{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXwIL2ugHSgK3TGPihD0FY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarErak/EDGE/blob/master/EDGE_MI_Estimator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Val0aqs4Idl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load and preprocess data\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
        "\n",
        "# Model parameters\n",
        "L1, L2, L3, L4, L5 = 1024, 20, 20, 20, 10\n",
        "NUM_ITERS = 1000\n",
        "BATCH = 128\n",
        "DISPLAY_STEP = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Assuming (train_images, train_labels) and (test_images, test_labels) are already loaded and preprocessed as shown above\n",
        "\n",
        "def prepare_datasets(batch_size=128):\n",
        "    # Convert the numpy arrays into TensorFlow Dataset objects\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "    # Shuffle the training data (using a buffer larger than the number of examples to ensure good randomness)\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=len(train_images))\n",
        "\n",
        "    # Batch the datasets\n",
        "    train_dataset = train_dataset.batch(batch_size)\n",
        "    test_dataset = test_dataset.batch(batch_size)\n",
        "\n",
        "    # Prefetch the data (1 batch) to speed up training\n",
        "    train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "# Example usage\n",
        "batch_size = 10000\n",
        "train_dataset, test_dataset = prepare_datasets(batch_size)\n"
      ],
      "metadata": {
        "id": "1iJb7ZaP6ACh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T= 10000\n",
        "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Convert images to the right shape and type\n",
        "X_MI = tf.convert_to_tensor(mnist_images[:T], dtype=tf.float32) / 255.0  # Normalize the data\n",
        "X_MI = tf.expand_dims(X_MI, axis=-1)  # Add an extra dimension for channel\n",
        "\n",
        "# Convert labels to one-hot encoded format if necessary\n",
        "Y_MI = tf.convert_to_tensor(mnist_labels[:T])\n",
        "Y_MI = tf.one_hot(Y_MI, depth=10)  # Assuming 10 classes for the digits 0-9\n"
      ],
      "metadata": {
        "id": "Yn9kryySBtgi"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Model with MI Calculation\n",
        "L1, L2, L3, L4, L5 = 1024, 20, 20, 20, 10\n",
        "class CustomModel(models.Model):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.flatten = layers.Flatten(input_shape=(28, 28, 1))\n",
        "        self.dense1 = layers.Dense(L1, activation='tanh')\n",
        "        self.dense2 = layers.Dense(L2, activation='tanh')\n",
        "        self.dense3 = layers.Dense(L3, activation='tanh')\n",
        "        self.dense4 = layers.Dense(L4, activation='tanh')\n",
        "        self.dense5 = layers.Dense(L5, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.flatten(inputs)\n",
        "        y1 = self.dense1(x)\n",
        "        y2 = self.dense2(y1)\n",
        "        y3 = self.dense3(y2)\n",
        "        y4 = self.dense4(y3)\n",
        "        y5 = self.dense5(y4)\n",
        "        return y5, y1, y2, y3, y4"
      ],
      "metadata": {
        "id": "mdfwI4dr5ra2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate and compile model\n",
        "model = CustomModel()\n",
        "model.compile(optimizer=optimizers.Adam(0.003),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "pd9PonuT5wy0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_MI_EDGE(model_outputs, ep_idx):\n",
        "    \"\"\"\n",
        "    Calculates mutual information for all given model outputs.\n",
        "\n",
        "    Args:\n",
        "    model_outputs (list of np.array): Outputs from the model layers during inference.\n",
        "    ep_idx (int): Current epoch index, used for calculations.\n",
        "\n",
        "    Returns:\n",
        "    list of tuples: Each tuple contains mutual information values (MI_XT, MI_TY).\n",
        "    \"\"\"\n",
        "    mi_xt_list = []; mi_ty_list = []\n",
        "    hidden_idx = 0\n",
        "    for hidden_output in model_outputs:\n",
        "        mi_xt, mi_ty = calc_MI_EDGE(hidden_output, hidden_idx, ep_idx)\n",
        "        mi_xt_list.append(mi_xt)\n",
        "        mi_ty_list.append(mi_ty)\n",
        "        hidden_idx += 1\n",
        "\n",
        "    return mi_xt_list, mi_ty_list"
      ],
      "metadata": {
        "id": "DJ78iXtS51h2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hidden_layers(model, names):\n",
        "    \"\"\"\n",
        "    Retrieves hidden layers from a Keras model based on layer names.\n",
        "\n",
        "    Args:\n",
        "    model (tf.keras.Model): The model from which to retrieve layers.\n",
        "    names (list of str): Names of the layers to retrieve.\n",
        "\n",
        "    Returns:\n",
        "    list of tf.Tensor: List of tensors corresponding to the outputs of the layers.\n",
        "    \"\"\"\n",
        "    hidden_layers = []\n",
        "    for name in names:\n",
        "        # Access the layer directly from the model by name\n",
        "        layer = model.get_layer(name)\n",
        "        hidden_layers.append(layer.output)\n",
        "    return hidden_layers"
      ],
      "metadata": {
        "id": "UkmYCL785057"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "global dist0\n",
        "dist0 = np.zeros(4)\n",
        "\n",
        "\n",
        "def calc_MI_EDGE(hidden, layer_idx, ep_idx):\n",
        "    \"\"\"\n",
        "    Calculate mutual information metrics between hidden layer activations and both inputs and outputs.\n",
        "\n",
        "    Args:\n",
        "    hidden (np.array): Hidden layer activations.\n",
        "    layer_idx (int): Index of the current layer.\n",
        "    ep_idx (int): Current epoch index, for adjusting calculations.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Contains mutual information values between inputs and hidden activations (MI_XT)\n",
        "           and outputs and hidden activations (MI_TY).\n",
        "    \"\"\"\n",
        "    # Assure hidden is a numpy array, and truncate samples for calculation\n",
        "    hidden = np.array(hidden)[:T, :]\n",
        "    X_reshaped = np.reshape(X_MI, [-1, 784])  # Vectorize X\n",
        "    Y_reshaped = np.argmax(Y_MI, axis=0)  # Convert 10-dim data to class integer in [0,9]\n",
        "\n",
        "    # Calculate or update distance measures\n",
        "    if ep_idx <= 20:\n",
        "        dist0[layer_idx] = av_distance(hidden)\n",
        "        r = 1\n",
        "    else:\n",
        "        dist = av_distance(hidden)\n",
        "        r = dist / dist0[layer_idx]\n",
        "\n",
        "    print('Epoch index and hidden dimension and ratio:', ep_idx, hidden.shape[1], r)\n",
        "\n",
        "    # Define smoothing vectors for mutual information calculations\n",
        "    smoothness_vector_xt = np.array([0.8, 1.0, 1.2, 1.8])\n",
        "    smoothness_vector_ty = np.array([0.4, 0.5, 0.6, 0.8])\n",
        "\n",
        "    # Call the EDGE function to calculate mutual information\n",
        "    mi_xt_py = EDGE(X_reshaped, hidden, U=20, L_ensemble=10, gamma=[0.2, smoothness_vector_xt[layer_idx]], epsilon_vector='range')\n",
        "    mi_ty_py = EDGE(Y_reshaped, hidden, U=10, L_ensemble=10, gamma=[0.0001, smoothness_vector_ty[layer_idx]], epsilon=[0.2, 0.2], epsilon_vector='range')\n",
        "\n",
        "    return mi_xt_py, mi_ty_py\n",
        "\n",
        "# def av_distance(X):\n",
        "#     \"\"\"\n",
        "#     Compute the average distance between random pairs of points in a given array.\n",
        "\n",
        "#     Args:\n",
        "#     X (np.array): Array of points.\n",
        "\n",
        "#     Returns:\n",
        "#     float: The average distance.\n",
        "#     \"\"\"\n",
        "#     r = 1000\n",
        "#     N = X.shape[0]\n",
        "#     indices = np.random.choice(N, size=2*r, replace=False)\n",
        "#     D = norm(X[indices[:r]] - X[indices[r:]], axis=1)\n",
        "#     return np.mean(D)\n",
        "\n",
        "# Find average distances between points\n",
        "from numpy import linalg as LA\n",
        "def av_distance(X):\n",
        "\n",
        "  r = 1000\n",
        "\n",
        "  N = X.shape[0]\n",
        "\n",
        "  np.random.seed(1234)\n",
        "  T1= np.random.choice(range(N), size=2*r)[:r]\n",
        "  T2= np.random.choice(range(N), size=2*r)[r:]\n",
        "  np.random.seed()\n",
        "  D = LA.norm(X[T2,:] - X[T1,:], ord=2, axis=1)\n",
        "  d = np.mean(D)\n",
        "\n",
        "  return d"
      ],
      "metadata": {
        "id": "MzvXpQdF57NO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def train_with_mi(model, train_dataset, test_dataset, epochs, batch_size):\n",
        "    print('train_with_mi')\n",
        "\n",
        "    # Compilation of the model should already be done outside this function, including optimizer and loss configuration\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    test_losses = []\n",
        "    test_acc = []\n",
        "    mi_xt_all = []\n",
        "    mi_ty_all = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # Iterate over batches of the dataset\n",
        "        for step, (batch_X, batch_Y) in enumerate(train_dataset):\n",
        "            # Run a training batch\n",
        "            with tf.GradientTape() as tape:\n",
        "                outputs = model(batch_X, training=True)\n",
        "                predictions = outputs[0]\n",
        "                loss = tf.keras.losses.categorical_crossentropy(batch_Y, predictions)\n",
        "\n",
        "            # Calculate gradients and update model weights\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            # Calculate accuracy\n",
        "            correct = tf.equal(tf.argmax(predictions, axis=1), tf.argmax(batch_Y, axis=1))\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "            # Append loss and accuracy for visualization\n",
        "            if step % DISPLAY_STEP == 0:\n",
        "                train_losses.append(loss.numpy().mean())\n",
        "                train_acc.append(accuracy.numpy())\n",
        "\n",
        "                # Evaluate on test set\n",
        "                # test_loss, test_accuracy = model.evaluate(test_dataset, verbose=0)\n",
        "                # test_losses.append(test_loss)\n",
        "                # test_acc.append(test_accuracy)\n",
        "                print(f\"Step {step}: Train Loss: {loss.numpy().mean()}, Train Acc: {accuracy.numpy()}\")\n",
        "\n",
        "            # Compute Mutual Information at specified intervals\n",
        "            q=1\n",
        "            if (epoch <= 10 and step % 1 == 0) or \\\n",
        "              (epoch > 10 and epoch <= 100 and step % (3*q) == 0) or \\\n",
        "              (epoch > 100 and epoch <= 1000 and step % (25*q) == 0) or \\\n",
        "              (epoch > 1000 and epoch <= 2000 and step % (50*q) == 0) or \\\n",
        "              (epoch > 2000 and epoch <= 4000 and step % (200*q) == 0) or \\\n",
        "              (epoch > 4000 and step % (400*q) == 0):\n",
        "                final_output, hidden1, hidden2, hidden3, hidden4 = model(batch_X, training=False)  # Forward pass to get hidden layer outputs\n",
        "                hidden_outputs = [hidden1, hidden2, hidden3, hidden4]\n",
        "                mi_xt, mi_ty = get_MI_EDGE(hidden_outputs, epoch)\n",
        "                mi_xt_all.append(mi_xt)\n",
        "                mi_ty_all.append(mi_ty)\n",
        "                print(f'MI(X;T): {mi_xt}, MI(Y;T): {mi_ty}')\n",
        "\n",
        "    return np.array(mi_xt_all), np.array(mi_ty_all)"
      ],
      "metadata": {
        "id": "zc7Vyc-o59Y0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# EDGE Estimator for Shannon Mutual Information\n",
        "#\n",
        "# Created by Morteza Noshad (noshad@umich.edu)\n",
        "#\n",
        "# Based on the paper: Scalable Mutual Information Estimation using Dependence Graphs\n",
        "#\n",
        "################\n",
        "# The estimator is in the following form:\n",
        "#\n",
        "# I = EDGE(X,Y,U=u,gamma=[gamma_X,gamma_Y], epsilon=[eps_X, eps_Y], hashing)\n",
        "#\n",
        "# Arguments:\n",
        "#\n",
        "# X is N * d_x and Y is N * d_Y data sets\n",
        "# U (optional) is an upper bound on the MI. It doesn't need to be accurate, but more accurate upper bound we set, faster convergence rates we get\n",
        "# gamma=[gamma_X,gamma_Y] (optional) is the vector of soothness for X and Y.\n",
        "#\t\tFor example, if the data is discrete we set gamma close to 0,\n",
        "#\t\tand if the data is continuous we set gamma close to 1 (or maybe higher if it is very smooth)\n",
        "# epsilon=[eps_X, eps_Y] (optional) is the vector of bandwidths for X and Y. If no epsilon is set,\n",
        "#\t\tautomatic bandwidths will be set.\n",
        "# hashing (optional): possible arguments are 'p-stable' (default) which is a common type of LSH\n",
        "#\t\tor 'floor' which uses the simple floor function as hashing\n",
        "#\n",
        "# Output: I is the estimation of mutual information between X snd Y\n",
        "###########################\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "#import cvxpy as cvx # Need to install CVXPY package,\n",
        "\t\t\t\t\t#  it is also possible to run this code without cvxpy, by setting uniform weights\n",
        "import time\n",
        "from scipy.special import *\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "def patch_asscalar(a):\n",
        "    return a.item()\n",
        "\n",
        "setattr(np, \"asscalar\", patch_asscalar)\n",
        "#from random import randint, seed\n",
        "#np.random.seed(seed=0)\n",
        "\n",
        "#####################\n",
        "#####################\n",
        "\n",
        "# Generate W and V matrices (used in LSH)\n",
        "def gen_W(X,Y):\n",
        "\tnp.random.seed(3334)\n",
        "\t# Num of Samples and dimensions\n",
        "\tN = X.shape[0]\n",
        "\tdim_X , dim_Y  = X.shape[1], Y.shape[1]\n",
        "\n",
        "\t# parameters to control the dimension of W and V\n",
        "\tkx,ky = 2, 2\n",
        "\trx,ry = 10,10\n",
        "\n",
        "\t# Find standard deviation vectors\n",
        "\tstd_X = np.array([np.std(X[:,[i]]) for i in range(dim_X)])\n",
        "\tstd_Y = np.array([np.std(Y[:,[i]]) for i in range(dim_Y)])\n",
        "\n",
        "\tstd_X = np.reshape(std_X,(dim_X,1))\n",
        "\tstd_Y = np.reshape(std_Y,(dim_Y,1))\n",
        "\n",
        "\t# Compute dimensions of W and V\n",
        "\td_X_shrink=min(dim_X,math.floor(math.log(1.0*N/rx,kx)))\n",
        "\td_Y_shrink=min(dim_Y,math.floor(math.log(1.0*N/ry,ky)))\n",
        "\n",
        "\t# Repeat columns of std_X and Y to be in the same size as W and V\n",
        "\tstd_X_mat= np.tile(std_X,(1,d_X_shrink))\n",
        "\tstd_Y_mat= np.tile(std_Y,(1,d_Y_shrink))\n",
        "\n",
        "\t# avoid devision by zero\n",
        "\tstd_X_mat[std_X_mat<0.0001]=1\n",
        "\tstd_Y_mat[std_Y_mat<0.0001]=1\n",
        "\n",
        "\t# Mean and standard deviation of Normal pdf for elements of W and V\n",
        "\tmu_X = np.zeros((dim_X, d_X_shrink))\n",
        "\tmu_Y = np.zeros((dim_Y, d_Y_shrink))\n",
        "\n",
        "\tsigma_X = 1.0/(std_X_mat *np.sqrt(dim_X))\n",
        "\tsigma_Y = 1.0/(std_Y_mat *np.sqrt(dim_Y))\n",
        "\n",
        "\t# Generate normal matrices W and V\n",
        "\t#np.random.seed(seed=0)\n",
        "\tW = np.random.normal(mu_X, sigma_X, (dim_X, d_X_shrink))\n",
        "\tV = np.random.normal(mu_Y, sigma_Y, (dim_Y, d_Y_shrink))\n",
        "\n",
        "\treturn (W,V)\n",
        "\n",
        "# Find KNN distances for a number of samples for normalizing bandwidth\n",
        "def find_knn(A,d):\n",
        "\tnp.random.seed(3334)\n",
        "\t#np.random.seed()\n",
        "\t#np.random.seed(seed=int(time.time()))\n",
        "\tr = 500\n",
        "\t# random samples from A\n",
        "\tA = A.reshape((-1,1))\n",
        "\tN = A.shape[0]\n",
        "\n",
        "\tk=math.floor(0.43*N**(2/3 + 0.17*(d/(d+1)) )*math.exp(-1.0/np.max([10000, d**4])))\n",
        "\n",
        "\tT= np.random.choice(A.reshape(-1,), size=r).reshape(-1,1)\n",
        "\tnbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(A)\n",
        "\tdistances, indices = nbrs.kneighbors(T)\n",
        "\td = np.mean(distances[:,-1])\n",
        "\treturn d\n",
        "\n",
        "# Returns epsilon and random shifts b\n",
        "def gen_eps(XW,YV):\n",
        "\td_X , d_Y  = XW.shape[1], YV.shape[1]\n",
        "\t# Find KNN distances for a number of samples for normalizing bandwidth\n",
        "\teps_X = np.array([find_knn(XW[:,[i]],d_X) for i in range(d_X)]) + 0.0001\n",
        "\teps_Y = np.array([find_knn(YV[:,[i]],d_Y) for i in range(d_Y)]) + 0.0001\n",
        "\n",
        "\treturn (eps_X,eps_Y)\n",
        "\n",
        "# Define H1 (LSH) for a vector X (X is just one sample)\n",
        "def H1(XW,b,eps):\n",
        "\n",
        "\t# dimension of X\n",
        "\td_X = XW.shape[0]\n",
        "\t#d_W = W.shape[1]\n",
        "\tXW=XW.reshape(1,d_X)\n",
        "\n",
        "\t# If not scalar\n",
        "\tif d_X > 1:\n",
        "\t\tX_te = 1.0*(np.squeeze(XW)+b)/eps\n",
        "\telif eps>0:\n",
        "\t\tX_te = 1.0*(XW+b)/eps\n",
        "\telse:\n",
        "\t\tX_te=XW\n",
        "\n",
        "\t# Discretize X\n",
        "\tX_t = np.floor(X_te)\n",
        "\tif d_X>1:\n",
        "\t\tR = tuple(X_t.tolist())\n",
        "\telse: R=np.asscalar(np.squeeze(X_t))\n",
        "\treturn R\n",
        "\n",
        "# Compuate Hashing: Compute the number of collisions in each bucket\n",
        "def Hash(XW,YV,eps_X,eps_Y,b_X,b_Y):\n",
        "\n",
        "  # Num of Samples and dimensions\n",
        "  N = XW.shape[0]\n",
        "  # Hash vectors as dictionaries\n",
        "  CX, CY, CXY = {}, {}, {}\n",
        "\n",
        "  # Computing Collisions\n",
        "\n",
        "  for i in range(N):\n",
        "    # Compute H_1 hashing of X_i and Y_i: Convert to tuple (vectors cannot be taken as keys in dict)\n",
        "\n",
        "    X_l, Y_l = H1(XW[i],b_X,eps_X), H1(YV[i],b_Y,eps_Y)\n",
        "\n",
        "    # X collisions: compute H_2\n",
        "    if X_l in CX:\n",
        "      CX[X_l].append(i)\n",
        "    else:\n",
        "      CX[X_l] = [i]\n",
        "\n",
        "    # Y collisions: compute H_2\n",
        "    if Y_l in CY:\n",
        "      CY[Y_l].append(i)\n",
        "    else:\n",
        "      CY[Y_l] = [i]\n",
        "\n",
        "    # XY collisions\n",
        "    if (X_l,Y_l) in CXY:\n",
        "      CXY[(X_l,Y_l)].append(i)\n",
        "    else:\n",
        "      CXY[(X_l,Y_l)] = [i]\n",
        "\n",
        "  return (CX, CY, CXY)\n",
        "\n",
        "\n",
        "# Compute mutual information and gradient given epsilons and radom shifts\n",
        "def Compute_MI(XW,YV,U,eps_X,eps_Y,b_X,b_Y):\n",
        "  N = XW.shape[0]\n",
        "\n",
        "  (CX, CY, CXY) = Hash(XW,YV,eps_X,eps_Y,b_X,b_Y)\n",
        "\n",
        "  # Computing Mutual Information Function\n",
        "  I = 0\n",
        "  N_c = 0\n",
        "  for e in CXY.keys():\n",
        "    Ni, Mj, Nij = len(CX[e[0]]), len(CY[e[1]]), len(CXY[e])\n",
        "\n",
        "    if 1==1:\n",
        "      I += Nij* max(min(math.log(1.0*Nij*N/(Ni*Mj),2), U),0.001)\n",
        "      N_c+=Nij\n",
        "\n",
        "  I = 1.0* I / N_c\n",
        "\n",
        "  return I\n",
        "\n",
        "def EDGE(X,Y,U=10, gamma=[1, 1], epsilon=[0,0], epsilon_vector = 'fixed', eps_range_factor=0.1, normalize_epsilon = True ,\n",
        "        ensemble_estimation = 'median', L_ensemble=5 ,hashing='p-stable', stochastic = False):\n",
        "\n",
        "  gamma = np.array(gamma)\n",
        "  epsilon = np.array(epsilon)\n",
        "  if X.ndim==1:\n",
        "    X=X.reshape((-1,1))\n",
        "  if Y.ndim==1:\n",
        "    Y=Y.reshape((-1,1))\n",
        "  # Num of Samples and dim\n",
        "  N, d = X.shape[0], X.shape[1]\n",
        "\n",
        "  # Find dimensions\n",
        "  dim_X, dim_Y  = X.shape[1], Y.shape[1]\n",
        "  dim = dim_X + dim_Y\n",
        "\n",
        "  ## Hash type\n",
        "\n",
        "  if hashing == 'p-stable':\n",
        "    # Generate random transformation matrices W and V\n",
        "    (W,V) = gen_W(X,Y)\n",
        "    d_X_shrink, d_Y_shrink=W.shape[1], V.shape[1]\n",
        "    # Find inner products\n",
        "    XW, YV = np.dot(X,W), np.dot(Y,V)\n",
        "\n",
        "  elif hashing == 'floor':\n",
        "    #W = np.identity(dim_X)\n",
        "    #V = np.identity(dim_Y)\n",
        "    d_X_shrink, d_Y_shrink=dim_X, dim_Y\n",
        "    XW, YV = X, Y\n",
        "\n",
        "  ## Initial epsilon and apply smoothness gamma\n",
        "\n",
        "  # If no manual epsilon is set for computing MI:\n",
        "  if epsilon[0] ==0:\n",
        "    # Generate auto epsilon and b\n",
        "    (eps_X_temp,eps_Y_temp) = gen_eps(XW,YV)\n",
        "\n",
        "    # Normalizing factors for the bandwidths\n",
        "    cx, cy = 3*d_X_shrink, 3*d_Y_shrink\n",
        "    eps_X0, eps_Y0 = eps_X_temp * cx*gamma[0], eps_Y_temp * cy*gamma[1]\n",
        "  else:\n",
        "    eps_X_temp = np.ones(d_X_shrink,)*epsilon[0]\n",
        "    eps_Y_temp = np.ones(d_Y_shrink,)*epsilon[1]\n",
        "    cx, cy = 3*d_X_shrink, 3*d_Y_shrink\n",
        "    eps_X0, eps_Y0 = eps_X_temp * cx*gamma[0], eps_Y_temp * cy*gamma[1]\n",
        "\n",
        "  ## epsilon_vector\n",
        "  if epsilon_vector == 'fixed':\n",
        "    T = np.ones(L_ensemble)\n",
        "  elif epsilon_vector == 'range':\n",
        "    T = np.linspace(1,1+eps_range_factor,L_ensemble)\n",
        "\n",
        "\n",
        "  ## Compute MI Vector\n",
        "\n",
        "  # MI Vector\n",
        "  I_vec = np.zeros(L_ensemble)\n",
        "\n",
        "  for j in range(L_ensemble):\n",
        "\n",
        "    # Apply epsilon_vector\n",
        "    eps_X, eps_Y = eps_X0 * T[j], eps_Y0 * T[j]\n",
        "\n",
        "    ## Shifts of hashing\n",
        "    if stochastic== True:\n",
        "      np.random.seed()\n",
        "      f=0.1\n",
        "      b_X = f*np.random.rand(d_X_shrink,)*eps_X\n",
        "      b_Y = f*np.random.rand(d_Y_shrink,)*eps_Y\n",
        "    else:\n",
        "      b_X = np.linspace(0,1,L_ensemble,endpoint=False)[j]*eps_X\n",
        "      b_Y = np.linspace(0,1,L_ensemble,endpoint=False)[j]*eps_Y\n",
        "\n",
        "    I_vec[j] = Compute_MI(XW,YV,U,eps_X,eps_Y,b_X,b_Y)\n",
        "\n",
        "  ## Ensemble method\n",
        "\n",
        "  if ensemble_estimation == 'average':\n",
        "    I = np.mean(I_vec)\n",
        "  elif ensemble_estimation == 'optimal_weights':\n",
        "    weights = compute_weights(L_ensemble, d, T, N)\n",
        "    weights=weights.reshape(L_ensemble,)\n",
        "    I = np.dot(I_vec, weights)\n",
        "  elif ensemble_estimation == 'median':\n",
        "    I = np.median(I_vec)\n",
        "\n",
        "  ## Normalize epsilon according to MI estimation (cross validation)\n",
        "  if normalize_epsilon == True:\n",
        "    gamma=gamma * math.pow(2,-math.sqrt(I*2.0)+(0.5/I))\n",
        "    normalize_epsilon = False\n",
        "    I = EDGE(X,Y,U, gamma, epsilon, epsilon_vector, eps_range_factor, normalize_epsilon, ensemble_estimation, L_ensemble,hashing, stochastic)\n",
        "\n",
        "\n",
        "  return I\n"
      ],
      "metadata": {
        "id": "MEne0LSTHGIg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = train_with_mi(model, train_dataset, test_dataset, epochs=20000, batch_size=128)\n",
        "results.append(result)\n",
        "\n",
        "np.save('mi_all_tanh', results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "tG1STW-d6G2k",
        "outputId": "5b4cc95d-f306-4e68-f789-6cd96b666600"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_with_mi\n",
            "Epoch 1/20000\n",
            "Step 0: Train Loss: 1.3803001642227173, Train Acc: 0.7059000134468079\n",
            "Epoch index and hidden dimension and ratio: 0 1024 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "tuple index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-796639076206>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_with_mi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mi_all_tanh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b2ab8538d5ab>\u001b[0m in \u001b[0;36mtrain_with_mi\u001b[0;34m(model, train_dataset, test_dataset, epochs, batch_size)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mfinal_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass to get hidden layer outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mhidden_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mmi_xt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmi_ty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_MI_EDGE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mmi_xt_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmi_xt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mmi_ty_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmi_ty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f15fa543d3d7>\u001b[0m in \u001b[0;36mget_MI_EDGE\u001b[0;34m(model_outputs, ep_idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mhidden_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhidden_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mmi_xt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmi_ty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_MI_EDGE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mmi_xt_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmi_xt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmi_ty_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmi_ty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-2ba8b10cc631>\u001b[0m in \u001b[0;36mcalc_MI_EDGE\u001b[0;34m(hidden, layer_idx, ep_idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Call the EDGE function to calculate mutual information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mmi_xt_py\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEDGE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_ensemble\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothness_vector_xt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_vector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'range'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmi_ty_py\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEDGE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_ensemble\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothness_vector_ty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_vector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'range'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmi_xt_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmi_ty_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-a5579d686bbe>\u001b[0m in \u001b[0;36mEDGE\u001b[0;34m(X, Y, U, gamma, epsilon, epsilon_vector, eps_range_factor, normalize_epsilon, ensemble_estimation, L_ensemble, hashing, stochastic)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;31m# Num of Samples and dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m   \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m   \u001b[0;31m# Find dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = np.load('mi_all.npy')\n",
        "print(a)"
      ],
      "metadata": {
        "id": "D7y47XvyPsGs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}