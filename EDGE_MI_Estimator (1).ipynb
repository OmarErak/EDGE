{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Val0aqs4Idl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load and preprocess data\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
        "\n",
        "# Model parameters\n",
        "L1, L2, L3, L4, L5 = 1024, 20, 20, 20, 10\n",
        "NUM_ITERS = 1000\n",
        "BATCH = 128\n",
        "DISPLAY_STEP = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1iJb7ZaP6ACh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# Assuming (train_images, train_labels) and (test_images, test_labels) are already loaded and preprocessed as shown above\n",
        "\n",
        "def prepare_datasets(batch_size=128):\n",
        "    # Convert the numpy arrays into TensorFlow Dataset objects\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "    # Shuffle the training data (using a buffer larger than the number of examples to ensure good randomness)\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=len(train_images))\n",
        "\n",
        "    # Batch the datasets\n",
        "    train_dataset = train_dataset.batch(batch_size)\n",
        "    test_dataset = test_dataset.batch(batch_size)\n",
        "\n",
        "    # Prefetch the data (1 batch) to speed up training\n",
        "    train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "# Example usage\n",
        "batch_size = 10000\n",
        "train_dataset, test_dataset = prepare_datasets(batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yn9kryySBtgi"
      },
      "outputs": [],
      "source": [
        "T= 10000\n",
        "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Convert images to the right shape and type\n",
        "X_MI = tf.convert_to_tensor(mnist_images[:T], dtype=tf.float32) / 255.0  # Normalize the data\n",
        "X_MI = tf.expand_dims(X_MI, axis=-1)  # Add an extra dimension for channel\n",
        "\n",
        "# Convert labels to one-hot encoded format if necessary\n",
        "Y_MI = tf.convert_to_tensor(mnist_labels[:T])\n",
        "Y_MI = tf.keras.utils.to_categorical(Y_MI, num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mdfwI4dr5ra2"
      },
      "outputs": [],
      "source": [
        "# Custom Model with MI Calculation\n",
        "L1, L2, L3, L4, L5 = 1024, 20, 20, 20, 10\n",
        "class CustomModel(models.Model):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.flatten = layers.Flatten(input_shape=(28, 28, 1))\n",
        "        self.dense1 = layers.Dense(L1, activation='relu')\n",
        "        self.dense2 = layers.Dense(L2, activation='relu')\n",
        "        self.dense3 = layers.Dense(L3, activation='relu')\n",
        "        self.dense4 = layers.Dense(L4, activation='relu')\n",
        "        self.dense5 = layers.Dense(L5, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.flatten(inputs)\n",
        "        y1 = self.dense1(x)\n",
        "        y2 = self.dense2(y1)\n",
        "        y3 = self.dense3(y2)\n",
        "        y4 = self.dense4(y3)\n",
        "        y5 = self.dense5(y4)\n",
        "        return y5, y1, y2, y3, y4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pd9PonuT5wy0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Instantiate and compile model\n",
        "model = CustomModel()\n",
        "model.compile(optimizer=optimizers.Adam(0.003),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DJ78iXtS51h2"
      },
      "outputs": [],
      "source": [
        "def get_MI_EDGE(model_outputs, ep_idx):\n",
        "    \"\"\"\n",
        "    Calculates mutual information for all given model outputs.\n",
        "\n",
        "    Args:\n",
        "    model_outputs (list of np.array): Outputs from the model layers during inference.\n",
        "    ep_idx (int): Current epoch index, used for calculations.\n",
        "\n",
        "    Returns:\n",
        "    list of tuples: Each tuple contains mutual information values (MI_XT, MI_TY).\n",
        "    \"\"\"\n",
        "    mi_xt_list = []; mi_ty_list = []\n",
        "    hidden_idx = 0\n",
        "    for hidden_output in model_outputs:\n",
        "        mi_xt, mi_ty = calc_MI_EDGE(hidden_output, hidden_idx, ep_idx)\n",
        "        mi_xt_list.append(mi_xt)\n",
        "        mi_ty_list.append(mi_ty)\n",
        "        hidden_idx += 1\n",
        "\n",
        "    return mi_xt_list, mi_ty_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UkmYCL785057"
      },
      "outputs": [],
      "source": [
        "def get_hidden_layers(model, names):\n",
        "    \"\"\"\n",
        "    Retrieves hidden layers from a Keras model based on layer names.\n",
        "\n",
        "    Args:\n",
        "    model (tf.keras.Model): The model from which to retrieve layers.\n",
        "    names (list of str): Names of the layers to retrieve.\n",
        "\n",
        "    Returns:\n",
        "    list of tf.Tensor: List of tensors corresponding to the outputs of the layers.\n",
        "    \"\"\"\n",
        "    hidden_layers = []\n",
        "    for name in names:\n",
        "        # Access the layer directly from the model by name\n",
        "        layer = model.get_layer(name)\n",
        "        hidden_layers.append(layer.output)\n",
        "    return hidden_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MzvXpQdF57NO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "global dist0\n",
        "dist0 = np.zeros(4)\n",
        "\n",
        "\n",
        "def calc_MI_EDGE(hidden, layer_idx, ep_idx):\n",
        "    \"\"\"\n",
        "    Calculate mutual information metrics between hidden layer activations and both inputs and outputs.\n",
        "\n",
        "    Args:\n",
        "    hidden (np.array): Hidden layer activations.\n",
        "    layer_idx (int): Index of the current layer.\n",
        "    ep_idx (int): Current epoch index, for adjusting calculations.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Contains mutual information values between inputs and hidden activations (MI_XT)\n",
        "           and outputs and hidden activations (MI_TY).\n",
        "    \"\"\"\n",
        "    # Assure hidden is a numpy array, and truncate samples for calculation\n",
        "    hidden = np.array(hidden)[:T, :]\n",
        "    X_reshaped = np.reshape(X_MI, [-1, 784])  # Vectorize X\n",
        "    Y_reshaped = np.argmax(Y_MI, axis=1)  # Convert 10-dim data to class integer in [0,9]\n",
        "\n",
        "    # Calculate or update distance measures\n",
        "    if ep_idx <= 20:\n",
        "        dist0[layer_idx] = av_distance(hidden)\n",
        "        r = 1\n",
        "    else:\n",
        "        dist = av_distance(hidden)\n",
        "        r = dist / dist0[layer_idx]\n",
        "\n",
        "    print('Epoch index and hidden dimension and ratio:', ep_idx, hidden.shape[1], r)\n",
        "\n",
        "    # Define smoothing vectors for mutual information calculations\n",
        "    smoothness_vector_xt = np.array([0.8, 1.0, 1.2, 1.8])\n",
        "    smoothness_vector_ty = np.array([0.4, 0.5, 0.6, 0.8])\n",
        "\n",
        "    # Call the EDGE function to calculate mutual information\n",
        "    mi_xt_py = EDGE(X_reshaped, hidden, U=20, L_ensemble=10, gamma=[0.2, smoothness_vector_xt[layer_idx]], epsilon_vector='range')\n",
        "    mi_ty_py = EDGE(Y_reshaped, hidden, U=10, L_ensemble=10, gamma=[0.0001, smoothness_vector_ty[layer_idx]], epsilon=[0.2, 0.2], epsilon_vector='range')\n",
        "\n",
        "    return mi_xt_py, mi_ty_py\n",
        "\n",
        "# def av_distance(X):\n",
        "#     \"\"\"\n",
        "#     Compute the average distance between random pairs of points in a given array.\n",
        "\n",
        "#     Args:\n",
        "#     X (np.array): Array of points.\n",
        "\n",
        "#     Returns:\n",
        "#     float: The average distance.\n",
        "#     \"\"\"\n",
        "#     r = 1000\n",
        "#     N = X.shape[0]\n",
        "#     indices = np.random.choice(N, size=2*r, replace=False)\n",
        "#     D = norm(X[indices[:r]] - X[indices[r:]], axis=1)\n",
        "#     return np.mean(D)\n",
        "\n",
        "# Find average distances between points\n",
        "from numpy import linalg as LA\n",
        "def av_distance(X):\n",
        "\n",
        "  r = 1000\n",
        "\n",
        "  N = X.shape[0]\n",
        "\n",
        "  np.random.seed(1234)\n",
        "  T1= np.random.choice(range(N), size=2*r)[:r]\n",
        "  T2= np.random.choice(range(N), size=2*r)[r:]\n",
        "  np.random.seed()\n",
        "  D = LA.norm(X[T2,:] - X[T1,:], ord=2, axis=1)\n",
        "  d = np.mean(D)\n",
        "\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zc7Vyc-o59Y0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def train_with_mi(model, train_dataset, test_dataset, epochs, batch_size):\n",
        "    print('train_with_mi')\n",
        "\n",
        "    # Compilation of the model should already be done outside this function, including optimizer and loss configuration\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    test_losses = []\n",
        "    test_acc = []\n",
        "    mi_xt_all = []\n",
        "    mi_ty_all = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # Iterate over batches of the dataset\n",
        "        for step, (batch_X, batch_Y) in enumerate(train_dataset):\n",
        "            # Run a training batch\n",
        "            with tf.GradientTape() as tape:\n",
        "                outputs = model(batch_X, training=True)\n",
        "                predictions = outputs[0]\n",
        "                loss = tf.keras.losses.categorical_crossentropy(batch_Y, predictions)\n",
        "\n",
        "            # Calculate gradients and update model weights\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            # Calculate accuracy\n",
        "            correct = tf.equal(tf.argmax(predictions, axis=1), tf.argmax(batch_Y, axis=1))\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "            # Append loss and accuracy for visualization\n",
        "            if step % DISPLAY_STEP == 0:\n",
        "                train_losses.append(loss.numpy().mean())\n",
        "                train_acc.append(accuracy.numpy())\n",
        "\n",
        "                # Evaluate on test set\n",
        "                # test_loss, test_accuracy = model.evaluate(test_dataset, verbose=0)\n",
        "                # test_losses.append(test_loss)\n",
        "                # test_acc.append(test_accuracy)\n",
        "                print(f\"Step {step}: Train Loss: {loss.numpy().mean()}, Train Acc: {accuracy.numpy()}\")\n",
        "\n",
        "            # Compute Mutual Information at specified intervals\n",
        "            # q=1\n",
        "            # if (epoch <= 10 and step % 1 == 0) or \\\n",
        "            #   (epoch > 10 and epoch <= 100 and step % (3*q) == 0) or \\\n",
        "            #   (epoch > 100 and epoch <= 1000 and step % (25*q) == 0) or \\\n",
        "            #   (epoch > 1000 and epoch <= 2000 and step % (50*q) == 0) or \\\n",
        "            #   (epoch > 2000 and epoch <= 4000 and step % (200*q) == 0) or \\\n",
        "            #   (epoch > 4000 and step % (400*q) == 0):\n",
        "            if epoch == 0 or epoch == 10 or epoch ==30 or epoch == 60 or (epoch >= 500 and epoch % 500 == 0):\n",
        "                final_output, hidden1, hidden2, hidden3, hidden4 = model(X_MI, training=True)  # Forward pass to get hidden layer outputs\n",
        "                hidden_outputs = [hidden1, hidden2, hidden3, hidden4]\n",
        "                mi_xt, mi_ty = get_MI_EDGE(hidden_outputs, epoch)\n",
        "                mi_xt_all.append(mi_xt)\n",
        "                mi_ty_all.append(mi_ty)\n",
        "                print(f'MI(X;T): {mi_xt}, MI(Y;T): {mi_ty}')\n",
        "\n",
        "    return np.array(mi_xt_all), np.array(mi_ty_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MEne0LSTHGIg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# EDGE Estimator for Shannon Mutual Information\n",
        "#\n",
        "# Created by Morteza Noshad (noshad@umich.edu)\n",
        "#\n",
        "# Based on the paper: Scalable Mutual Information Estimation using Dependence Graphs\n",
        "#\n",
        "################\n",
        "# The estimator is in the following form:\n",
        "#\n",
        "# I = EDGE(X,Y,U=u,gamma=[gamma_X,gamma_Y], epsilon=[eps_X, eps_Y], hashing)\n",
        "#\n",
        "# Arguments:\n",
        "#\n",
        "# X is N * d_x and Y is N * d_Y data sets\n",
        "# U (optional) is an upper bound on the MI. It doesn't need to be accurate, but more accurate upper bound we set, faster convergence rates we get\n",
        "# gamma=[gamma_X,gamma_Y] (optional) is the vector of soothness for X and Y.\n",
        "#\t\tFor example, if the data is discrete we set gamma close to 0,\n",
        "#\t\tand if the data is continuous we set gamma close to 1 (or maybe higher if it is very smooth)\n",
        "# epsilon=[eps_X, eps_Y] (optional) is the vector of bandwidths for X and Y. If no epsilon is set,\n",
        "#\t\tautomatic bandwidths will be set.\n",
        "# hashing (optional): possible arguments are 'p-stable' (default) which is a common type of LSH\n",
        "#\t\tor 'floor' which uses the simple floor function as hashing\n",
        "#\n",
        "# Output: I is the estimation of mutual information between X snd Y\n",
        "###########################\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "#import cvxpy as cvx # Need to install CVXPY package,\n",
        "\t\t\t\t\t#  it is also possible to run this code without cvxpy, by setting uniform weights\n",
        "import time\n",
        "from scipy.special import *\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "def patch_asscalar(a):\n",
        "    return a.item()\n",
        "\n",
        "setattr(np, \"asscalar\", patch_asscalar)\n",
        "#from random import randint, seed\n",
        "#np.random.seed(seed=0)\n",
        "\n",
        "#####################\n",
        "#####################\n",
        "\n",
        "# Generate W and V matrices (used in LSH)\n",
        "def gen_W(X,Y):\n",
        "\tnp.random.seed(3334)\n",
        "\t# Num of Samples and dimensions\n",
        "\tN = X.shape[0]\n",
        "\tdim_X , dim_Y  = X.shape[1], Y.shape[1]\n",
        "\n",
        "\t# parameters to control the dimension of W and V\n",
        "\tkx,ky = 2, 2\n",
        "\trx,ry = 10,10\n",
        "\n",
        "\t# Find standard deviation vectors\n",
        "\tstd_X = np.array([np.std(X[:,[i]]) for i in range(dim_X)])\n",
        "\tstd_Y = np.array([np.std(Y[:,[i]]) for i in range(dim_Y)])\n",
        "\n",
        "\tstd_X = np.reshape(std_X,(dim_X,1))\n",
        "\tstd_Y = np.reshape(std_Y,(dim_Y,1))\n",
        "\n",
        "\t# Compute dimensions of W and V\n",
        "\td_X_shrink=min(dim_X,math.floor(math.log(1.0*N/rx,kx)))\n",
        "\td_Y_shrink=min(dim_Y,math.floor(math.log(1.0*N/ry,ky)))\n",
        "\n",
        "\t# Repeat columns of std_X and Y to be in the same size as W and V\n",
        "\tstd_X_mat= np.tile(std_X,(1,d_X_shrink))\n",
        "\tstd_Y_mat= np.tile(std_Y,(1,d_Y_shrink))\n",
        "\n",
        "\t# avoid devision by zero\n",
        "\tstd_X_mat[std_X_mat<0.0001]=1\n",
        "\tstd_Y_mat[std_Y_mat<0.0001]=1\n",
        "\n",
        "\t# Mean and standard deviation of Normal pdf for elements of W and V\n",
        "\tmu_X = np.zeros((dim_X, d_X_shrink))\n",
        "\tmu_Y = np.zeros((dim_Y, d_Y_shrink))\n",
        "\n",
        "\tsigma_X = 1.0/(std_X_mat *np.sqrt(dim_X))\n",
        "\tsigma_Y = 1.0/(std_Y_mat *np.sqrt(dim_Y))\n",
        "\n",
        "\t# Generate normal matrices W and V\n",
        "\t#np.random.seed(seed=0)\n",
        "\tW = np.random.normal(mu_X, sigma_X, (dim_X, d_X_shrink))\n",
        "\tV = np.random.normal(mu_Y, sigma_Y, (dim_Y, d_Y_shrink))\n",
        "\n",
        "\treturn (W,V)\n",
        "\n",
        "# Find KNN distances for a number of samples for normalizing bandwidth\n",
        "def find_knn(A,d):\n",
        "\tnp.random.seed(3334)\n",
        "\t#np.random.seed()\n",
        "\t#np.random.seed(seed=int(time.time()))\n",
        "\tr = 500\n",
        "\t# random samples from A\n",
        "\tA = A.reshape((-1,1))\n",
        "\tN = A.shape[0]\n",
        "\n",
        "\tk=math.floor(0.43*N**(2/3 + 0.17*(d/(d+1)) )*math.exp(-1.0/np.max([10000, d**4])))\n",
        "\n",
        "\tT= np.random.choice(A.reshape(-1,), size=r).reshape(-1,1)\n",
        "\tnbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(A)\n",
        "\tdistances, indices = nbrs.kneighbors(T)\n",
        "\td = np.mean(distances[:,-1])\n",
        "\treturn d\n",
        "\n",
        "# Returns epsilon and random shifts b\n",
        "def gen_eps(XW,YV):\n",
        "\td_X , d_Y  = XW.shape[1], YV.shape[1]\n",
        "\t# Find KNN distances for a number of samples for normalizing bandwidth\n",
        "\teps_X = np.array([find_knn(XW[:,[i]],d_X) for i in range(d_X)]) + 0.0001\n",
        "\teps_Y = np.array([find_knn(YV[:,[i]],d_Y) for i in range(d_Y)]) + 0.0001\n",
        "\n",
        "\treturn (eps_X,eps_Y)\n",
        "\n",
        "# Define H1 (LSH) for a vector X (X is just one sample)\n",
        "def H1(XW,b,eps):\n",
        "\n",
        "\t# dimension of X\n",
        "\td_X = XW.shape[0]\n",
        "\t#d_W = W.shape[1]\n",
        "\tXW=XW.reshape(1,d_X)\n",
        "\n",
        "\t# If not scalar\n",
        "\tif d_X > 1:\n",
        "\t\tX_te = 1.0*(np.squeeze(XW)+b)/eps\n",
        "\telif eps>0:\n",
        "\t\tX_te = 1.0*(XW+b)/eps\n",
        "\telse:\n",
        "\t\tX_te=XW\n",
        "\n",
        "\t# Discretize X\n",
        "\tX_t = np.floor(X_te)\n",
        "\tif d_X>1:\n",
        "\t\tR = tuple(X_t.tolist())\n",
        "\telse: R=np.asscalar(np.squeeze(X_t))\n",
        "\treturn R\n",
        "\n",
        "# Compuate Hashing: Compute the number of collisions in each bucket\n",
        "def Hash(XW,YV,eps_X,eps_Y,b_X,b_Y):\n",
        "\n",
        "  # Num of Samples and dimensions\n",
        "  N = XW.shape[0]\n",
        "  # Hash vectors as dictionaries\n",
        "  CX, CY, CXY = {}, {}, {}\n",
        "\n",
        "  # Computing Collisions\n",
        "\n",
        "  for i in range(N):\n",
        "    # Compute H_1 hashing of X_i and Y_i: Convert to tuple (vectors cannot be taken as keys in dict)\n",
        "\n",
        "    X_l, Y_l = H1(XW[i],b_X,eps_X), H1(YV[i],b_Y,eps_Y)\n",
        "\n",
        "    # X collisions: compute H_2\n",
        "    if X_l in CX:\n",
        "      CX[X_l].append(i)\n",
        "    else:\n",
        "      CX[X_l] = [i]\n",
        "\n",
        "    # Y collisions: compute H_2\n",
        "    if Y_l in CY:\n",
        "      CY[Y_l].append(i)\n",
        "    else:\n",
        "      CY[Y_l] = [i]\n",
        "\n",
        "    # XY collisions\n",
        "    if (X_l,Y_l) in CXY:\n",
        "      CXY[(X_l,Y_l)].append(i)\n",
        "    else:\n",
        "      CXY[(X_l,Y_l)] = [i]\n",
        "\n",
        "  return (CX, CY, CXY)\n",
        "\n",
        "\n",
        "# Compute mutual information and gradient given epsilons and radom shifts\n",
        "def Compute_MI(XW,YV,U,eps_X,eps_Y,b_X,b_Y):\n",
        "  N = XW.shape[0]\n",
        "\n",
        "  (CX, CY, CXY) = Hash(XW,YV,eps_X,eps_Y,b_X,b_Y)\n",
        "\n",
        "  # Computing Mutual Information Function\n",
        "  I = 0\n",
        "  N_c = 0\n",
        "  for e in CXY.keys():\n",
        "    Ni, Mj, Nij = len(CX[e[0]]), len(CY[e[1]]), len(CXY[e])\n",
        "\n",
        "    if 1==1:\n",
        "      I += Nij* max(min(math.log(1.0*Nij*N/(Ni*Mj),2), U),0.001)\n",
        "      N_c+=Nij\n",
        "\n",
        "  I = 1.0* I / N_c\n",
        "\n",
        "  return I\n",
        "\n",
        "def EDGE(X,Y,U=10, gamma=[1, 1], epsilon=[0,0], epsilon_vector = 'fixed', eps_range_factor=0.1, normalize_epsilon = True ,\n",
        "        ensemble_estimation = 'median', L_ensemble=5 ,hashing='p-stable', stochastic = False):\n",
        "\n",
        "  gamma = np.array(gamma)\n",
        "  epsilon = np.array(epsilon)\n",
        "  if X.ndim==1:\n",
        "    X=X.reshape((-1,1))\n",
        "  if Y.ndim==1:\n",
        "    Y=Y.reshape((-1,1))\n",
        "  # Num of Samples and dim\n",
        "  N, d = X.shape[0], X.shape[1]\n",
        "\n",
        "  # Find dimensions\n",
        "  dim_X, dim_Y  = X.shape[1], Y.shape[1]\n",
        "  dim = dim_X + dim_Y\n",
        "\n",
        "  ## Hash type\n",
        "\n",
        "  if hashing == 'p-stable':\n",
        "    # Generate random transformation matrices W and V\n",
        "    (W,V) = gen_W(X,Y)\n",
        "    d_X_shrink, d_Y_shrink=W.shape[1], V.shape[1]\n",
        "    # Find inner products\n",
        "    XW, YV = np.dot(X,W), np.dot(Y,V)\n",
        "\n",
        "  elif hashing == 'floor':\n",
        "    #W = np.identity(dim_X)\n",
        "    #V = np.identity(dim_Y)\n",
        "    d_X_shrink, d_Y_shrink=dim_X, dim_Y\n",
        "    XW, YV = X, Y\n",
        "\n",
        "  ## Initial epsilon and apply smoothness gamma\n",
        "\n",
        "  # If no manual epsilon is set for computing MI:\n",
        "  if epsilon[0] ==0:\n",
        "    # Generate auto epsilon and b\n",
        "    (eps_X_temp,eps_Y_temp) = gen_eps(XW,YV)\n",
        "\n",
        "    # Normalizing factors for the bandwidths\n",
        "    cx, cy = 3*d_X_shrink, 3*d_Y_shrink\n",
        "    eps_X0, eps_Y0 = eps_X_temp * cx*gamma[0], eps_Y_temp * cy*gamma[1]\n",
        "  else:\n",
        "    eps_X_temp = np.ones(d_X_shrink,)*epsilon[0]\n",
        "    eps_Y_temp = np.ones(d_Y_shrink,)*epsilon[1]\n",
        "    cx, cy = 3*d_X_shrink, 3*d_Y_shrink\n",
        "    eps_X0, eps_Y0 = eps_X_temp * cx*gamma[0], eps_Y_temp * cy*gamma[1]\n",
        "\n",
        "  ## epsilon_vector\n",
        "  if epsilon_vector == 'fixed':\n",
        "    T = np.ones(L_ensemble)\n",
        "  elif epsilon_vector == 'range':\n",
        "    T = np.linspace(1,1+eps_range_factor,L_ensemble)\n",
        "\n",
        "\n",
        "  ## Compute MI Vector\n",
        "\n",
        "  # MI Vector\n",
        "  I_vec = np.zeros(L_ensemble)\n",
        "\n",
        "  for j in range(L_ensemble):\n",
        "\n",
        "    # Apply epsilon_vector\n",
        "    eps_X, eps_Y = eps_X0 * T[j], eps_Y0 * T[j]\n",
        "\n",
        "    ## Shifts of hashing\n",
        "    if stochastic== True:\n",
        "      np.random.seed()\n",
        "      f=0.1\n",
        "      b_X = f*np.random.rand(d_X_shrink,)*eps_X\n",
        "      b_Y = f*np.random.rand(d_Y_shrink,)*eps_Y\n",
        "    else:\n",
        "      b_X = np.linspace(0,1,L_ensemble,endpoint=False)[j]*eps_X\n",
        "      b_Y = np.linspace(0,1,L_ensemble,endpoint=False)[j]*eps_Y\n",
        "\n",
        "    I_vec[j] = Compute_MI(XW,YV,U,eps_X,eps_Y,b_X,b_Y)\n",
        "\n",
        "  ## Ensemble method\n",
        "\n",
        "  if ensemble_estimation == 'average':\n",
        "    I = np.mean(I_vec)\n",
        "  elif ensemble_estimation == 'optimal_weights':\n",
        "    weights = compute_weights(L_ensemble, d, T, N)\n",
        "    weights=weights.reshape(L_ensemble,)\n",
        "    I = np.dot(I_vec, weights)\n",
        "  elif ensemble_estimation == 'median':\n",
        "    I = np.median(I_vec)\n",
        "\n",
        "  ## Normalize epsilon according to MI estimation (cross validation)\n",
        "  if normalize_epsilon == True:\n",
        "    gamma=gamma * math.pow(2,-math.sqrt(I*2.0)+(0.5/I))\n",
        "    normalize_epsilon = False\n",
        "    I = EDGE(X,Y,U, gamma, epsilon, epsilon_vector, eps_range_factor, normalize_epsilon, ensemble_estimation, L_ensemble,hashing, stochastic)\n",
        "\n",
        "\n",
        "  return I\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG1STW-d6G2k",
        "outputId": "1d584ea6-614a-49e9-a083-da8633ab9093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_with_mi\n",
            "Epoch 1/10000\n",
            "Step 0: Train Loss: 2.3213179111480713, Train Acc: 0.09880000352859497\n",
            "Epoch index and hidden dimension and ratio: 0 1024 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "MI(X;T): [13.240540348919023, 7.6596081404018985, 5.912137219587784, 0.8469481709650073], MI(Y;T): [3.3117817805539245, 2.4744824620879498, 1.9575007257953074, 1.4382945225111665]\n",
            "Epoch index and hidden dimension and ratio: 0 1024 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "MI(X;T): [13.217864196745362, 7.862591134208376, 5.141146383524216, 3.051013357689377], MI(Y;T): [3.307644036178815, 2.6795736356563897, 2.337078410056077, 1.5621708070970106]\n",
            "Epoch index and hidden dimension and ratio: 0 1024 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "MI(X;T): [13.162361754594308, 8.199870249020409, 5.875897550767484, 2.9111005740663174], MI(Y;T): [3.3025798255530567, 2.8115767286083804, 2.489053217955262, 1.848336063272868]\n",
            "Epoch index and hidden dimension and ratio: 0 1024 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "MI(X;T): [13.106799082926784, 8.866291757833956, 5.552925677552967, 3.0682808428717445], MI(Y;T): [3.299642081177948, 2.773616203422052, 2.5373633989896485, 1.7210961491671717]\n",
            "Epoch index and hidden dimension and ratio: 0 1024 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "MI(X;T): [13.077901702281462, 8.607976123769681, 5.243626450309071, 3.293906984491872], MI(Y;T): [3.2981861104040084, 2.8173581754970707, 2.540929385362041, 1.7703275102872196]\n",
            "Epoch index and hidden dimension and ratio: 0 1024 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "Epoch index and hidden dimension and ratio: 0 20 1\n",
            "MI(X;T): [13.080346197020802, 8.692012131866397, 5.904147895399932, 3.212990380028763], MI(Y;T): [3.2876179897765123, 2.8450892154346716, 2.568441175474934, 1.9225574909996397]\n",
            "Epoch 2/10000\n",
            "Step 0: Train Loss: 1.6048407554626465, Train Acc: 0.3930000066757202\n",
            "Epoch 3/10000\n",
            "Step 0: Train Loss: 0.960899829864502, Train Acc: 0.7069000005722046\n",
            "Epoch 4/10000\n",
            "Step 0: Train Loss: 0.5388622283935547, Train Acc: 0.8770999908447266\n",
            "Epoch 5/10000\n",
            "Step 0: Train Loss: 0.35282811522483826, Train Acc: 0.9041000008583069\n",
            "Epoch 6/10000\n",
            "Step 0: Train Loss: 0.2706722021102905, Train Acc: 0.9204000234603882\n",
            "Epoch 7/10000\n",
            "Step 0: Train Loss: 0.23908430337905884, Train Acc: 0.9329000115394592\n",
            "Epoch 8/10000\n",
            "Step 0: Train Loss: 0.19340458512306213, Train Acc: 0.942300021648407\n",
            "Epoch 9/10000\n",
            "Step 0: Train Loss: 0.1770225167274475, Train Acc: 0.9476000070571899\n",
            "Epoch 10/10000\n",
            "Step 0: Train Loss: 0.14068779349327087, Train Acc: 0.9605000019073486\n",
            "Epoch 11/10000\n",
            "Step 0: Train Loss: 0.11965351551771164, Train Acc: 0.9677000045776367\n",
            "Epoch index and hidden dimension and ratio: 10 1024 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "MI(X;T): [12.99217469998283, 9.984044755720884, 7.535688161529017, 5.129538103740214], MI(Y;T): [3.3053192737951207, 3.2209357145481947, 2.9941117363382803, 2.5461464202542086]\n",
            "Epoch index and hidden dimension and ratio: 10 1024 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "MI(X;T): [12.990444862837832, 10.0011532314137, 7.508720762453105, 5.093836501644101], MI(Y;T): [3.305290059495102, 3.223277346071468, 2.9941440713843117, 2.5265224263527095]\n",
            "Epoch index and hidden dimension and ratio: 10 1024 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "MI(X;T): [12.993096548587609, 10.014829474134006, 7.505043002030902, 5.07555473644325], MI(Y;T): [3.3066125766546444, 3.2247220550132303, 2.9968581696512118, 2.5086220662564402]\n",
            "Epoch index and hidden dimension and ratio: 10 1024 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "MI(X;T): [12.989768705321076, 9.987235463527636, 7.596888694103935, 5.133337238247183], MI(Y;T): [3.3064837746441453, 3.2247036266738935, 3.0003766570927963, 2.527895413526769]\n",
            "Epoch index and hidden dimension and ratio: 10 1024 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "MI(X;T): [12.989216387030398, 9.986531456738552, 7.62050891397841, 5.1971878444395845], MI(Y;T): [3.305017569928151, 3.2223866137814956, 2.9954570319669056, 2.5417409219088847]\n",
            "Epoch index and hidden dimension and ratio: 10 1024 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "Epoch index and hidden dimension and ratio: 10 20 1\n",
            "MI(X;T): [12.988151348720486, 9.985977472246251, 7.60703969542018, 5.186809666486801], MI(Y;T): [3.305717569928153, 3.2214062006061184, 2.9967068507243724, 2.536463005682309]\n",
            "Epoch 12/10000\n",
            "Step 0: Train Loss: 0.11063440889120102, Train Acc: 0.9682999849319458\n",
            "Epoch 13/10000\n",
            "Step 0: Train Loss: 0.09886796027421951, Train Acc: 0.9742000102996826\n",
            "Epoch 14/10000\n",
            "Step 0: Train Loss: 0.09160855412483215, Train Acc: 0.9732000231742859\n",
            "Epoch 15/10000\n",
            "Step 0: Train Loss: 0.0807361826300621, Train Acc: 0.9772999882698059\n",
            "Epoch 16/10000\n",
            "Step 0: Train Loss: 0.07074533402919769, Train Acc: 0.9793999791145325\n",
            "Epoch 17/10000\n",
            "Step 0: Train Loss: 0.06325540691614151, Train Acc: 0.9829000234603882\n",
            "Epoch 18/10000\n",
            "Step 0: Train Loss: 0.058139532804489136, Train Acc: 0.984000027179718\n",
            "Epoch 19/10000\n",
            "Step 0: Train Loss: 0.05882065370678902, Train Acc: 0.9835000038146973\n",
            "Epoch 20/10000\n",
            "Step 0: Train Loss: 0.05389212816953659, Train Acc: 0.9865999817848206\n",
            "Epoch 21/10000\n",
            "Step 0: Train Loss: 0.043732598423957825, Train Acc: 0.9882000088691711\n",
            "Epoch 22/10000\n",
            "Step 0: Train Loss: 0.04263737052679062, Train Acc: 0.9872999787330627\n",
            "Epoch 23/10000\n",
            "Step 0: Train Loss: 0.035067640244960785, Train Acc: 0.991599977016449\n",
            "Epoch 24/10000\n",
            "Step 0: Train Loss: 0.034356437623500824, Train Acc: 0.9919000267982483\n",
            "Epoch 25/10000\n",
            "Step 0: Train Loss: 0.03130005672574043, Train Acc: 0.9926999807357788\n",
            "Epoch 26/10000\n",
            "Step 0: Train Loss: 0.032376375049352646, Train Acc: 0.9919999837875366\n",
            "Epoch 27/10000\n",
            "Step 0: Train Loss: 0.026604199782013893, Train Acc: 0.9934999942779541\n",
            "Epoch 28/10000\n",
            "Step 0: Train Loss: 0.02401278354227543, Train Acc: 0.9947999715805054\n",
            "Epoch 29/10000\n",
            "Step 0: Train Loss: 0.02095327153801918, Train Acc: 0.9962999820709229\n",
            "Epoch 30/10000\n",
            "Step 0: Train Loss: 0.020142095163464546, Train Acc: 0.9958000183105469\n",
            "Epoch 31/10000\n",
            "Step 0: Train Loss: 0.01884816586971283, Train Acc: 0.9965000152587891\n",
            "Epoch index and hidden dimension and ratio: 30 1024 1.070059210333581\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.0990371987639105\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.177778920412923\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.2947847912757482\n",
            "MI(X;T): [12.924656939480366, 10.149052259443042, 7.8402759569573215, 5.248921979861962], MI(Y;T): [3.308317569928149, 3.2536812642769033, 3.119683444932593, 2.61206126906436]\n",
            "Epoch index and hidden dimension and ratio: 30 1024 1.070895862927303\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.1006252697138739\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.1785039858922648\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.2996077153294392\n",
            "MI(X;T): [12.923116225623643, 10.162985908529514, 7.847634056199986, 5.24268528386315], MI(Y;T): [3.3082727400805343, 3.2541191519908867, 3.1225134207087875, 2.6307072322276364]\n",
            "Epoch index and hidden dimension and ratio: 30 1024 1.0716654805216337\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.103042470131053\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.1788901965713328\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.302912715118445\n",
            "MI(X;T): [12.923747210702036, 10.203941395307965, 7.837852349794175, 5.246114148332343], MI(Y;T): [3.309237087904427, 3.253602336022089, 3.1270075417666208, 2.6468381436355046]\n",
            "Epoch index and hidden dimension and ratio: 30 1024 1.0721719508675305\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.1049490448250627\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.181732564974678\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.3051181596239714\n",
            "MI(X;T): [12.925814623691672, 10.228833322259534, 7.8262536528604825, 5.265556642593538], MI(Y;T): [3.3090075833811357, 3.2533773654134137, 3.1256549871163597, 2.6477864102700575]\n",
            "Epoch index and hidden dimension and ratio: 30 1024 1.0725035281482018\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.1057679769653233\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.1865190635978027\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.306707484650687\n",
            "MI(X;T): [12.925283584180907, 10.208914557883194, 7.8481840881251035, 5.289000137024599], MI(Y;T): [3.308061599154212, 3.2520197222829124, 3.1248613896779345, 2.6449627430145437]\n",
            "Epoch index and hidden dimension and ratio: 30 1024 1.0729555922131029\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.1054719290370187\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.1883788927357606\n",
            "Epoch index and hidden dimension and ratio: 30 20 1.310724846881148\n",
            "MI(X;T): [12.925484797559236, 10.192379958704066, 7.849872424998525, 5.278419121286668], MI(Y;T): [3.3082607985326566, 3.254881818892924, 3.1225627701924465, 2.6350501012108474]\n",
            "Epoch 32/10000\n",
            "Step 0: Train Loss: 0.012570889666676521, Train Acc: 0.9980000257492065\n",
            "Epoch 33/10000\n",
            "Step 0: Train Loss: 0.016582831740379333, Train Acc: 0.9973000288009644\n",
            "Epoch 34/10000\n",
            "Step 0: Train Loss: 0.013687798753380775, Train Acc: 0.9979000091552734\n",
            "Epoch 35/10000\n",
            "Step 0: Train Loss: 0.014918896369636059, Train Acc: 0.9970999956130981\n",
            "Epoch 36/10000\n",
            "Step 0: Train Loss: 0.014502209611237049, Train Acc: 0.9973999857902527\n",
            "Epoch 37/10000\n",
            "Step 0: Train Loss: 0.009802442975342274, Train Acc: 0.9987000226974487\n",
            "Epoch 38/10000\n",
            "Step 0: Train Loss: 0.01044852938503027, Train Acc: 0.9983999729156494\n",
            "Epoch 39/10000\n",
            "Step 0: Train Loss: 0.00818635057657957, Train Acc: 0.9984999895095825\n",
            "Epoch 40/10000\n",
            "Step 0: Train Loss: 0.006343676708638668, Train Acc: 0.9994000196456909\n",
            "Epoch 41/10000\n",
            "Step 0: Train Loss: 0.006982192397117615, Train Acc: 0.9995999932289124\n",
            "Epoch 42/10000\n",
            "Step 0: Train Loss: 0.005083604250103235, Train Acc: 0.9997000098228455\n",
            "Epoch 43/10000\n",
            "Step 0: Train Loss: 0.005629129242151976, Train Acc: 0.9995999932289124\n",
            "Epoch 44/10000\n",
            "Step 0: Train Loss: 0.005091208964586258, Train Acc: 0.9995999932289124\n",
            "Epoch 45/10000\n",
            "Step 0: Train Loss: 0.005161297507584095, Train Acc: 0.9995999932289124\n",
            "Epoch 46/10000\n",
            "Step 0: Train Loss: 0.0049288468435406685, Train Acc: 0.9994999766349792\n",
            "Epoch 47/10000\n",
            "Step 0: Train Loss: 0.004710858222097158, Train Acc: 0.9998000264167786\n",
            "Epoch 48/10000\n",
            "Step 0: Train Loss: 0.0036356139462441206, Train Acc: 0.9997000098228455\n",
            "Epoch 49/10000\n",
            "Step 0: Train Loss: 0.0038211129140108824, Train Acc: 0.9994999766349792\n",
            "Epoch 50/10000\n",
            "Step 0: Train Loss: 0.003200794570147991, Train Acc: 0.9997000098228455\n",
            "Epoch 51/10000\n",
            "Step 0: Train Loss: 0.004029654432088137, Train Acc: 0.9998000264167786\n",
            "Epoch 52/10000\n",
            "Step 0: Train Loss: 0.003025203477591276, Train Acc: 0.9998999834060669\n",
            "Epoch 53/10000\n",
            "Step 0: Train Loss: 0.0035632748622447252, Train Acc: 0.9998000264167786\n",
            "Epoch 54/10000\n",
            "Step 0: Train Loss: 0.002180913696065545, Train Acc: 0.9998999834060669\n",
            "Epoch 55/10000\n",
            "Step 0: Train Loss: 0.0022055343724787235, Train Acc: 0.9998999834060669\n",
            "Epoch 56/10000\n",
            "Step 0: Train Loss: 0.002482254523783922, Train Acc: 0.9998999834060669\n",
            "Epoch 57/10000\n",
            "Step 0: Train Loss: 0.0022693518549203873, Train Acc: 0.9998000264167786\n",
            "Epoch 58/10000\n",
            "Step 0: Train Loss: 0.001983961556106806, Train Acc: 1.0\n",
            "Epoch 59/10000\n",
            "Step 0: Train Loss: 0.0022271769121289253, Train Acc: 0.9998999834060669\n",
            "Epoch 60/10000\n",
            "Step 0: Train Loss: 0.0015282119857147336, Train Acc: 1.0\n",
            "Epoch 61/10000\n",
            "Step 0: Train Loss: 0.002040922874584794, Train Acc: 0.9998999834060669\n",
            "Epoch index and hidden dimension and ratio: 60 1024 1.1279825159799177\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.2300051572438664\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.3957890086116602\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.6708949682451693\n",
            "MI(X;T): [12.955320301906927, 10.282679827787803, 7.870343619618122, 5.239793882147286], MI(Y;T): [3.3087615991542103, 3.2580516029356805, 3.144683329430852, 2.702624317914103]\n",
            "Epoch index and hidden dimension and ratio: 60 1024 1.128180684066838\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.2299978889363559\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.3970718070013064\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.6719322535092283\n",
            "MI(X;T): [12.954151639619594, 10.279229167669296, 7.875914715973832, 5.2427223838172665], MI(Y;T): [3.3084238547791025, 3.258030011436746, 3.1453052690367507, 2.704078769064166]\n",
            "Epoch index and hidden dimension and ratio: 60 1024 1.1283730517540298\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.2299938750948949\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.3980671691722115\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.6731549504295273\n",
            "MI(X;T): [12.95337326927725, 10.279613517666482, 7.876552438130531, 5.245985096770067], MI(Y;T): [3.3079175699281484, 3.2581379840145095, 3.146611294212593, 2.6998689238011533]\n",
            "Epoch index and hidden dimension and ratio: 60 1024 1.1285638041400317\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.2301752573360547\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.398462520932441\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.6742142810901581\n",
            "MI(X;T): [12.95117822612486, 10.278245463057088, 7.87024136039149, 5.242560342453256], MI(Y;T): [3.3078175699281482, 3.257894530523389, 3.1453671623140487, 2.7006715061624655]\n",
            "Epoch index and hidden dimension and ratio: 60 1024 1.128755878136098\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.2305730615695074\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.3984913407299915\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.6754167423106192\n",
            "MI(X;T): [12.95149327391277, 10.277771462523061, 7.876261974913095, 5.244083782937678], MI(Y;T): [3.3076658986151397, 3.2586670048762882, 3.1447363281389054, 2.7027000604286404]\n",
            "Epoch index and hidden dimension and ratio: 60 1024 1.1289180690601466\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.2312843793657307\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.3987540198539203\n",
            "Epoch index and hidden dimension and ratio: 60 20 1.6761575663461556\n",
            "MI(X;T): [12.952252989331868, 10.278680693873355, 7.877609824768575, 5.249166393123515], MI(Y;T): [3.3077658986151395, 3.258916441700667, 3.14566247316476, 2.704279394061661]\n",
            "Epoch 62/10000\n",
            "Step 0: Train Loss: 0.0013039541663601995, Train Acc: 1.0\n",
            "Epoch 63/10000\n",
            "Step 0: Train Loss: 0.0012640554923564196, Train Acc: 1.0\n",
            "Epoch 64/10000\n",
            "Step 0: Train Loss: 0.002228201599791646, Train Acc: 0.9998999834060669\n",
            "Epoch 65/10000\n",
            "Step 0: Train Loss: 0.001986025832593441, Train Acc: 0.9998999834060669\n",
            "Epoch 66/10000\n",
            "Step 0: Train Loss: 0.001146757393144071, Train Acc: 1.0\n",
            "Epoch 67/10000\n",
            "Step 0: Train Loss: 0.001096629537642002, Train Acc: 1.0\n",
            "Epoch 68/10000\n",
            "Step 0: Train Loss: 0.0020914445631206036, Train Acc: 0.9998000264167786\n",
            "Epoch 69/10000\n",
            "Step 0: Train Loss: 0.001200430328026414, Train Acc: 0.9998999834060669\n",
            "Epoch 70/10000\n",
            "Step 0: Train Loss: 0.0009072439279407263, Train Acc: 1.0\n",
            "Epoch 71/10000\n",
            "Step 0: Train Loss: 0.0008472898625768721, Train Acc: 1.0\n",
            "Epoch 72/10000\n",
            "Step 0: Train Loss: 0.0008705876534804702, Train Acc: 1.0\n",
            "Epoch 73/10000\n",
            "Step 0: Train Loss: 0.0008579968707635999, Train Acc: 1.0\n",
            "Epoch 74/10000\n",
            "Step 0: Train Loss: 0.0008166436455212533, Train Acc: 1.0\n",
            "Epoch 75/10000\n",
            "Step 0: Train Loss: 0.0007617271621711552, Train Acc: 1.0\n",
            "Epoch 76/10000\n",
            "Step 0: Train Loss: 0.0008195667178370059, Train Acc: 1.0\n",
            "Epoch 77/10000\n",
            "Step 0: Train Loss: 0.00066596973920241, Train Acc: 1.0\n",
            "Epoch 78/10000\n",
            "Step 0: Train Loss: 0.00065850984537974, Train Acc: 1.0\n",
            "Epoch 79/10000\n",
            "Step 0: Train Loss: 0.0008183742756955326, Train Acc: 1.0\n",
            "Epoch 80/10000\n",
            "Step 0: Train Loss: 0.0005978237022645772, Train Acc: 1.0\n",
            "Epoch 81/10000\n",
            "Step 0: Train Loss: 0.0006700576632283628, Train Acc: 1.0\n",
            "Epoch 82/10000\n",
            "Step 0: Train Loss: 0.0006313583580777049, Train Acc: 1.0\n",
            "Epoch 83/10000\n",
            "Step 0: Train Loss: 0.0008652942487969995, Train Acc: 0.9998999834060669\n",
            "Epoch 84/10000\n",
            "Step 0: Train Loss: 0.0005767347174696624, Train Acc: 1.0\n",
            "Epoch 85/10000\n",
            "Step 0: Train Loss: 0.0005474420031532645, Train Acc: 1.0\n",
            "Epoch 86/10000\n",
            "Step 0: Train Loss: 0.0005539461853913963, Train Acc: 1.0\n",
            "Epoch 87/10000\n",
            "Step 0: Train Loss: 0.000546569237485528, Train Acc: 1.0\n",
            "Epoch 88/10000\n",
            "Step 0: Train Loss: 0.00048185462947003543, Train Acc: 1.0\n",
            "Epoch 89/10000\n",
            "Step 0: Train Loss: 0.0005162025918252766, Train Acc: 1.0\n",
            "Epoch 90/10000\n",
            "Step 0: Train Loss: 0.0004404045466799289, Train Acc: 1.0\n",
            "Epoch 91/10000\n",
            "Step 0: Train Loss: 0.00043555229785852134, Train Acc: 1.0\n",
            "Epoch 92/10000\n",
            "Step 0: Train Loss: 0.0004755199479404837, Train Acc: 1.0\n",
            "Epoch 93/10000\n",
            "Step 0: Train Loss: 0.0004232419596519321, Train Acc: 1.0\n",
            "Epoch 94/10000\n",
            "Step 0: Train Loss: 0.0004064432287123054, Train Acc: 1.0\n",
            "Epoch 95/10000\n",
            "Step 0: Train Loss: 0.000390579312806949, Train Acc: 1.0\n",
            "Epoch 96/10000\n",
            "Step 0: Train Loss: 0.0003731472825165838, Train Acc: 1.0\n",
            "Epoch 97/10000\n",
            "Step 0: Train Loss: 0.00038256781408563256, Train Acc: 1.0\n",
            "Epoch 98/10000\n",
            "Step 0: Train Loss: 0.0003425933828111738, Train Acc: 1.0\n",
            "Epoch 99/10000\n",
            "Step 0: Train Loss: 0.00033131774398498237, Train Acc: 1.0\n",
            "Epoch 100/10000\n",
            "Step 0: Train Loss: 0.0003631274157669395, Train Acc: 1.0\n",
            "Epoch 101/10000\n",
            "Step 0: Train Loss: 0.0003495838027447462, Train Acc: 1.0\n",
            "Epoch 102/10000\n",
            "Step 0: Train Loss: 0.0003268574073445052, Train Acc: 1.0\n",
            "Epoch 103/10000\n",
            "Step 0: Train Loss: 0.00030530898948200047, Train Acc: 1.0\n",
            "Epoch 104/10000\n",
            "Step 0: Train Loss: 0.00030315457843244076, Train Acc: 1.0\n",
            "Epoch 105/10000\n",
            "Step 0: Train Loss: 0.00030148206860758364, Train Acc: 1.0\n",
            "Epoch 106/10000\n",
            "Step 0: Train Loss: 0.00029897261993028224, Train Acc: 1.0\n",
            "Epoch 107/10000\n",
            "Step 0: Train Loss: 0.0002950330381281674, Train Acc: 1.0\n",
            "Epoch 108/10000\n",
            "Step 0: Train Loss: 0.0002741592761594802, Train Acc: 1.0\n",
            "Epoch 109/10000\n",
            "Step 0: Train Loss: 0.000267141149379313, Train Acc: 1.0\n",
            "Epoch 110/10000\n",
            "Step 0: Train Loss: 0.0002459482057020068, Train Acc: 1.0\n",
            "Epoch 111/10000\n",
            "Step 0: Train Loss: 0.00024787947768345475, Train Acc: 1.0\n",
            "Epoch 112/10000\n",
            "Step 0: Train Loss: 0.00022000126773491502, Train Acc: 1.0\n",
            "Epoch 113/10000\n",
            "Step 0: Train Loss: 0.00023909173614811152, Train Acc: 1.0\n",
            "Epoch 114/10000\n",
            "Step 0: Train Loss: 0.000233535174629651, Train Acc: 1.0\n",
            "Epoch 115/10000\n",
            "Step 0: Train Loss: 0.00024143514747265726, Train Acc: 1.0\n",
            "Epoch 116/10000\n",
            "Step 0: Train Loss: 0.00021357278455980122, Train Acc: 1.0\n",
            "Epoch 117/10000\n",
            "Step 0: Train Loss: 0.00021424965234473348, Train Acc: 1.0\n",
            "Epoch 118/10000\n",
            "Step 0: Train Loss: 0.00019919789338018745, Train Acc: 1.0\n",
            "Epoch 119/10000\n",
            "Step 0: Train Loss: 0.0002027425216510892, Train Acc: 1.0\n",
            "Epoch 120/10000\n",
            "Step 0: Train Loss: 0.0001963692920980975, Train Acc: 1.0\n",
            "Epoch 121/10000\n",
            "Step 0: Train Loss: 0.00019303019507788122, Train Acc: 1.0\n",
            "Epoch 122/10000\n",
            "Step 0: Train Loss: 0.00021067116176709533, Train Acc: 1.0\n",
            "Epoch 123/10000\n",
            "Step 0: Train Loss: 0.0001787951769074425, Train Acc: 1.0\n",
            "Epoch 124/10000\n",
            "Step 0: Train Loss: 0.00021453635417856276, Train Acc: 1.0\n",
            "Epoch 125/10000\n",
            "Step 0: Train Loss: 0.00017152514192275703, Train Acc: 1.0\n",
            "Epoch 126/10000\n",
            "Step 0: Train Loss: 0.00016891425184439868, Train Acc: 1.0\n",
            "Epoch 127/10000\n",
            "Step 0: Train Loss: 0.00016947815311141312, Train Acc: 1.0\n",
            "Epoch 128/10000\n",
            "Step 0: Train Loss: 0.0001782698673196137, Train Acc: 1.0\n",
            "Epoch 129/10000\n",
            "Step 0: Train Loss: 0.0001742880849633366, Train Acc: 1.0\n",
            "Epoch 130/10000\n",
            "Step 0: Train Loss: 0.0001612017658771947, Train Acc: 1.0\n",
            "Epoch 131/10000\n",
            "Step 0: Train Loss: 0.00016400418826378882, Train Acc: 1.0\n",
            "Epoch 132/10000\n",
            "Step 0: Train Loss: 0.00016925910313148052, Train Acc: 1.0\n",
            "Epoch 133/10000\n",
            "Step 0: Train Loss: 0.0001492961309850216, Train Acc: 1.0\n",
            "Epoch 134/10000\n",
            "Step 0: Train Loss: 0.000165735007612966, Train Acc: 1.0\n",
            "Epoch 135/10000\n",
            "Step 0: Train Loss: 0.00014910628669895232, Train Acc: 1.0\n",
            "Epoch 136/10000\n",
            "Step 0: Train Loss: 0.0001536615309305489, Train Acc: 1.0\n",
            "Epoch 137/10000\n",
            "Step 0: Train Loss: 0.00014161896251607686, Train Acc: 1.0\n",
            "Epoch 138/10000\n",
            "Step 0: Train Loss: 0.0001581773831276223, Train Acc: 1.0\n",
            "Epoch 139/10000\n",
            "Step 0: Train Loss: 0.0001393322745570913, Train Acc: 1.0\n",
            "Epoch 140/10000\n",
            "Step 0: Train Loss: 0.00013620476238429546, Train Acc: 1.0\n",
            "Epoch 141/10000\n",
            "Step 0: Train Loss: 0.00013013958232477307, Train Acc: 1.0\n",
            "Epoch 142/10000\n",
            "Step 0: Train Loss: 0.00012761344260070473, Train Acc: 1.0\n",
            "Epoch 143/10000\n",
            "Step 0: Train Loss: 0.00014469180314335972, Train Acc: 1.0\n",
            "Epoch 144/10000\n",
            "Step 0: Train Loss: 0.00013619603123515844, Train Acc: 1.0\n",
            "Epoch 145/10000\n",
            "Step 0: Train Loss: 0.00013111064617987722, Train Acc: 1.0\n",
            "Epoch 146/10000\n",
            "Step 0: Train Loss: 0.00011192086094524711, Train Acc: 1.0\n",
            "Epoch 147/10000\n",
            "Step 0: Train Loss: 0.0001298281567869708, Train Acc: 1.0\n",
            "Epoch 148/10000\n",
            "Step 0: Train Loss: 0.00012626634270418435, Train Acc: 1.0\n",
            "Epoch 149/10000\n",
            "Step 0: Train Loss: 0.0001316705165663734, Train Acc: 1.0\n",
            "Epoch 150/10000\n",
            "Step 0: Train Loss: 0.00012226829130668193, Train Acc: 1.0\n",
            "Epoch 151/10000\n",
            "Step 0: Train Loss: 0.00011171448568347842, Train Acc: 1.0\n",
            "Epoch 152/10000\n",
            "Step 0: Train Loss: 0.00012370794138405472, Train Acc: 1.0\n",
            "Epoch 153/10000\n",
            "Step 0: Train Loss: 0.00011649954103631899, Train Acc: 1.0\n",
            "Epoch 154/10000\n",
            "Step 0: Train Loss: 0.00011591236398089677, Train Acc: 1.0\n",
            "Epoch 155/10000\n",
            "Step 0: Train Loss: 0.0001046832330757752, Train Acc: 1.0\n",
            "Epoch 156/10000\n",
            "Step 0: Train Loss: 0.00011196480772923678, Train Acc: 1.0\n",
            "Epoch 157/10000\n",
            "Step 0: Train Loss: 0.00010127265704795718, Train Acc: 1.0\n",
            "Epoch 158/10000\n",
            "Step 0: Train Loss: 0.00010815721907420084, Train Acc: 1.0\n",
            "Epoch 159/10000\n",
            "Step 0: Train Loss: 0.00010083406959893182, Train Acc: 1.0\n",
            "Epoch 160/10000\n",
            "Step 0: Train Loss: 9.269780275644735e-05, Train Acc: 1.0\n",
            "Epoch 161/10000\n",
            "Step 0: Train Loss: 9.96441813185811e-05, Train Acc: 1.0\n",
            "Epoch 162/10000\n",
            "Step 0: Train Loss: 0.00010893747821683064, Train Acc: 1.0\n",
            "Epoch 163/10000\n",
            "Step 0: Train Loss: 0.00010146210115635768, Train Acc: 1.0\n",
            "Epoch 164/10000\n",
            "Step 0: Train Loss: 0.00010062207002192736, Train Acc: 1.0\n",
            "Epoch 165/10000\n",
            "Step 0: Train Loss: 9.487460920354351e-05, Train Acc: 1.0\n",
            "Epoch 166/10000\n",
            "Step 0: Train Loss: 8.540520502720028e-05, Train Acc: 1.0\n",
            "Epoch 167/10000\n",
            "Step 0: Train Loss: 8.277770393760875e-05, Train Acc: 1.0\n",
            "Epoch 168/10000\n",
            "Step 0: Train Loss: 9.342385601485148e-05, Train Acc: 1.0\n",
            "Epoch 169/10000\n",
            "Step 0: Train Loss: 8.967554458649829e-05, Train Acc: 1.0\n",
            "Epoch 170/10000\n",
            "Step 0: Train Loss: 8.651269308757037e-05, Train Acc: 1.0\n",
            "Epoch 171/10000\n",
            "Step 0: Train Loss: 8.919797255657613e-05, Train Acc: 1.0\n",
            "Epoch 172/10000\n",
            "Step 0: Train Loss: 8.435577910859138e-05, Train Acc: 1.0\n",
            "Epoch 173/10000\n",
            "Step 0: Train Loss: 9.155344014288858e-05, Train Acc: 1.0\n",
            "Epoch 174/10000\n",
            "Step 0: Train Loss: 8.123595762299374e-05, Train Acc: 1.0\n",
            "Epoch 175/10000\n",
            "Step 0: Train Loss: 8.172908565029502e-05, Train Acc: 1.0\n",
            "Epoch 176/10000\n",
            "Step 0: Train Loss: 8.948264439823106e-05, Train Acc: 1.0\n",
            "Epoch 177/10000\n",
            "Step 0: Train Loss: 7.780751184327528e-05, Train Acc: 1.0\n",
            "Epoch 178/10000\n",
            "Step 0: Train Loss: 7.536402699770406e-05, Train Acc: 1.0\n",
            "Epoch 179/10000\n",
            "Step 0: Train Loss: 7.685005402890965e-05, Train Acc: 1.0\n",
            "Epoch 180/10000\n",
            "Step 0: Train Loss: 8.800422074273229e-05, Train Acc: 1.0\n",
            "Epoch 181/10000\n",
            "Step 0: Train Loss: 7.352662942139432e-05, Train Acc: 1.0\n",
            "Epoch 182/10000\n",
            "Step 0: Train Loss: 7.485468086088076e-05, Train Acc: 1.0\n",
            "Epoch 183/10000\n",
            "Step 0: Train Loss: 6.726744322804734e-05, Train Acc: 1.0\n",
            "Epoch 184/10000\n",
            "Step 0: Train Loss: 7.61427654651925e-05, Train Acc: 1.0\n",
            "Epoch 185/10000\n",
            "Step 0: Train Loss: 7.812739931978285e-05, Train Acc: 1.0\n",
            "Epoch 186/10000\n",
            "Step 0: Train Loss: 7.006448868196458e-05, Train Acc: 1.0\n",
            "Epoch 187/10000\n",
            "Step 0: Train Loss: 7.189153257058933e-05, Train Acc: 1.0\n",
            "Epoch 188/10000\n",
            "Step 0: Train Loss: 6.49198773317039e-05, Train Acc: 1.0\n",
            "Epoch 189/10000\n",
            "Step 0: Train Loss: 6.53233946650289e-05, Train Acc: 1.0\n",
            "Epoch 190/10000\n",
            "Step 0: Train Loss: 6.432981172110885e-05, Train Acc: 1.0\n",
            "Epoch 191/10000\n",
            "Step 0: Train Loss: 6.775895599275827e-05, Train Acc: 1.0\n",
            "Epoch 192/10000\n",
            "Step 0: Train Loss: 6.676217162748799e-05, Train Acc: 1.0\n",
            "Epoch 193/10000\n",
            "Step 0: Train Loss: 6.804863369325176e-05, Train Acc: 1.0\n",
            "Epoch 194/10000\n",
            "Step 0: Train Loss: 6.133197894087061e-05, Train Acc: 1.0\n",
            "Epoch 195/10000\n",
            "Step 0: Train Loss: 5.911362313781865e-05, Train Acc: 1.0\n",
            "Epoch 196/10000\n",
            "Step 0: Train Loss: 5.426842471933924e-05, Train Acc: 1.0\n",
            "Epoch 197/10000\n",
            "Step 0: Train Loss: 6.255127664189786e-05, Train Acc: 1.0\n",
            "Epoch 198/10000\n",
            "Step 0: Train Loss: 6.384473090292886e-05, Train Acc: 1.0\n",
            "Epoch 199/10000\n",
            "Step 0: Train Loss: 5.4531854402739555e-05, Train Acc: 1.0\n",
            "Epoch 200/10000\n",
            "Step 0: Train Loss: 5.252100163488649e-05, Train Acc: 1.0\n",
            "Epoch 201/10000\n",
            "Step 0: Train Loss: 5.8741046814247966e-05, Train Acc: 1.0\n",
            "Epoch 202/10000\n",
            "Step 0: Train Loss: 5.7325429224874824e-05, Train Acc: 1.0\n",
            "Epoch 203/10000\n",
            "Step 0: Train Loss: 5.5334483477054164e-05, Train Acc: 1.0\n",
            "Epoch 204/10000\n",
            "Step 0: Train Loss: 5.503219290403649e-05, Train Acc: 1.0\n",
            "Epoch 205/10000\n",
            "Step 0: Train Loss: 5.358026464818977e-05, Train Acc: 1.0\n",
            "Epoch 206/10000\n",
            "Step 0: Train Loss: 5.255988799035549e-05, Train Acc: 1.0\n",
            "Epoch 207/10000\n",
            "Step 0: Train Loss: 5.84438384976238e-05, Train Acc: 1.0\n",
            "Epoch 208/10000\n",
            "Step 0: Train Loss: 5.6778309954097494e-05, Train Acc: 1.0\n",
            "Epoch 209/10000\n",
            "Step 0: Train Loss: 5.3286017646314576e-05, Train Acc: 1.0\n",
            "Epoch 210/10000\n",
            "Step 0: Train Loss: 5.0838567403843626e-05, Train Acc: 1.0\n",
            "Epoch 211/10000\n",
            "Step 0: Train Loss: 4.985804480384104e-05, Train Acc: 1.0\n",
            "Epoch 212/10000\n",
            "Step 0: Train Loss: 5.779469574918039e-05, Train Acc: 1.0\n",
            "Epoch 213/10000\n",
            "Step 0: Train Loss: 5.4527950851479545e-05, Train Acc: 1.0\n",
            "Epoch 214/10000\n",
            "Step 0: Train Loss: 4.6615867177024484e-05, Train Acc: 1.0\n",
            "Epoch 215/10000\n",
            "Step 0: Train Loss: 4.979017103323713e-05, Train Acc: 1.0\n",
            "Epoch 216/10000\n",
            "Step 0: Train Loss: 5.16635955136735e-05, Train Acc: 1.0\n",
            "Epoch 217/10000\n",
            "Step 0: Train Loss: 4.540255758911371e-05, Train Acc: 1.0\n",
            "Epoch 218/10000\n",
            "Step 0: Train Loss: 5.204436820349656e-05, Train Acc: 1.0\n",
            "Epoch 219/10000\n",
            "Step 0: Train Loss: 4.8138801503228024e-05, Train Acc: 1.0\n",
            "Epoch 220/10000\n",
            "Step 0: Train Loss: 4.7314992116298527e-05, Train Acc: 1.0\n",
            "Epoch 221/10000\n",
            "Step 0: Train Loss: 4.902430373476818e-05, Train Acc: 1.0\n",
            "Epoch 222/10000\n",
            "Step 0: Train Loss: 4.3866399209946394e-05, Train Acc: 1.0\n",
            "Epoch 223/10000\n",
            "Step 0: Train Loss: 4.714302849606611e-05, Train Acc: 1.0\n",
            "Epoch 224/10000\n",
            "Step 0: Train Loss: 4.657403042074293e-05, Train Acc: 1.0\n",
            "Epoch 225/10000\n",
            "Step 0: Train Loss: 4.4054395402781665e-05, Train Acc: 1.0\n",
            "Epoch 226/10000\n",
            "Step 0: Train Loss: 4.4975273340241984e-05, Train Acc: 1.0\n",
            "Epoch 227/10000\n",
            "Step 0: Train Loss: 4.476067260839045e-05, Train Acc: 1.0\n",
            "Epoch 228/10000\n",
            "Step 0: Train Loss: 4.318037463235669e-05, Train Acc: 1.0\n",
            "Epoch 229/10000\n",
            "Step 0: Train Loss: 4.4149164750706404e-05, Train Acc: 1.0\n",
            "Epoch 230/10000\n",
            "Step 0: Train Loss: 4.187257945886813e-05, Train Acc: 1.0\n",
            "Epoch 231/10000\n",
            "Step 0: Train Loss: 3.949347592424601e-05, Train Acc: 1.0\n",
            "Epoch 232/10000\n",
            "Step 0: Train Loss: 3.6489622289082035e-05, Train Acc: 1.0\n",
            "Epoch 233/10000\n",
            "Step 0: Train Loss: 4.09448730351869e-05, Train Acc: 1.0\n",
            "Epoch 234/10000\n",
            "Step 0: Train Loss: 4.130815796088427e-05, Train Acc: 1.0\n",
            "Epoch 235/10000\n",
            "Step 0: Train Loss: 3.728476076503284e-05, Train Acc: 1.0\n",
            "Epoch 236/10000\n",
            "Step 0: Train Loss: 3.961022957810201e-05, Train Acc: 1.0\n",
            "Epoch 237/10000\n",
            "Step 0: Train Loss: 4.029495903523639e-05, Train Acc: 1.0\n",
            "Epoch 238/10000\n",
            "Step 0: Train Loss: 3.8947368011577055e-05, Train Acc: 1.0\n",
            "Epoch 239/10000\n",
            "Step 0: Train Loss: 4.0827395423548296e-05, Train Acc: 1.0\n",
            "Epoch 240/10000\n",
            "Step 0: Train Loss: 3.795825978158973e-05, Train Acc: 1.0\n",
            "Epoch 241/10000\n",
            "Step 0: Train Loss: 4.031770731671713e-05, Train Acc: 1.0\n",
            "Epoch 242/10000\n",
            "Step 0: Train Loss: 3.660230140667409e-05, Train Acc: 1.0\n",
            "Epoch 243/10000\n",
            "Step 0: Train Loss: 4.0650105802342296e-05, Train Acc: 1.0\n",
            "Epoch 244/10000\n",
            "Step 0: Train Loss: 3.833222217508592e-05, Train Acc: 1.0\n",
            "Epoch 245/10000\n",
            "Step 0: Train Loss: 3.654671672848053e-05, Train Acc: 1.0\n",
            "Epoch 246/10000\n",
            "Step 0: Train Loss: 3.754148565349169e-05, Train Acc: 1.0\n",
            "Epoch 247/10000\n",
            "Step 0: Train Loss: 3.5684792237589136e-05, Train Acc: 1.0\n",
            "Epoch 248/10000\n",
            "Step 0: Train Loss: 3.413483136682771e-05, Train Acc: 1.0\n",
            "Epoch 249/10000\n",
            "Step 0: Train Loss: 3.53827781509608e-05, Train Acc: 1.0\n",
            "Epoch 250/10000\n",
            "Step 0: Train Loss: 3.526194268488325e-05, Train Acc: 1.0\n",
            "Epoch 251/10000\n",
            "Step 0: Train Loss: 3.57014978362713e-05, Train Acc: 1.0\n",
            "Epoch 252/10000\n",
            "Step 0: Train Loss: 3.166306487401016e-05, Train Acc: 1.0\n",
            "Epoch 253/10000\n",
            "Step 0: Train Loss: 3.210029171896167e-05, Train Acc: 1.0\n",
            "Epoch 254/10000\n",
            "Step 0: Train Loss: 3.3320502552669495e-05, Train Acc: 1.0\n",
            "Epoch 255/10000\n",
            "Step 0: Train Loss: 3.237840792280622e-05, Train Acc: 1.0\n",
            "Epoch 256/10000\n",
            "Step 0: Train Loss: 3.435660255490802e-05, Train Acc: 1.0\n",
            "Epoch 257/10000\n",
            "Step 0: Train Loss: 3.168988405377604e-05, Train Acc: 1.0\n",
            "Epoch 258/10000\n",
            "Step 0: Train Loss: 3.057317735510878e-05, Train Acc: 1.0\n",
            "Epoch 259/10000\n",
            "Step 0: Train Loss: 3.103834023931995e-05, Train Acc: 1.0\n",
            "Epoch 260/10000\n",
            "Step 0: Train Loss: 3.317851587780751e-05, Train Acc: 1.0\n",
            "Epoch 261/10000\n",
            "Step 0: Train Loss: 3.0498107662424445e-05, Train Acc: 1.0\n",
            "Epoch 262/10000\n",
            "Step 0: Train Loss: 2.9838192858733237e-05, Train Acc: 1.0\n",
            "Epoch 263/10000\n",
            "Step 0: Train Loss: 3.2396113965660334e-05, Train Acc: 1.0\n",
            "Epoch 264/10000\n",
            "Step 0: Train Loss: 2.9599255867651664e-05, Train Acc: 1.0\n",
            "Epoch 265/10000\n",
            "Step 0: Train Loss: 3.31777409883216e-05, Train Acc: 1.0\n",
            "Epoch 266/10000\n",
            "Step 0: Train Loss: 3.050897976208944e-05, Train Acc: 1.0\n",
            "Epoch 267/10000\n",
            "Step 0: Train Loss: 2.860705899365712e-05, Train Acc: 1.0\n",
            "Epoch 268/10000\n",
            "Step 0: Train Loss: 3.2113195629790425e-05, Train Acc: 1.0\n",
            "Epoch 269/10000\n",
            "Step 0: Train Loss: 2.8582033337443136e-05, Train Acc: 1.0\n",
            "Epoch 270/10000\n",
            "Step 0: Train Loss: 2.9599395929835737e-05, Train Acc: 1.0\n",
            "Epoch 271/10000\n",
            "Step 0: Train Loss: 2.9021528462180868e-05, Train Acc: 1.0\n",
            "Epoch 272/10000\n",
            "Step 0: Train Loss: 2.663523810042534e-05, Train Acc: 1.0\n",
            "Epoch 273/10000\n",
            "Step 0: Train Loss: 2.8061267585144378e-05, Train Acc: 1.0\n",
            "Epoch 274/10000\n",
            "Step 0: Train Loss: 2.7185160433873534e-05, Train Acc: 1.0\n",
            "Epoch 275/10000\n",
            "Step 0: Train Loss: 3.124264185316861e-05, Train Acc: 1.0\n",
            "Epoch 276/10000\n",
            "Step 0: Train Loss: 2.646973189257551e-05, Train Acc: 1.0\n",
            "Epoch 277/10000\n",
            "Step 0: Train Loss: 2.4369805032620206e-05, Train Acc: 1.0\n",
            "Epoch 278/10000\n",
            "Step 0: Train Loss: 2.66989827650832e-05, Train Acc: 1.0\n",
            "Epoch 279/10000\n",
            "Step 0: Train Loss: 2.5852703402051702e-05, Train Acc: 1.0\n",
            "Epoch 280/10000\n",
            "Step 0: Train Loss: 2.5220271709258668e-05, Train Acc: 1.0\n",
            "Epoch 281/10000\n",
            "Step 0: Train Loss: 2.7505657271831296e-05, Train Acc: 1.0\n",
            "Epoch 282/10000\n",
            "Step 0: Train Loss: 2.6709294616011903e-05, Train Acc: 1.0\n",
            "Epoch 283/10000\n",
            "Step 0: Train Loss: 2.237868648080621e-05, Train Acc: 1.0\n",
            "Epoch 284/10000\n",
            "Step 0: Train Loss: 2.6397401597932912e-05, Train Acc: 1.0\n",
            "Epoch 285/10000\n",
            "Step 0: Train Loss: 2.400017729087267e-05, Train Acc: 1.0\n",
            "Epoch 286/10000\n",
            "Step 0: Train Loss: 2.7511998268892057e-05, Train Acc: 1.0\n",
            "Epoch 287/10000\n",
            "Step 0: Train Loss: 2.6819947379408404e-05, Train Acc: 1.0\n",
            "Epoch 288/10000\n",
            "Step 0: Train Loss: 2.5800183721003123e-05, Train Acc: 1.0\n",
            "Epoch 289/10000\n",
            "Step 0: Train Loss: 2.707873318286147e-05, Train Acc: 1.0\n",
            "Epoch 290/10000\n",
            "Step 0: Train Loss: 2.5760462449397892e-05, Train Acc: 1.0\n",
            "Epoch 291/10000\n",
            "Step 0: Train Loss: 2.547051371948328e-05, Train Acc: 1.0\n",
            "Epoch 292/10000\n",
            "Step 0: Train Loss: 2.3779166440363042e-05, Train Acc: 1.0\n",
            "Epoch 293/10000\n",
            "Step 0: Train Loss: 2.3546599550172687e-05, Train Acc: 1.0\n",
            "Epoch 294/10000\n",
            "Step 0: Train Loss: 2.2869908207212575e-05, Train Acc: 1.0\n",
            "Epoch 295/10000\n",
            "Step 0: Train Loss: 2.4216225938289426e-05, Train Acc: 1.0\n",
            "Epoch 296/10000\n",
            "Step 0: Train Loss: 2.1687759726773947e-05, Train Acc: 1.0\n",
            "Epoch 297/10000\n",
            "Step 0: Train Loss: 2.5591933081159368e-05, Train Acc: 1.0\n",
            "Epoch 298/10000\n",
            "Step 0: Train Loss: 2.3151262212195434e-05, Train Acc: 1.0\n",
            "Epoch 299/10000\n",
            "Step 0: Train Loss: 2.392790702288039e-05, Train Acc: 1.0\n",
            "Epoch 300/10000\n",
            "Step 0: Train Loss: 2.2696036467095837e-05, Train Acc: 1.0\n",
            "Epoch 301/10000\n",
            "Step 0: Train Loss: 2.1391957488958724e-05, Train Acc: 1.0\n",
            "Epoch 302/10000\n",
            "Step 0: Train Loss: 2.1127732907189056e-05, Train Acc: 1.0\n",
            "Epoch 303/10000\n",
            "Step 0: Train Loss: 2.2552696464117616e-05, Train Acc: 1.0\n",
            "Epoch 304/10000\n",
            "Step 0: Train Loss: 2.1451020074891858e-05, Train Acc: 1.0\n",
            "Epoch 305/10000\n",
            "Step 0: Train Loss: 1.9885868823621422e-05, Train Acc: 1.0\n",
            "Epoch 306/10000\n",
            "Step 0: Train Loss: 2.014890378632117e-05, Train Acc: 1.0\n",
            "Epoch 307/10000\n",
            "Step 0: Train Loss: 2.2750593416276388e-05, Train Acc: 1.0\n",
            "Epoch 308/10000\n",
            "Step 0: Train Loss: 2.3817909095669165e-05, Train Acc: 1.0\n",
            "Epoch 309/10000\n",
            "Step 0: Train Loss: 2.1466130419867113e-05, Train Acc: 1.0\n",
            "Epoch 310/10000\n",
            "Step 0: Train Loss: 1.901677569549065e-05, Train Acc: 1.0\n",
            "Epoch 311/10000\n",
            "Step 0: Train Loss: 2.295146077813115e-05, Train Acc: 1.0\n",
            "Epoch 312/10000\n",
            "Step 0: Train Loss: 1.7657455828157254e-05, Train Acc: 1.0\n",
            "Epoch 313/10000\n",
            "Step 0: Train Loss: 2.217786095570773e-05, Train Acc: 1.0\n",
            "Epoch 314/10000\n",
            "Step 0: Train Loss: 2.089252302539535e-05, Train Acc: 1.0\n",
            "Epoch 315/10000\n",
            "Step 0: Train Loss: 1.9683793652802706e-05, Train Acc: 1.0\n",
            "Epoch 316/10000\n",
            "Step 0: Train Loss: 2.1480385839822702e-05, Train Acc: 1.0\n",
            "Epoch 317/10000\n",
            "Step 0: Train Loss: 2.0561235942295752e-05, Train Acc: 1.0\n",
            "Epoch 318/10000\n",
            "Step 0: Train Loss: 1.9745286408578977e-05, Train Acc: 1.0\n",
            "Epoch 319/10000\n",
            "Step 0: Train Loss: 2.1275114704621956e-05, Train Acc: 1.0\n",
            "Epoch 320/10000\n",
            "Step 0: Train Loss: 1.7829037460614927e-05, Train Acc: 1.0\n",
            "Epoch 321/10000\n",
            "Step 0: Train Loss: 1.891871215775609e-05, Train Acc: 1.0\n",
            "Epoch 322/10000\n",
            "Step 0: Train Loss: 1.780626189429313e-05, Train Acc: 1.0\n",
            "Epoch 323/10000\n",
            "Step 0: Train Loss: 1.9991972294519655e-05, Train Acc: 1.0\n",
            "Epoch 324/10000\n",
            "Step 0: Train Loss: 1.954818617377896e-05, Train Acc: 1.0\n",
            "Epoch 325/10000\n",
            "Step 0: Train Loss: 1.7067277440219186e-05, Train Acc: 1.0\n",
            "Epoch 326/10000\n",
            "Step 0: Train Loss: 2.0077728549949825e-05, Train Acc: 1.0\n",
            "Epoch 327/10000\n",
            "Step 0: Train Loss: 1.7821614164859056e-05, Train Acc: 1.0\n",
            "Epoch 328/10000\n",
            "Step 0: Train Loss: 1.891722422442399e-05, Train Acc: 1.0\n",
            "Epoch 329/10000\n",
            "Step 0: Train Loss: 1.9708053514477797e-05, Train Acc: 1.0\n",
            "Epoch 330/10000\n",
            "Step 0: Train Loss: 1.8080956579069607e-05, Train Acc: 1.0\n",
            "Epoch 331/10000\n",
            "Step 0: Train Loss: 1.7634321920922957e-05, Train Acc: 1.0\n",
            "Epoch 332/10000\n",
            "Step 0: Train Loss: 1.6829546439112164e-05, Train Acc: 1.0\n",
            "Epoch 333/10000\n",
            "Step 0: Train Loss: 1.812260234146379e-05, Train Acc: 1.0\n",
            "Epoch 334/10000\n",
            "Step 0: Train Loss: 1.7522837879369035e-05, Train Acc: 1.0\n",
            "Epoch 335/10000\n",
            "Step 0: Train Loss: 1.758334292389918e-05, Train Acc: 1.0\n",
            "Epoch 336/10000\n",
            "Step 0: Train Loss: 1.6860294635989703e-05, Train Acc: 1.0\n",
            "Epoch 337/10000\n",
            "Step 0: Train Loss: 1.808961314964108e-05, Train Acc: 1.0\n",
            "Epoch 338/10000\n",
            "Step 0: Train Loss: 1.631710256333463e-05, Train Acc: 1.0\n",
            "Epoch 339/10000\n",
            "Step 0: Train Loss: 1.8064602045342326e-05, Train Acc: 1.0\n",
            "Epoch 340/10000\n",
            "Step 0: Train Loss: 1.816808980947826e-05, Train Acc: 1.0\n",
            "Epoch 341/10000\n",
            "Step 0: Train Loss: 1.765792148944456e-05, Train Acc: 1.0\n",
            "Epoch 342/10000\n",
            "Step 0: Train Loss: 1.71141200553393e-05, Train Acc: 1.0\n",
            "Epoch 343/10000\n",
            "Step 0: Train Loss: 1.714243262540549e-05, Train Acc: 1.0\n",
            "Epoch 344/10000\n",
            "Step 0: Train Loss: 1.634702130104415e-05, Train Acc: 1.0\n",
            "Epoch 345/10000\n",
            "Step 0: Train Loss: 1.579043237143196e-05, Train Acc: 1.0\n",
            "Epoch 346/10000\n",
            "Step 0: Train Loss: 1.3786299859930295e-05, Train Acc: 1.0\n",
            "Epoch 347/10000\n",
            "Step 0: Train Loss: 1.5802339476067573e-05, Train Acc: 1.0\n",
            "Epoch 348/10000\n",
            "Step 0: Train Loss: 1.7045449567376636e-05, Train Acc: 1.0\n",
            "Epoch 349/10000\n",
            "Step 0: Train Loss: 1.43115012178896e-05, Train Acc: 1.0\n",
            "Epoch 350/10000\n",
            "Step 0: Train Loss: 1.5091675777512137e-05, Train Acc: 1.0\n",
            "Epoch 351/10000\n",
            "Step 0: Train Loss: 1.5564906789222732e-05, Train Acc: 1.0\n",
            "Epoch 352/10000\n",
            "Step 0: Train Loss: 1.5270990843418986e-05, Train Acc: 1.0\n",
            "Epoch 353/10000\n",
            "Step 0: Train Loss: 1.5772864571772516e-05, Train Acc: 1.0\n",
            "Epoch 354/10000\n",
            "Step 0: Train Loss: 1.578803130541928e-05, Train Acc: 1.0\n",
            "Epoch 355/10000\n",
            "Step 0: Train Loss: 1.6217312804656103e-05, Train Acc: 1.0\n",
            "Epoch 356/10000\n",
            "Step 0: Train Loss: 1.54376684804447e-05, Train Acc: 1.0\n",
            "Epoch 357/10000\n",
            "Step 0: Train Loss: 1.4116701095190365e-05, Train Acc: 1.0\n",
            "Epoch 358/10000\n",
            "Step 0: Train Loss: 1.3436841982183978e-05, Train Acc: 1.0\n",
            "Epoch 359/10000\n",
            "Step 0: Train Loss: 1.6323465388268232e-05, Train Acc: 1.0\n",
            "Epoch 360/10000\n",
            "Step 0: Train Loss: 1.3490037417795975e-05, Train Acc: 1.0\n",
            "Epoch 361/10000\n",
            "Step 0: Train Loss: 1.4037643268238753e-05, Train Acc: 1.0\n",
            "Epoch 362/10000\n",
            "Step 0: Train Loss: 1.3721455616177991e-05, Train Acc: 1.0\n",
            "Epoch 363/10000\n",
            "Step 0: Train Loss: 1.3561546438722871e-05, Train Acc: 1.0\n",
            "Epoch 364/10000\n",
            "Step 0: Train Loss: 1.3362053323362488e-05, Train Acc: 1.0\n",
            "Epoch 365/10000\n",
            "Step 0: Train Loss: 1.3939746168034617e-05, Train Acc: 1.0\n",
            "Epoch 366/10000\n",
            "Step 0: Train Loss: 1.426494236511644e-05, Train Acc: 1.0\n",
            "Epoch 367/10000\n",
            "Step 0: Train Loss: 1.4548383660439868e-05, Train Acc: 1.0\n",
            "Epoch 368/10000\n",
            "Step 0: Train Loss: 1.4196794836607296e-05, Train Acc: 1.0\n",
            "Epoch 369/10000\n",
            "Step 0: Train Loss: 1.3992047570354771e-05, Train Acc: 1.0\n",
            "Epoch 370/10000\n",
            "Step 0: Train Loss: 1.4023044968780596e-05, Train Acc: 1.0\n",
            "Epoch 371/10000\n",
            "Step 0: Train Loss: 1.2269661056052428e-05, Train Acc: 1.0\n",
            "Epoch 372/10000\n",
            "Step 0: Train Loss: 1.4331917554954998e-05, Train Acc: 1.0\n",
            "Epoch 373/10000\n",
            "Step 0: Train Loss: 1.521453850727994e-05, Train Acc: 1.0\n",
            "Epoch 374/10000\n",
            "Step 0: Train Loss: 1.4446344721363857e-05, Train Acc: 1.0\n",
            "Epoch 375/10000\n",
            "Step 0: Train Loss: 1.3593810763268266e-05, Train Acc: 1.0\n",
            "Epoch 376/10000\n",
            "Step 0: Train Loss: 1.274840542464517e-05, Train Acc: 1.0\n",
            "Epoch 377/10000\n",
            "Step 0: Train Loss: 1.3545012734539341e-05, Train Acc: 1.0\n",
            "Epoch 378/10000\n",
            "Step 0: Train Loss: 1.2588293429871555e-05, Train Acc: 1.0\n",
            "Epoch 379/10000\n",
            "Step 0: Train Loss: 1.2786793377017602e-05, Train Acc: 1.0\n",
            "Epoch 380/10000\n",
            "Step 0: Train Loss: 1.1480326975288335e-05, Train Acc: 1.0\n",
            "Epoch 381/10000\n",
            "Step 0: Train Loss: 1.303542831010418e-05, Train Acc: 1.0\n",
            "Epoch 382/10000\n",
            "Step 0: Train Loss: 1.3043069884588476e-05, Train Acc: 1.0\n",
            "Epoch 383/10000\n",
            "Step 0: Train Loss: 1.2082490684406366e-05, Train Acc: 1.0\n",
            "Epoch 384/10000\n",
            "Step 0: Train Loss: 1.3147507161193062e-05, Train Acc: 1.0\n",
            "Epoch 385/10000\n",
            "Step 0: Train Loss: 1.2245553989487235e-05, Train Acc: 1.0\n",
            "Epoch 386/10000\n",
            "Step 0: Train Loss: 1.1721756891347468e-05, Train Acc: 1.0\n",
            "Epoch 387/10000\n",
            "Step 0: Train Loss: 1.2842991964134853e-05, Train Acc: 1.0\n",
            "Epoch 388/10000\n",
            "Step 0: Train Loss: 1.013057044474408e-05, Train Acc: 1.0\n",
            "Epoch 389/10000\n",
            "Step 0: Train Loss: 1.1965968951699324e-05, Train Acc: 1.0\n",
            "Epoch 390/10000\n",
            "Step 0: Train Loss: 1.1907811312994454e-05, Train Acc: 1.0\n",
            "Epoch 391/10000\n",
            "Step 0: Train Loss: 1.191446790471673e-05, Train Acc: 1.0\n",
            "Epoch 392/10000\n",
            "Step 0: Train Loss: 1.1491607438074425e-05, Train Acc: 1.0\n",
            "Epoch 393/10000\n",
            "Step 0: Train Loss: 1.1850802366097923e-05, Train Acc: 1.0\n",
            "Epoch 394/10000\n",
            "Step 0: Train Loss: 1.2405292181938421e-05, Train Acc: 1.0\n",
            "Epoch 395/10000\n",
            "Step 0: Train Loss: 1.1596018339332659e-05, Train Acc: 1.0\n",
            "Epoch 396/10000\n",
            "Step 0: Train Loss: 1.0816274880198762e-05, Train Acc: 1.0\n",
            "Epoch 397/10000\n",
            "Step 0: Train Loss: 1.1379770512576215e-05, Train Acc: 1.0\n",
            "Epoch 398/10000\n",
            "Step 0: Train Loss: 1.0715785720094573e-05, Train Acc: 1.0\n",
            "Epoch 399/10000\n",
            "Step 0: Train Loss: 1.1284827451163437e-05, Train Acc: 1.0\n",
            "Epoch 400/10000\n",
            "Step 0: Train Loss: 1.1165157047798857e-05, Train Acc: 1.0\n",
            "Epoch 401/10000\n",
            "Step 0: Train Loss: 1.0706080502131954e-05, Train Acc: 1.0\n",
            "Epoch 402/10000\n",
            "Step 0: Train Loss: 1.1380851901776623e-05, Train Acc: 1.0\n",
            "Epoch 403/10000\n",
            "Step 0: Train Loss: 1.1513123354234267e-05, Train Acc: 1.0\n",
            "Epoch 404/10000\n",
            "Step 0: Train Loss: 1.0101768566528335e-05, Train Acc: 1.0\n",
            "Epoch 405/10000\n",
            "Step 0: Train Loss: 1.1109102160844486e-05, Train Acc: 1.0\n",
            "Epoch 406/10000\n",
            "Step 0: Train Loss: 1.1019758858310524e-05, Train Acc: 1.0\n",
            "Epoch 407/10000\n",
            "Step 0: Train Loss: 1.0500684766157065e-05, Train Acc: 1.0\n",
            "Epoch 408/10000\n",
            "Step 0: Train Loss: 1.046098714141408e-05, Train Acc: 1.0\n",
            "Epoch 409/10000\n",
            "Step 0: Train Loss: 1.0473728252691217e-05, Train Acc: 1.0\n",
            "Epoch 410/10000\n",
            "Step 0: Train Loss: 1.0840942195500247e-05, Train Acc: 1.0\n",
            "Epoch 411/10000\n",
            "Step 0: Train Loss: 9.829692316998262e-06, Train Acc: 1.0\n",
            "Epoch 412/10000\n",
            "Step 0: Train Loss: 1.1022871149179991e-05, Train Acc: 1.0\n",
            "Epoch 413/10000\n",
            "Step 0: Train Loss: 1.0512590051803272e-05, Train Acc: 1.0\n",
            "Epoch 414/10000\n",
            "Step 0: Train Loss: 1.1005795386154205e-05, Train Acc: 1.0\n",
            "Epoch 415/10000\n",
            "Step 0: Train Loss: 1.0373502846050542e-05, Train Acc: 1.0\n",
            "Epoch 416/10000\n",
            "Step 0: Train Loss: 1.0110492439707741e-05, Train Acc: 1.0\n",
            "Epoch 417/10000\n",
            "Step 0: Train Loss: 1.0112726158695295e-05, Train Acc: 1.0\n",
            "Epoch 418/10000\n",
            "Step 0: Train Loss: 9.622476682125125e-06, Train Acc: 1.0\n",
            "Epoch 419/10000\n",
            "Step 0: Train Loss: 1.0315842700947542e-05, Train Acc: 1.0\n",
            "Epoch 420/10000\n",
            "Step 0: Train Loss: 1.0248723810946103e-05, Train Acc: 1.0\n",
            "Epoch 421/10000\n",
            "Step 0: Train Loss: 1.1220056876481976e-05, Train Acc: 1.0\n",
            "Epoch 422/10000\n",
            "Step 0: Train Loss: 9.969745406124275e-06, Train Acc: 1.0\n",
            "Epoch 423/10000\n",
            "Step 0: Train Loss: 9.669584869698156e-06, Train Acc: 1.0\n",
            "Epoch 424/10000\n",
            "Step 0: Train Loss: 9.515610145172104e-06, Train Acc: 1.0\n",
            "Epoch 425/10000\n",
            "Step 0: Train Loss: 9.675828550825827e-06, Train Acc: 1.0\n",
            "Epoch 426/10000\n",
            "Step 0: Train Loss: 9.920953743858263e-06, Train Acc: 1.0\n",
            "Epoch 427/10000\n",
            "Step 0: Train Loss: 1.0483387086424045e-05, Train Acc: 1.0\n",
            "Epoch 428/10000\n",
            "Step 0: Train Loss: 9.972764928534161e-06, Train Acc: 1.0\n",
            "Epoch 429/10000\n",
            "Step 0: Train Loss: 9.33859519136604e-06, Train Acc: 1.0\n",
            "Epoch 430/10000\n",
            "Step 0: Train Loss: 9.041365956363734e-06, Train Acc: 1.0\n",
            "Epoch 431/10000\n",
            "Step 0: Train Loss: 1.0497659786778968e-05, Train Acc: 1.0\n",
            "Epoch 432/10000\n",
            "Step 0: Train Loss: 8.39144649944501e-06, Train Acc: 1.0\n",
            "Epoch 433/10000\n",
            "Step 0: Train Loss: 9.470498298469465e-06, Train Acc: 1.0\n",
            "Epoch 434/10000\n",
            "Step 0: Train Loss: 9.84621965471888e-06, Train Acc: 1.0\n",
            "Epoch 435/10000\n",
            "Step 0: Train Loss: 9.193022378894966e-06, Train Acc: 1.0\n",
            "Epoch 436/10000\n",
            "Step 0: Train Loss: 9.25178756006062e-06, Train Acc: 1.0\n",
            "Epoch 437/10000\n",
            "Step 0: Train Loss: 9.113980922847986e-06, Train Acc: 1.0\n",
            "Epoch 438/10000\n",
            "Step 0: Train Loss: 9.454656719753984e-06, Train Acc: 1.0\n",
            "Epoch 439/10000\n",
            "Step 0: Train Loss: 9.165335541183595e-06, Train Acc: 1.0\n",
            "Epoch 440/10000\n",
            "Step 0: Train Loss: 8.838781468512025e-06, Train Acc: 1.0\n",
            "Epoch 441/10000\n",
            "Step 0: Train Loss: 9.00006853044033e-06, Train Acc: 1.0\n",
            "Epoch 442/10000\n",
            "Step 0: Train Loss: 8.97756308404496e-06, Train Acc: 1.0\n",
            "Epoch 443/10000\n",
            "Step 0: Train Loss: 9.051410415850114e-06, Train Acc: 1.0\n",
            "Epoch 444/10000\n",
            "Step 0: Train Loss: 8.7723874457879e-06, Train Acc: 1.0\n",
            "Epoch 445/10000\n",
            "Step 0: Train Loss: 8.638541658001486e-06, Train Acc: 1.0\n",
            "Epoch 446/10000\n",
            "Step 0: Train Loss: 8.798406270216219e-06, Train Acc: 1.0\n",
            "Epoch 447/10000\n",
            "Step 0: Train Loss: 7.990379344846588e-06, Train Acc: 1.0\n",
            "Epoch 448/10000\n",
            "Step 0: Train Loss: 7.837465091142803e-06, Train Acc: 1.0\n",
            "Epoch 449/10000\n",
            "Step 0: Train Loss: 8.25215920485789e-06, Train Acc: 1.0\n",
            "Epoch 450/10000\n",
            "Step 0: Train Loss: 7.95100368122803e-06, Train Acc: 1.0\n",
            "Epoch 451/10000\n",
            "Step 0: Train Loss: 7.940934665384702e-06, Train Acc: 1.0\n",
            "Epoch 452/10000\n",
            "Step 0: Train Loss: 8.114209776977077e-06, Train Acc: 1.0\n",
            "Epoch 453/10000\n",
            "Step 0: Train Loss: 8.39094900584314e-06, Train Acc: 1.0\n",
            "Epoch 454/10000\n",
            "Step 0: Train Loss: 8.43927227833774e-06, Train Acc: 1.0\n",
            "Epoch 455/10000\n",
            "Step 0: Train Loss: 7.908241059340071e-06, Train Acc: 1.0\n",
            "Epoch 456/10000\n",
            "Step 0: Train Loss: 8.507403435942251e-06, Train Acc: 1.0\n",
            "Epoch 457/10000\n",
            "Step 0: Train Loss: 8.752670510148164e-06, Train Acc: 1.0\n",
            "Epoch 458/10000\n",
            "Step 0: Train Loss: 8.165465260390192e-06, Train Acc: 1.0\n",
            "Epoch 459/10000\n",
            "Step 0: Train Loss: 7.931198524602223e-06, Train Acc: 1.0\n",
            "Epoch 460/10000\n",
            "Step 0: Train Loss: 7.841498700145166e-06, Train Acc: 1.0\n",
            "Epoch 461/10000\n",
            "Step 0: Train Loss: 8.086054549494293e-06, Train Acc: 1.0\n",
            "Epoch 462/10000\n",
            "Step 0: Train Loss: 8.098557373159565e-06, Train Acc: 1.0\n",
            "Epoch 463/10000\n",
            "Step 0: Train Loss: 7.500723768316675e-06, Train Acc: 1.0\n",
            "Epoch 464/10000\n",
            "Step 0: Train Loss: 7.823428859410342e-06, Train Acc: 1.0\n",
            "Epoch 465/10000\n",
            "Step 0: Train Loss: 7.497115348087391e-06, Train Acc: 1.0\n",
            "Epoch 466/10000\n",
            "Step 0: Train Loss: 7.633353561686818e-06, Train Acc: 1.0\n",
            "Epoch 467/10000\n",
            "Step 0: Train Loss: 7.5755328907689545e-06, Train Acc: 1.0\n",
            "Epoch 468/10000\n",
            "Step 0: Train Loss: 7.7504091677838e-06, Train Acc: 1.0\n",
            "Epoch 469/10000\n",
            "Step 0: Train Loss: 7.296060630324064e-06, Train Acc: 1.0\n",
            "Epoch 470/10000\n",
            "Step 0: Train Loss: 7.560211997770239e-06, Train Acc: 1.0\n",
            "Epoch 471/10000\n",
            "Step 0: Train Loss: 7.65885033615632e-06, Train Acc: 1.0\n",
            "Epoch 472/10000\n",
            "Step 0: Train Loss: 7.52305686546606e-06, Train Acc: 1.0\n",
            "Epoch 473/10000\n",
            "Step 0: Train Loss: 7.693067345826421e-06, Train Acc: 1.0\n",
            "Epoch 474/10000\n",
            "Step 0: Train Loss: 8.38388496049447e-06, Train Acc: 1.0\n",
            "Epoch 475/10000\n",
            "Step 0: Train Loss: 6.703765848214971e-06, Train Acc: 1.0\n",
            "Epoch 476/10000\n",
            "Step 0: Train Loss: 7.053538411128102e-06, Train Acc: 1.0\n",
            "Epoch 477/10000\n",
            "Step 0: Train Loss: 7.036006536509376e-06, Train Acc: 1.0\n",
            "Epoch 478/10000\n",
            "Step 0: Train Loss: 7.0967653300613165e-06, Train Acc: 1.0\n",
            "Epoch 479/10000\n",
            "Step 0: Train Loss: 6.953295269340742e-06, Train Acc: 1.0\n",
            "Epoch 480/10000\n",
            "Step 0: Train Loss: 6.843101346021285e-06, Train Acc: 1.0\n",
            "Epoch 481/10000\n",
            "Step 0: Train Loss: 6.996753199928207e-06, Train Acc: 1.0\n",
            "Epoch 482/10000\n",
            "Step 0: Train Loss: 6.815387223468861e-06, Train Acc: 1.0\n",
            "Epoch 483/10000\n",
            "Step 0: Train Loss: 6.774384473828832e-06, Train Acc: 1.0\n",
            "Epoch 484/10000\n",
            "Step 0: Train Loss: 6.844492418167647e-06, Train Acc: 1.0\n",
            "Epoch 485/10000\n",
            "Step 0: Train Loss: 6.768907496734755e-06, Train Acc: 1.0\n",
            "Epoch 486/10000\n",
            "Step 0: Train Loss: 6.864307124487823e-06, Train Acc: 1.0\n",
            "Epoch 487/10000\n",
            "Step 0: Train Loss: 6.9128491304581985e-06, Train Acc: 1.0\n",
            "Epoch 488/10000\n",
            "Step 0: Train Loss: 6.417349140974693e-06, Train Acc: 1.0\n",
            "Epoch 489/10000\n",
            "Step 0: Train Loss: 6.742012374161277e-06, Train Acc: 1.0\n",
            "Epoch 490/10000\n",
            "Step 0: Train Loss: 6.306298928393517e-06, Train Acc: 1.0\n",
            "Epoch 491/10000\n",
            "Step 0: Train Loss: 5.927172878728015e-06, Train Acc: 1.0\n",
            "Epoch 492/10000\n",
            "Step 0: Train Loss: 7.224773071357049e-06, Train Acc: 1.0\n",
            "Epoch 493/10000\n",
            "Step 0: Train Loss: 6.949983344384236e-06, Train Acc: 1.0\n",
            "Epoch 494/10000\n",
            "Step 0: Train Loss: 5.953904292255174e-06, Train Acc: 1.0\n",
            "Epoch 495/10000\n",
            "Step 0: Train Loss: 7.117674613255076e-06, Train Acc: 1.0\n",
            "Epoch 496/10000\n",
            "Step 0: Train Loss: 6.859482709842268e-06, Train Acc: 1.0\n",
            "Epoch 497/10000\n",
            "Step 0: Train Loss: 6.517737347166985e-06, Train Acc: 1.0\n",
            "Epoch 498/10000\n",
            "Step 0: Train Loss: 6.744839993189089e-06, Train Acc: 1.0\n",
            "Epoch 499/10000\n",
            "Step 0: Train Loss: 6.0276133808656596e-06, Train Acc: 1.0\n",
            "Epoch 500/10000\n",
            "Step 0: Train Loss: 6.233960448298603e-06, Train Acc: 1.0\n",
            "Epoch 501/10000\n",
            "Step 0: Train Loss: 6.0061493059038185e-06, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 500 1024 1.2379948796420728\n",
            "Epoch index and hidden dimension and ratio: 500 20 1.5186068672271147\n",
            "Epoch index and hidden dimension and ratio: 500 20 1.8753862424189331\n",
            "Epoch index and hidden dimension and ratio: 500 20 2.525831570044353\n",
            "MI(X;T): [12.937086732858216, 10.133709340911507, 7.918461652280165, 5.106376837275666], MI(Y;T): [3.3096514348011885, 3.257755015051199, 3.1685748399539846, 2.7440080507063036]\n",
            "Epoch index and hidden dimension and ratio: 500 1024 1.2380097110439097\n",
            "Epoch index and hidden dimension and ratio: 500 20 1.518656769039874\n",
            "Epoch index and hidden dimension and ratio: 500 20 1.8754815889738248\n",
            "Epoch index and hidden dimension and ratio: 500 20 2.5259801296943842\n",
            "MI(X;T): [12.937508851111758, 10.133759220886175, 7.9182324228174155, 5.106273243727102], MI(Y;T): [3.3096514348011885, 3.2578550150511987, 3.1687094417708845, 2.743765449736083]\n",
            "Epoch index and hidden dimension and ratio: 500 1024 1.238024395600184\n",
            "Epoch index and hidden dimension and ratio: 500 20 1.5186970159366864\n",
            "Epoch index and hidden dimension and ratio: 500 20 1.875572872825978\n",
            "Epoch index and hidden dimension and ratio: 500 20 2.526116844056705\n",
            "MI(X;T): [12.937708851111758, 10.133382687103158, 7.919377002698392, 5.106144384107402], MI(Y;T): [3.3096514348011885, 3.2579550547026686, 3.168733900544196, 2.743787707069557]\n",
            "Epoch index and hidden dimension and ratio: 500 1024 1.2380384927742072\n",
            "Epoch index and hidden dimension and ratio: 500 20 1.5187316217590128\n",
            "Epoch index and hidden dimension and ratio: 500 20 1.8756611096510774\n",
            "Epoch index and hidden dimension and ratio: 500 20 2.5262083160006887\n",
            "MI(X;T): [12.937708851111758, 10.133441268674623, 7.9185472410000095, 5.1063228419605], MI(Y;T): [3.3096514348011885, 3.2580355367263905, 3.1685597419039917, 2.7434380157135667]\n",
            "Epoch index and hidden dimension and ratio: 500 1024 1.2380542052494206\n",
            "Epoch index and hidden dimension and ratio: 500 20 1.5187642749017096\n",
            "Epoch index and hidden dimension and ratio: 500 20 1.8757413480301606\n",
            "Epoch index and hidden dimension and ratio: 500 20 2.526315746179504\n",
            "MI(X;T): [12.937708851111758, 10.13395503306025, 7.918175001306222, 5.106222896126276], MI(Y;T): [3.3096514348011885, 3.257955054702668, 3.1685777307918457, 2.7434810210849663]\n",
            "Epoch index and hidden dimension and ratio: 500 1024 1.2380694771879457\n",
            "Epoch index and hidden dimension and ratio: 500 20 1.5188024606366908\n",
            "Epoch index and hidden dimension and ratio: 500 20 1.8758115566118583\n",
            "Epoch index and hidden dimension and ratio: 500 20 2.5264327183956414\n",
            "MI(X;T): [12.937935317362406, 10.134038602952936, 7.918931287742889, 5.10657243463344], MI(Y;T): [3.3095514348011887, 3.257879526300981, 3.1684761054175654, 2.743480020117027]\n",
            "Epoch 502/10000\n",
            "Step 0: Train Loss: 6.036539161868859e-06, Train Acc: 1.0\n",
            "Epoch 503/10000\n",
            "Step 0: Train Loss: 5.980019977869233e-06, Train Acc: 1.0\n",
            "Epoch 504/10000\n",
            "Step 0: Train Loss: 6.4657033362891525e-06, Train Acc: 1.0\n",
            "Epoch 505/10000\n",
            "Step 0: Train Loss: 6.421171747206245e-06, Train Acc: 1.0\n",
            "Epoch 506/10000\n",
            "Step 0: Train Loss: 6.602406301681185e-06, Train Acc: 1.0\n",
            "Epoch 507/10000\n",
            "Step 0: Train Loss: 5.714380222343607e-06, Train Acc: 1.0\n",
            "Epoch 508/10000\n",
            "Step 0: Train Loss: 5.887933639314724e-06, Train Acc: 1.0\n",
            "Epoch 509/10000\n",
            "Step 0: Train Loss: 6.548531473526964e-06, Train Acc: 1.0\n",
            "Epoch 510/10000\n",
            "Step 0: Train Loss: 5.704347131540999e-06, Train Acc: 1.0\n",
            "Epoch 511/10000\n",
            "Step 0: Train Loss: 5.556617907132022e-06, Train Acc: 1.0\n",
            "Epoch 512/10000\n",
            "Step 0: Train Loss: 5.77640230403631e-06, Train Acc: 1.0\n",
            "Epoch 513/10000\n",
            "Step 0: Train Loss: 5.83453447688953e-06, Train Acc: 1.0\n",
            "Epoch 514/10000\n",
            "Step 0: Train Loss: 5.842184236826142e-06, Train Acc: 1.0\n",
            "Epoch 515/10000\n",
            "Step 0: Train Loss: 5.691273599950364e-06, Train Acc: 1.0\n",
            "Epoch 516/10000\n",
            "Step 0: Train Loss: 5.8178911785944365e-06, Train Acc: 1.0\n",
            "Epoch 517/10000\n",
            "Step 0: Train Loss: 5.646817953675054e-06, Train Acc: 1.0\n",
            "Epoch 518/10000\n",
            "Step 0: Train Loss: 6.045460850145901e-06, Train Acc: 1.0\n",
            "Epoch 519/10000\n",
            "Step 0: Train Loss: 5.593734385911375e-06, Train Acc: 1.0\n",
            "Epoch 520/10000\n",
            "Step 0: Train Loss: 5.8038363022205885e-06, Train Acc: 1.0\n",
            "Epoch 521/10000\n",
            "Step 0: Train Loss: 5.3265707720129285e-06, Train Acc: 1.0\n",
            "Epoch 522/10000\n",
            "Step 0: Train Loss: 5.994920229568379e-06, Train Acc: 1.0\n",
            "Epoch 523/10000\n",
            "Step 0: Train Loss: 5.820181741000852e-06, Train Acc: 1.0\n",
            "Epoch 524/10000\n",
            "Step 0: Train Loss: 5.468521976581542e-06, Train Acc: 1.0\n",
            "Epoch 525/10000\n",
            "Step 0: Train Loss: 5.389871603256324e-06, Train Acc: 1.0\n",
            "Epoch 526/10000\n",
            "Step 0: Train Loss: 5.936499746894697e-06, Train Acc: 1.0\n",
            "Epoch 527/10000\n",
            "Step 0: Train Loss: 5.385269105318002e-06, Train Acc: 1.0\n",
            "Epoch 528/10000\n",
            "Step 0: Train Loss: 5.135015271662269e-06, Train Acc: 1.0\n",
            "Epoch 529/10000\n",
            "Step 0: Train Loss: 5.0622679736989085e-06, Train Acc: 1.0\n",
            "Epoch 530/10000\n",
            "Step 0: Train Loss: 5.271563168207649e-06, Train Acc: 1.0\n",
            "Epoch 531/10000\n",
            "Step 0: Train Loss: 5.307160336087691e-06, Train Acc: 1.0\n",
            "Epoch 532/10000\n",
            "Step 0: Train Loss: 5.706946467398666e-06, Train Acc: 1.0\n",
            "Epoch 533/10000\n",
            "Step 0: Train Loss: 5.076541128801182e-06, Train Acc: 1.0\n",
            "Epoch 534/10000\n",
            "Step 0: Train Loss: 5.675968623108929e-06, Train Acc: 1.0\n",
            "Epoch 535/10000\n",
            "Step 0: Train Loss: 5.327809503796743e-06, Train Acc: 1.0\n",
            "Epoch 536/10000\n",
            "Step 0: Train Loss: 5.904356385144638e-06, Train Acc: 1.0\n",
            "Epoch 537/10000\n",
            "Step 0: Train Loss: 5.19087552675046e-06, Train Acc: 1.0\n",
            "Epoch 538/10000\n",
            "Step 0: Train Loss: 5.11144389747642e-06, Train Acc: 1.0\n",
            "Epoch 539/10000\n",
            "Step 0: Train Loss: 5.361679086490767e-06, Train Acc: 1.0\n",
            "Epoch 540/10000\n",
            "Step 0: Train Loss: 4.942663053952856e-06, Train Acc: 1.0\n",
            "Epoch 541/10000\n",
            "Step 0: Train Loss: 5.333480658009648e-06, Train Acc: 1.0\n",
            "Epoch 542/10000\n",
            "Step 0: Train Loss: 4.970911959389923e-06, Train Acc: 1.0\n",
            "Epoch 543/10000\n",
            "Step 0: Train Loss: 5.0517060117272194e-06, Train Acc: 1.0\n",
            "Epoch 544/10000\n",
            "Step 0: Train Loss: 5.26287340107956e-06, Train Acc: 1.0\n",
            "Epoch 545/10000\n",
            "Step 0: Train Loss: 4.911932592222001e-06, Train Acc: 1.0\n",
            "Epoch 546/10000\n",
            "Step 0: Train Loss: 5.476026672113221e-06, Train Acc: 1.0\n",
            "Epoch 547/10000\n",
            "Step 0: Train Loss: 4.615937996277353e-06, Train Acc: 1.0\n",
            "Epoch 548/10000\n",
            "Step 0: Train Loss: 4.633821845345665e-06, Train Acc: 1.0\n",
            "Epoch 549/10000\n",
            "Step 0: Train Loss: 5.2758837227884214e-06, Train Acc: 1.0\n",
            "Epoch 550/10000\n",
            "Step 0: Train Loss: 5.01437580169295e-06, Train Acc: 1.0\n",
            "Epoch 551/10000\n",
            "Step 0: Train Loss: 5.073863576399162e-06, Train Acc: 1.0\n",
            "Epoch 552/10000\n",
            "Step 0: Train Loss: 5.037201390223345e-06, Train Acc: 1.0\n",
            "Epoch 553/10000\n",
            "Step 0: Train Loss: 4.470409294299316e-06, Train Acc: 1.0\n",
            "Epoch 554/10000\n",
            "Step 0: Train Loss: 4.71700059279101e-06, Train Acc: 1.0\n",
            "Epoch 555/10000\n",
            "Step 0: Train Loss: 4.695296865975251e-06, Train Acc: 1.0\n",
            "Epoch 556/10000\n",
            "Step 0: Train Loss: 4.3658233153109904e-06, Train Acc: 1.0\n",
            "Epoch 557/10000\n",
            "Step 0: Train Loss: 4.576775609166361e-06, Train Acc: 1.0\n",
            "Epoch 558/10000\n",
            "Step 0: Train Loss: 4.7032312977535184e-06, Train Acc: 1.0\n",
            "Epoch 559/10000\n",
            "Step 0: Train Loss: 4.965711013937835e-06, Train Acc: 1.0\n",
            "Epoch 560/10000\n",
            "Step 0: Train Loss: 4.4380508370522875e-06, Train Acc: 1.0\n",
            "Epoch 561/10000\n",
            "Step 0: Train Loss: 4.229148089507362e-06, Train Acc: 1.0\n",
            "Epoch 562/10000\n",
            "Step 0: Train Loss: 4.295198777981568e-06, Train Acc: 1.0\n",
            "Epoch 563/10000\n",
            "Step 0: Train Loss: 4.179665666015353e-06, Train Acc: 1.0\n",
            "Epoch 564/10000\n",
            "Step 0: Train Loss: 4.596532562572975e-06, Train Acc: 1.0\n",
            "Epoch 565/10000\n",
            "Step 0: Train Loss: 4.749957952299155e-06, Train Acc: 1.0\n",
            "Epoch 566/10000\n",
            "Step 0: Train Loss: 4.416046976984944e-06, Train Acc: 1.0\n",
            "Epoch 567/10000\n",
            "Step 0: Train Loss: 4.51824780611787e-06, Train Acc: 1.0\n",
            "Epoch 568/10000\n",
            "Step 0: Train Loss: 4.904249635728775e-06, Train Acc: 1.0\n",
            "Epoch 569/10000\n",
            "Step 0: Train Loss: 4.65986886410974e-06, Train Acc: 1.0\n",
            "Epoch 570/10000\n",
            "Step 0: Train Loss: 4.7843586798990145e-06, Train Acc: 1.0\n",
            "Epoch 571/10000\n",
            "Step 0: Train Loss: 4.425359293236397e-06, Train Acc: 1.0\n",
            "Epoch 572/10000\n",
            "Step 0: Train Loss: 4.192780579614919e-06, Train Acc: 1.0\n",
            "Epoch 573/10000\n",
            "Step 0: Train Loss: 3.941273462260142e-06, Train Acc: 1.0\n",
            "Epoch 574/10000\n",
            "Step 0: Train Loss: 4.576720584736904e-06, Train Acc: 1.0\n",
            "Epoch 575/10000\n",
            "Step 0: Train Loss: 4.4907046685693786e-06, Train Acc: 1.0\n",
            "Epoch 576/10000\n",
            "Step 0: Train Loss: 4.336533038440393e-06, Train Acc: 1.0\n",
            "Epoch 577/10000\n",
            "Step 0: Train Loss: 4.298776275390992e-06, Train Acc: 1.0\n",
            "Epoch 578/10000\n",
            "Step 0: Train Loss: 4.076099230587715e-06, Train Acc: 1.0\n",
            "Epoch 579/10000\n",
            "Step 0: Train Loss: 4.0931449802883435e-06, Train Acc: 1.0\n",
            "Epoch 580/10000\n",
            "Step 0: Train Loss: 4.1367029552930035e-06, Train Acc: 1.0\n",
            "Epoch 581/10000\n",
            "Step 0: Train Loss: 3.96217683373834e-06, Train Acc: 1.0\n",
            "Epoch 582/10000\n",
            "Step 0: Train Loss: 4.183765668130945e-06, Train Acc: 1.0\n",
            "Epoch 583/10000\n",
            "Step 0: Train Loss: 3.948119228880387e-06, Train Acc: 1.0\n",
            "Epoch 584/10000\n",
            "Step 0: Train Loss: 4.016965704067843e-06, Train Acc: 1.0\n",
            "Epoch 585/10000\n",
            "Step 0: Train Loss: 4.552893187792506e-06, Train Acc: 1.0\n",
            "Epoch 586/10000\n",
            "Step 0: Train Loss: 4.624330813385313e-06, Train Acc: 1.0\n",
            "Epoch 587/10000\n",
            "Step 0: Train Loss: 4.2828528421523515e-06, Train Acc: 1.0\n",
            "Epoch 588/10000\n",
            "Step 0: Train Loss: 3.895712779922178e-06, Train Acc: 1.0\n",
            "Epoch 589/10000\n",
            "Step 0: Train Loss: 4.187130798527505e-06, Train Acc: 1.0\n",
            "Epoch 590/10000\n",
            "Step 0: Train Loss: 4.128228283661883e-06, Train Acc: 1.0\n",
            "Epoch 591/10000\n",
            "Step 0: Train Loss: 3.881899829139002e-06, Train Acc: 1.0\n",
            "Epoch 592/10000\n",
            "Step 0: Train Loss: 4.149503183725756e-06, Train Acc: 1.0\n",
            "Epoch 593/10000\n",
            "Step 0: Train Loss: 3.888178525812691e-06, Train Acc: 1.0\n",
            "Epoch 594/10000\n",
            "Step 0: Train Loss: 3.9138467400334775e-06, Train Acc: 1.0\n",
            "Epoch 595/10000\n",
            "Step 0: Train Loss: 3.851441306323977e-06, Train Acc: 1.0\n",
            "Epoch 596/10000\n",
            "Step 0: Train Loss: 3.897902388416696e-06, Train Acc: 1.0\n",
            "Epoch 597/10000\n",
            "Step 0: Train Loss: 3.95336383007816e-06, Train Acc: 1.0\n",
            "Epoch 598/10000\n",
            "Step 0: Train Loss: 3.868949988827808e-06, Train Acc: 1.0\n",
            "Epoch 599/10000\n",
            "Step 0: Train Loss: 3.826216470770305e-06, Train Acc: 1.0\n",
            "Epoch 600/10000\n",
            "Step 0: Train Loss: 3.621159521571826e-06, Train Acc: 1.0\n",
            "Epoch 601/10000\n",
            "Step 0: Train Loss: 3.6159847240924137e-06, Train Acc: 1.0\n",
            "Epoch 602/10000\n",
            "Step 0: Train Loss: 3.7599954794131918e-06, Train Acc: 1.0\n",
            "Epoch 603/10000\n",
            "Step 0: Train Loss: 3.7685413190047257e-06, Train Acc: 1.0\n",
            "Epoch 604/10000\n",
            "Step 0: Train Loss: 3.872006345773116e-06, Train Acc: 1.0\n",
            "Epoch 605/10000\n",
            "Step 0: Train Loss: 3.880081749230158e-06, Train Acc: 1.0\n",
            "Epoch 606/10000\n",
            "Step 0: Train Loss: 3.876119990309235e-06, Train Acc: 1.0\n",
            "Epoch 607/10000\n",
            "Step 0: Train Loss: 3.381337592145428e-06, Train Acc: 1.0\n",
            "Epoch 608/10000\n",
            "Step 0: Train Loss: 3.6081380585528677e-06, Train Acc: 1.0\n",
            "Epoch 609/10000\n",
            "Step 0: Train Loss: 3.7308057017071405e-06, Train Acc: 1.0\n",
            "Epoch 610/10000\n",
            "Step 0: Train Loss: 3.5941732221544953e-06, Train Acc: 1.0\n",
            "Epoch 611/10000\n",
            "Step 0: Train Loss: 3.4635081647138577e-06, Train Acc: 1.0\n",
            "Epoch 612/10000\n",
            "Step 0: Train Loss: 3.491343250061618e-06, Train Acc: 1.0\n",
            "Epoch 613/10000\n",
            "Step 0: Train Loss: 3.782999101531459e-06, Train Acc: 1.0\n",
            "Epoch 614/10000\n",
            "Step 0: Train Loss: 3.5520297387847677e-06, Train Acc: 1.0\n",
            "Epoch 615/10000\n",
            "Step 0: Train Loss: 3.593487690523034e-06, Train Acc: 1.0\n",
            "Epoch 616/10000\n",
            "Step 0: Train Loss: 3.5233304060966475e-06, Train Acc: 1.0\n",
            "Epoch 617/10000\n",
            "Step 0: Train Loss: 3.4162960673711495e-06, Train Acc: 1.0\n",
            "Epoch 618/10000\n",
            "Step 0: Train Loss: 3.4009367482212838e-06, Train Acc: 1.0\n",
            "Epoch 619/10000\n",
            "Step 0: Train Loss: 3.497512352623744e-06, Train Acc: 1.0\n",
            "Epoch 620/10000\n",
            "Step 0: Train Loss: 3.5943933198723244e-06, Train Acc: 1.0\n",
            "Epoch 621/10000\n",
            "Step 0: Train Loss: 3.52463212038856e-06, Train Acc: 1.0\n",
            "Epoch 622/10000\n",
            "Step 0: Train Loss: 3.855640443362063e-06, Train Acc: 1.0\n",
            "Epoch 623/10000\n",
            "Step 0: Train Loss: 3.3389028430974577e-06, Train Acc: 1.0\n",
            "Epoch 624/10000\n",
            "Step 0: Train Loss: 3.5653695249493467e-06, Train Acc: 1.0\n",
            "Epoch 625/10000\n",
            "Step 0: Train Loss: 3.53747236658819e-06, Train Acc: 1.0\n",
            "Epoch 626/10000\n",
            "Step 0: Train Loss: 3.528400611685356e-06, Train Acc: 1.0\n",
            "Epoch 627/10000\n",
            "Step 0: Train Loss: 3.388613777133287e-06, Train Acc: 1.0\n",
            "Epoch 628/10000\n",
            "Step 0: Train Loss: 3.4016959489235887e-06, Train Acc: 1.0\n",
            "Epoch 629/10000\n",
            "Step 0: Train Loss: 3.517108098094468e-06, Train Acc: 1.0\n",
            "Epoch 630/10000\n",
            "Step 0: Train Loss: 3.2279378956445726e-06, Train Acc: 1.0\n",
            "Epoch 631/10000\n",
            "Step 0: Train Loss: 3.235745907659293e-06, Train Acc: 1.0\n",
            "Epoch 632/10000\n",
            "Step 0: Train Loss: 2.9843167794751935e-06, Train Acc: 1.0\n",
            "Epoch 633/10000\n",
            "Step 0: Train Loss: 3.387336391824647e-06, Train Acc: 1.0\n",
            "Epoch 634/10000\n",
            "Step 0: Train Loss: 2.8088336421205895e-06, Train Acc: 1.0\n",
            "Epoch 635/10000\n",
            "Step 0: Train Loss: 3.3836354305094574e-06, Train Acc: 1.0\n",
            "Epoch 636/10000\n",
            "Step 0: Train Loss: 3.236805696360534e-06, Train Acc: 1.0\n",
            "Epoch 637/10000\n",
            "Step 0: Train Loss: 3.193267730239313e-06, Train Acc: 1.0\n",
            "Epoch 638/10000\n",
            "Step 0: Train Loss: 3.493684516797657e-06, Train Acc: 1.0\n",
            "Epoch 639/10000\n",
            "Step 0: Train Loss: 3.3387448183930246e-06, Train Acc: 1.0\n",
            "Epoch 640/10000\n",
            "Step 0: Train Loss: 3.005360895258491e-06, Train Acc: 1.0\n",
            "Epoch 641/10000\n",
            "Step 0: Train Loss: 2.977893473143922e-06, Train Acc: 1.0\n",
            "Epoch 642/10000\n",
            "Step 0: Train Loss: 3.3488254302938003e-06, Train Acc: 1.0\n",
            "Epoch 643/10000\n",
            "Step 0: Train Loss: 3.1216643492371077e-06, Train Acc: 1.0\n",
            "Epoch 644/10000\n",
            "Step 0: Train Loss: 3.088190851485706e-06, Train Acc: 1.0\n",
            "Epoch 645/10000\n",
            "Step 0: Train Loss: 3.0866024189890595e-06, Train Acc: 1.0\n",
            "Epoch 646/10000\n",
            "Step 0: Train Loss: 3.131969378955546e-06, Train Acc: 1.0\n",
            "Epoch 647/10000\n",
            "Step 0: Train Loss: 2.8828817448811606e-06, Train Acc: 1.0\n",
            "Epoch 648/10000\n",
            "Step 0: Train Loss: 3.087751338171074e-06, Train Acc: 1.0\n",
            "Epoch 649/10000\n",
            "Step 0: Train Loss: 3.1099050374905346e-06, Train Acc: 1.0\n",
            "Epoch 650/10000\n",
            "Step 0: Train Loss: 2.9495217859221157e-06, Train Acc: 1.0\n",
            "Epoch 651/10000\n",
            "Step 0: Train Loss: 3.0449143650912447e-06, Train Acc: 1.0\n",
            "Epoch 652/10000\n",
            "Step 0: Train Loss: 2.968978606077144e-06, Train Acc: 1.0\n",
            "Epoch 653/10000\n",
            "Step 0: Train Loss: 3.2473110422870377e-06, Train Acc: 1.0\n",
            "Epoch 654/10000\n",
            "Step 0: Train Loss: 3.15786974169896e-06, Train Acc: 1.0\n",
            "Epoch 655/10000\n",
            "Step 0: Train Loss: 2.9592665669042617e-06, Train Acc: 1.0\n",
            "Epoch 656/10000\n",
            "Step 0: Train Loss: 3.072388835789752e-06, Train Acc: 1.0\n",
            "Epoch 657/10000\n",
            "Step 0: Train Loss: 2.963131009892095e-06, Train Acc: 1.0\n",
            "Epoch 658/10000\n",
            "Step 0: Train Loss: 2.777025883915485e-06, Train Acc: 1.0\n",
            "Epoch 659/10000\n",
            "Step 0: Train Loss: 2.9607265332742827e-06, Train Acc: 1.0\n",
            "Epoch 660/10000\n",
            "Step 0: Train Loss: 2.875774953281507e-06, Train Acc: 1.0\n",
            "Epoch 661/10000\n",
            "Step 0: Train Loss: 2.720679049161845e-06, Train Acc: 1.0\n",
            "Epoch 662/10000\n",
            "Step 0: Train Loss: 2.9690384053537855e-06, Train Acc: 1.0\n",
            "Epoch 663/10000\n",
            "Step 0: Train Loss: 2.6069144496432273e-06, Train Acc: 1.0\n",
            "Epoch 664/10000\n",
            "Step 0: Train Loss: 2.933569930974045e-06, Train Acc: 1.0\n",
            "Epoch 665/10000\n",
            "Step 0: Train Loss: 2.653164528965135e-06, Train Acc: 1.0\n",
            "Epoch 666/10000\n",
            "Step 0: Train Loss: 2.7673363547364715e-06, Train Acc: 1.0\n",
            "Epoch 667/10000\n",
            "Step 0: Train Loss: 2.675472387636546e-06, Train Acc: 1.0\n",
            "Epoch 668/10000\n",
            "Step 0: Train Loss: 2.668614797585178e-06, Train Acc: 1.0\n",
            "Epoch 669/10000\n",
            "Step 0: Train Loss: 2.8368342555040726e-06, Train Acc: 1.0\n",
            "Epoch 670/10000\n",
            "Step 0: Train Loss: 2.7444020815892145e-06, Train Acc: 1.0\n",
            "Epoch 671/10000\n",
            "Step 0: Train Loss: 2.573324763943674e-06, Train Acc: 1.0\n",
            "Epoch 672/10000\n",
            "Step 0: Train Loss: 2.5623648980399594e-06, Train Acc: 1.0\n",
            "Epoch 673/10000\n",
            "Step 0: Train Loss: 2.772493189695524e-06, Train Acc: 1.0\n",
            "Epoch 674/10000\n",
            "Step 0: Train Loss: 2.7364476409275085e-06, Train Acc: 1.0\n",
            "Epoch 675/10000\n",
            "Step 0: Train Loss: 2.8408057914930396e-06, Train Acc: 1.0\n",
            "Epoch 676/10000\n",
            "Step 0: Train Loss: 2.612532625789754e-06, Train Acc: 1.0\n",
            "Epoch 677/10000\n",
            "Step 0: Train Loss: 2.6969967166223796e-06, Train Acc: 1.0\n",
            "Epoch 678/10000\n",
            "Step 0: Train Loss: 2.736240730882855e-06, Train Acc: 1.0\n",
            "Epoch 679/10000\n",
            "Step 0: Train Loss: 2.756139565462945e-06, Train Acc: 1.0\n",
            "Epoch 680/10000\n",
            "Step 0: Train Loss: 2.5780377654882614e-06, Train Acc: 1.0\n",
            "Epoch 681/10000\n",
            "Step 0: Train Loss: 2.677525799299474e-06, Train Acc: 1.0\n",
            "Epoch 682/10000\n",
            "Step 0: Train Loss: 2.703593963815365e-06, Train Acc: 1.0\n",
            "Epoch 683/10000\n",
            "Step 0: Train Loss: 2.675242512850673e-06, Train Acc: 1.0\n",
            "Epoch 684/10000\n",
            "Step 0: Train Loss: 2.5806305075093405e-06, Train Acc: 1.0\n",
            "Epoch 685/10000\n",
            "Step 0: Train Loss: 2.59056423601578e-06, Train Acc: 1.0\n",
            "Epoch 686/10000\n",
            "Step 0: Train Loss: 2.688799895622651e-06, Train Acc: 1.0\n",
            "Epoch 687/10000\n",
            "Step 0: Train Loss: 2.5206404643540736e-06, Train Acc: 1.0\n",
            "Epoch 688/10000\n",
            "Step 0: Train Loss: 2.617602376631112e-06, Train Acc: 1.0\n",
            "Epoch 689/10000\n",
            "Step 0: Train Loss: 2.543851451264345e-06, Train Acc: 1.0\n",
            "Epoch 690/10000\n",
            "Step 0: Train Loss: 2.5871727302728686e-06, Train Acc: 1.0\n",
            "Epoch 691/10000\n",
            "Step 0: Train Loss: 2.4531268536520656e-06, Train Acc: 1.0\n",
            "Epoch 692/10000\n",
            "Step 0: Train Loss: 2.428013658573036e-06, Train Acc: 1.0\n",
            "Epoch 693/10000\n",
            "Step 0: Train Loss: 2.515653932277928e-06, Train Acc: 1.0\n",
            "Epoch 694/10000\n",
            "Step 0: Train Loss: 2.508465968276141e-06, Train Acc: 1.0\n",
            "Epoch 695/10000\n",
            "Step 0: Train Loss: 2.1693904272979125e-06, Train Acc: 1.0\n",
            "Epoch 696/10000\n",
            "Step 0: Train Loss: 2.5914571324392455e-06, Train Acc: 1.0\n",
            "Epoch 697/10000\n",
            "Step 0: Train Loss: 2.3649031390959863e-06, Train Acc: 1.0\n",
            "Epoch 698/10000\n",
            "Step 0: Train Loss: 2.6316724870412145e-06, Train Acc: 1.0\n",
            "Epoch 699/10000\n",
            "Step 0: Train Loss: 2.2966873984842096e-06, Train Acc: 1.0\n",
            "Epoch 700/10000\n",
            "Step 0: Train Loss: 2.508608531570644e-06, Train Acc: 1.0\n",
            "Epoch 701/10000\n",
            "Step 0: Train Loss: 2.3823249648557976e-06, Train Acc: 1.0\n",
            "Epoch 702/10000\n",
            "Step 0: Train Loss: 2.5318472580693197e-06, Train Acc: 1.0\n",
            "Epoch 703/10000\n",
            "Step 0: Train Loss: 2.4563225906604202e-06, Train Acc: 1.0\n",
            "Epoch 704/10000\n",
            "Step 0: Train Loss: 2.4109338028210914e-06, Train Acc: 1.0\n",
            "Epoch 705/10000\n",
            "Step 0: Train Loss: 2.2645399440079927e-06, Train Acc: 1.0\n",
            "Epoch 706/10000\n",
            "Step 0: Train Loss: 2.2739925498171942e-06, Train Acc: 1.0\n",
            "Epoch 707/10000\n",
            "Step 0: Train Loss: 2.4055250378296478e-06, Train Acc: 1.0\n",
            "Epoch 708/10000\n",
            "Step 0: Train Loss: 2.156549498977256e-06, Train Acc: 1.0\n",
            "Epoch 709/10000\n",
            "Step 0: Train Loss: 2.3854884148022393e-06, Train Acc: 1.0\n",
            "Epoch 710/10000\n",
            "Step 0: Train Loss: 2.3097300072549842e-06, Train Acc: 1.0\n",
            "Epoch 711/10000\n",
            "Step 0: Train Loss: 2.3478805815102533e-06, Train Acc: 1.0\n",
            "Epoch 712/10000\n",
            "Step 0: Train Loss: 2.2622509732173057e-06, Train Acc: 1.0\n",
            "Epoch 713/10000\n",
            "Step 0: Train Loss: 2.461278427290381e-06, Train Acc: 1.0\n",
            "Epoch 714/10000\n",
            "Step 0: Train Loss: 2.4335104171768762e-06, Train Acc: 1.0\n",
            "Epoch 715/10000\n",
            "Step 0: Train Loss: 2.332159965590108e-06, Train Acc: 1.0\n",
            "Epoch 716/10000\n",
            "Step 0: Train Loss: 2.339380671401159e-06, Train Acc: 1.0\n",
            "Epoch 717/10000\n",
            "Step 0: Train Loss: 2.1426610601338325e-06, Train Acc: 1.0\n",
            "Epoch 718/10000\n",
            "Step 0: Train Loss: 2.2158058072818676e-06, Train Acc: 1.0\n",
            "Epoch 719/10000\n",
            "Step 0: Train Loss: 2.039736955339322e-06, Train Acc: 1.0\n",
            "Epoch 720/10000\n",
            "Step 0: Train Loss: 1.973759253814933e-06, Train Acc: 1.0\n",
            "Epoch 721/10000\n",
            "Step 0: Train Loss: 2.1158643903618213e-06, Train Acc: 1.0\n",
            "Epoch 722/10000\n",
            "Step 0: Train Loss: 2.166603053410654e-06, Train Acc: 1.0\n",
            "Epoch 723/10000\n",
            "Step 0: Train Loss: 2.0120990029681707e-06, Train Acc: 1.0\n",
            "Epoch 724/10000\n",
            "Step 0: Train Loss: 2.086917675114819e-06, Train Acc: 1.0\n",
            "Epoch 725/10000\n",
            "Step 0: Train Loss: 2.326249614270637e-06, Train Acc: 1.0\n",
            "Epoch 726/10000\n",
            "Step 0: Train Loss: 2.3109648736863164e-06, Train Acc: 1.0\n",
            "Epoch 727/10000\n",
            "Step 0: Train Loss: 2.101377504004631e-06, Train Acc: 1.0\n",
            "Epoch 728/10000\n",
            "Step 0: Train Loss: 2.1470916635735193e-06, Train Acc: 1.0\n",
            "Epoch 729/10000\n",
            "Step 0: Train Loss: 2.078609441014123e-06, Train Acc: 1.0\n",
            "Epoch 730/10000\n",
            "Step 0: Train Loss: 2.0209981812513433e-06, Train Acc: 1.0\n",
            "Epoch 731/10000\n",
            "Step 0: Train Loss: 2.0465586203499697e-06, Train Acc: 1.0\n",
            "Epoch 732/10000\n",
            "Step 0: Train Loss: 2.1578127871180186e-06, Train Acc: 1.0\n",
            "Epoch 733/10000\n",
            "Step 0: Train Loss: 2.2976826130616246e-06, Train Acc: 1.0\n",
            "Epoch 734/10000\n",
            "Step 0: Train Loss: 1.8404664388071978e-06, Train Acc: 1.0\n",
            "Epoch 735/10000\n",
            "Step 0: Train Loss: 2.0630022845580243e-06, Train Acc: 1.0\n",
            "Epoch 736/10000\n",
            "Step 0: Train Loss: 2.0885995581920724e-06, Train Acc: 1.0\n",
            "Epoch 737/10000\n",
            "Step 0: Train Loss: 2.1870732780371327e-06, Train Acc: 1.0\n",
            "Epoch 738/10000\n",
            "Step 0: Train Loss: 2.0995591967221117e-06, Train Acc: 1.0\n",
            "Epoch 739/10000\n",
            "Step 0: Train Loss: 2.100420715578366e-06, Train Acc: 1.0\n",
            "Epoch 740/10000\n",
            "Step 0: Train Loss: 1.917183681143797e-06, Train Acc: 1.0\n",
            "Epoch 741/10000\n",
            "Step 0: Train Loss: 2.103347696902347e-06, Train Acc: 1.0\n",
            "Epoch 742/10000\n",
            "Step 0: Train Loss: 2.1235648546280572e-06, Train Acc: 1.0\n",
            "Epoch 743/10000\n",
            "Step 0: Train Loss: 1.8958249938805238e-06, Train Acc: 1.0\n",
            "Epoch 744/10000\n",
            "Step 0: Train Loss: 1.9798399080173112e-06, Train Acc: 1.0\n",
            "Epoch 745/10000\n",
            "Step 0: Train Loss: 1.8538602262196946e-06, Train Acc: 1.0\n",
            "Epoch 746/10000\n",
            "Step 0: Train Loss: 1.883214508779929e-06, Train Acc: 1.0\n",
            "Epoch 747/10000\n",
            "Step 0: Train Loss: 1.8974121758219553e-06, Train Acc: 1.0\n",
            "Epoch 748/10000\n",
            "Step 0: Train Loss: 1.8190495438830112e-06, Train Acc: 1.0\n",
            "Epoch 749/10000\n",
            "Step 0: Train Loss: 1.954553908944945e-06, Train Acc: 1.0\n",
            "Epoch 750/10000\n",
            "Step 0: Train Loss: 1.9253543541708495e-06, Train Acc: 1.0\n",
            "Epoch 751/10000\n",
            "Step 0: Train Loss: 1.891839133350004e-06, Train Acc: 1.0\n",
            "Epoch 752/10000\n",
            "Step 0: Train Loss: 1.796067522263911e-06, Train Acc: 1.0\n",
            "Epoch 753/10000\n",
            "Step 0: Train Loss: 1.9120222987112356e-06, Train Acc: 1.0\n",
            "Epoch 754/10000\n",
            "Step 0: Train Loss: 1.7213490082212957e-06, Train Acc: 1.0\n",
            "Epoch 755/10000\n",
            "Step 0: Train Loss: 1.953309038071893e-06, Train Acc: 1.0\n",
            "Epoch 756/10000\n",
            "Step 0: Train Loss: 1.9145927581121214e-06, Train Acc: 1.0\n",
            "Epoch 757/10000\n",
            "Step 0: Train Loss: 1.7033953554346226e-06, Train Acc: 1.0\n",
            "Epoch 758/10000\n",
            "Step 0: Train Loss: 1.9739375147764804e-06, Train Acc: 1.0\n",
            "Epoch 759/10000\n",
            "Step 0: Train Loss: 1.8185429553341237e-06, Train Acc: 1.0\n",
            "Epoch 760/10000\n",
            "Step 0: Train Loss: 1.8676970512387925e-06, Train Acc: 1.0\n",
            "Epoch 761/10000\n",
            "Step 0: Train Loss: 1.848248075475567e-06, Train Acc: 1.0\n",
            "Epoch 762/10000\n",
            "Step 0: Train Loss: 1.8916445014838246e-06, Train Acc: 1.0\n",
            "Epoch 763/10000\n",
            "Step 0: Train Loss: 1.8042038618659717e-06, Train Acc: 1.0\n",
            "Epoch 764/10000\n",
            "Step 0: Train Loss: 1.7559863181304536e-06, Train Acc: 1.0\n",
            "Epoch 765/10000\n",
            "Step 0: Train Loss: 1.8763103071250953e-06, Train Acc: 1.0\n",
            "Epoch 766/10000\n",
            "Step 0: Train Loss: 1.8626968767421204e-06, Train Acc: 1.0\n",
            "Epoch 767/10000\n",
            "Step 0: Train Loss: 1.8562394643595326e-06, Train Acc: 1.0\n",
            "Epoch 768/10000\n",
            "Step 0: Train Loss: 1.8084384691974265e-06, Train Acc: 1.0\n",
            "Epoch 769/10000\n",
            "Step 0: Train Loss: 1.8189999764217646e-06, Train Acc: 1.0\n",
            "Epoch 770/10000\n",
            "Step 0: Train Loss: 1.8672883470571833e-06, Train Acc: 1.0\n",
            "Epoch 771/10000\n",
            "Step 0: Train Loss: 1.8386699593975209e-06, Train Acc: 1.0\n",
            "Epoch 772/10000\n",
            "Step 0: Train Loss: 1.77117794919468e-06, Train Acc: 1.0\n",
            "Epoch 773/10000\n",
            "Step 0: Train Loss: 1.7482911971455906e-06, Train Acc: 1.0\n",
            "Epoch 774/10000\n",
            "Step 0: Train Loss: 1.945620851984131e-06, Train Acc: 1.0\n",
            "Epoch 775/10000\n",
            "Step 0: Train Loss: 1.6779292764113052e-06, Train Acc: 1.0\n",
            "Epoch 776/10000\n",
            "Step 0: Train Loss: 1.7708498489810154e-06, Train Acc: 1.0\n",
            "Epoch 777/10000\n",
            "Step 0: Train Loss: 1.751451918607927e-06, Train Acc: 1.0\n",
            "Epoch 778/10000\n",
            "Step 0: Train Loss: 1.7636702978052199e-06, Train Acc: 1.0\n",
            "Epoch 779/10000\n",
            "Step 0: Train Loss: 1.5560733572783647e-06, Train Acc: 1.0\n",
            "Epoch 780/10000\n",
            "Step 0: Train Loss: 1.6641037063891417e-06, Train Acc: 1.0\n",
            "Epoch 781/10000\n",
            "Step 0: Train Loss: 1.9015770931218867e-06, Train Acc: 1.0\n",
            "Epoch 782/10000\n",
            "Step 0: Train Loss: 1.7448874132242054e-06, Train Acc: 1.0\n",
            "Epoch 783/10000\n",
            "Step 0: Train Loss: 1.7813312069847598e-06, Train Acc: 1.0\n",
            "Epoch 784/10000\n",
            "Step 0: Train Loss: 1.684626795395161e-06, Train Acc: 1.0\n",
            "Epoch 785/10000\n",
            "Step 0: Train Loss: 1.5238549622154096e-06, Train Acc: 1.0\n",
            "Epoch 786/10000\n",
            "Step 0: Train Loss: 1.5678261888751877e-06, Train Acc: 1.0\n",
            "Epoch 787/10000\n",
            "Step 0: Train Loss: 1.6819096799736144e-06, Train Acc: 1.0\n",
            "Epoch 788/10000\n",
            "Step 0: Train Loss: 1.5768245020808536e-06, Train Acc: 1.0\n",
            "Epoch 789/10000\n",
            "Step 0: Train Loss: 1.6502507378390874e-06, Train Acc: 1.0\n",
            "Epoch 790/10000\n",
            "Step 0: Train Loss: 1.7533527625346323e-06, Train Acc: 1.0\n",
            "Epoch 791/10000\n",
            "Step 0: Train Loss: 1.7198800605910947e-06, Train Acc: 1.0\n",
            "Epoch 792/10000\n",
            "Step 0: Train Loss: 1.6507967757206643e-06, Train Acc: 1.0\n",
            "Epoch 793/10000\n",
            "Step 0: Train Loss: 1.6509976603629184e-06, Train Acc: 1.0\n",
            "Epoch 794/10000\n",
            "Step 0: Train Loss: 1.6304122709698277e-06, Train Acc: 1.0\n",
            "Epoch 795/10000\n",
            "Step 0: Train Loss: 1.5647377722416422e-06, Train Acc: 1.0\n",
            "Epoch 796/10000\n",
            "Step 0: Train Loss: 1.5739910850243177e-06, Train Acc: 1.0\n",
            "Epoch 797/10000\n",
            "Step 0: Train Loss: 1.7061935295714648e-06, Train Acc: 1.0\n",
            "Epoch 798/10000\n",
            "Step 0: Train Loss: 1.632506723581173e-06, Train Acc: 1.0\n",
            "Epoch 799/10000\n",
            "Step 0: Train Loss: 1.6009738601496792e-06, Train Acc: 1.0\n",
            "Epoch 800/10000\n",
            "Step 0: Train Loss: 1.5959446955093881e-06, Train Acc: 1.0\n",
            "Epoch 801/10000\n",
            "Step 0: Train Loss: 1.5418788734677946e-06, Train Acc: 1.0\n",
            "Epoch 802/10000\n",
            "Step 0: Train Loss: 1.5217576674331212e-06, Train Acc: 1.0\n",
            "Epoch 803/10000\n",
            "Step 0: Train Loss: 1.6957465049927123e-06, Train Acc: 1.0\n",
            "Epoch 804/10000\n",
            "Step 0: Train Loss: 1.4343072507472243e-06, Train Acc: 1.0\n",
            "Epoch 805/10000\n",
            "Step 0: Train Loss: 1.3989393892188673e-06, Train Acc: 1.0\n",
            "Epoch 806/10000\n",
            "Step 0: Train Loss: 1.57137731093826e-06, Train Acc: 1.0\n",
            "Epoch 807/10000\n",
            "Step 0: Train Loss: 1.4903895362294861e-06, Train Acc: 1.0\n",
            "Epoch 808/10000\n",
            "Step 0: Train Loss: 1.3954357882539625e-06, Train Acc: 1.0\n",
            "Epoch 809/10000\n",
            "Step 0: Train Loss: 1.4513976793750771e-06, Train Acc: 1.0\n",
            "Epoch 810/10000\n",
            "Step 0: Train Loss: 1.4924146398698213e-06, Train Acc: 1.0\n",
            "Epoch 811/10000\n",
            "Step 0: Train Loss: 1.5892728697508574e-06, Train Acc: 1.0\n",
            "Epoch 812/10000\n",
            "Step 0: Train Loss: 1.5722444004495628e-06, Train Acc: 1.0\n",
            "Epoch 813/10000\n",
            "Step 0: Train Loss: 1.5010351717137382e-06, Train Acc: 1.0\n",
            "Epoch 814/10000\n",
            "Step 0: Train Loss: 1.4878934280204703e-06, Train Acc: 1.0\n",
            "Epoch 815/10000\n",
            "Step 0: Train Loss: 1.4858387658023275e-06, Train Acc: 1.0\n",
            "Epoch 816/10000\n",
            "Step 0: Train Loss: 1.3865280834579607e-06, Train Acc: 1.0\n",
            "Epoch 817/10000\n",
            "Step 0: Train Loss: 1.4287886642705416e-06, Train Acc: 1.0\n",
            "Epoch 818/10000\n",
            "Step 0: Train Loss: 1.4751219623576617e-06, Train Acc: 1.0\n",
            "Epoch 819/10000\n",
            "Step 0: Train Loss: 1.3848247135683778e-06, Train Acc: 1.0\n",
            "Epoch 820/10000\n",
            "Step 0: Train Loss: 1.4438309108300018e-06, Train Acc: 1.0\n",
            "Epoch 821/10000\n",
            "Step 0: Train Loss: 1.3782328096567653e-06, Train Acc: 1.0\n",
            "Epoch 822/10000\n",
            "Step 0: Train Loss: 1.3707472135138232e-06, Train Acc: 1.0\n",
            "Epoch 823/10000\n",
            "Step 0: Train Loss: 1.5120842817850644e-06, Train Acc: 1.0\n",
            "Epoch 824/10000\n",
            "Step 0: Train Loss: 1.478351350669982e-06, Train Acc: 1.0\n",
            "Epoch 825/10000\n",
            "Step 0: Train Loss: 1.4334621027956018e-06, Train Acc: 1.0\n",
            "Epoch 826/10000\n",
            "Step 0: Train Loss: 1.3911061387261725e-06, Train Acc: 1.0\n",
            "Epoch 827/10000\n",
            "Step 0: Train Loss: 1.392356011820084e-06, Train Acc: 1.0\n",
            "Epoch 828/10000\n",
            "Step 0: Train Loss: 1.4191655282047577e-06, Train Acc: 1.0\n",
            "Epoch 829/10000\n",
            "Step 0: Train Loss: 1.346562157777953e-06, Train Acc: 1.0\n",
            "Epoch 830/10000\n",
            "Step 0: Train Loss: 1.3973624390928308e-06, Train Acc: 1.0\n",
            "Epoch 831/10000\n",
            "Step 0: Train Loss: 1.3725118606089382e-06, Train Acc: 1.0\n",
            "Epoch 832/10000\n",
            "Step 0: Train Loss: 1.3750510561294504e-06, Train Acc: 1.0\n",
            "Epoch 833/10000\n",
            "Step 0: Train Loss: 1.4129117289485293e-06, Train Acc: 1.0\n",
            "Epoch 834/10000\n",
            "Step 0: Train Loss: 1.2881148450105684e-06, Train Acc: 1.0\n",
            "Epoch 835/10000\n",
            "Step 0: Train Loss: 1.3354273278309847e-06, Train Acc: 1.0\n",
            "Epoch 836/10000\n",
            "Step 0: Train Loss: 1.465586933591112e-06, Train Acc: 1.0\n",
            "Epoch 837/10000\n",
            "Step 0: Train Loss: 1.4805694945607684e-06, Train Acc: 1.0\n",
            "Epoch 838/10000\n",
            "Step 0: Train Loss: 1.4242687029764056e-06, Train Acc: 1.0\n",
            "Epoch 839/10000\n",
            "Step 0: Train Loss: 1.4035296089787153e-06, Train Acc: 1.0\n",
            "Epoch 840/10000\n",
            "Step 0: Train Loss: 1.3828955616190797e-06, Train Acc: 1.0\n",
            "Epoch 841/10000\n",
            "Step 0: Train Loss: 1.445392513232946e-06, Train Acc: 1.0\n",
            "Epoch 842/10000\n",
            "Step 0: Train Loss: 1.289628016820643e-06, Train Acc: 1.0\n",
            "Epoch 843/10000\n",
            "Step 0: Train Loss: 1.3623687209474156e-06, Train Acc: 1.0\n",
            "Epoch 844/10000\n",
            "Step 0: Train Loss: 1.2702912499662489e-06, Train Acc: 1.0\n",
            "Epoch 845/10000\n",
            "Step 0: Train Loss: 1.3417454738373635e-06, Train Acc: 1.0\n",
            "Epoch 846/10000\n",
            "Step 0: Train Loss: 1.30311161683494e-06, Train Acc: 1.0\n",
            "Epoch 847/10000\n",
            "Step 0: Train Loss: 1.2168309240223607e-06, Train Acc: 1.0\n",
            "Epoch 848/10000\n",
            "Step 0: Train Loss: 1.2819509720429778e-06, Train Acc: 1.0\n",
            "Epoch 849/10000\n",
            "Step 0: Train Loss: 1.227526695402048e-06, Train Acc: 1.0\n",
            "Epoch 850/10000\n",
            "Step 0: Train Loss: 1.4603922409150982e-06, Train Acc: 1.0\n",
            "Epoch 851/10000\n",
            "Step 0: Train Loss: 1.2602455399246537e-06, Train Acc: 1.0\n",
            "Epoch 852/10000\n",
            "Step 0: Train Loss: 1.3938733900431544e-06, Train Acc: 1.0\n",
            "Epoch 853/10000\n",
            "Step 0: Train Loss: 1.380884555146622e-06, Train Acc: 1.0\n",
            "Epoch 854/10000\n",
            "Step 0: Train Loss: 1.1989817494395538e-06, Train Acc: 1.0\n",
            "Epoch 855/10000\n",
            "Step 0: Train Loss: 1.4218754813555279e-06, Train Acc: 1.0\n",
            "Epoch 856/10000\n",
            "Step 0: Train Loss: 1.1877676797666936e-06, Train Acc: 1.0\n",
            "Epoch 857/10000\n",
            "Step 0: Train Loss: 1.2687665957855643e-06, Train Acc: 1.0\n",
            "Epoch 858/10000\n",
            "Step 0: Train Loss: 1.274558371733292e-06, Train Acc: 1.0\n",
            "Epoch 859/10000\n",
            "Step 0: Train Loss: 1.14790782390628e-06, Train Acc: 1.0\n",
            "Epoch 860/10000\n",
            "Step 0: Train Loss: 1.3412567341219983e-06, Train Acc: 1.0\n",
            "Epoch 861/10000\n",
            "Step 0: Train Loss: 1.300000917581201e-06, Train Acc: 1.0\n",
            "Epoch 862/10000\n",
            "Step 0: Train Loss: 1.144817247222818e-06, Train Acc: 1.0\n",
            "Epoch 863/10000\n",
            "Step 0: Train Loss: 1.2301561582717113e-06, Train Acc: 1.0\n",
            "Epoch 864/10000\n",
            "Step 0: Train Loss: 1.243185693056148e-06, Train Acc: 1.0\n",
            "Epoch 865/10000\n",
            "Step 0: Train Loss: 1.1622593092397437e-06, Train Acc: 1.0\n",
            "Epoch 866/10000\n",
            "Step 0: Train Loss: 1.3072616411591298e-06, Train Acc: 1.0\n",
            "Epoch 867/10000\n",
            "Step 0: Train Loss: 1.1363782732587424e-06, Train Acc: 1.0\n",
            "Epoch 868/10000\n",
            "Step 0: Train Loss: 1.115708187171549e-06, Train Acc: 1.0\n",
            "Epoch 869/10000\n",
            "Step 0: Train Loss: 1.2612689488378237e-06, Train Acc: 1.0\n",
            "Epoch 870/10000\n",
            "Step 0: Train Loss: 1.2021696420561057e-06, Train Acc: 1.0\n",
            "Epoch 871/10000\n",
            "Step 0: Train Loss: 1.23167262700008e-06, Train Acc: 1.0\n",
            "Epoch 872/10000\n",
            "Step 0: Train Loss: 1.2503953712439397e-06, Train Acc: 1.0\n",
            "Epoch 873/10000\n",
            "Step 0: Train Loss: 1.2169138017270598e-06, Train Acc: 1.0\n",
            "Epoch 874/10000\n",
            "Step 0: Train Loss: 1.1729531479431898e-06, Train Acc: 1.0\n",
            "Epoch 875/10000\n",
            "Step 0: Train Loss: 1.2588363915710943e-06, Train Acc: 1.0\n",
            "Epoch 876/10000\n",
            "Step 0: Train Loss: 1.1224805120946257e-06, Train Acc: 1.0\n",
            "Epoch 877/10000\n",
            "Step 0: Train Loss: 1.1151843182233279e-06, Train Acc: 1.0\n",
            "Epoch 878/10000\n",
            "Step 0: Train Loss: 1.2020479971397435e-06, Train Acc: 1.0\n",
            "Epoch 879/10000\n",
            "Step 0: Train Loss: 1.0977233841913403e-06, Train Acc: 1.0\n",
            "Epoch 880/10000\n",
            "Step 0: Train Loss: 1.0937983461190015e-06, Train Acc: 1.0\n",
            "Epoch 881/10000\n",
            "Step 0: Train Loss: 1.1263622354817926e-06, Train Acc: 1.0\n",
            "Epoch 882/10000\n",
            "Step 0: Train Loss: 1.1612446542130783e-06, Train Acc: 1.0\n",
            "Epoch 883/10000\n",
            "Step 0: Train Loss: 1.1068981393691502e-06, Train Acc: 1.0\n",
            "Epoch 884/10000\n",
            "Step 0: Train Loss: 1.0884115226872382e-06, Train Acc: 1.0\n",
            "Epoch 885/10000\n",
            "Step 0: Train Loss: 1.0651500588210183e-06, Train Acc: 1.0\n",
            "Epoch 886/10000\n",
            "Step 0: Train Loss: 1.1843234233310795e-06, Train Acc: 1.0\n",
            "Epoch 887/10000\n",
            "Step 0: Train Loss: 1.111249503082945e-06, Train Acc: 1.0\n",
            "Epoch 888/10000\n",
            "Step 0: Train Loss: 1.1767623391278903e-06, Train Acc: 1.0\n",
            "Epoch 889/10000\n",
            "Step 0: Train Loss: 1.032955424307147e-06, Train Acc: 1.0\n",
            "Epoch 890/10000\n",
            "Step 0: Train Loss: 1.1666079444694333e-06, Train Acc: 1.0\n",
            "Epoch 891/10000\n",
            "Step 0: Train Loss: 1.0874111922021257e-06, Train Acc: 1.0\n",
            "Epoch 892/10000\n",
            "Step 0: Train Loss: 1.050693867910013e-06, Train Acc: 1.0\n",
            "Epoch 893/10000\n",
            "Step 0: Train Loss: 1.1220137139389408e-06, Train Acc: 1.0\n",
            "Epoch 894/10000\n",
            "Step 0: Train Loss: 1.0486661494724103e-06, Train Acc: 1.0\n",
            "Epoch 895/10000\n",
            "Step 0: Train Loss: 1.1171625828865217e-06, Train Acc: 1.0\n",
            "Epoch 896/10000\n",
            "Step 0: Train Loss: 9.281351935896964e-07, Train Acc: 1.0\n",
            "Epoch 897/10000\n",
            "Step 0: Train Loss: 1.0842153415069333e-06, Train Acc: 1.0\n",
            "Epoch 898/10000\n",
            "Step 0: Train Loss: 1.041706241267093e-06, Train Acc: 1.0\n",
            "Epoch 899/10000\n",
            "Step 0: Train Loss: 1.138785819421173e-06, Train Acc: 1.0\n",
            "Epoch 900/10000\n",
            "Step 0: Train Loss: 1.086415863937873e-06, Train Acc: 1.0\n",
            "Epoch 901/10000\n",
            "Step 0: Train Loss: 9.797079201234737e-07, Train Acc: 1.0\n",
            "Epoch 902/10000\n",
            "Step 0: Train Loss: 1.055603888744372e-06, Train Acc: 1.0\n",
            "Epoch 903/10000\n",
            "Step 0: Train Loss: 1.1309916772006545e-06, Train Acc: 1.0\n",
            "Epoch 904/10000\n",
            "Step 0: Train Loss: 1.0182355936194654e-06, Train Acc: 1.0\n",
            "Epoch 905/10000\n",
            "Step 0: Train Loss: 1.0290694945069845e-06, Train Acc: 1.0\n",
            "Epoch 906/10000\n",
            "Step 0: Train Loss: 1.0403844044049038e-06, Train Acc: 1.0\n",
            "Epoch 907/10000\n",
            "Step 0: Train Loss: 1.0747249916676083e-06, Train Acc: 1.0\n",
            "Epoch 908/10000\n",
            "Step 0: Train Loss: 1.0910328001045855e-06, Train Acc: 1.0\n",
            "Epoch 909/10000\n",
            "Step 0: Train Loss: 1.0414684084025794e-06, Train Acc: 1.0\n",
            "Epoch 910/10000\n",
            "Step 0: Train Loss: 1.090009959625604e-06, Train Acc: 1.0\n",
            "Epoch 911/10000\n",
            "Step 0: Train Loss: 1.0423603953313432e-06, Train Acc: 1.0\n",
            "Epoch 912/10000\n",
            "Step 0: Train Loss: 9.918644536810461e-07, Train Acc: 1.0\n",
            "Epoch 913/10000\n",
            "Step 0: Train Loss: 1.0106276704391348e-06, Train Acc: 1.0\n",
            "Epoch 914/10000\n",
            "Step 0: Train Loss: 1.0111306210092152e-06, Train Acc: 1.0\n",
            "Epoch 915/10000\n",
            "Step 0: Train Loss: 9.738308790474548e-07, Train Acc: 1.0\n",
            "Epoch 916/10000\n",
            "Step 0: Train Loss: 1.0768562788143754e-06, Train Acc: 1.0\n",
            "Epoch 917/10000\n",
            "Step 0: Train Loss: 9.592398555469117e-07, Train Acc: 1.0\n",
            "Epoch 918/10000\n",
            "Step 0: Train Loss: 9.038565735863813e-07, Train Acc: 1.0\n",
            "Epoch 919/10000\n",
            "Step 0: Train Loss: 9.866336085906369e-07, Train Acc: 1.0\n",
            "Epoch 920/10000\n",
            "Step 0: Train Loss: 9.830437193159014e-07, Train Acc: 1.0\n",
            "Epoch 921/10000\n",
            "Step 0: Train Loss: 8.512753311151755e-07, Train Acc: 1.0\n",
            "Epoch 922/10000\n",
            "Step 0: Train Loss: 9.22476033338171e-07, Train Acc: 1.0\n",
            "Epoch 923/10000\n",
            "Step 0: Train Loss: 8.945939953264315e-07, Train Acc: 1.0\n",
            "Epoch 924/10000\n",
            "Step 0: Train Loss: 9.237630251845985e-07, Train Acc: 1.0\n",
            "Epoch 925/10000\n",
            "Step 0: Train Loss: 9.84117491498182e-07, Train Acc: 1.0\n",
            "Epoch 926/10000\n",
            "Step 0: Train Loss: 8.823514008327038e-07, Train Acc: 1.0\n",
            "Epoch 927/10000\n",
            "Step 0: Train Loss: 8.835539233587042e-07, Train Acc: 1.0\n",
            "Epoch 928/10000\n",
            "Step 0: Train Loss: 9.781929293239955e-07, Train Acc: 1.0\n",
            "Epoch 929/10000\n",
            "Step 0: Train Loss: 9.565923164700507e-07, Train Acc: 1.0\n",
            "Epoch 930/10000\n",
            "Step 0: Train Loss: 8.649467417853884e-07, Train Acc: 1.0\n",
            "Epoch 931/10000\n",
            "Step 0: Train Loss: 9.496689585830609e-07, Train Acc: 1.0\n",
            "Epoch 932/10000\n",
            "Step 0: Train Loss: 9.220682954946824e-07, Train Acc: 1.0\n",
            "Epoch 933/10000\n",
            "Step 0: Train Loss: 8.418210768468271e-07, Train Acc: 1.0\n",
            "Epoch 934/10000\n",
            "Step 0: Train Loss: 9.329431804872002e-07, Train Acc: 1.0\n",
            "Epoch 935/10000\n",
            "Step 0: Train Loss: 1.003405373012356e-06, Train Acc: 1.0\n",
            "Epoch 936/10000\n",
            "Step 0: Train Loss: 8.813140652819129e-07, Train Acc: 1.0\n",
            "Epoch 937/10000\n",
            "Step 0: Train Loss: 9.027003216033336e-07, Train Acc: 1.0\n",
            "Epoch 938/10000\n",
            "Step 0: Train Loss: 8.521091103830258e-07, Train Acc: 1.0\n",
            "Epoch 939/10000\n",
            "Step 0: Train Loss: 8.718254207451537e-07, Train Acc: 1.0\n",
            "Epoch 940/10000\n",
            "Step 0: Train Loss: 8.736860195313056e-07, Train Acc: 1.0\n",
            "Epoch 941/10000\n",
            "Step 0: Train Loss: 9.383309702570841e-07, Train Acc: 1.0\n",
            "Epoch 942/10000\n",
            "Step 0: Train Loss: 8.910777182791207e-07, Train Acc: 1.0\n",
            "Epoch 943/10000\n",
            "Step 0: Train Loss: 8.779872473496653e-07, Train Acc: 1.0\n",
            "Epoch 944/10000\n",
            "Step 0: Train Loss: 9.370183420287503e-07, Train Acc: 1.0\n",
            "Epoch 945/10000\n",
            "Step 0: Train Loss: 8.971205716079567e-07, Train Acc: 1.0\n",
            "Epoch 946/10000\n",
            "Step 0: Train Loss: 8.817805792205036e-07, Train Acc: 1.0\n",
            "Epoch 947/10000\n",
            "Step 0: Train Loss: 8.491888365824707e-07, Train Acc: 1.0\n",
            "Epoch 948/10000\n",
            "Step 0: Train Loss: 9.276251375922584e-07, Train Acc: 1.0\n",
            "Epoch 949/10000\n",
            "Step 0: Train Loss: 8.414400554102031e-07, Train Acc: 1.0\n",
            "Epoch 950/10000\n",
            "Step 0: Train Loss: 8.113991611935489e-07, Train Acc: 1.0\n",
            "Epoch 951/10000\n",
            "Step 0: Train Loss: 8.32153091323562e-07, Train Acc: 1.0\n",
            "Epoch 952/10000\n",
            "Step 0: Train Loss: 8.439547514171863e-07, Train Acc: 1.0\n",
            "Epoch 953/10000\n",
            "Step 0: Train Loss: 8.633030574856093e-07, Train Acc: 1.0\n",
            "Epoch 954/10000\n",
            "Step 0: Train Loss: 7.951641691761324e-07, Train Acc: 1.0\n",
            "Epoch 955/10000\n",
            "Step 0: Train Loss: 8.303888421323791e-07, Train Acc: 1.0\n",
            "Epoch 956/10000\n",
            "Step 0: Train Loss: 8.765948109612509e-07, Train Acc: 1.0\n",
            "Epoch 957/10000\n",
            "Step 0: Train Loss: 8.867378369359358e-07, Train Acc: 1.0\n",
            "Epoch 958/10000\n",
            "Step 0: Train Loss: 8.710266001799027e-07, Train Acc: 1.0\n",
            "Epoch 959/10000\n",
            "Step 0: Train Loss: 8.653750569465046e-07, Train Acc: 1.0\n",
            "Epoch 960/10000\n",
            "Step 0: Train Loss: 8.606447750025836e-07, Train Acc: 1.0\n",
            "Epoch 961/10000\n",
            "Step 0: Train Loss: 8.158461355378677e-07, Train Acc: 1.0\n",
            "Epoch 962/10000\n",
            "Step 0: Train Loss: 8.482342650495411e-07, Train Acc: 1.0\n",
            "Epoch 963/10000\n",
            "Step 0: Train Loss: 8.802169872978993e-07, Train Acc: 1.0\n",
            "Epoch 964/10000\n",
            "Step 0: Train Loss: 8.247158689300704e-07, Train Acc: 1.0\n",
            "Epoch 965/10000\n",
            "Step 0: Train Loss: 8.5877178435112e-07, Train Acc: 1.0\n",
            "Epoch 966/10000\n",
            "Step 0: Train Loss: 8.095512384898029e-07, Train Acc: 1.0\n",
            "Epoch 967/10000\n",
            "Step 0: Train Loss: 7.652436693206255e-07, Train Acc: 1.0\n",
            "Epoch 968/10000\n",
            "Step 0: Train Loss: 7.809790076862555e-07, Train Acc: 1.0\n",
            "Epoch 969/10000\n",
            "Step 0: Train Loss: 8.121631935864571e-07, Train Acc: 1.0\n",
            "Epoch 970/10000\n",
            "Step 0: Train Loss: 7.218526434371597e-07, Train Acc: 1.0\n",
            "Epoch 971/10000\n",
            "Step 0: Train Loss: 7.616197308379924e-07, Train Acc: 1.0\n",
            "Epoch 972/10000\n",
            "Step 0: Train Loss: 8.942124622990377e-07, Train Acc: 1.0\n",
            "Epoch 973/10000\n",
            "Step 0: Train Loss: 7.899915885900555e-07, Train Acc: 1.0\n",
            "Epoch 974/10000\n",
            "Step 0: Train Loss: 7.639679324711324e-07, Train Acc: 1.0\n",
            "Epoch 975/10000\n",
            "Step 0: Train Loss: 8.200900651900156e-07, Train Acc: 1.0\n",
            "Epoch 976/10000\n",
            "Step 0: Train Loss: 7.918136475382198e-07, Train Acc: 1.0\n",
            "Epoch 977/10000\n",
            "Step 0: Train Loss: 7.607598035974661e-07, Train Acc: 1.0\n",
            "Epoch 978/10000\n",
            "Step 0: Train Loss: 7.63359594202484e-07, Train Acc: 1.0\n",
            "Epoch 979/10000\n",
            "Step 0: Train Loss: 7.678297606616979e-07, Train Acc: 1.0\n",
            "Epoch 980/10000\n",
            "Step 0: Train Loss: 7.32734235953103e-07, Train Acc: 1.0\n",
            "Epoch 981/10000\n",
            "Step 0: Train Loss: 7.879158374635153e-07, Train Acc: 1.0\n",
            "Epoch 982/10000\n",
            "Step 0: Train Loss: 7.196825890787295e-07, Train Acc: 1.0\n",
            "Epoch 983/10000\n",
            "Step 0: Train Loss: 7.200039817689685e-07, Train Acc: 1.0\n",
            "Epoch 984/10000\n",
            "Step 0: Train Loss: 8.003027005543117e-07, Train Acc: 1.0\n",
            "Epoch 985/10000\n",
            "Step 0: Train Loss: 7.728595505795965e-07, Train Acc: 1.0\n",
            "Epoch 986/10000\n",
            "Step 0: Train Loss: 7.416164180540363e-07, Train Acc: 1.0\n",
            "Epoch 987/10000\n",
            "Step 0: Train Loss: 7.927568503873772e-07, Train Acc: 1.0\n",
            "Epoch 988/10000\n",
            "Step 0: Train Loss: 7.678539759581327e-07, Train Acc: 1.0\n",
            "Epoch 989/10000\n",
            "Step 0: Train Loss: 7.28063184851635e-07, Train Acc: 1.0\n",
            "Epoch 990/10000\n",
            "Step 0: Train Loss: 7.260725283231295e-07, Train Acc: 1.0\n",
            "Epoch 991/10000\n",
            "Step 0: Train Loss: 7.471716116924654e-07, Train Acc: 1.0\n",
            "Epoch 992/10000\n",
            "Step 0: Train Loss: 7.32903345124214e-07, Train Acc: 1.0\n",
            "Epoch 993/10000\n",
            "Step 0: Train Loss: 6.937543730600737e-07, Train Acc: 1.0\n",
            "Epoch 994/10000\n",
            "Step 0: Train Loss: 7.69642554132588e-07, Train Acc: 1.0\n",
            "Epoch 995/10000\n",
            "Step 0: Train Loss: 7.611660635120643e-07, Train Acc: 1.0\n",
            "Epoch 996/10000\n",
            "Step 0: Train Loss: 6.969975174797582e-07, Train Acc: 1.0\n",
            "Epoch 997/10000\n",
            "Step 0: Train Loss: 7.095028422554606e-07, Train Acc: 1.0\n",
            "Epoch 998/10000\n",
            "Step 0: Train Loss: 7.436781856995367e-07, Train Acc: 1.0\n",
            "Epoch 999/10000\n",
            "Step 0: Train Loss: 7.725976161054859e-07, Train Acc: 1.0\n",
            "Epoch 1000/10000\n",
            "Step 0: Train Loss: 7.304944915631495e-07, Train Acc: 1.0\n",
            "Epoch 1001/10000\n",
            "Step 0: Train Loss: 7.191216582214111e-07, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 1000 1024 1.271425740455901\n",
            "Epoch index and hidden dimension and ratio: 1000 20 1.613843825985002\n",
            "Epoch index and hidden dimension and ratio: 1000 20 2.0439038510613177\n",
            "Epoch index and hidden dimension and ratio: 1000 20 2.8489183730530234\n",
            "MI(X;T): [12.933805791777754, 10.092499353517006, 7.962748313582428, 5.042165875013669], MI(Y;T): [3.309453359302389, 3.258791345522921, 3.1797418844850256, 2.7463862750212042]\n",
            "Epoch index and hidden dimension and ratio: 1000 1024 1.2714351385719167\n",
            "Epoch index and hidden dimension and ratio: 1000 20 1.6138789742183368\n",
            "Epoch index and hidden dimension and ratio: 1000 20 2.0439579357915223\n",
            "Epoch index and hidden dimension and ratio: 1000 20 2.8490368259301246\n",
            "MI(X;T): [12.933998911735548, 10.09149640309614, 7.962353236384983, 5.0421800133786645], MI(Y;T): [3.309508384002685, 3.2585931032100284, 3.179729564808629, 2.74570357773833]\n",
            "Epoch index and hidden dimension and ratio: 1000 1024 1.2714452709157458\n",
            "Epoch index and hidden dimension and ratio: 1000 20 1.613912820665252\n",
            "Epoch index and hidden dimension and ratio: 1000 20 2.0439946270756284\n",
            "Epoch index and hidden dimension and ratio: 1000 20 2.849127968838338\n",
            "MI(X;T): [12.933940635078358, 10.09142082298957, 7.962213828830247, 5.04220672881355], MI(Y;T): [3.309508384002685, 3.2586672238437826, 3.1798507005460683, 2.745663625848273]\n",
            "Epoch index and hidden dimension and ratio: 1000 1024 1.2714549627228868\n",
            "Epoch index and hidden dimension and ratio: 1000 20 1.6139403751444712\n",
            "Epoch index and hidden dimension and ratio: 1000 20 2.044024589508324\n",
            "Epoch index and hidden dimension and ratio: 1000 20 2.849203647065375\n",
            "MI(X;T): [12.933736656110655, 10.092680458601578, 7.961903002825348, 5.041761350442949], MI(Y;T): [3.30945335930239, 3.258771202811481, 3.1796415060650274, 2.74588764206356]\n",
            "Epoch index and hidden dimension and ratio: 1000 1024 1.2714646545300277\n",
            "Epoch index and hidden dimension and ratio: 1000 20 1.6139669532838756\n",
            "Epoch index and hidden dimension and ratio: 1000 20 2.0440476961301486\n",
            "Epoch index and hidden dimension and ratio: 1000 20 2.8492714284339384\n",
            "MI(X;T): [12.933636656110655, 10.092525910584772, 7.961979441695837, 5.040663955818651], MI(Y;T): [3.30945335930239, 3.258671202811481, 3.179291618803955, 2.7458417773313766]\n",
            "Epoch index and hidden dimension and ratio: 1000 1024 1.2714724373448532\n",
            "Epoch index and hidden dimension and ratio: 1000 20 1.6139872394555843\n",
            "Epoch index and hidden dimension and ratio: 1000 20 2.044076896806081\n",
            "Epoch index and hidden dimension and ratio: 1000 20 2.8493415130528903\n",
            "MI(X;T): [12.933798911735547, 10.092957733218967, 7.9616628484964425, 5.040462346605835], MI(Y;T): [3.30945335930239, 3.258698096483534, 3.179156980933652, 2.745831670980942]\n",
            "Epoch 1002/10000\n",
            "Step 0: Train Loss: 7.774372647872951e-07, Train Acc: 1.0\n",
            "Epoch 1003/10000\n",
            "Step 0: Train Loss: 7.425227863677719e-07, Train Acc: 1.0\n",
            "Epoch 1004/10000\n",
            "Step 0: Train Loss: 7.052348678371345e-07, Train Acc: 1.0\n",
            "Epoch 1005/10000\n",
            "Step 0: Train Loss: 6.752302397217136e-07, Train Acc: 1.0\n",
            "Epoch 1006/10000\n",
            "Step 0: Train Loss: 6.984875540183566e-07, Train Acc: 1.0\n",
            "Epoch 1007/10000\n",
            "Step 0: Train Loss: 6.339489573292667e-07, Train Acc: 1.0\n",
            "Epoch 1008/10000\n",
            "Step 0: Train Loss: 6.916212100804842e-07, Train Acc: 1.0\n",
            "Epoch 1009/10000\n",
            "Step 0: Train Loss: 7.175844984885771e-07, Train Acc: 1.0\n",
            "Epoch 1010/10000\n",
            "Step 0: Train Loss: 7.129594905563863e-07, Train Acc: 1.0\n",
            "Epoch 1011/10000\n",
            "Step 0: Train Loss: 6.258185294427676e-07, Train Acc: 1.0\n",
            "Epoch 1012/10000\n",
            "Step 0: Train Loss: 6.093206934565387e-07, Train Acc: 1.0\n",
            "Epoch 1013/10000\n",
            "Step 0: Train Loss: 6.834322334725584e-07, Train Acc: 1.0\n",
            "Epoch 1014/10000\n",
            "Step 0: Train Loss: 6.55394160276046e-07, Train Acc: 1.0\n",
            "Epoch 1015/10000\n",
            "Step 0: Train Loss: 6.691380463053065e-07, Train Acc: 1.0\n",
            "Epoch 1016/10000\n",
            "Step 0: Train Loss: 7.228774165923824e-07, Train Acc: 1.0\n",
            "Epoch 1017/10000\n",
            "Step 0: Train Loss: 6.888192842779972e-07, Train Acc: 1.0\n",
            "Epoch 1018/10000\n",
            "Step 0: Train Loss: 6.682678304059664e-07, Train Acc: 1.0\n",
            "Epoch 1019/10000\n",
            "Step 0: Train Loss: 7.005980933172395e-07, Train Acc: 1.0\n",
            "Epoch 1020/10000\n",
            "Step 0: Train Loss: 6.267122216740972e-07, Train Acc: 1.0\n",
            "Epoch 1021/10000\n",
            "Step 0: Train Loss: 6.359161943692015e-07, Train Acc: 1.0\n",
            "Epoch 1022/10000\n",
            "Step 0: Train Loss: 6.64489618884545e-07, Train Acc: 1.0\n",
            "Epoch 1023/10000\n",
            "Step 0: Train Loss: 6.439857997975196e-07, Train Acc: 1.0\n",
            "Epoch 1024/10000\n",
            "Step 0: Train Loss: 6.307309377007186e-07, Train Acc: 1.0\n",
            "Epoch 1025/10000\n",
            "Step 0: Train Loss: 6.362617455124564e-07, Train Acc: 1.0\n",
            "Epoch 1026/10000\n",
            "Step 0: Train Loss: 6.47371507511707e-07, Train Acc: 1.0\n",
            "Epoch 1027/10000\n",
            "Step 0: Train Loss: 6.346884902086458e-07, Train Acc: 1.0\n",
            "Epoch 1028/10000\n",
            "Step 0: Train Loss: 6.390040425685584e-07, Train Acc: 1.0\n",
            "Epoch 1029/10000\n",
            "Step 0: Train Loss: 6.361547661981604e-07, Train Acc: 1.0\n",
            "Epoch 1030/10000\n",
            "Step 0: Train Loss: 6.838477020210121e-07, Train Acc: 1.0\n",
            "Epoch 1031/10000\n",
            "Step 0: Train Loss: 5.529956865757413e-07, Train Acc: 1.0\n",
            "Epoch 1032/10000\n",
            "Step 0: Train Loss: 6.313863423201838e-07, Train Acc: 1.0\n",
            "Epoch 1033/10000\n",
            "Step 0: Train Loss: 6.444755058510054e-07, Train Acc: 1.0\n",
            "Epoch 1034/10000\n",
            "Step 0: Train Loss: 6.02323382281611e-07, Train Acc: 1.0\n",
            "Epoch 1035/10000\n",
            "Step 0: Train Loss: 6.287512519520533e-07, Train Acc: 1.0\n",
            "Epoch 1036/10000\n",
            "Step 0: Train Loss: 6.282272124735755e-07, Train Acc: 1.0\n",
            "Epoch 1037/10000\n",
            "Step 0: Train Loss: 6.31421357866202e-07, Train Acc: 1.0\n",
            "Epoch 1038/10000\n",
            "Step 0: Train Loss: 6.174633426780929e-07, Train Acc: 1.0\n",
            "Epoch 1039/10000\n",
            "Step 0: Train Loss: 5.854672053828835e-07, Train Acc: 1.0\n",
            "Epoch 1040/10000\n",
            "Step 0: Train Loss: 6.038479796188767e-07, Train Acc: 1.0\n",
            "Epoch 1041/10000\n",
            "Step 0: Train Loss: 6.540352615047595e-07, Train Acc: 1.0\n",
            "Epoch 1042/10000\n",
            "Step 0: Train Loss: 5.953501158728614e-07, Train Acc: 1.0\n",
            "Epoch 1043/10000\n",
            "Step 0: Train Loss: 6.001057499815943e-07, Train Acc: 1.0\n",
            "Epoch 1044/10000\n",
            "Step 0: Train Loss: 5.830590907862643e-07, Train Acc: 1.0\n",
            "Epoch 1045/10000\n",
            "Step 0: Train Loss: 6.036941044840205e-07, Train Acc: 1.0\n",
            "Epoch 1046/10000\n",
            "Step 0: Train Loss: 5.888527425668144e-07, Train Acc: 1.0\n",
            "Epoch 1047/10000\n",
            "Step 0: Train Loss: 6.381213211170689e-07, Train Acc: 1.0\n",
            "Epoch 1048/10000\n",
            "Step 0: Train Loss: 6.148513875814388e-07, Train Acc: 1.0\n",
            "Epoch 1049/10000\n",
            "Step 0: Train Loss: 5.699945404558093e-07, Train Acc: 1.0\n",
            "Epoch 1050/10000\n",
            "Step 0: Train Loss: 5.599216592599987e-07, Train Acc: 1.0\n",
            "Epoch 1051/10000\n",
            "Step 0: Train Loss: 6.043734970262449e-07, Train Acc: 1.0\n",
            "Epoch 1052/10000\n",
            "Step 0: Train Loss: 5.658575901179574e-07, Train Acc: 1.0\n",
            "Epoch 1053/10000\n",
            "Step 0: Train Loss: 6.500176823465154e-07, Train Acc: 1.0\n",
            "Epoch 1054/10000\n",
            "Step 0: Train Loss: 5.723547360503289e-07, Train Acc: 1.0\n",
            "Epoch 1055/10000\n",
            "Step 0: Train Loss: 5.59707132197218e-07, Train Acc: 1.0\n",
            "Epoch 1056/10000\n",
            "Step 0: Train Loss: 5.636640594275377e-07, Train Acc: 1.0\n",
            "Epoch 1057/10000\n",
            "Step 0: Train Loss: 5.40645544333529e-07, Train Acc: 1.0\n",
            "Epoch 1058/10000\n",
            "Step 0: Train Loss: 5.505387434823206e-07, Train Acc: 1.0\n",
            "Epoch 1059/10000\n",
            "Step 0: Train Loss: 5.577879846896394e-07, Train Acc: 1.0\n",
            "Epoch 1060/10000\n",
            "Step 0: Train Loss: 5.781832896900596e-07, Train Acc: 1.0\n",
            "Epoch 1061/10000\n",
            "Step 0: Train Loss: 5.337909669833607e-07, Train Acc: 1.0\n",
            "Epoch 1062/10000\n",
            "Step 0: Train Loss: 5.392744242271874e-07, Train Acc: 1.0\n",
            "Epoch 1063/10000\n",
            "Step 0: Train Loss: 5.452350819723506e-07, Train Acc: 1.0\n",
            "Epoch 1064/10000\n",
            "Step 0: Train Loss: 5.905458806410024e-07, Train Acc: 1.0\n",
            "Epoch 1065/10000\n",
            "Step 0: Train Loss: 5.329924874786229e-07, Train Acc: 1.0\n",
            "Epoch 1066/10000\n",
            "Step 0: Train Loss: 5.706381216441514e-07, Train Acc: 1.0\n",
            "Epoch 1067/10000\n",
            "Step 0: Train Loss: 5.879589934920659e-07, Train Acc: 1.0\n",
            "Epoch 1068/10000\n",
            "Step 0: Train Loss: 5.499911708284344e-07, Train Acc: 1.0\n",
            "Epoch 1069/10000\n",
            "Step 0: Train Loss: 5.293331923894584e-07, Train Acc: 1.0\n",
            "Epoch 1070/10000\n",
            "Step 0: Train Loss: 5.590632667917816e-07, Train Acc: 1.0\n",
            "Epoch 1071/10000\n",
            "Step 0: Train Loss: 4.724232098851644e-07, Train Acc: 1.0\n",
            "Epoch 1072/10000\n",
            "Step 0: Train Loss: 5.312277266966703e-07, Train Acc: 1.0\n",
            "Epoch 1073/10000\n",
            "Step 0: Train Loss: 5.736069965678325e-07, Train Acc: 1.0\n",
            "Epoch 1074/10000\n",
            "Step 0: Train Loss: 6.094636546549737e-07, Train Acc: 1.0\n",
            "Epoch 1075/10000\n",
            "Step 0: Train Loss: 5.788634211967292e-07, Train Acc: 1.0\n",
            "Epoch 1076/10000\n",
            "Step 0: Train Loss: 5.201538328947208e-07, Train Acc: 1.0\n",
            "Epoch 1077/10000\n",
            "Step 0: Train Loss: 5.492406103257963e-07, Train Acc: 1.0\n",
            "Epoch 1078/10000\n",
            "Step 0: Train Loss: 5.424095093076176e-07, Train Acc: 1.0\n",
            "Epoch 1079/10000\n",
            "Step 0: Train Loss: 5.665010576194618e-07, Train Acc: 1.0\n",
            "Epoch 1080/10000\n",
            "Step 0: Train Loss: 5.219542345002992e-07, Train Acc: 1.0\n",
            "Epoch 1081/10000\n",
            "Step 0: Train Loss: 5.082926577415492e-07, Train Acc: 1.0\n",
            "Epoch 1082/10000\n",
            "Step 0: Train Loss: 5.338392838893924e-07, Train Acc: 1.0\n",
            "Epoch 1083/10000\n",
            "Step 0: Train Loss: 5.65035293220717e-07, Train Acc: 1.0\n",
            "Epoch 1084/10000\n",
            "Step 0: Train Loss: 5.170186909708718e-07, Train Acc: 1.0\n",
            "Epoch 1085/10000\n",
            "Step 0: Train Loss: 5.017722060074448e-07, Train Acc: 1.0\n",
            "Epoch 1086/10000\n",
            "Step 0: Train Loss: 5.547954060602933e-07, Train Acc: 1.0\n",
            "Epoch 1087/10000\n",
            "Step 0: Train Loss: 5.015209012526611e-07, Train Acc: 1.0\n",
            "Epoch 1088/10000\n",
            "Step 0: Train Loss: 5.178293918106647e-07, Train Acc: 1.0\n",
            "Epoch 1089/10000\n",
            "Step 0: Train Loss: 5.053363452134363e-07, Train Acc: 1.0\n",
            "Epoch 1090/10000\n",
            "Step 0: Train Loss: 5.605771775663015e-07, Train Acc: 1.0\n",
            "Epoch 1091/10000\n",
            "Step 0: Train Loss: 5.146820285517606e-07, Train Acc: 1.0\n",
            "Epoch 1092/10000\n",
            "Step 0: Train Loss: 5.046684918852407e-07, Train Acc: 1.0\n",
            "Epoch 1093/10000\n",
            "Step 0: Train Loss: 4.790273919752508e-07, Train Acc: 1.0\n",
            "Epoch 1094/10000\n",
            "Step 0: Train Loss: 5.021658466830559e-07, Train Acc: 1.0\n",
            "Epoch 1095/10000\n",
            "Step 0: Train Loss: 4.7614187792532903e-07, Train Acc: 1.0\n",
            "Epoch 1096/10000\n",
            "Step 0: Train Loss: 4.819718810722406e-07, Train Acc: 1.0\n",
            "Epoch 1097/10000\n",
            "Step 0: Train Loss: 5.379510525926889e-07, Train Acc: 1.0\n",
            "Epoch 1098/10000\n",
            "Step 0: Train Loss: 4.825554924536846e-07, Train Acc: 1.0\n",
            "Epoch 1099/10000\n",
            "Step 0: Train Loss: 4.826156327908393e-07, Train Acc: 1.0\n",
            "Epoch 1100/10000\n",
            "Step 0: Train Loss: 4.856900090999261e-07, Train Acc: 1.0\n",
            "Epoch 1101/10000\n",
            "Step 0: Train Loss: 4.763214462855103e-07, Train Acc: 1.0\n",
            "Epoch 1102/10000\n",
            "Step 0: Train Loss: 4.969202223037428e-07, Train Acc: 1.0\n",
            "Epoch 1103/10000\n",
            "Step 0: Train Loss: 5.112243570692954e-07, Train Acc: 1.0\n",
            "Epoch 1104/10000\n",
            "Step 0: Train Loss: 4.912100166620803e-07, Train Acc: 1.0\n",
            "Epoch 1105/10000\n",
            "Step 0: Train Loss: 4.467571272925852e-07, Train Acc: 1.0\n",
            "Epoch 1106/10000\n",
            "Step 0: Train Loss: 5.25208292856405e-07, Train Acc: 1.0\n",
            "Epoch 1107/10000\n",
            "Step 0: Train Loss: 4.824842108064331e-07, Train Acc: 1.0\n",
            "Epoch 1108/10000\n",
            "Step 0: Train Loss: 4.661173136355501e-07, Train Acc: 1.0\n",
            "Epoch 1109/10000\n",
            "Step 0: Train Loss: 4.249069718298415e-07, Train Acc: 1.0\n",
            "Epoch 1110/10000\n",
            "Step 0: Train Loss: 4.550187782115245e-07, Train Acc: 1.0\n",
            "Epoch 1111/10000\n",
            "Step 0: Train Loss: 4.380677864901372e-07, Train Acc: 1.0\n",
            "Epoch 1112/10000\n",
            "Step 0: Train Loss: 4.6213557425289764e-07, Train Acc: 1.0\n",
            "Epoch 1113/10000\n",
            "Step 0: Train Loss: 4.413695648963767e-07, Train Acc: 1.0\n",
            "Epoch 1114/10000\n",
            "Step 0: Train Loss: 4.77763762773975e-07, Train Acc: 1.0\n",
            "Epoch 1115/10000\n",
            "Step 0: Train Loss: 4.375785067622928e-07, Train Acc: 1.0\n",
            "Epoch 1116/10000\n",
            "Step 0: Train Loss: 4.966219080415613e-07, Train Acc: 1.0\n",
            "Epoch 1117/10000\n",
            "Step 0: Train Loss: 4.563656545997219e-07, Train Acc: 1.0\n",
            "Epoch 1118/10000\n",
            "Step 0: Train Loss: 4.788005867339962e-07, Train Acc: 1.0\n",
            "Epoch 1119/10000\n",
            "Step 0: Train Loss: 4.558771422580321e-07, Train Acc: 1.0\n",
            "Epoch 1120/10000\n",
            "Step 0: Train Loss: 4.3363255031181325e-07, Train Acc: 1.0\n",
            "Epoch 1121/10000\n",
            "Step 0: Train Loss: 4.6912114726183063e-07, Train Acc: 1.0\n",
            "Epoch 1122/10000\n",
            "Step 0: Train Loss: 4.320949358316284e-07, Train Acc: 1.0\n",
            "Epoch 1123/10000\n",
            "Step 0: Train Loss: 4.840934479943826e-07, Train Acc: 1.0\n",
            "Epoch 1124/10000\n",
            "Step 0: Train Loss: 4.668202109314734e-07, Train Acc: 1.0\n",
            "Epoch 1125/10000\n",
            "Step 0: Train Loss: 4.325598013110721e-07, Train Acc: 1.0\n",
            "Epoch 1126/10000\n",
            "Step 0: Train Loss: 4.436222695858305e-07, Train Acc: 1.0\n",
            "Epoch 1127/10000\n",
            "Step 0: Train Loss: 4.7461594476772007e-07, Train Acc: 1.0\n",
            "Epoch 1128/10000\n",
            "Step 0: Train Loss: 4.379721474379039e-07, Train Acc: 1.0\n",
            "Epoch 1129/10000\n",
            "Step 0: Train Loss: 4.1162715547216067e-07, Train Acc: 1.0\n",
            "Epoch 1130/10000\n",
            "Step 0: Train Loss: 4.5727173869636317e-07, Train Acc: 1.0\n",
            "Epoch 1131/10000\n",
            "Step 0: Train Loss: 4.550306016426475e-07, Train Acc: 1.0\n",
            "Epoch 1132/10000\n",
            "Step 0: Train Loss: 4.270525550964521e-07, Train Acc: 1.0\n",
            "Epoch 1133/10000\n",
            "Step 0: Train Loss: 3.982283089953853e-07, Train Acc: 1.0\n",
            "Epoch 1134/10000\n",
            "Step 0: Train Loss: 4.4717481273437443e-07, Train Acc: 1.0\n",
            "Epoch 1135/10000\n",
            "Step 0: Train Loss: 4.4777087282454886e-07, Train Acc: 1.0\n",
            "Epoch 1136/10000\n",
            "Step 0: Train Loss: 4.527062174020102e-07, Train Acc: 1.0\n",
            "Epoch 1137/10000\n",
            "Step 0: Train Loss: 4.499280805703165e-07, Train Acc: 1.0\n",
            "Epoch 1138/10000\n",
            "Step 0: Train Loss: 3.9921746974869166e-07, Train Acc: 1.0\n",
            "Epoch 1139/10000\n",
            "Step 0: Train Loss: 4.1222344293601054e-07, Train Acc: 1.0\n",
            "Epoch 1140/10000\n",
            "Step 0: Train Loss: 4.5885704480497225e-07, Train Acc: 1.0\n",
            "Epoch 1141/10000\n",
            "Step 0: Train Loss: 4.3563562712733983e-07, Train Acc: 1.0\n",
            "Epoch 1142/10000\n",
            "Step 0: Train Loss: 4.39080878322784e-07, Train Acc: 1.0\n",
            "Epoch 1143/10000\n",
            "Step 0: Train Loss: 4.2172413827756827e-07, Train Acc: 1.0\n",
            "Epoch 1144/10000\n",
            "Step 0: Train Loss: 4.500480770275317e-07, Train Acc: 1.0\n",
            "Epoch 1145/10000\n",
            "Step 0: Train Loss: 4.012800332020561e-07, Train Acc: 1.0\n",
            "Epoch 1146/10000\n",
            "Step 0: Train Loss: 4.1697904862303403e-07, Train Acc: 1.0\n",
            "Epoch 1147/10000\n",
            "Step 0: Train Loss: 4.0915981003308843e-07, Train Acc: 1.0\n",
            "Epoch 1148/10000\n",
            "Step 0: Train Loss: 4.1442862652729673e-07, Train Acc: 1.0\n",
            "Epoch 1149/10000\n",
            "Step 0: Train Loss: 4.178735082405183e-07, Train Acc: 1.0\n",
            "Epoch 1150/10000\n",
            "Step 0: Train Loss: 4.018522474780184e-07, Train Acc: 1.0\n",
            "Epoch 1151/10000\n",
            "Step 0: Train Loss: 4.359809224752098e-07, Train Acc: 1.0\n",
            "Epoch 1152/10000\n",
            "Step 0: Train Loss: 4.3980796249343257e-07, Train Acc: 1.0\n",
            "Epoch 1153/10000\n",
            "Step 0: Train Loss: 4.214617774778162e-07, Train Acc: 1.0\n",
            "Epoch 1154/10000\n",
            "Step 0: Train Loss: 4.110908946586278e-07, Train Acc: 1.0\n",
            "Epoch 1155/10000\n",
            "Step 0: Train Loss: 4.214617490561068e-07, Train Acc: 1.0\n",
            "Epoch 1156/10000\n",
            "Step 0: Train Loss: 3.955341014716396e-07, Train Acc: 1.0\n",
            "Epoch 1157/10000\n",
            "Step 0: Train Loss: 4.092547669642954e-07, Train Acc: 1.0\n",
            "Epoch 1158/10000\n",
            "Step 0: Train Loss: 4.097675798675482e-07, Train Acc: 1.0\n",
            "Epoch 1159/10000\n",
            "Step 0: Train Loss: 4.0292488279192185e-07, Train Acc: 1.0\n",
            "Epoch 1160/10000\n",
            "Step 0: Train Loss: 3.737070528586628e-07, Train Acc: 1.0\n",
            "Epoch 1161/10000\n",
            "Step 0: Train Loss: 3.526311331825127e-07, Train Acc: 1.0\n",
            "Epoch 1162/10000\n",
            "Step 0: Train Loss: 4.080866347067058e-07, Train Acc: 1.0\n",
            "Epoch 1163/10000\n",
            "Step 0: Train Loss: 3.5428791989033925e-07, Train Acc: 1.0\n",
            "Epoch 1164/10000\n",
            "Step 0: Train Loss: 3.9148096675489796e-07, Train Acc: 1.0\n",
            "Epoch 1165/10000\n",
            "Step 0: Train Loss: 3.781891564358375e-07, Train Acc: 1.0\n",
            "Epoch 1166/10000\n",
            "Step 0: Train Loss: 4.021026995815191e-07, Train Acc: 1.0\n",
            "Epoch 1167/10000\n",
            "Step 0: Train Loss: 3.7151397691559396e-07, Train Acc: 1.0\n",
            "Epoch 1168/10000\n",
            "Step 0: Train Loss: 3.868558735575789e-07, Train Acc: 1.0\n",
            "Epoch 1169/10000\n",
            "Step 0: Train Loss: 4.07121149237355e-07, Train Acc: 1.0\n",
            "Epoch 1170/10000\n",
            "Step 0: Train Loss: 3.6512415135803167e-07, Train Acc: 1.0\n",
            "Epoch 1171/10000\n",
            "Step 0: Train Loss: 3.610830390243791e-07, Train Acc: 1.0\n",
            "Epoch 1172/10000\n",
            "Step 0: Train Loss: 3.4417934102748404e-07, Train Acc: 1.0\n",
            "Epoch 1173/10000\n",
            "Step 0: Train Loss: 3.501039884667989e-07, Train Acc: 1.0\n",
            "Epoch 1174/10000\n",
            "Step 0: Train Loss: 3.8820306258458004e-07, Train Acc: 1.0\n",
            "Epoch 1175/10000\n",
            "Step 0: Train Loss: 3.353459874233522e-07, Train Acc: 1.0\n",
            "Epoch 1176/10000\n",
            "Step 0: Train Loss: 3.55730634282736e-07, Train Acc: 1.0\n",
            "Epoch 1177/10000\n",
            "Step 0: Train Loss: 3.873800267228944e-07, Train Acc: 1.0\n",
            "Epoch 1178/10000\n",
            "Step 0: Train Loss: 3.9152851627477503e-07, Train Acc: 1.0\n",
            "Epoch 1179/10000\n",
            "Step 0: Train Loss: 3.6737708342116093e-07, Train Acc: 1.0\n",
            "Epoch 1180/10000\n",
            "Step 0: Train Loss: 3.7055991697343416e-07, Train Acc: 1.0\n",
            "Epoch 1181/10000\n",
            "Step 0: Train Loss: 3.42570132261244e-07, Train Acc: 1.0\n",
            "Epoch 1182/10000\n",
            "Step 0: Train Loss: 3.4689719541347586e-07, Train Acc: 1.0\n",
            "Epoch 1183/10000\n",
            "Step 0: Train Loss: 3.6885546705889283e-07, Train Acc: 1.0\n",
            "Epoch 1184/10000\n",
            "Step 0: Train Loss: 3.4677808002925303e-07, Train Acc: 1.0\n",
            "Epoch 1185/10000\n",
            "Step 0: Train Loss: 3.8346962583091226e-07, Train Acc: 1.0\n",
            "Epoch 1186/10000\n",
            "Step 0: Train Loss: 3.602723666062957e-07, Train Acc: 1.0\n",
            "Epoch 1187/10000\n",
            "Step 0: Train Loss: 3.57113407289944e-07, Train Acc: 1.0\n",
            "Epoch 1188/10000\n",
            "Step 0: Train Loss: 3.647071480372688e-07, Train Acc: 1.0\n",
            "Epoch 1189/10000\n",
            "Step 0: Train Loss: 3.452642545198614e-07, Train Acc: 1.0\n",
            "Epoch 1190/10000\n",
            "Step 0: Train Loss: 3.6707916706291144e-07, Train Acc: 1.0\n",
            "Epoch 1191/10000\n",
            "Step 0: Train Loss: 3.5235680684309045e-07, Train Acc: 1.0\n",
            "Epoch 1192/10000\n",
            "Step 0: Train Loss: 3.3513120456518664e-07, Train Acc: 1.0\n",
            "Epoch 1193/10000\n",
            "Step 0: Train Loss: 3.343445200698625e-07, Train Acc: 1.0\n",
            "Epoch 1194/10000\n",
            "Step 0: Train Loss: 3.7445784073497634e-07, Train Acc: 1.0\n",
            "Epoch 1195/10000\n",
            "Step 0: Train Loss: 3.383021294212085e-07, Train Acc: 1.0\n",
            "Epoch 1196/10000\n",
            "Step 0: Train Loss: 3.241882780002925e-07, Train Acc: 1.0\n",
            "Epoch 1197/10000\n",
            "Step 0: Train Loss: 3.283124385689007e-07, Train Acc: 1.0\n",
            "Epoch 1198/10000\n",
            "Step 0: Train Loss: 3.393035399312794e-07, Train Acc: 1.0\n",
            "Epoch 1199/10000\n",
            "Step 0: Train Loss: 3.451089582995337e-07, Train Acc: 1.0\n",
            "Epoch 1200/10000\n",
            "Step 0: Train Loss: 3.376109987129894e-07, Train Acc: 1.0\n",
            "Epoch 1201/10000\n",
            "Step 0: Train Loss: 3.5152248756276094e-07, Train Acc: 1.0\n",
            "Epoch 1202/10000\n",
            "Step 0: Train Loss: 3.543355262536352e-07, Train Acc: 1.0\n",
            "Epoch 1203/10000\n",
            "Step 0: Train Loss: 3.504734991111036e-07, Train Acc: 1.0\n",
            "Epoch 1204/10000\n",
            "Step 0: Train Loss: 3.722885821844102e-07, Train Acc: 1.0\n",
            "Epoch 1205/10000\n",
            "Step 0: Train Loss: 3.192172073340771e-07, Train Acc: 1.0\n",
            "Epoch 1206/10000\n",
            "Step 0: Train Loss: 3.0686712193528365e-07, Train Acc: 1.0\n",
            "Epoch 1207/10000\n",
            "Step 0: Train Loss: 3.2239967140412773e-07, Train Acc: 1.0\n",
            "Epoch 1208/10000\n",
            "Step 0: Train Loss: 3.758406705856032e-07, Train Acc: 1.0\n",
            "Epoch 1209/10000\n",
            "Step 0: Train Loss: 3.2771666269582056e-07, Train Acc: 1.0\n",
            "Epoch 1210/10000\n",
            "Step 0: Train Loss: 3.293616828159429e-07, Train Acc: 1.0\n",
            "Epoch 1211/10000\n",
            "Step 0: Train Loss: 3.1040775638757623e-07, Train Acc: 1.0\n",
            "Epoch 1212/10000\n",
            "Step 0: Train Loss: 3.380282009857183e-07, Train Acc: 1.0\n",
            "Epoch 1213/10000\n",
            "Step 0: Train Loss: 3.353699185026926e-07, Train Acc: 1.0\n",
            "Epoch 1214/10000\n",
            "Step 0: Train Loss: 3.1390047183776915e-07, Train Acc: 1.0\n",
            "Epoch 1215/10000\n",
            "Step 0: Train Loss: 3.290636527708557e-07, Train Acc: 1.0\n",
            "Epoch 1216/10000\n",
            "Step 0: Train Loss: 3.34392495915381e-07, Train Acc: 1.0\n",
            "Epoch 1217/10000\n",
            "Step 0: Train Loss: 3.180728356255713e-07, Train Acc: 1.0\n",
            "Epoch 1218/10000\n",
            "Step 0: Train Loss: 3.406865118904534e-07, Train Acc: 1.0\n",
            "Epoch 1219/10000\n",
            "Step 0: Train Loss: 3.1792944810149493e-07, Train Acc: 1.0\n",
            "Epoch 1220/10000\n",
            "Step 0: Train Loss: 3.3314066172351886e-07, Train Acc: 1.0\n",
            "Epoch 1221/10000\n",
            "Step 0: Train Loss: 3.0723666100129776e-07, Train Acc: 1.0\n",
            "Epoch 1222/10000\n",
            "Step 0: Train Loss: 3.26715536402844e-07, Train Acc: 1.0\n",
            "Epoch 1223/10000\n",
            "Step 0: Train Loss: 3.1219568086271465e-07, Train Acc: 1.0\n",
            "Epoch 1224/10000\n",
            "Step 0: Train Loss: 3.1123002486310725e-07, Train Acc: 1.0\n",
            "Epoch 1225/10000\n",
            "Step 0: Train Loss: 3.1950312973094697e-07, Train Acc: 1.0\n",
            "Epoch 1226/10000\n",
            "Step 0: Train Loss: 2.8003347551930347e-07, Train Acc: 1.0\n",
            "Epoch 1227/10000\n",
            "Step 0: Train Loss: 3.274305697686941e-07, Train Acc: 1.0\n",
            "Epoch 1228/10000\n",
            "Step 0: Train Loss: 2.904878613207984e-07, Train Acc: 1.0\n",
            "Epoch 1229/10000\n",
            "Step 0: Train Loss: 2.91525111606461e-07, Train Acc: 1.0\n",
            "Epoch 1230/10000\n",
            "Step 0: Train Loss: 3.1741711836730246e-07, Train Acc: 1.0\n",
            "Epoch 1231/10000\n",
            "Step 0: Train Loss: 3.172383173932758e-07, Train Acc: 1.0\n",
            "Epoch 1232/10000\n",
            "Step 0: Train Loss: 2.99202071118998e-07, Train Acc: 1.0\n",
            "Epoch 1233/10000\n",
            "Step 0: Train Loss: 2.7593259233071876e-07, Train Acc: 1.0\n",
            "Epoch 1234/10000\n",
            "Step 0: Train Loss: 2.766597901882051e-07, Train Acc: 1.0\n",
            "Epoch 1235/10000\n",
            "Step 0: Train Loss: 2.996905266172689e-07, Train Acc: 1.0\n",
            "Epoch 1236/10000\n",
            "Step 0: Train Loss: 2.8968929655093234e-07, Train Acc: 1.0\n",
            "Epoch 1237/10000\n",
            "Step 0: Train Loss: 3.120404414858058e-07, Train Acc: 1.0\n",
            "Epoch 1238/10000\n",
            "Step 0: Train Loss: 2.866137549517589e-07, Train Acc: 1.0\n",
            "Epoch 1239/10000\n",
            "Step 0: Train Loss: 2.661693656591524e-07, Train Acc: 1.0\n",
            "Epoch 1240/10000\n",
            "Step 0: Train Loss: 3.0311184673337266e-07, Train Acc: 1.0\n",
            "Epoch 1241/10000\n",
            "Step 0: Train Loss: 2.905117924001388e-07, Train Acc: 1.0\n",
            "Epoch 1242/10000\n",
            "Step 0: Train Loss: 2.9630510312017577e-07, Train Acc: 1.0\n",
            "Epoch 1243/10000\n",
            "Step 0: Train Loss: 2.708423778585711e-07, Train Acc: 1.0\n",
            "Epoch 1244/10000\n",
            "Step 0: Train Loss: 2.7607549668573483e-07, Train Acc: 1.0\n",
            "Epoch 1245/10000\n",
            "Step 0: Train Loss: 2.674091206245066e-07, Train Acc: 1.0\n",
            "Epoch 1246/10000\n",
            "Step 0: Train Loss: 2.7774461841545417e-07, Train Acc: 1.0\n",
            "Epoch 1247/10000\n",
            "Step 0: Train Loss: 2.8369319693410944e-07, Train Acc: 1.0\n",
            "Epoch 1248/10000\n",
            "Step 0: Train Loss: 2.6887528292718343e-07, Train Acc: 1.0\n",
            "Epoch 1249/10000\n",
            "Step 0: Train Loss: 2.8926015716024267e-07, Train Acc: 1.0\n",
            "Epoch 1250/10000\n",
            "Step 0: Train Loss: 2.778637622213864e-07, Train Acc: 1.0\n",
            "Epoch 1251/10000\n",
            "Step 0: Train Loss: 2.8322824618953746e-07, Train Acc: 1.0\n",
            "Epoch 1252/10000\n",
            "Step 0: Train Loss: 2.7886531484000443e-07, Train Acc: 1.0\n",
            "Epoch 1253/10000\n",
            "Step 0: Train Loss: 2.96603133165263e-07, Train Acc: 1.0\n",
            "Epoch 1254/10000\n",
            "Step 0: Train Loss: 2.995240322434256e-07, Train Acc: 1.0\n",
            "Epoch 1255/10000\n",
            "Step 0: Train Loss: 2.504339704501035e-07, Train Acc: 1.0\n",
            "Epoch 1256/10000\n",
            "Step 0: Train Loss: 2.5809910653151746e-07, Train Acc: 1.0\n",
            "Epoch 1257/10000\n",
            "Step 0: Train Loss: 2.822385170020425e-07, Train Acc: 1.0\n",
            "Epoch 1258/10000\n",
            "Step 0: Train Loss: 2.8653005301748635e-07, Train Acc: 1.0\n",
            "Epoch 1259/10000\n",
            "Step 0: Train Loss: 2.798901164169365e-07, Train Acc: 1.0\n",
            "Epoch 1260/10000\n",
            "Step 0: Train Loss: 2.88485324517751e-07, Train Acc: 1.0\n",
            "Epoch 1261/10000\n",
            "Step 0: Train Loss: 2.6600264391163364e-07, Train Acc: 1.0\n",
            "Epoch 1262/10000\n",
            "Step 0: Train Loss: 2.770053981748788e-07, Train Acc: 1.0\n",
            "Epoch 1263/10000\n",
            "Step 0: Train Loss: 2.656448714333237e-07, Train Acc: 1.0\n",
            "Epoch 1264/10000\n",
            "Step 0: Train Loss: 2.7215372710998054e-07, Train Acc: 1.0\n",
            "Epoch 1265/10000\n",
            "Step 0: Train Loss: 2.7162920446244243e-07, Train Acc: 1.0\n",
            "Epoch 1266/10000\n",
            "Step 0: Train Loss: 2.820003430770157e-07, Train Acc: 1.0\n",
            "Epoch 1267/10000\n",
            "Step 0: Train Loss: 2.649890120665077e-07, Train Acc: 1.0\n",
            "Epoch 1268/10000\n",
            "Step 0: Train Loss: 2.5734814812494733e-07, Train Acc: 1.0\n",
            "Epoch 1269/10000\n",
            "Step 0: Train Loss: 2.707589885631023e-07, Train Acc: 1.0\n",
            "Epoch 1270/10000\n",
            "Step 0: Train Loss: 2.516975428079604e-07, Train Acc: 1.0\n",
            "Epoch 1271/10000\n",
            "Step 0: Train Loss: 2.634037628013175e-07, Train Acc: 1.0\n",
            "Epoch 1272/10000\n",
            "Step 0: Train Loss: 2.616752396988886e-07, Train Acc: 1.0\n",
            "Epoch 1273/10000\n",
            "Step 0: Train Loss: 2.6733755476016086e-07, Train Acc: 1.0\n",
            "Epoch 1274/10000\n",
            "Step 0: Train Loss: 2.75455704468186e-07, Train Acc: 1.0\n",
            "Epoch 1275/10000\n",
            "Step 0: Train Loss: 2.419343445581035e-07, Train Acc: 1.0\n",
            "Epoch 1276/10000\n",
            "Step 0: Train Loss: 2.6246192419421277e-07, Train Acc: 1.0\n",
            "Epoch 1277/10000\n",
            "Step 0: Train Loss: 2.3314849784128455e-07, Train Acc: 1.0\n",
            "Epoch 1278/10000\n",
            "Step 0: Train Loss: 2.5994674501816917e-07, Train Acc: 1.0\n",
            "Epoch 1279/10000\n",
            "Step 0: Train Loss: 2.6079308668158774e-07, Train Acc: 1.0\n",
            "Epoch 1280/10000\n",
            "Step 0: Train Loss: 2.577056079644535e-07, Train Acc: 1.0\n",
            "Epoch 1281/10000\n",
            "Step 0: Train Loss: 2.525676450204628e-07, Train Acc: 1.0\n",
            "Epoch 1282/10000\n",
            "Step 0: Train Loss: 2.6233095695715747e-07, Train Acc: 1.0\n",
            "Epoch 1283/10000\n",
            "Step 0: Train Loss: 2.659786844105838e-07, Train Acc: 1.0\n",
            "Epoch 1284/10000\n",
            "Step 0: Train Loss: 2.472153539656574e-07, Train Acc: 1.0\n",
            "Epoch 1285/10000\n",
            "Step 0: Train Loss: 2.549876683133334e-07, Train Acc: 1.0\n",
            "Epoch 1286/10000\n",
            "Step 0: Train Loss: 2.3376863111934654e-07, Train Acc: 1.0\n",
            "Epoch 1287/10000\n",
            "Step 0: Train Loss: 2.3856082975726167e-07, Train Acc: 1.0\n",
            "Epoch 1288/10000\n",
            "Step 0: Train Loss: 2.566328021202935e-07, Train Acc: 1.0\n",
            "Epoch 1289/10000\n",
            "Step 0: Train Loss: 2.332083681721997e-07, Train Acc: 1.0\n",
            "Epoch 1290/10000\n",
            "Step 0: Train Loss: 2.4691726707715134e-07, Train Acc: 1.0\n",
            "Epoch 1291/10000\n",
            "Step 0: Train Loss: 2.560129530593258e-07, Train Acc: 1.0\n",
            "Epoch 1292/10000\n",
            "Step 0: Train Loss: 2.3004926674730086e-07, Train Acc: 1.0\n",
            "Epoch 1293/10000\n",
            "Step 0: Train Loss: 2.5173338258355216e-07, Train Acc: 1.0\n",
            "Epoch 1294/10000\n",
            "Step 0: Train Loss: 2.4598753611826396e-07, Train Acc: 1.0\n",
            "Epoch 1295/10000\n",
            "Step 0: Train Loss: 2.3630774137473054e-07, Train Acc: 1.0\n",
            "Epoch 1296/10000\n",
            "Step 0: Train Loss: 2.127757596781521e-07, Train Acc: 1.0\n",
            "Epoch 1297/10000\n",
            "Step 0: Train Loss: 2.5836138206614123e-07, Train Acc: 1.0\n",
            "Epoch 1298/10000\n",
            "Step 0: Train Loss: 2.4135027842930867e-07, Train Acc: 1.0\n",
            "Epoch 1299/10000\n",
            "Step 0: Train Loss: 2.416363429347257e-07, Train Acc: 1.0\n",
            "Epoch 1300/10000\n",
            "Step 0: Train Loss: 2.475968301496323e-07, Train Acc: 1.0\n",
            "Epoch 1301/10000\n",
            "Step 0: Train Loss: 2.452007379361021e-07, Train Acc: 1.0\n",
            "Epoch 1302/10000\n",
            "Step 0: Train Loss: 2.4136221554726944e-07, Train Acc: 1.0\n",
            "Epoch 1303/10000\n",
            "Step 0: Train Loss: 2.2674724675653124e-07, Train Acc: 1.0\n",
            "Epoch 1304/10000\n",
            "Step 0: Train Loss: 2.42423169538597e-07, Train Acc: 1.0\n",
            "Epoch 1305/10000\n",
            "Step 0: Train Loss: 2.2796318432938278e-07, Train Acc: 1.0\n",
            "Epoch 1306/10000\n",
            "Step 0: Train Loss: 2.4189841951738345e-07, Train Acc: 1.0\n",
            "Epoch 1307/10000\n",
            "Step 0: Train Loss: 2.336015398896052e-07, Train Acc: 1.0\n",
            "Epoch 1308/10000\n",
            "Step 0: Train Loss: 2.2715258296557295e-07, Train Acc: 1.0\n",
            "Epoch 1309/10000\n",
            "Step 0: Train Loss: 2.3572349050482444e-07, Train Acc: 1.0\n",
            "Epoch 1310/10000\n",
            "Step 0: Train Loss: 2.2512604402891156e-07, Train Acc: 1.0\n",
            "Epoch 1311/10000\n",
            "Step 0: Train Loss: 2.2389814091638982e-07, Train Acc: 1.0\n",
            "Epoch 1312/10000\n",
            "Step 0: Train Loss: 2.4087330530164763e-07, Train Acc: 1.0\n",
            "Epoch 1313/10000\n",
            "Step 0: Train Loss: 2.2523333598201134e-07, Train Acc: 1.0\n",
            "Epoch 1314/10000\n",
            "Step 0: Train Loss: 2.2443458647103398e-07, Train Acc: 1.0\n",
            "Epoch 1315/10000\n",
            "Step 0: Train Loss: 2.001278858188016e-07, Train Acc: 1.0\n",
            "Epoch 1316/10000\n",
            "Step 0: Train Loss: 2.4703646772650245e-07, Train Acc: 1.0\n",
            "Epoch 1317/10000\n",
            "Step 0: Train Loss: 2.0804336031687853e-07, Train Acc: 1.0\n",
            "Epoch 1318/10000\n",
            "Step 0: Train Loss: 2.1949938400211977e-07, Train Acc: 1.0\n",
            "Epoch 1319/10000\n",
            "Step 0: Train Loss: 2.1225149282599887e-07, Train Acc: 1.0\n",
            "Epoch 1320/10000\n",
            "Step 0: Train Loss: 2.3187313047401403e-07, Train Acc: 1.0\n",
            "Epoch 1321/10000\n",
            "Step 0: Train Loss: 2.0900900210563123e-07, Train Acc: 1.0\n",
            "Epoch 1322/10000\n",
            "Step 0: Train Loss: 2.1569658770204114e-07, Train Acc: 1.0\n",
            "Epoch 1323/10000\n",
            "Step 0: Train Loss: 2.2119209575066634e-07, Train Acc: 1.0\n",
            "Epoch 1324/10000\n",
            "Step 0: Train Loss: 2.3621220179848024e-07, Train Acc: 1.0\n",
            "Epoch 1325/10000\n",
            "Step 0: Train Loss: 2.119652862120347e-07, Train Acc: 1.0\n",
            "Epoch 1326/10000\n",
            "Step 0: Train Loss: 2.2491140327929315e-07, Train Acc: 1.0\n",
            "Epoch 1327/10000\n",
            "Step 0: Train Loss: 2.0514663390258647e-07, Train Acc: 1.0\n",
            "Epoch 1328/10000\n",
            "Step 0: Train Loss: 2.37630914057263e-07, Train Acc: 1.0\n",
            "Epoch 1329/10000\n",
            "Step 0: Train Loss: 1.996988174823855e-07, Train Acc: 1.0\n",
            "Epoch 1330/10000\n",
            "Step 0: Train Loss: 2.0352541696411208e-07, Train Acc: 1.0\n",
            "Epoch 1331/10000\n",
            "Step 0: Train Loss: 2.1144093409475317e-07, Train Acc: 1.0\n",
            "Epoch 1332/10000\n",
            "Step 0: Train Loss: 2.0630290009648888e-07, Train Acc: 1.0\n",
            "Epoch 1333/10000\n",
            "Step 0: Train Loss: 2.051346967846257e-07, Train Acc: 1.0\n",
            "Epoch 1334/10000\n",
            "Step 0: Train Loss: 2.1275218387017958e-07, Train Acc: 1.0\n",
            "Epoch 1335/10000\n",
            "Step 0: Train Loss: 2.1966630470160453e-07, Train Acc: 1.0\n",
            "Epoch 1336/10000\n",
            "Step 0: Train Loss: 2.2498294072192948e-07, Train Acc: 1.0\n",
            "Epoch 1337/10000\n",
            "Step 0: Train Loss: 2.1113075376888446e-07, Train Acc: 1.0\n",
            "Epoch 1338/10000\n",
            "Step 0: Train Loss: 2.1887947809773323e-07, Train Acc: 1.0\n",
            "Epoch 1339/10000\n",
            "Step 0: Train Loss: 2.0912823117669177e-07, Train Acc: 1.0\n",
            "Epoch 1340/10000\n",
            "Step 0: Train Loss: 2.1209650924447487e-07, Train Acc: 1.0\n",
            "Epoch 1341/10000\n",
            "Step 0: Train Loss: 1.987570925621185e-07, Train Acc: 1.0\n",
            "Epoch 1342/10000\n",
            "Step 0: Train Loss: 1.9749347757169744e-07, Train Acc: 1.0\n",
            "Epoch 1343/10000\n",
            "Step 0: Train Loss: 2.2033378854757757e-07, Train Acc: 1.0\n",
            "Epoch 1344/10000\n",
            "Step 0: Train Loss: 2.038115667346574e-07, Train Acc: 1.0\n",
            "Epoch 1345/10000\n",
            "Step 0: Train Loss: 2.0924741761518817e-07, Train Acc: 1.0\n",
            "Epoch 1346/10000\n",
            "Step 0: Train Loss: 1.8941111079584516e-07, Train Acc: 1.0\n",
            "Epoch 1347/10000\n",
            "Step 0: Train Loss: 2.0160612734798633e-07, Train Acc: 1.0\n",
            "Epoch 1348/10000\n",
            "Step 0: Train Loss: 1.9774375914494158e-07, Train Acc: 1.0\n",
            "Epoch 1349/10000\n",
            "Step 0: Train Loss: 1.8963757497658662e-07, Train Acc: 1.0\n",
            "Epoch 1350/10000\n",
            "Step 0: Train Loss: 2.0520619159469788e-07, Train Acc: 1.0\n",
            "Epoch 1351/10000\n",
            "Step 0: Train Loss: 2.064101352061698e-07, Train Acc: 1.0\n",
            "Epoch 1352/10000\n",
            "Step 0: Train Loss: 2.0281014201373182e-07, Train Acc: 1.0\n",
            "Epoch 1353/10000\n",
            "Step 0: Train Loss: 2.1068989042305475e-07, Train Acc: 1.0\n",
            "Epoch 1354/10000\n",
            "Step 0: Train Loss: 2.044551763447089e-07, Train Acc: 1.0\n",
            "Epoch 1355/10000\n",
            "Step 0: Train Loss: 1.8483348185327486e-07, Train Acc: 1.0\n",
            "Epoch 1356/10000\n",
            "Step 0: Train Loss: 2.0352541696411208e-07, Train Acc: 1.0\n",
            "Epoch 1357/10000\n",
            "Step 0: Train Loss: 2.1338381372970616e-07, Train Acc: 1.0\n",
            "Epoch 1358/10000\n",
            "Step 0: Train Loss: 1.929516031395906e-07, Train Acc: 1.0\n",
            "Epoch 1359/10000\n",
            "Step 0: Train Loss: 1.9043628185499983e-07, Train Acc: 1.0\n",
            "Epoch 1360/10000\n",
            "Step 0: Train Loss: 1.848335102749843e-07, Train Acc: 1.0\n",
            "Epoch 1361/10000\n",
            "Step 0: Train Loss: 2.0697056868357322e-07, Train Acc: 1.0\n",
            "Epoch 1362/10000\n",
            "Step 0: Train Loss: 1.862401148855497e-07, Train Acc: 1.0\n",
            "Epoch 1363/10000\n",
            "Step 0: Train Loss: 1.9919818328162364e-07, Train Acc: 1.0\n",
            "Epoch 1364/10000\n",
            "Step 0: Train Loss: 1.9814898166714556e-07, Train Acc: 1.0\n",
            "Epoch 1365/10000\n",
            "Step 0: Train Loss: 2.051943397418654e-07, Train Acc: 1.0\n",
            "Epoch 1366/10000\n",
            "Step 0: Train Loss: 1.886600244915826e-07, Train Acc: 1.0\n",
            "Epoch 1367/10000\n",
            "Step 0: Train Loss: 2.0507502540567657e-07, Train Acc: 1.0\n",
            "Epoch 1368/10000\n",
            "Step 0: Train Loss: 1.9631329450930934e-07, Train Acc: 1.0\n",
            "Epoch 1369/10000\n",
            "Step 0: Train Loss: 1.7540396868298558e-07, Train Acc: 1.0\n",
            "Epoch 1370/10000\n",
            "Step 0: Train Loss: 1.8733688023075956e-07, Train Acc: 1.0\n",
            "Epoch 1371/10000\n",
            "Step 0: Train Loss: 1.824612780865209e-07, Train Acc: 1.0\n",
            "Epoch 1372/10000\n",
            "Step 0: Train Loss: 1.8910122889792547e-07, Train Acc: 1.0\n",
            "Epoch 1373/10000\n",
            "Step 0: Train Loss: 1.8602553097935015e-07, Train Acc: 1.0\n",
            "Epoch 1374/10000\n",
            "Step 0: Train Loss: 1.7383051442720898e-07, Train Acc: 1.0\n",
            "Epoch 1375/10000\n",
            "Step 0: Train Loss: 1.742120190328933e-07, Train Acc: 1.0\n",
            "Epoch 1376/10000\n",
            "Step 0: Train Loss: 1.8401100021492311e-07, Train Acc: 1.0\n",
            "Epoch 1377/10000\n",
            "Step 0: Train Loss: 1.814836565472433e-07, Train Acc: 1.0\n",
            "Epoch 1378/10000\n",
            "Step 0: Train Loss: 1.799101028154837e-07, Train Acc: 1.0\n",
            "Epoch 1379/10000\n",
            "Step 0: Train Loss: 1.8930381884274539e-07, Train Acc: 1.0\n",
            "Epoch 1380/10000\n",
            "Step 0: Train Loss: 1.7883731118217838e-07, Train Acc: 1.0\n",
            "Epoch 1381/10000\n",
            "Step 0: Train Loss: 1.7286483000589214e-07, Train Acc: 1.0\n",
            "Epoch 1382/10000\n",
            "Step 0: Train Loss: 1.8217514252683031e-07, Train Acc: 1.0\n",
            "Epoch 1383/10000\n",
            "Step 0: Train Loss: 1.64901763355374e-07, Train Acc: 1.0\n",
            "Epoch 1384/10000\n",
            "Step 0: Train Loss: 1.7539210261929838e-07, Train Acc: 1.0\n",
            "Epoch 1385/10000\n",
            "Step 0: Train Loss: 1.6858528795182792e-07, Train Acc: 1.0\n",
            "Epoch 1386/10000\n",
            "Step 0: Train Loss: 1.7175622701870452e-07, Train Acc: 1.0\n",
            "Epoch 1387/10000\n",
            "Step 0: Train Loss: 1.6704747451967705e-07, Train Acc: 1.0\n",
            "Epoch 1388/10000\n",
            "Step 0: Train Loss: 1.5402993369662e-07, Train Acc: 1.0\n",
            "Epoch 1389/10000\n",
            "Step 0: Train Loss: 1.886601239675656e-07, Train Acc: 1.0\n",
            "Epoch 1390/10000\n",
            "Step 0: Train Loss: 1.7251923623007315e-07, Train Acc: 1.0\n",
            "Epoch 1391/10000\n",
            "Step 0: Train Loss: 1.6473494213187223e-07, Train Acc: 1.0\n",
            "Epoch 1392/10000\n",
            "Step 0: Train Loss: 1.7918296180141624e-07, Train Acc: 1.0\n",
            "Epoch 1393/10000\n",
            "Step 0: Train Loss: 1.824492130708677e-07, Train Acc: 1.0\n",
            "Epoch 1394/10000\n",
            "Step 0: Train Loss: 1.7577364985754684e-07, Train Acc: 1.0\n",
            "Epoch 1395/10000\n",
            "Step 0: Train Loss: 1.760120795779585e-07, Train Acc: 1.0\n",
            "Epoch 1396/10000\n",
            "Step 0: Train Loss: 1.783962773060921e-07, Train Acc: 1.0\n",
            "Epoch 1397/10000\n",
            "Step 0: Train Loss: 1.737470540774666e-07, Train Acc: 1.0\n",
            "Epoch 1398/10000\n",
            "Step 0: Train Loss: 1.549478270135296e-07, Train Acc: 1.0\n",
            "Epoch 1399/10000\n",
            "Step 0: Train Loss: 1.6462756491364416e-07, Train Acc: 1.0\n",
            "Epoch 1400/10000\n",
            "Step 0: Train Loss: 1.650567753586074e-07, Train Acc: 1.0\n",
            "Epoch 1401/10000\n",
            "Step 0: Train Loss: 1.6179045303488238e-07, Train Acc: 1.0\n",
            "Epoch 1402/10000\n",
            "Step 0: Train Loss: 1.7224503778834332e-07, Train Acc: 1.0\n",
            "Epoch 1403/10000\n",
            "Step 0: Train Loss: 1.5860759106089972e-07, Train Acc: 1.0\n",
            "Epoch 1404/10000\n",
            "Step 0: Train Loss: 1.64150847581368e-07, Train Acc: 1.0\n",
            "Epoch 1405/10000\n",
            "Step 0: Train Loss: 1.6165934368927992e-07, Train Acc: 1.0\n",
            "Epoch 1406/10000\n",
            "Step 0: Train Loss: 1.7175628386212338e-07, Train Acc: 1.0\n",
            "Epoch 1407/10000\n",
            "Step 0: Train Loss: 1.6270837477350142e-07, Train Acc: 1.0\n",
            "Epoch 1408/10000\n",
            "Step 0: Train Loss: 1.494046273364802e-07, Train Acc: 1.0\n",
            "Epoch 1409/10000\n",
            "Step 0: Train Loss: 1.4386148450284963e-07, Train Acc: 1.0\n",
            "Epoch 1410/10000\n",
            "Step 0: Train Loss: 1.654978092346937e-07, Train Acc: 1.0\n",
            "Epoch 1411/10000\n",
            "Step 0: Train Loss: 1.5944202402806695e-07, Train Acc: 1.0\n",
            "Epoch 1412/10000\n",
            "Step 0: Train Loss: 1.6784619560894498e-07, Train Acc: 1.0\n",
            "Epoch 1413/10000\n",
            "Step 0: Train Loss: 1.5630685368250852e-07, Train Acc: 1.0\n",
            "Epoch 1414/10000\n",
            "Step 0: Train Loss: 1.6884762032987055e-07, Train Acc: 1.0\n",
            "Epoch 1415/10000\n",
            "Step 0: Train Loss: 1.5619956172940874e-07, Train Acc: 1.0\n",
            "Epoch 1416/10000\n",
            "Step 0: Train Loss: 1.3941492227331764e-07, Train Acc: 1.0\n",
            "Epoch 1417/10000\n",
            "Step 0: Train Loss: 1.7017086406667659e-07, Train Acc: 1.0\n",
            "Epoch 1418/10000\n",
            "Step 0: Train Loss: 1.496907913178802e-07, Train Acc: 1.0\n",
            "Epoch 1419/10000\n",
            "Step 0: Train Loss: 1.5288557619896892e-07, Train Acc: 1.0\n",
            "Epoch 1420/10000\n",
            "Step 0: Train Loss: 1.5737964531581383e-07, Train Acc: 1.0\n",
            "Epoch 1421/10000\n",
            "Step 0: Train Loss: 1.6764353460985149e-07, Train Acc: 1.0\n",
            "Epoch 1422/10000\n",
            "Step 0: Train Loss: 1.5675981046570087e-07, Train Acc: 1.0\n",
            "Epoch 1423/10000\n",
            "Step 0: Train Loss: 1.4384954738488887e-07, Train Acc: 1.0\n",
            "Epoch 1424/10000\n",
            "Step 0: Train Loss: 1.5430417477091396e-07, Train Acc: 1.0\n",
            "Epoch 1425/10000\n",
            "Step 0: Train Loss: 1.4966688866024924e-07, Train Acc: 1.0\n",
            "Epoch 1426/10000\n",
            "Step 0: Train Loss: 1.3097499618197617e-07, Train Acc: 1.0\n",
            "Epoch 1427/10000\n",
            "Step 0: Train Loss: 1.549955896962274e-07, Train Acc: 1.0\n",
            "Epoch 1428/10000\n",
            "Step 0: Train Loss: 1.5866723401813942e-07, Train Acc: 1.0\n",
            "Epoch 1429/10000\n",
            "Step 0: Train Loss: 1.4208525556114182e-07, Train Acc: 1.0\n",
            "Epoch 1430/10000\n",
            "Step 0: Train Loss: 1.498576409630914e-07, Train Acc: 1.0\n",
            "Epoch 1431/10000\n",
            "Step 0: Train Loss: 1.5832149813377328e-07, Train Acc: 1.0\n",
            "Epoch 1432/10000\n",
            "Step 0: Train Loss: 1.4827216432422574e-07, Train Acc: 1.0\n",
            "Epoch 1433/10000\n",
            "Step 0: Train Loss: 1.6351887666132825e-07, Train Acc: 1.0\n",
            "Epoch 1434/10000\n",
            "Step 0: Train Loss: 1.5667639274852263e-07, Train Acc: 1.0\n",
            "Epoch 1435/10000\n",
            "Step 0: Train Loss: 1.3962959144464548e-07, Train Acc: 1.0\n",
            "Epoch 1436/10000\n",
            "Step 0: Train Loss: 1.4154878158478823e-07, Train Acc: 1.0\n",
            "Epoch 1437/10000\n",
            "Step 0: Train Loss: 1.5197952052403707e-07, Train Acc: 1.0\n",
            "Epoch 1438/10000\n",
            "Step 0: Train Loss: 1.3892615413624299e-07, Train Acc: 1.0\n",
            "Epoch 1439/10000\n",
            "Step 0: Train Loss: 1.399036904103923e-07, Train Acc: 1.0\n",
            "Epoch 1440/10000\n",
            "Step 0: Train Loss: 1.4705628359479306e-07, Train Acc: 1.0\n",
            "Epoch 1441/10000\n",
            "Step 0: Train Loss: 1.4700860617722356e-07, Train Acc: 1.0\n",
            "Epoch 1442/10000\n",
            "Step 0: Train Loss: 1.5571073674891522e-07, Train Acc: 1.0\n",
            "Epoch 1443/10000\n",
            "Step 0: Train Loss: 1.4226408495687792e-07, Train Acc: 1.0\n",
            "Epoch 1444/10000\n",
            "Step 0: Train Loss: 1.3688772071418498e-07, Train Acc: 1.0\n",
            "Epoch 1445/10000\n",
            "Step 0: Train Loss: 1.4156070449189428e-07, Train Acc: 1.0\n",
            "Epoch 1446/10000\n",
            "Step 0: Train Loss: 1.363870580917137e-07, Train Acc: 1.0\n",
            "Epoch 1447/10000\n",
            "Step 0: Train Loss: 1.4387336477739154e-07, Train Acc: 1.0\n",
            "Epoch 1448/10000\n",
            "Step 0: Train Loss: 1.4331308761939e-07, Train Acc: 1.0\n",
            "Epoch 1449/10000\n",
            "Step 0: Train Loss: 1.3796058340176387e-07, Train Acc: 1.0\n",
            "Epoch 1450/10000\n",
            "Step 0: Train Loss: 1.3692351785721257e-07, Train Acc: 1.0\n",
            "Epoch 1451/10000\n",
            "Step 0: Train Loss: 1.3722153369144507e-07, Train Acc: 1.0\n",
            "Epoch 1452/10000\n",
            "Step 0: Train Loss: 1.3275121091282926e-07, Train Acc: 1.0\n",
            "Epoch 1453/10000\n",
            "Step 0: Train Loss: 1.412745689322037e-07, Train Acc: 1.0\n",
            "Epoch 1454/10000\n",
            "Step 0: Train Loss: 1.3679233745733654e-07, Train Acc: 1.0\n",
            "Epoch 1455/10000\n",
            "Step 0: Train Loss: 1.380559524477576e-07, Train Acc: 1.0\n",
            "Epoch 1456/10000\n",
            "Step 0: Train Loss: 1.455303646480388e-07, Train Acc: 1.0\n",
            "Epoch 1457/10000\n",
            "Step 0: Train Loss: 1.4134614900740416e-07, Train Acc: 1.0\n",
            "Epoch 1458/10000\n",
            "Step 0: Train Loss: 1.3513532337583456e-07, Train Acc: 1.0\n",
            "Epoch 1459/10000\n",
            "Step 0: Train Loss: 1.3055775127668312e-07, Train Acc: 1.0\n",
            "Epoch 1460/10000\n",
            "Step 0: Train Loss: 1.254794170790774e-07, Train Acc: 1.0\n",
            "Epoch 1461/10000\n",
            "Step 0: Train Loss: 1.2822125938782847e-07, Train Acc: 1.0\n",
            "Epoch 1462/10000\n",
            "Step 0: Train Loss: 1.2941336535732262e-07, Train Acc: 1.0\n",
            "Epoch 1463/10000\n",
            "Step 0: Train Loss: 1.341101665275346e-07, Train Acc: 1.0\n",
            "Epoch 1464/10000\n",
            "Step 0: Train Loss: 1.2669538307363837e-07, Train Acc: 1.0\n",
            "Epoch 1465/10000\n",
            "Step 0: Train Loss: 1.3197633563777345e-07, Train Acc: 1.0\n",
            "Epoch 1466/10000\n",
            "Step 0: Train Loss: 1.3475388982442382e-07, Train Acc: 1.0\n",
            "Epoch 1467/10000\n",
            "Step 0: Train Loss: 1.311179573804111e-07, Train Acc: 1.0\n",
            "Epoch 1468/10000\n",
            "Step 0: Train Loss: 1.4222824518128618e-07, Train Acc: 1.0\n",
            "Epoch 1469/10000\n",
            "Step 0: Train Loss: 1.3332339676708216e-07, Train Acc: 1.0\n",
            "Epoch 1470/10000\n",
            "Step 0: Train Loss: 1.2996173381907283e-07, Train Acc: 1.0\n",
            "Epoch 1471/10000\n",
            "Step 0: Train Loss: 1.2916302694065962e-07, Train Acc: 1.0\n",
            "Epoch 1472/10000\n",
            "Step 0: Train Loss: 1.3250088670702098e-07, Train Acc: 1.0\n",
            "Epoch 1473/10000\n",
            "Step 0: Train Loss: 1.3688770650333026e-07, Train Acc: 1.0\n",
            "Epoch 1474/10000\n",
            "Step 0: Train Loss: 1.2587285880272248e-07, Train Acc: 1.0\n",
            "Epoch 1475/10000\n",
            "Step 0: Train Loss: 1.2327407716838934e-07, Train Acc: 1.0\n",
            "Epoch 1476/10000\n",
            "Step 0: Train Loss: 1.3316841318555817e-07, Train Acc: 1.0\n",
            "Epoch 1477/10000\n",
            "Step 0: Train Loss: 1.2197470766750484e-07, Train Acc: 1.0\n",
            "Epoch 1478/10000\n",
            "Step 0: Train Loss: 1.385924548458206e-07, Train Acc: 1.0\n",
            "Epoch 1479/10000\n",
            "Step 0: Train Loss: 1.3204787308040977e-07, Train Acc: 1.0\n",
            "Epoch 1480/10000\n",
            "Step 0: Train Loss: 1.2564633777856216e-07, Train Acc: 1.0\n",
            "Epoch 1481/10000\n",
            "Step 0: Train Loss: 1.1532283394899423e-07, Train Acc: 1.0\n",
            "Epoch 1482/10000\n",
            "Step 0: Train Loss: 1.2679078054134152e-07, Train Acc: 1.0\n",
            "Epoch 1483/10000\n",
            "Step 0: Train Loss: 1.2160516860149073e-07, Train Acc: 1.0\n",
            "Epoch 1484/10000\n",
            "Step 0: Train Loss: 1.2410855276812072e-07, Train Acc: 1.0\n",
            "Epoch 1485/10000\n",
            "Step 0: Train Loss: 1.2925832493237976e-07, Train Acc: 1.0\n",
            "Epoch 1486/10000\n",
            "Step 0: Train Loss: 1.1793353138500606e-07, Train Acc: 1.0\n",
            "Epoch 1487/10000\n",
            "Step 0: Train Loss: 1.0730011723580901e-07, Train Acc: 1.0\n",
            "Epoch 1488/10000\n",
            "Step 0: Train Loss: 1.3650627295191953e-07, Train Acc: 1.0\n",
            "Epoch 1489/10000\n",
            "Step 0: Train Loss: 1.1811235367531481e-07, Train Acc: 1.0\n",
            "Epoch 1490/10000\n",
            "Step 0: Train Loss: 1.2309529040521738e-07, Train Acc: 1.0\n",
            "Epoch 1491/10000\n",
            "Step 0: Train Loss: 1.2369132207368239e-07, Train Acc: 1.0\n",
            "Epoch 1492/10000\n",
            "Step 0: Train Loss: 1.1132937061120174e-07, Train Acc: 1.0\n",
            "Epoch 1493/10000\n",
            "Step 0: Train Loss: 1.1321285597887254e-07, Train Acc: 1.0\n",
            "Epoch 1494/10000\n",
            "Step 0: Train Loss: 1.184341513749132e-07, Train Acc: 1.0\n",
            "Epoch 1495/10000\n",
            "Step 0: Train Loss: 1.2164092311195418e-07, Train Acc: 1.0\n",
            "Epoch 1496/10000\n",
            "Step 0: Train Loss: 1.383659338216603e-07, Train Acc: 1.0\n",
            "Epoch 1497/10000\n",
            "Step 0: Train Loss: 1.0497554825406041e-07, Train Acc: 1.0\n",
            "Epoch 1498/10000\n",
            "Step 0: Train Loss: 1.2313103070482612e-07, Train Acc: 1.0\n",
            "Epoch 1499/10000\n",
            "Step 0: Train Loss: 1.2185552122900845e-07, Train Acc: 1.0\n",
            "Epoch 1500/10000\n",
            "Step 0: Train Loss: 1.2029387619350018e-07, Train Acc: 1.0\n",
            "Epoch 1501/10000\n",
            "Step 0: Train Loss: 1.1302215341402189e-07, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 1500 1024 1.2962853726180457\n",
            "Epoch index and hidden dimension and ratio: 1500 20 1.686802228918708\n",
            "Epoch index and hidden dimension and ratio: 1500 20 2.1751408297816424\n",
            "Epoch index and hidden dimension and ratio: 1500 20 3.1059601292551213\n",
            "MI(X;T): [12.926540434044849, 10.063936087016469, 7.975414946027284, 5.002776818802791], MI(Y;T): [3.310051121651302, 3.2610960458128284, 3.182854576785308, 2.754119696923283]\n",
            "Epoch index and hidden dimension and ratio: 1500 1024 1.296293155432871\n",
            "Epoch index and hidden dimension and ratio: 1500 20 1.686827722236096\n",
            "Epoch index and hidden dimension and ratio: 1500 20 2.1751766323495243\n",
            "Epoch index and hidden dimension and ratio: 1500 20 3.1060387688040856\n",
            "MI(X;T): [12.926402689669741, 10.063920466703955, 7.975333467322589, 5.0028601271806465], MI(Y;T): [3.310051121651302, 3.2610533081642146, 3.1830284444635444, 2.7540784274175465]\n",
            "Epoch index and hidden dimension and ratio: 1500 1024 1.2963009382476962\n",
            "Epoch index and hidden dimension and ratio: 1500 20 1.686854408857702\n",
            "Epoch index and hidden dimension and ratio: 1500 20 2.1752116731606432\n",
            "Epoch index and hidden dimension and ratio: 1500 20 3.1061200406392078\n",
            "MI(X;T): [12.926540434044849, 10.063749684584732, 7.975728933473816, 5.002546211477185], MI(Y;T): [3.310051121651302, 3.2611031312853314, 3.183070715641345, 2.7538941433392328]\n",
            "Epoch index and hidden dimension and ratio: 1500 1024 1.2963078399891452\n",
            "Epoch index and hidden dimension and ratio: 1500 20 1.6868744780650073\n",
            "Epoch index and hidden dimension and ratio: 1500 20 2.1752324945121773\n",
            "Epoch index and hidden dimension and ratio: 1500 20 3.1061733444339032\n",
            "MI(X;T): [12.92620268966974, 10.064779593928375, 7.97538910697379, 5.00245506826346], MI(Y;T): [3.310051121651302, 3.261203131285331, 3.182872308703864, 2.754200050692946]\n",
            "Epoch index and hidden dimension and ratio: 1500 1024 1.2963148885761568\n",
            "Epoch index and hidden dimension and ratio: 1500 20 1.6868945472723127\n",
            "Epoch index and hidden dimension and ratio: 1500 20 2.1752550932961596\n",
            "Epoch index and hidden dimension and ratio: 1500 20 3.1062335579797633\n",
            "MI(X;T): [12.92644043404485, 10.065517273211778, 7.975587389558816, 5.0025996749090655], MI(Y;T): [3.310051121651302, 3.2610707339414944, 3.1828266864152828, 2.7540488494538606]\n",
            "Epoch index and hidden dimension and ratio: 1500 1024 1.2963225245454193\n",
            "Epoch index and hidden dimension and ratio: 1500 20 1.6869150504084247\n",
            "Epoch index and hidden dimension and ratio: 1500 20 2.1752870870802243\n",
            "Epoch index and hidden dimension and ratio: 1500 20 3.1063003522410173\n",
            "MI(X;T): [12.92644043404485, 10.065307286664765, 7.975674507892743, 5.002787819543921], MI(Y;T): [3.310051121651302, 3.2609329895663857, 3.1828527907275856, 2.7541611804455224]\n",
            "Epoch 1502/10000\n",
            "Step 0: Train Loss: 1.2962792084181274e-07, Train Acc: 1.0\n",
            "Epoch 1503/10000\n",
            "Step 0: Train Loss: 1.114724099693376e-07, Train Acc: 1.0\n",
            "Epoch 1504/10000\n",
            "Step 0: Train Loss: 1.1916139897039102e-07, Train Acc: 1.0\n",
            "Epoch 1505/10000\n",
            "Step 0: Train Loss: 1.2237997282227298e-07, Train Acc: 1.0\n",
            "Epoch 1506/10000\n",
            "Step 0: Train Loss: 1.1280751976983083e-07, Train Acc: 1.0\n",
            "Epoch 1507/10000\n",
            "Step 0: Train Loss: 1.185057598718231e-07, Train Acc: 1.0\n",
            "Epoch 1508/10000\n",
            "Step 0: Train Loss: 1.0269861405731717e-07, Train Acc: 1.0\n",
            "Epoch 1509/10000\n",
            "Step 0: Train Loss: 1.1725403936679868e-07, Train Acc: 1.0\n",
            "Epoch 1510/10000\n",
            "Step 0: Train Loss: 1.1519173881424649e-07, Train Acc: 1.0\n",
            "Epoch 1511/10000\n",
            "Step 0: Train Loss: 1.2136671045936964e-07, Train Acc: 1.0\n",
            "Epoch 1512/10000\n",
            "Step 0: Train Loss: 1.1728980808811684e-07, Train Acc: 1.0\n",
            "Epoch 1513/10000\n",
            "Step 0: Train Loss: 1.0886176227131728e-07, Train Acc: 1.0\n",
            "Epoch 1514/10000\n",
            "Step 0: Train Loss: 1.1383276188325908e-07, Train Acc: 1.0\n",
            "Epoch 1515/10000\n",
            "Step 0: Train Loss: 1.166818677234005e-07, Train Acc: 1.0\n",
            "Epoch 1516/10000\n",
            "Step 0: Train Loss: 1.0856371090994799e-07, Train Acc: 1.0\n",
            "Epoch 1517/10000\n",
            "Step 0: Train Loss: 1.0015949669650581e-07, Train Acc: 1.0\n",
            "Epoch 1518/10000\n",
            "Step 0: Train Loss: 1.1234264007953243e-07, Train Acc: 1.0\n",
            "Epoch 1519/10000\n",
            "Step 0: Train Loss: 1.1014915202167685e-07, Train Acc: 1.0\n",
            "Epoch 1520/10000\n",
            "Step 0: Train Loss: 1.1142473965719546e-07, Train Acc: 1.0\n",
            "Epoch 1521/10000\n",
            "Step 0: Train Loss: 1.077531166515655e-07, Train Acc: 1.0\n",
            "Epoch 1522/10000\n",
            "Step 0: Train Loss: 1.0426028040910751e-07, Train Acc: 1.0\n",
            "Epoch 1523/10000\n",
            "Step 0: Train Loss: 1.1155587742450734e-07, Train Acc: 1.0\n",
            "Epoch 1524/10000\n",
            "Step 0: Train Loss: 1.0875446321279014e-07, Train Acc: 1.0\n",
            "Epoch 1525/10000\n",
            "Step 0: Train Loss: 1.0747891110440833e-07, Train Acc: 1.0\n",
            "Epoch 1526/10000\n",
            "Step 0: Train Loss: 1.1452414128143573e-07, Train Acc: 1.0\n",
            "Epoch 1527/10000\n",
            "Step 0: Train Loss: 1.0098203517827642e-07, Train Acc: 1.0\n",
            "Epoch 1528/10000\n",
            "Step 0: Train Loss: 1.1433344582201244e-07, Train Acc: 1.0\n",
            "Epoch 1529/10000\n",
            "Step 0: Train Loss: 1.0126814231625758e-07, Train Acc: 1.0\n",
            "Epoch 1530/10000\n",
            "Step 0: Train Loss: 9.930117528256233e-08, Train Acc: 1.0\n",
            "Epoch 1531/10000\n",
            "Step 0: Train Loss: 1.0852797771576661e-07, Train Acc: 1.0\n",
            "Epoch 1532/10000\n",
            "Step 0: Train Loss: 9.896741204329373e-08, Train Acc: 1.0\n",
            "Epoch 1533/10000\n",
            "Step 0: Train Loss: 1.0547621798195905e-07, Train Acc: 1.0\n",
            "Epoch 1534/10000\n",
            "Step 0: Train Loss: 8.938300055660875e-08, Train Acc: 1.0\n",
            "Epoch 1535/10000\n",
            "Step 0: Train Loss: 1.0851603349237848e-07, Train Acc: 1.0\n",
            "Epoch 1536/10000\n",
            "Step 0: Train Loss: 1.0334235867048847e-07, Train Acc: 1.0\n",
            "Epoch 1537/10000\n",
            "Step 0: Train Loss: 1.0499937985741781e-07, Train Acc: 1.0\n",
            "Epoch 1538/10000\n",
            "Step 0: Train Loss: 1.042841120124649e-07, Train Acc: 1.0\n",
            "Epoch 1539/10000\n",
            "Step 0: Train Loss: 9.970649017532196e-08, Train Acc: 1.0\n",
            "Epoch 1540/10000\n",
            "Step 0: Train Loss: 1.0609608125378145e-07, Train Acc: 1.0\n",
            "Epoch 1541/10000\n",
            "Step 0: Train Loss: 1.0738355626926932e-07, Train Acc: 1.0\n",
            "Epoch 1542/10000\n",
            "Step 0: Train Loss: 9.325729166675956e-08, Train Acc: 1.0\n",
            "Epoch 1543/10000\n",
            "Step 0: Train Loss: 9.647593657291509e-08, Train Acc: 1.0\n",
            "Epoch 1544/10000\n",
            "Step 0: Train Loss: 1.1213997908043893e-07, Train Acc: 1.0\n",
            "Epoch 1545/10000\n",
            "Step 0: Train Loss: 9.518848997913665e-08, Train Acc: 1.0\n",
            "Epoch 1546/10000\n",
            "Step 0: Train Loss: 9.94800259945805e-08, Train Acc: 1.0\n",
            "Epoch 1547/10000\n",
            "Step 0: Train Loss: 1.0626299484783885e-07, Train Acc: 1.0\n",
            "Epoch 1548/10000\n",
            "Step 0: Train Loss: 9.652360688505723e-08, Train Acc: 1.0\n",
            "Epoch 1549/10000\n",
            "Step 0: Train Loss: 1.0731202593206035e-07, Train Acc: 1.0\n",
            "Epoch 1550/10000\n",
            "Step 0: Train Loss: 9.54745829062631e-08, Train Acc: 1.0\n",
            "Epoch 1551/10000\n",
            "Step 0: Train Loss: 1.0285359763884117e-07, Train Acc: 1.0\n",
            "Epoch 1552/10000\n",
            "Step 0: Train Loss: 1.0174494491366204e-07, Train Acc: 1.0\n",
            "Epoch 1553/10000\n",
            "Step 0: Train Loss: 1.031039644772136e-07, Train Acc: 1.0\n",
            "Epoch 1554/10000\n",
            "Step 0: Train Loss: 1.0280590601041695e-07, Train Acc: 1.0\n",
            "Epoch 1555/10000\n",
            "Step 0: Train Loss: 1.0368808744942726e-07, Train Acc: 1.0\n",
            "Epoch 1556/10000\n",
            "Step 0: Train Loss: 9.516460863778775e-08, Train Acc: 1.0\n",
            "Epoch 1557/10000\n",
            "Step 0: Train Loss: 1.0074361966871948e-07, Train Acc: 1.0\n",
            "Epoch 1558/10000\n",
            "Step 0: Train Loss: 9.758455377095743e-08, Train Acc: 1.0\n",
            "Epoch 1559/10000\n",
            "Step 0: Train Loss: 9.510504384024898e-08, Train Acc: 1.0\n",
            "Epoch 1560/10000\n",
            "Step 0: Train Loss: 9.164796210825443e-08, Train Acc: 1.0\n",
            "Epoch 1561/10000\n",
            "Step 0: Train Loss: 9.64759507837698e-08, Train Acc: 1.0\n",
            "Epoch 1562/10000\n",
            "Step 0: Train Loss: 9.976610471085223e-08, Train Acc: 1.0\n",
            "Epoch 1563/10000\n",
            "Step 0: Train Loss: 9.026513936305491e-08, Train Acc: 1.0\n",
            "Epoch 1564/10000\n",
            "Step 0: Train Loss: 9.428249114762366e-08, Train Acc: 1.0\n",
            "Epoch 1565/10000\n",
            "Step 0: Train Loss: 8.98479228794713e-08, Train Acc: 1.0\n",
            "Epoch 1566/10000\n",
            "Step 0: Train Loss: 9.086119234780199e-08, Train Acc: 1.0\n",
            "Epoch 1567/10000\n",
            "Step 0: Train Loss: 1.0216216139724565e-07, Train Acc: 1.0\n",
            "Epoch 1568/10000\n",
            "Step 0: Train Loss: 9.490237573572813e-08, Train Acc: 1.0\n",
            "Epoch 1569/10000\n",
            "Step 0: Train Loss: 9.213672313990173e-08, Train Acc: 1.0\n",
            "Epoch 1570/10000\n",
            "Step 0: Train Loss: 9.077774620891432e-08, Train Acc: 1.0\n",
            "Epoch 1571/10000\n",
            "Step 0: Train Loss: 9.161222891407306e-08, Train Acc: 1.0\n",
            "Epoch 1572/10000\n",
            "Step 0: Train Loss: 8.723724675974154e-08, Train Acc: 1.0\n",
            "Epoch 1573/10000\n",
            "Step 0: Train Loss: 9.435402148483263e-08, Train Acc: 1.0\n",
            "Epoch 1574/10000\n",
            "Step 0: Train Loss: 9.47354834579528e-08, Train Acc: 1.0\n",
            "Epoch 1575/10000\n",
            "Step 0: Train Loss: 8.741605483919557e-08, Train Acc: 1.0\n",
            "Epoch 1576/10000\n",
            "Step 0: Train Loss: 8.659350214657024e-08, Train Acc: 1.0\n",
            "Epoch 1577/10000\n",
            "Step 0: Train Loss: 9.177909987556632e-08, Train Acc: 1.0\n",
            "Epoch 1578/10000\n",
            "Step 0: Train Loss: 8.925188410557894e-08, Train Acc: 1.0\n",
            "Epoch 1579/10000\n",
            "Step 0: Train Loss: 9.520038446453327e-08, Train Acc: 1.0\n",
            "Epoch 1580/10000\n",
            "Step 0: Train Loss: 8.635509374244066e-08, Train Acc: 1.0\n",
            "Epoch 1581/10000\n",
            "Step 0: Train Loss: 8.991942479497084e-08, Train Acc: 1.0\n",
            "Epoch 1582/10000\n",
            "Step 0: Train Loss: 7.86661118468146e-08, Train Acc: 1.0\n",
            "Epoch 1583/10000\n",
            "Step 0: Train Loss: 9.542689838326623e-08, Train Acc: 1.0\n",
            "Epoch 1584/10000\n",
            "Step 0: Train Loss: 8.870351564382872e-08, Train Acc: 1.0\n",
            "Epoch 1585/10000\n",
            "Step 0: Train Loss: 9.083735363901724e-08, Train Acc: 1.0\n",
            "Epoch 1586/10000\n",
            "Step 0: Train Loss: 8.267154072427729e-08, Train Acc: 1.0\n",
            "Epoch 1587/10000\n",
            "Step 0: Train Loss: 8.721340094552943e-08, Train Acc: 1.0\n",
            "Epoch 1588/10000\n",
            "Step 0: Train Loss: 8.238543358629613e-08, Train Acc: 1.0\n",
            "Epoch 1589/10000\n",
            "Step 0: Train Loss: 8.92160940679787e-08, Train Acc: 1.0\n",
            "Epoch 1590/10000\n",
            "Step 0: Train Loss: 7.9929733942663e-08, Train Acc: 1.0\n",
            "Epoch 1591/10000\n",
            "Step 0: Train Loss: 7.672300483818617e-08, Train Acc: 1.0\n",
            "Epoch 1592/10000\n",
            "Step 0: Train Loss: 7.92025502960314e-08, Train Acc: 1.0\n",
            "Epoch 1593/10000\n",
            "Step 0: Train Loss: 7.543554403355301e-08, Train Acc: 1.0\n",
            "Epoch 1594/10000\n",
            "Step 0: Train Loss: 8.438815513045483e-08, Train Acc: 1.0\n",
            "Epoch 1595/10000\n",
            "Step 0: Train Loss: 7.488716846637544e-08, Train Acc: 1.0\n",
            "Epoch 1596/10000\n",
            "Step 0: Train Loss: 9.062277683824504e-08, Train Acc: 1.0\n",
            "Epoch 1597/10000\n",
            "Step 0: Train Loss: 8.569943332759067e-08, Train Acc: 1.0\n",
            "Epoch 1598/10000\n",
            "Step 0: Train Loss: 9.040817872119078e-08, Train Acc: 1.0\n",
            "Epoch 1599/10000\n",
            "Step 0: Train Loss: 7.666340451351061e-08, Train Acc: 1.0\n",
            "Epoch 1600/10000\n",
            "Step 0: Train Loss: 8.105030246952083e-08, Train Acc: 1.0\n",
            "Epoch 1601/10000\n",
            "Step 0: Train Loss: 8.348214208808713e-08, Train Acc: 1.0\n",
            "Epoch 1602/10000\n",
            "Step 0: Train Loss: 8.349407210062054e-08, Train Acc: 1.0\n",
            "Epoch 1603/10000\n",
            "Step 0: Train Loss: 8.453118738316334e-08, Train Acc: 1.0\n",
            "Epoch 1604/10000\n",
            "Step 0: Train Loss: 8.589017852500547e-08, Train Acc: 1.0\n",
            "Epoch 1605/10000\n",
            "Step 0: Train Loss: 8.182517063914929e-08, Train Acc: 1.0\n",
            "Epoch 1606/10000\n",
            "Step 0: Train Loss: 8.25046413410746e-08, Train Acc: 1.0\n",
            "Epoch 1607/10000\n",
            "Step 0: Train Loss: 7.7843573365044e-08, Train Acc: 1.0\n",
            "Epoch 1608/10000\n",
            "Step 0: Train Loss: 8.419740282761268e-08, Train Acc: 1.0\n",
            "Epoch 1609/10000\n",
            "Step 0: Train Loss: 7.25149291724847e-08, Train Acc: 1.0\n",
            "Epoch 1610/10000\n",
            "Step 0: Train Loss: 7.662765000304717e-08, Train Acc: 1.0\n",
            "Epoch 1611/10000\n",
            "Step 0: Train Loss: 7.78912507826135e-08, Train Acc: 1.0\n",
            "Epoch 1612/10000\n",
            "Step 0: Train Loss: 7.98462949092027e-08, Train Acc: 1.0\n",
            "Epoch 1613/10000\n",
            "Step 0: Train Loss: 7.63892202826355e-08, Train Acc: 1.0\n",
            "Epoch 1614/10000\n",
            "Step 0: Train Loss: 7.55190043832954e-08, Train Acc: 1.0\n",
            "Epoch 1615/10000\n",
            "Step 0: Train Loss: 7.74024897509662e-08, Train Acc: 1.0\n",
            "Epoch 1616/10000\n",
            "Step 0: Train Loss: 7.936944257380674e-08, Train Acc: 1.0\n",
            "Epoch 1617/10000\n",
            "Step 0: Train Loss: 7.877339669448702e-08, Train Acc: 1.0\n",
            "Epoch 1618/10000\n",
            "Step 0: Train Loss: 7.745018848481777e-08, Train Acc: 1.0\n",
            "Epoch 1619/10000\n",
            "Step 0: Train Loss: 7.925023481902826e-08, Train Acc: 1.0\n",
            "Epoch 1620/10000\n",
            "Step 0: Train Loss: 7.754556463623885e-08, Train Acc: 1.0\n",
            "Epoch 1621/10000\n",
            "Step 0: Train Loss: 7.964362680468184e-08, Train Acc: 1.0\n",
            "Epoch 1622/10000\n",
            "Step 0: Train Loss: 7.119170675196074e-08, Train Acc: 1.0\n",
            "Epoch 1623/10000\n",
            "Step 0: Train Loss: 7.476798202787904e-08, Train Acc: 1.0\n",
            "Epoch 1624/10000\n",
            "Step 0: Train Loss: 7.902374932200473e-08, Train Acc: 1.0\n",
            "Epoch 1625/10000\n",
            "Step 0: Train Loss: 8.200395740232125e-08, Train Acc: 1.0\n",
            "Epoch 1626/10000\n",
            "Step 0: Train Loss: 7.203809104794345e-08, Train Acc: 1.0\n",
            "Epoch 1627/10000\n",
            "Step 0: Train Loss: 8.018006525389865e-08, Train Acc: 1.0\n",
            "Epoch 1628/10000\n",
            "Step 0: Train Loss: 7.256261369548156e-08, Train Acc: 1.0\n",
            "Epoch 1629/10000\n",
            "Step 0: Train Loss: 7.441035876354363e-08, Train Acc: 1.0\n",
            "Epoch 1630/10000\n",
            "Step 0: Train Loss: 7.354012154792144e-08, Train Acc: 1.0\n",
            "Epoch 1631/10000\n",
            "Step 0: Train Loss: 8.041848786888295e-08, Train Acc: 1.0\n",
            "Epoch 1632/10000\n",
            "Step 0: Train Loss: 7.312289795891047e-08, Train Acc: 1.0\n",
            "Epoch 1633/10000\n",
            "Step 0: Train Loss: 7.20261681408374e-08, Train Acc: 1.0\n",
            "Epoch 1634/10000\n",
            "Step 0: Train Loss: 6.79492160315931e-08, Train Acc: 1.0\n",
            "Epoch 1635/10000\n",
            "Step 0: Train Loss: 7.489911268976357e-08, Train Acc: 1.0\n",
            "Epoch 1636/10000\n",
            "Step 0: Train Loss: 7.970324134021212e-08, Train Acc: 1.0\n",
            "Epoch 1637/10000\n",
            "Step 0: Train Loss: 7.683028258043123e-08, Train Acc: 1.0\n",
            "Epoch 1638/10000\n",
            "Step 0: Train Loss: 6.304973254600554e-08, Train Acc: 1.0\n",
            "Epoch 1639/10000\n",
            "Step 0: Train Loss: 7.64011360843142e-08, Train Acc: 1.0\n",
            "Epoch 1640/10000\n",
            "Step 0: Train Loss: 7.808198176917358e-08, Train Acc: 1.0\n",
            "Epoch 1641/10000\n",
            "Step 0: Train Loss: 6.750815373379737e-08, Train Acc: 1.0\n",
            "Epoch 1642/10000\n",
            "Step 0: Train Loss: 7.756936781788681e-08, Train Acc: 1.0\n",
            "Epoch 1643/10000\n",
            "Step 0: Train Loss: 6.924859263790495e-08, Train Acc: 1.0\n",
            "Epoch 1644/10000\n",
            "Step 0: Train Loss: 6.780616246260252e-08, Train Acc: 1.0\n",
            "Epoch 1645/10000\n",
            "Step 0: Train Loss: 6.928435425379575e-08, Train Acc: 1.0\n",
            "Epoch 1646/10000\n",
            "Step 0: Train Loss: 7.284871372803536e-08, Train Acc: 1.0\n",
            "Epoch 1647/10000\n",
            "Step 0: Train Loss: 7.128707579795446e-08, Train Acc: 1.0\n",
            "Epoch 1648/10000\n",
            "Step 0: Train Loss: 6.921284523286886e-08, Train Acc: 1.0\n",
            "Epoch 1649/10000\n",
            "Step 0: Train Loss: 7.55785976025436e-08, Train Acc: 1.0\n",
            "Epoch 1650/10000\n",
            "Step 0: Train Loss: 6.881944614178792e-08, Train Acc: 1.0\n",
            "Epoch 1651/10000\n",
            "Step 0: Train Loss: 7.133476032095132e-08, Train Acc: 1.0\n",
            "Epoch 1652/10000\n",
            "Step 0: Train Loss: 7.324210571368894e-08, Train Acc: 1.0\n",
            "Epoch 1653/10000\n",
            "Step 0: Train Loss: 6.346697034587123e-08, Train Acc: 1.0\n",
            "Epoch 1654/10000\n",
            "Step 0: Train Loss: 6.431334753642659e-08, Train Acc: 1.0\n",
            "Epoch 1655/10000\n",
            "Step 0: Train Loss: 6.501668536884608e-08, Train Acc: 1.0\n",
            "Epoch 1656/10000\n",
            "Step 0: Train Loss: 6.400339458423332e-08, Train Acc: 1.0\n",
            "Epoch 1657/10000\n",
            "Step 0: Train Loss: 6.400340879508803e-08, Train Acc: 1.0\n",
            "Epoch 1658/10000\n",
            "Step 0: Train Loss: 6.707900013225299e-08, Train Acc: 1.0\n",
            "Epoch 1659/10000\n",
            "Step 0: Train Loss: 6.730549273470388e-08, Train Acc: 1.0\n",
            "Epoch 1660/10000\n",
            "Step 0: Train Loss: 6.970160626451616e-08, Train Acc: 1.0\n",
            "Epoch 1661/10000\n",
            "Step 0: Train Loss: 6.577961642051378e-08, Train Acc: 1.0\n",
            "Epoch 1662/10000\n",
            "Step 0: Train Loss: 5.917543788314106e-08, Train Acc: 1.0\n",
            "Epoch 1663/10000\n",
            "Step 0: Train Loss: 6.856910772512492e-08, Train Acc: 1.0\n",
            "Epoch 1664/10000\n",
            "Step 0: Train Loss: 6.92843471483684e-08, Train Acc: 1.0\n",
            "Epoch 1665/10000\n",
            "Step 0: Train Loss: 6.568424737452006e-08, Train Acc: 1.0\n",
            "Epoch 1666/10000\n",
            "Step 0: Train Loss: 6.093973325960178e-08, Train Acc: 1.0\n",
            "Epoch 1667/10000\n",
            "Step 0: Train Loss: 7.042877570029304e-08, Train Acc: 1.0\n",
            "Epoch 1668/10000\n",
            "Step 0: Train Loss: 6.46471249865499e-08, Train Acc: 1.0\n",
            "Epoch 1669/10000\n",
            "Step 0: Train Loss: 5.8472103603435244e-08, Train Acc: 1.0\n",
            "Epoch 1670/10000\n",
            "Step 0: Train Loss: 6.815188413611395e-08, Train Acc: 1.0\n",
            "Epoch 1671/10000\n",
            "Step 0: Train Loss: 6.810419961311709e-08, Train Acc: 1.0\n",
            "Epoch 1672/10000\n",
            "Step 0: Train Loss: 6.055824997019954e-08, Train Acc: 1.0\n",
            "Epoch 1673/10000\n",
            "Step 0: Train Loss: 6.674520847127496e-08, Train Acc: 1.0\n",
            "Epoch 1674/10000\n",
            "Step 0: Train Loss: 6.246560246836452e-08, Train Acc: 1.0\n",
            "Epoch 1675/10000\n",
            "Step 0: Train Loss: 6.254904150182483e-08, Train Acc: 1.0\n",
            "Epoch 1676/10000\n",
            "Step 0: Train Loss: 6.276363251345174e-08, Train Acc: 1.0\n",
            "Epoch 1677/10000\n",
            "Step 0: Train Loss: 5.755419252295724e-08, Train Acc: 1.0\n",
            "Epoch 1678/10000\n",
            "Step 0: Train Loss: 6.066554192329932e-08, Train Acc: 1.0\n",
            "Epoch 1679/10000\n",
            "Step 0: Train Loss: 6.322854062545957e-08, Train Acc: 1.0\n",
            "Epoch 1680/10000\n",
            "Step 0: Train Loss: 6.470674662750753e-08, Train Acc: 1.0\n",
            "Epoch 1681/10000\n",
            "Step 0: Train Loss: 6.676906139091443e-08, Train Acc: 1.0\n",
            "Epoch 1682/10000\n",
            "Step 0: Train Loss: 5.966418115121996e-08, Train Acc: 1.0\n",
            "Epoch 1683/10000\n",
            "Step 0: Train Loss: 6.055826418105426e-08, Train Acc: 1.0\n",
            "Epoch 1684/10000\n",
            "Step 0: Train Loss: 6.619682579867003e-08, Train Acc: 1.0\n",
            "Epoch 1685/10000\n",
            "Step 0: Train Loss: 6.297820931422393e-08, Train Acc: 1.0\n",
            "Epoch 1686/10000\n",
            "Step 0: Train Loss: 6.406302333061831e-08, Train Acc: 1.0\n",
            "Epoch 1687/10000\n",
            "Step 0: Train Loss: 6.13450339415067e-08, Train Acc: 1.0\n",
            "Epoch 1688/10000\n",
            "Step 0: Train Loss: 5.346532105932056e-08, Train Acc: 1.0\n",
            "Epoch 1689/10000\n",
            "Step 0: Train Loss: 5.629057397982251e-08, Train Acc: 1.0\n",
            "Epoch 1690/10000\n",
            "Step 0: Train Loss: 6.271594799045488e-08, Train Acc: 1.0\n",
            "Epoch 1691/10000\n",
            "Step 0: Train Loss: 5.803103420021216e-08, Train Acc: 1.0\n",
            "Epoch 1692/10000\n",
            "Step 0: Train Loss: 5.800719549142741e-08, Train Acc: 1.0\n",
            "Epoch 1693/10000\n",
            "Step 0: Train Loss: 5.5909104901274986e-08, Train Acc: 1.0\n",
            "Epoch 1694/10000\n",
            "Step 0: Train Loss: 5.7113119567020476e-08, Train Acc: 1.0\n",
            "Epoch 1695/10000\n",
            "Step 0: Train Loss: 6.500474114545796e-08, Train Acc: 1.0\n",
            "Epoch 1696/10000\n",
            "Step 0: Train Loss: 5.573029682182096e-08, Train Acc: 1.0\n",
            "Epoch 1697/10000\n",
            "Step 0: Train Loss: 5.5313069680096305e-08, Train Acc: 1.0\n",
            "Epoch 1698/10000\n",
            "Step 0: Train Loss: 6.102317229306209e-08, Train Acc: 1.0\n",
            "Epoch 1699/10000\n",
            "Step 0: Train Loss: 6.07966796906112e-08, Train Acc: 1.0\n",
            "Epoch 1700/10000\n",
            "Step 0: Train Loss: 6.268017216370936e-08, Train Acc: 1.0\n",
            "Epoch 1701/10000\n",
            "Step 0: Train Loss: 5.663628854790659e-08, Train Acc: 1.0\n",
            "Epoch 1702/10000\n",
            "Step 0: Train Loss: 6.086820292239281e-08, Train Acc: 1.0\n",
            "Epoch 1703/10000\n",
            "Step 0: Train Loss: 5.9306568545025584e-08, Train Acc: 1.0\n",
            "Epoch 1704/10000\n",
            "Step 0: Train Loss: 6.033177157860337e-08, Train Acc: 1.0\n",
            "Epoch 1705/10000\n",
            "Step 0: Train Loss: 6.4670970800762e-08, Train Acc: 1.0\n",
            "Epoch 1706/10000\n",
            "Step 0: Train Loss: 5.208249120869368e-08, Train Acc: 1.0\n",
            "Epoch 1707/10000\n",
            "Step 0: Train Loss: 6.008142605651301e-08, Train Acc: 1.0\n",
            "Epoch 1708/10000\n",
            "Step 0: Train Loss: 5.8197922925273815e-08, Train Acc: 1.0\n",
            "Epoch 1709/10000\n",
            "Step 0: Train Loss: 5.78521976990487e-08, Train Acc: 1.0\n",
            "Epoch 1710/10000\n",
            "Step 0: Train Loss: 5.670780822697452e-08, Train Acc: 1.0\n",
            "Epoch 1711/10000\n",
            "Step 0: Train Loss: 5.5539562282547195e-08, Train Acc: 1.0\n",
            "Epoch 1712/10000\n",
            "Step 0: Train Loss: 5.1259952726923075e-08, Train Acc: 1.0\n",
            "Epoch 1713/10000\n",
            "Step 0: Train Loss: 5.663628144247923e-08, Train Acc: 1.0\n",
            "Epoch 1714/10000\n",
            "Step 0: Train Loss: 5.65647617634113e-08, Train Acc: 1.0\n",
            "Epoch 1715/10000\n",
            "Step 0: Train Loss: 5.841249262061865e-08, Train Acc: 1.0\n",
            "Epoch 1716/10000\n",
            "Step 0: Train Loss: 4.994865321350517e-08, Train Acc: 1.0\n",
            "Epoch 1717/10000\n",
            "Step 0: Train Loss: 4.879232662347022e-08, Train Acc: 1.0\n",
            "Epoch 1718/10000\n",
            "Step 0: Train Loss: 5.656475110527026e-08, Train Acc: 1.0\n",
            "Epoch 1719/10000\n",
            "Step 0: Train Loss: 5.1724871497071945e-08, Train Acc: 1.0\n",
            "Epoch 1720/10000\n",
            "Step 0: Train Loss: 5.985492634863476e-08, Train Acc: 1.0\n",
            "Epoch 1721/10000\n",
            "Step 0: Train Loss: 5.215402509861633e-08, Train Acc: 1.0\n",
            "Epoch 1722/10000\n",
            "Step 0: Train Loss: 6.11304571407345e-08, Train Acc: 1.0\n",
            "Epoch 1723/10000\n",
            "Step 0: Train Loss: 5.047316520290224e-08, Train Acc: 1.0\n",
            "Epoch 1724/10000\n",
            "Step 0: Train Loss: 5.3000409394599046e-08, Train Acc: 1.0\n",
            "Epoch 1725/10000\n",
            "Step 0: Train Loss: 5.77330112605523e-08, Train Acc: 1.0\n",
            "Epoch 1726/10000\n",
            "Step 0: Train Loss: 5.112882206503855e-08, Train Acc: 1.0\n",
            "Epoch 1727/10000\n",
            "Step 0: Train Loss: 5.6850851137824066e-08, Train Acc: 1.0\n",
            "Epoch 1728/10000\n",
            "Step 0: Train Loss: 5.725616958329738e-08, Train Acc: 1.0\n",
            "Epoch 1729/10000\n",
            "Step 0: Train Loss: 5.940191982745091e-08, Train Acc: 1.0\n",
            "Epoch 1730/10000\n",
            "Step 0: Train Loss: 4.925724184090541e-08, Train Acc: 1.0\n",
            "Epoch 1731/10000\n",
            "Step 0: Train Loss: 5.1832152792030683e-08, Train Acc: 1.0\n",
            "Epoch 1732/10000\n",
            "Step 0: Train Loss: 5.326266361294074e-08, Train Acc: 1.0\n",
            "Epoch 1733/10000\n",
            "Step 0: Train Loss: 5.130763014449258e-08, Train Acc: 1.0\n",
            "Epoch 1734/10000\n",
            "Step 0: Train Loss: 5.046124584850986e-08, Train Acc: 1.0\n",
            "Epoch 1735/10000\n",
            "Step 0: Train Loss: 5.2022887331304446e-08, Train Acc: 1.0\n",
            "Epoch 1736/10000\n",
            "Step 0: Train Loss: 5.122418045289123e-08, Train Acc: 1.0\n",
            "Epoch 1737/10000\n",
            "Step 0: Train Loss: 4.774328843382136e-08, Train Acc: 1.0\n",
            "Epoch 1738/10000\n",
            "Step 0: Train Loss: 5.390640112068468e-08, Train Acc: 1.0\n",
            "Epoch 1739/10000\n",
            "Step 0: Train Loss: 5.127187563402913e-08, Train Acc: 1.0\n",
            "Epoch 1740/10000\n",
            "Step 0: Train Loss: 4.854198820680722e-08, Train Acc: 1.0\n",
            "Epoch 1741/10000\n",
            "Step 0: Train Loss: 5.4168648233599015e-08, Train Acc: 1.0\n",
            "Epoch 1742/10000\n",
            "Step 0: Train Loss: 5.054468488197017e-08, Train Acc: 1.0\n",
            "Epoch 1743/10000\n",
            "Step 0: Train Loss: 4.951950316467446e-08, Train Acc: 1.0\n",
            "Epoch 1744/10000\n",
            "Step 0: Train Loss: 5.069966846349416e-08, Train Acc: 1.0\n",
            "Epoch 1745/10000\n",
            "Step 0: Train Loss: 4.7349889342740425e-08, Train Acc: 1.0\n",
            "Epoch 1746/10000\n",
            "Step 0: Train Loss: 5.5837585222207053e-08, Train Acc: 1.0\n",
            "Epoch 1747/10000\n",
            "Step 0: Train Loss: 5.0032099352392834e-08, Train Acc: 1.0\n",
            "Epoch 1748/10000\n",
            "Step 0: Train Loss: 4.967446898263006e-08, Train Acc: 1.0\n",
            "Epoch 1749/10000\n",
            "Step 0: Train Loss: 4.5084924238381063e-08, Train Acc: 1.0\n",
            "Epoch 1750/10000\n",
            "Step 0: Train Loss: 5.14029991904863e-08, Train Acc: 1.0\n",
            "Epoch 1751/10000\n",
            "Step 0: Train Loss: 4.845854562063323e-08, Train Acc: 1.0\n",
            "Epoch 1752/10000\n",
            "Step 0: Train Loss: 5.2905040348605326e-08, Train Acc: 1.0\n",
            "Epoch 1753/10000\n",
            "Step 0: Train Loss: 4.9436057025786795e-08, Train Acc: 1.0\n",
            "Epoch 1754/10000\n",
            "Step 0: Train Loss: 4.9209560870622227e-08, Train Acc: 1.0\n",
            "Epoch 1755/10000\n",
            "Step 0: Train Loss: 5.1701029235573515e-08, Train Acc: 1.0\n",
            "Epoch 1756/10000\n",
            "Step 0: Train Loss: 4.8804242425148914e-08, Train Acc: 1.0\n",
            "Epoch 1757/10000\n",
            "Step 0: Train Loss: 4.9149942782378275e-08, Train Acc: 1.0\n",
            "Epoch 1758/10000\n",
            "Step 0: Train Loss: 4.8375095929031886e-08, Train Acc: 1.0\n",
            "Epoch 1759/10000\n",
            "Step 0: Train Loss: 5.184407925185042e-08, Train Acc: 1.0\n",
            "Epoch 1760/10000\n",
            "Step 0: Train Loss: 4.6265089537200765e-08, Train Acc: 1.0\n",
            "Epoch 1761/10000\n",
            "Step 0: Train Loss: 4.861351499130251e-08, Train Acc: 1.0\n",
            "Epoch 1762/10000\n",
            "Step 0: Train Loss: 4.312988721721922e-08, Train Acc: 1.0\n",
            "Epoch 1763/10000\n",
            "Step 0: Train Loss: 4.519221263876716e-08, Train Acc: 1.0\n",
            "Epoch 1764/10000\n",
            "Step 0: Train Loss: 4.923338892126594e-08, Train Acc: 1.0\n",
            "Epoch 1765/10000\n",
            "Step 0: Train Loss: 4.8208196545829196e-08, Train Acc: 1.0\n",
            "Epoch 1766/10000\n",
            "Step 0: Train Loss: 4.5871701104260865e-08, Train Acc: 1.0\n",
            "Epoch 1767/10000\n",
            "Step 0: Train Loss: 4.726644675656644e-08, Train Acc: 1.0\n",
            "Epoch 1768/10000\n",
            "Step 0: Train Loss: 4.025694977372041e-08, Train Acc: 1.0\n",
            "Epoch 1769/10000\n",
            "Step 0: Train Loss: 5.043741424515247e-08, Train Acc: 1.0\n",
            "Epoch 1770/10000\n",
            "Step 0: Train Loss: 4.568097011770078e-08, Train Acc: 1.0\n",
            "Epoch 1771/10000\n",
            "Step 0: Train Loss: 4.42623786511831e-08, Train Acc: 1.0\n",
            "Epoch 1772/10000\n",
            "Step 0: Train Loss: 4.445311674317054e-08, Train Acc: 1.0\n",
            "Epoch 1773/10000\n",
            "Step 0: Train Loss: 4.345175241837751e-08, Train Acc: 1.0\n",
            "Epoch 1774/10000\n",
            "Step 0: Train Loss: 4.445311319045686e-08, Train Acc: 1.0\n",
            "Epoch 1775/10000\n",
            "Step 0: Train Loss: 4.3880909572635574e-08, Train Acc: 1.0\n",
            "Epoch 1776/10000\n",
            "Step 0: Train Loss: 4.763599292800791e-08, Train Acc: 1.0\n",
            "Epoch 1777/10000\n",
            "Step 0: Train Loss: 4.680153153913125e-08, Train Acc: 1.0\n",
            "Epoch 1778/10000\n",
            "Step 0: Train Loss: 4.606243209082095e-08, Train Acc: 1.0\n",
            "Epoch 1779/10000\n",
            "Step 0: Train Loss: 4.1151018592699984e-08, Train Acc: 1.0\n",
            "Epoch 1780/10000\n",
            "Step 0: Train Loss: 4.4143174449118305e-08, Train Acc: 1.0\n",
            "Epoch 1781/10000\n",
            "Step 0: Train Loss: 4.3928594095632434e-08, Train Acc: 1.0\n",
            "Epoch 1782/10000\n",
            "Step 0: Train Loss: 4.0042376525661894e-08, Train Acc: 1.0\n",
            "Epoch 1783/10000\n",
            "Step 0: Train Loss: 4.000660780434373e-08, Train Acc: 1.0\n",
            "Epoch 1784/10000\n",
            "Step 0: Train Loss: 4.3761691159716065e-08, Train Acc: 1.0\n",
            "Epoch 1785/10000\n",
            "Step 0: Train Loss: 4.259344521528874e-08, Train Acc: 1.0\n",
            "Epoch 1786/10000\n",
            "Step 0: Train Loss: 4.320141755442819e-08, Train Acc: 1.0\n",
            "Epoch 1787/10000\n",
            "Step 0: Train Loss: 4.199740644139638e-08, Train Acc: 1.0\n",
            "Epoch 1788/10000\n",
            "Step 0: Train Loss: 3.910061963097178e-08, Train Acc: 1.0\n",
            "Epoch 1789/10000\n",
            "Step 0: Train Loss: 4.283186783027304e-08, Train Acc: 1.0\n",
            "Epoch 1790/10000\n",
            "Step 0: Train Loss: 4.266496489435667e-08, Train Acc: 1.0\n",
            "Epoch 1791/10000\n",
            "Step 0: Train Loss: 4.158017574695805e-08, Train Acc: 1.0\n",
            "Epoch 1792/10000\n",
            "Step 0: Train Loss: 4.183051061090737e-08, Train Acc: 1.0\n",
            "Epoch 1793/10000\n",
            "Step 0: Train Loss: 4.4476948346527934e-08, Train Acc: 1.0\n",
            "Epoch 1794/10000\n",
            "Step 0: Train Loss: 4.175898382641208e-08, Train Acc: 1.0\n",
            "Epoch 1795/10000\n",
            "Step 0: Train Loss: 4.1544410578353563e-08, Train Acc: 1.0\n",
            "Epoch 1796/10000\n",
            "Step 0: Train Loss: 4.234311035133942e-08, Train Acc: 1.0\n",
            "Epoch 1797/10000\n",
            "Step 0: Train Loss: 4.0817237589863e-08, Train Acc: 1.0\n",
            "Epoch 1798/10000\n",
            "Step 0: Train Loss: 4.014965782062063e-08, Train Acc: 1.0\n",
            "Epoch 1799/10000\n",
            "Step 0: Train Loss: 4.349944049408805e-08, Train Acc: 1.0\n",
            "Epoch 1800/10000\n",
            "Step 0: Train Loss: 3.761051203809984e-08, Train Acc: 1.0\n",
            "Epoch 1801/10000\n",
            "Step 0: Train Loss: 4.347559823258962e-08, Train Acc: 1.0\n",
            "Epoch 1802/10000\n",
            "Step 0: Train Loss: 4.221197968945489e-08, Train Acc: 1.0\n",
            "Epoch 1803/10000\n",
            "Step 0: Train Loss: 3.818271565592113e-08, Train Acc: 1.0\n",
            "Epoch 1804/10000\n",
            "Step 0: Train Loss: 4.076954951415246e-08, Train Acc: 1.0\n",
            "Epoch 1805/10000\n",
            "Step 0: Train Loss: 3.8635711518963944e-08, Train Acc: 1.0\n",
            "Epoch 1806/10000\n",
            "Step 0: Train Loss: 3.857610408886103e-08, Train Acc: 1.0\n",
            "Epoch 1807/10000\n",
            "Step 0: Train Loss: 4.228350647395018e-08, Train Acc: 1.0\n",
            "Epoch 1808/10000\n",
            "Step 0: Train Loss: 3.978011520189284e-08, Train Acc: 1.0\n",
            "Epoch 1809/10000\n",
            "Step 0: Train Loss: 4.116293794709236e-08, Train Acc: 1.0\n",
            "Epoch 1810/10000\n",
            "Step 0: Train Loss: 3.635881284935749e-08, Train Acc: 1.0\n",
            "Epoch 1811/10000\n",
            "Step 0: Train Loss: 4.2033168057287185e-08, Train Acc: 1.0\n",
            "Epoch 1812/10000\n",
            "Step 0: Train Loss: 3.793237368654445e-08, Train Acc: 1.0\n",
            "Epoch 1813/10000\n",
            "Step 0: Train Loss: 3.76701123627754e-08, Train Acc: 1.0\n",
            "Epoch 1814/10000\n",
            "Step 0: Train Loss: 3.945825000073455e-08, Train Acc: 1.0\n",
            "Epoch 1815/10000\n",
            "Step 0: Train Loss: 3.6501862865634394e-08, Train Acc: 1.0\n",
            "Epoch 1816/10000\n",
            "Step 0: Train Loss: 4.4739209670296987e-08, Train Acc: 1.0\n",
            "Epoch 1817/10000\n",
            "Step 0: Train Loss: 4.010197685033745e-08, Train Acc: 1.0\n",
            "Epoch 1818/10000\n",
            "Step 0: Train Loss: 4.019735300175853e-08, Train Acc: 1.0\n",
            "Epoch 1819/10000\n",
            "Step 0: Train Loss: 3.858801633782605e-08, Train Acc: 1.0\n",
            "Epoch 1820/10000\n",
            "Step 0: Train Loss: 3.727672037712182e-08, Train Acc: 1.0\n",
            "Epoch 1821/10000\n",
            "Step 0: Train Loss: 3.6299212524681934e-08, Train Acc: 1.0\n",
            "Epoch 1822/10000\n",
            "Step 0: Train Loss: 3.5095194306222766e-08, Train Acc: 1.0\n",
            "Epoch 1823/10000\n",
            "Step 0: Train Loss: 3.738400522479424e-08, Train Acc: 1.0\n",
            "Epoch 1824/10000\n",
            "Step 0: Train Loss: 3.712175100645254e-08, Train Acc: 1.0\n",
            "Epoch 1825/10000\n",
            "Step 0: Train Loss: 3.788468916354759e-08, Train Acc: 1.0\n",
            "Epoch 1826/10000\n",
            "Step 0: Train Loss: 3.556010952365796e-08, Train Acc: 1.0\n",
            "Epoch 1827/10000\n",
            "Step 0: Train Loss: 3.6776039991082143e-08, Train Acc: 1.0\n",
            "Epoch 1828/10000\n",
            "Step 0: Train Loss: 3.848073504286731e-08, Train Acc: 1.0\n",
            "Epoch 1829/10000\n",
            "Step 0: Train Loss: 3.795621594804288e-08, Train Acc: 1.0\n",
            "Epoch 1830/10000\n",
            "Step 0: Train Loss: 3.6692597404908156e-08, Train Acc: 1.0\n",
            "Epoch 1831/10000\n",
            "Step 0: Train Loss: 3.871915055242425e-08, Train Acc: 1.0\n",
            "Epoch 1832/10000\n",
            "Step 0: Train Loss: 3.982779617217602e-08, Train Acc: 1.0\n",
            "Epoch 1833/10000\n",
            "Step 0: Train Loss: 3.550050209355504e-08, Train Acc: 1.0\n",
            "Epoch 1834/10000\n",
            "Step 0: Train Loss: 3.449914842690305e-08, Train Acc: 1.0\n",
            "Epoch 1835/10000\n",
            "Step 0: Train Loss: 3.683564386847138e-08, Train Acc: 1.0\n",
            "Epoch 1836/10000\n",
            "Step 0: Train Loss: 3.43799371194109e-08, Train Acc: 1.0\n",
            "Epoch 1837/10000\n",
            "Step 0: Train Loss: 3.5393217245882624e-08, Train Acc: 1.0\n",
            "Epoch 1838/10000\n",
            "Step 0: Train Loss: 3.612039023437319e-08, Train Acc: 1.0\n",
            "Epoch 1839/10000\n",
            "Step 0: Train Loss: 3.9052938660688596e-08, Train Acc: 1.0\n",
            "Epoch 1840/10000\n",
            "Step 0: Train Loss: 3.653762803423888e-08, Train Acc: 1.0\n",
            "Epoch 1841/10000\n",
            "Step 0: Train Loss: 3.635880929664381e-08, Train Acc: 1.0\n",
            "Epoch 1842/10000\n",
            "Step 0: Train Loss: 3.4701805873282865e-08, Train Acc: 1.0\n",
            "Epoch 1843/10000\n",
            "Step 0: Train Loss: 3.424881001024005e-08, Train Acc: 1.0\n",
            "Epoch 1844/10000\n",
            "Step 0: Train Loss: 3.5810447940320955e-08, Train Acc: 1.0\n",
            "Epoch 1845/10000\n",
            "Step 0: Train Loss: 3.501174461462142e-08, Train Acc: 1.0\n",
            "Epoch 1846/10000\n",
            "Step 0: Train Loss: 3.556010952365796e-08, Train Acc: 1.0\n",
            "Epoch 1847/10000\n",
            "Step 0: Train Loss: 3.4928298475733754e-08, Train Acc: 1.0\n",
            "Epoch 1848/10000\n",
            "Step 0: Train Loss: 3.885028476702246e-08, Train Acc: 1.0\n",
            "Epoch 1849/10000\n",
            "Step 0: Train Loss: 3.563163630815325e-08, Train Acc: 1.0\n",
            "Epoch 1850/10000\n",
            "Step 0: Train Loss: 3.2806383387651294e-08, Train Acc: 1.0\n",
            "Epoch 1851/10000\n",
            "Step 0: Train Loss: 3.788469271626127e-08, Train Acc: 1.0\n",
            "Epoch 1852/10000\n",
            "Step 0: Train Loss: 3.5047506230512226e-08, Train Acc: 1.0\n",
            "Epoch 1853/10000\n",
            "Step 0: Train Loss: 3.556010952365796e-08, Train Acc: 1.0\n",
            "Epoch 1854/10000\n",
            "Step 0: Train Loss: 3.2985191467105324e-08, Train Acc: 1.0\n",
            "Epoch 1855/10000\n",
            "Step 0: Train Loss: 3.1113611242972183e-08, Train Acc: 1.0\n",
            "Epoch 1856/10000\n",
            "Step 0: Train Loss: 3.3080556960385366e-08, Train Acc: 1.0\n",
            "Epoch 1857/10000\n",
            "Step 0: Train Loss: 3.255603786556094e-08, Train Acc: 1.0\n",
            "Epoch 1858/10000\n",
            "Step 0: Train Loss: 3.2949433403928197e-08, Train Acc: 1.0\n",
            "Epoch 1859/10000\n",
            "Step 0: Train Loss: 3.561971695376087e-08, Train Acc: 1.0\n",
            "Epoch 1860/10000\n",
            "Step 0: Train Loss: 3.137586901402756e-08, Train Acc: 1.0\n",
            "Epoch 1861/10000\n",
            "Step 0: Train Loss: 3.5905813433601e-08, Train Acc: 1.0\n",
            "Epoch 1862/10000\n",
            "Step 0: Train Loss: 3.5154798183612e-08, Train Acc: 1.0\n",
            "Epoch 1863/10000\n",
            "Step 0: Train Loss: 3.253219915677619e-08, Train Acc: 1.0\n",
            "Epoch 1864/10000\n",
            "Step 0: Train Loss: 3.457067521139834e-08, Train Acc: 1.0\n",
            "Epoch 1865/10000\n",
            "Step 0: Train Loss: 3.279445692783156e-08, Train Acc: 1.0\n",
            "Epoch 1866/10000\n",
            "Step 0: Train Loss: 3.163813033779661e-08, Train Acc: 1.0\n",
            "Epoch 1867/10000\n",
            "Step 0: Train Loss: 3.5953501509311536e-08, Train Acc: 1.0\n",
            "Epoch 1868/10000\n",
            "Step 0: Train Loss: 3.318784891348514e-08, Train Acc: 1.0\n",
            "Epoch 1869/10000\n",
            "Step 0: Train Loss: 3.480909072095528e-08, Train Acc: 1.0\n",
            "Epoch 1870/10000\n",
            "Step 0: Train Loss: 3.281829918932999e-08, Train Acc: 1.0\n",
            "Epoch 1871/10000\n",
            "Step 0: Train Loss: 3.390310254758333e-08, Train Acc: 1.0\n",
            "Epoch 1872/10000\n",
            "Step 0: Train Loss: 3.402231385507548e-08, Train Acc: 1.0\n",
            "Epoch 1873/10000\n",
            "Step 0: Train Loss: 3.184078067874907e-08, Train Acc: 1.0\n",
            "Epoch 1874/10000\n",
            "Step 0: Train Loss: 3.013608917967758e-08, Train Acc: 1.0\n",
            "Epoch 1875/10000\n",
            "Step 0: Train Loss: 3.0469880840655605e-08, Train Acc: 1.0\n",
            "Epoch 1876/10000\n",
            "Step 0: Train Loss: 3.378389834551854e-08, Train Acc: 1.0\n",
            "Epoch 1877/10000\n",
            "Step 0: Train Loss: 3.240106849489166e-08, Train Acc: 1.0\n",
            "Epoch 1878/10000\n",
            "Step 0: Train Loss: 3.1161292213255365e-08, Train Acc: 1.0\n",
            "Epoch 1879/10000\n",
            "Step 0: Train Loss: 3.253219915677619e-08, Train Acc: 1.0\n",
            "Epoch 1880/10000\n",
            "Step 0: Train Loss: 2.8979760813285793e-08, Train Acc: 1.0\n",
            "Epoch 1881/10000\n",
            "Step 0: Train Loss: 3.161428807629818e-08, Train Acc: 1.0\n",
            "Epoch 1882/10000\n",
            "Step 0: Train Loss: 3.051756181093879e-08, Train Acc: 1.0\n",
            "Epoch 1883/10000\n",
            "Step 0: Train Loss: 3.3152087297594335e-08, Train Acc: 1.0\n",
            "Epoch 1884/10000\n",
            "Step 0: Train Loss: 2.999303916340068e-08, Train Acc: 1.0\n",
            "Epoch 1885/10000\n",
            "Step 0: Train Loss: 3.075598087320941e-08, Train Acc: 1.0\n",
            "Epoch 1886/10000\n",
            "Step 0: Train Loss: 2.96950197764545e-08, Train Acc: 1.0\n",
            "Epoch 1887/10000\n",
            "Step 0: Train Loss: 2.9075128082922674e-08, Train Acc: 1.0\n",
            "Epoch 1888/10000\n",
            "Step 0: Train Loss: 3.119705382914617e-08, Train Acc: 1.0\n",
            "Epoch 1889/10000\n",
            "Step 0: Train Loss: 2.8359876225181324e-08, Train Acc: 1.0\n",
            "Epoch 1890/10000\n",
            "Step 0: Train Loss: 3.0076488855002026e-08, Train Acc: 1.0\n",
            "Epoch 1891/10000\n",
            "Step 0: Train Loss: 3.094671541248317e-08, Train Acc: 1.0\n",
            "Epoch 1892/10000\n",
            "Step 0: Train Loss: 2.7942643754386154e-08, Train Acc: 1.0\n",
            "Epoch 1893/10000\n",
            "Step 0: Train Loss: 2.8896314674398127e-08, Train Acc: 1.0\n",
            "Epoch 1894/10000\n",
            "Step 0: Train Loss: 3.17215729239706e-08, Train Acc: 1.0\n",
            "Epoch 1895/10000\n",
            "Step 0: Train Loss: 3.0100331116500456e-08, Train Acc: 1.0\n",
            "Epoch 1896/10000\n",
            "Step 0: Train Loss: 3.350971056192975e-08, Train Acc: 1.0\n",
            "Epoch 1897/10000\n",
            "Step 0: Train Loss: 3.1113604137544826e-08, Train Acc: 1.0\n",
            "Epoch 1898/10000\n",
            "Step 0: Train Loss: 3.051756181093879e-08, Train Acc: 1.0\n",
            "Epoch 1899/10000\n",
            "Step 0: Train Loss: 3.080366894891995e-08, Train Acc: 1.0\n",
            "Epoch 1900/10000\n",
            "Step 0: Train Loss: 3.306864115870667e-08, Train Acc: 1.0\n",
            "Epoch 1901/10000\n",
            "Step 0: Train Loss: 3.082750055227734e-08, Train Acc: 1.0\n",
            "Epoch 1902/10000\n",
            "Step 0: Train Loss: 2.8133378293659916e-08, Train Acc: 1.0\n",
            "Epoch 1903/10000\n",
            "Step 0: Train Loss: 2.7251232381786394e-08, Train Acc: 1.0\n",
            "Epoch 1904/10000\n",
            "Step 0: Train Loss: 2.800224407906171e-08, Train Acc: 1.0\n",
            "Epoch 1905/10000\n",
            "Step 0: Train Loss: 2.9146656643774804e-08, Train Acc: 1.0\n",
            "Epoch 1906/10000\n",
            "Step 0: Train Loss: 2.8574453025953517e-08, Train Acc: 1.0\n",
            "Epoch 1907/10000\n",
            "Step 0: Train Loss: 2.8741348856442528e-08, Train Acc: 1.0\n",
            "Epoch 1908/10000\n",
            "Step 0: Train Loss: 2.5510775714110423e-08, Train Acc: 1.0\n",
            "Epoch 1909/10000\n",
            "Step 0: Train Loss: 2.686976152688203e-08, Train Acc: 1.0\n",
            "Epoch 1910/10000\n",
            "Step 0: Train Loss: 2.8312191702184464e-08, Train Acc: 1.0\n",
            "Epoch 1911/10000\n",
            "Step 0: Train Loss: 2.9337391183048567e-08, Train Acc: 1.0\n",
            "Epoch 1912/10000\n",
            "Step 0: Train Loss: 2.9516202815216275e-08, Train Acc: 1.0\n",
            "Epoch 1913/10000\n",
            "Step 0: Train Loss: 2.6047217716040905e-08, Train Acc: 1.0\n",
            "Epoch 1914/10000\n",
            "Step 0: Train Loss: 2.5165071804167383e-08, Train Acc: 1.0\n",
            "Epoch 1915/10000\n",
            "Step 0: Train Loss: 3.023146177838498e-08, Train Acc: 1.0\n",
            "Epoch 1916/10000\n",
            "Step 0: Train Loss: 2.9313552474263815e-08, Train Acc: 1.0\n",
            "Epoch 1917/10000\n",
            "Step 0: Train Loss: 3.0183773702674443e-08, Train Acc: 1.0\n",
            "Epoch 1918/10000\n",
            "Step 0: Train Loss: 2.671479393256959e-08, Train Acc: 1.0\n",
            "Epoch 1919/10000\n",
            "Step 0: Train Loss: 2.4616706895130847e-08, Train Acc: 1.0\n",
            "Epoch 1920/10000\n",
            "Step 0: Train Loss: 2.8169139909550722e-08, Train Acc: 1.0\n",
            "Epoch 1921/10000\n",
            "Step 0: Train Loss: 3.021953887127893e-08, Train Acc: 1.0\n",
            "Epoch 1922/10000\n",
            "Step 0: Train Loss: 3.043411567205112e-08, Train Acc: 1.0\n",
            "Epoch 1923/10000\n",
            "Step 0: Train Loss: 2.9742697194024004e-08, Train Acc: 1.0\n",
            "Epoch 1924/10000\n",
            "Step 0: Train Loss: 2.6309479039809958e-08, Train Acc: 1.0\n",
            "Epoch 1925/10000\n",
            "Step 0: Train Loss: 2.8824794995330194e-08, Train Acc: 1.0\n",
            "Epoch 1926/10000\n",
            "Step 0: Train Loss: 2.539156618297511e-08, Train Acc: 1.0\n",
            "Epoch 1927/10000\n",
            "Step 0: Train Loss: 2.6345240655700763e-08, Train Acc: 1.0\n",
            "Epoch 1928/10000\n",
            "Step 0: Train Loss: 2.4569022372133986e-08, Train Acc: 1.0\n",
            "Epoch 1929/10000\n",
            "Step 0: Train Loss: 2.573727364563183e-08, Train Acc: 1.0\n",
            "Epoch 1930/10000\n",
            "Step 0: Train Loss: 2.7966487792241423e-08, Train Acc: 1.0\n",
            "Epoch 1931/10000\n",
            "Step 0: Train Loss: 2.475975691140775e-08, Train Acc: 1.0\n",
            "Epoch 1932/10000\n",
            "Step 0: Train Loss: 2.8133380070016756e-08, Train Acc: 1.0\n",
            "Epoch 1933/10000\n",
            "Step 0: Train Loss: 2.719162672804032e-08, Train Acc: 1.0\n",
            "Epoch 1934/10000\n",
            "Step 0: Train Loss: 2.5939929315654808e-08, Train Acc: 1.0\n",
            "Epoch 1935/10000\n",
            "Step 0: Train Loss: 2.635716178644998e-08, Train Acc: 1.0\n",
            "Epoch 1936/10000\n",
            "Step 0: Train Loss: 2.6607501979469816e-08, Train Acc: 1.0\n",
            "Epoch 1937/10000\n",
            "Step 0: Train Loss: 2.797840892299064e-08, Train Acc: 1.0\n",
            "Epoch 1938/10000\n",
            "Step 0: Train Loss: 2.6738632641354343e-08, Train Acc: 1.0\n",
            "Epoch 1939/10000\n",
            "Step 0: Train Loss: 2.6488288895620826e-08, Train Acc: 1.0\n",
            "Epoch 1940/10000\n",
            "Step 0: Train Loss: 2.416371103208803e-08, Train Acc: 1.0\n",
            "Epoch 1941/10000\n",
            "Step 0: Train Loss: 2.7644617262012616e-08, Train Acc: 1.0\n",
            "Epoch 1942/10000\n",
            "Step 0: Train Loss: 2.397297649281427e-08, Train Acc: 1.0\n",
            "Epoch 1943/10000\n",
            "Step 0: Train Loss: 2.520083164370135e-08, Train Acc: 1.0\n",
            "Epoch 1944/10000\n",
            "Step 0: Train Loss: 2.231597129309648e-08, Train Acc: 1.0\n",
            "Epoch 1945/10000\n",
            "Step 0: Train Loss: 2.5141227766312113e-08, Train Acc: 1.0\n",
            "Epoch 1946/10000\n",
            "Step 0: Train Loss: 2.381800712214499e-08, Train Acc: 1.0\n",
            "Epoch 1947/10000\n",
            "Step 0: Train Loss: 2.5141227766312113e-08, Train Acc: 1.0\n",
            "Epoch 1948/10000\n",
            "Step 0: Train Loss: 2.4318685731827827e-08, Train Acc: 1.0\n",
            "Epoch 1949/10000\n",
            "Step 0: Train Loss: 2.4914729834790705e-08, Train Acc: 1.0\n",
            "Epoch 1950/10000\n",
            "Step 0: Train Loss: 2.5379646828582736e-08, Train Acc: 1.0\n",
            "Epoch 1951/10000\n",
            "Step 0: Train Loss: 2.300738621840992e-08, Train Acc: 1.0\n",
            "Epoch 1952/10000\n",
            "Step 0: Train Loss: 2.458094705559688e-08, Train Acc: 1.0\n",
            "Epoch 1953/10000\n",
            "Step 0: Train Loss: 2.751349015284177e-08, Train Acc: 1.0\n",
            "Epoch 1954/10000\n",
            "Step 0: Train Loss: 2.46286298022369e-08, Train Acc: 1.0\n",
            "Epoch 1955/10000\n",
            "Step 0: Train Loss: 2.5928008184905593e-08, Train Acc: 1.0\n",
            "Epoch 1956/10000\n",
            "Step 0: Train Loss: 2.4306764601078612e-08, Train Acc: 1.0\n",
            "Epoch 1957/10000\n",
            "Step 0: Train Loss: 2.5546539106358068e-08, Train Acc: 1.0\n",
            "Epoch 1958/10000\n",
            "Step 0: Train Loss: 2.4700153034018513e-08, Train Acc: 1.0\n",
            "Epoch 1959/10000\n",
            "Step 0: Train Loss: 2.439020896360944e-08, Train Acc: 1.0\n",
            "Epoch 1960/10000\n",
            "Step 0: Train Loss: 2.251862518676262e-08, Train Acc: 1.0\n",
            "Epoch 1961/10000\n",
            "Step 0: Train Loss: 2.4521339625493965e-08, Train Acc: 1.0\n",
            "Epoch 1962/10000\n",
            "Step 0: Train Loss: 2.4366368478467848e-08, Train Acc: 1.0\n",
            "Epoch 1963/10000\n",
            "Step 0: Train Loss: 2.4271001208830967e-08, Train Acc: 1.0\n",
            "Epoch 1964/10000\n",
            "Step 0: Train Loss: 2.5033939365926017e-08, Train Acc: 1.0\n",
            "Epoch 1965/10000\n",
            "Step 0: Train Loss: 2.032517798511435e-08, Train Acc: 1.0\n",
            "Epoch 1966/10000\n",
            "Step 0: Train Loss: 2.3233882373574488e-08, Train Acc: 1.0\n",
            "Epoch 1967/10000\n",
            "Step 0: Train Loss: 2.3734559206900485e-08, Train Acc: 1.0\n",
            "Epoch 1968/10000\n",
            "Step 0: Train Loss: 2.3496141920986702e-08, Train Acc: 1.0\n",
            "Epoch 1969/10000\n",
            "Step 0: Train Loss: 2.150534861300457e-08, Train Acc: 1.0\n",
            "Epoch 1970/10000\n",
            "Step 0: Train Loss: 2.222060224710276e-08, Train Acc: 1.0\n",
            "Epoch 1971/10000\n",
            "Step 0: Train Loss: 2.1100035496601777e-08, Train Acc: 1.0\n",
            "Epoch 1972/10000\n",
            "Step 0: Train Loss: 2.1004670003321735e-08, Train Acc: 1.0\n",
            "Epoch 1973/10000\n",
            "Step 0: Train Loss: 2.2351732908987287e-08, Train Acc: 1.0\n",
            "Epoch 1974/10000\n",
            "Step 0: Train Loss: 2.1219246804093927e-08, Train Acc: 1.0\n",
            "Epoch 1975/10000\n",
            "Step 0: Train Loss: 2.2721282633142437e-08, Train Acc: 1.0\n",
            "Epoch 1976/10000\n",
            "Step 0: Train Loss: 2.2530546317511835e-08, Train Acc: 1.0\n",
            "Epoch 1977/10000\n",
            "Step 0: Train Loss: 2.2327888871132018e-08, Train Acc: 1.0\n",
            "Epoch 1978/10000\n",
            "Step 0: Train Loss: 2.0706647063661876e-08, Train Acc: 1.0\n",
            "Epoch 1979/10000\n",
            "Step 0: Train Loss: 2.2506705832370244e-08, Train Acc: 1.0\n",
            "Epoch 1980/10000\n",
            "Step 0: Train Loss: 2.2304050162347266e-08, Train Acc: 1.0\n",
            "Epoch 1981/10000\n",
            "Step 0: Train Loss: 2.3686876460260464e-08, Train Acc: 1.0\n",
            "Epoch 1982/10000\n",
            "Step 0: Train Loss: 2.1255008419984733e-08, Train Acc: 1.0\n",
            "Epoch 1983/10000\n",
            "Step 0: Train Loss: 2.1469585220756926e-08, Train Acc: 1.0\n",
            "Epoch 1984/10000\n",
            "Step 0: Train Loss: 2.1862977206410505e-08, Train Acc: 1.0\n",
            "Epoch 1985/10000\n",
            "Step 0: Train Loss: 2.3984897623563484e-08, Train Acc: 1.0\n",
            "Epoch 1986/10000\n",
            "Step 0: Train Loss: 2.2232525154208815e-08, Train Acc: 1.0\n",
            "Epoch 1987/10000\n",
            "Step 0: Train Loss: 2.2435180824231793e-08, Train Acc: 1.0\n",
            "Epoch 1988/10000\n",
            "Step 0: Train Loss: 2.0837775949189563e-08, Train Acc: 1.0\n",
            "Epoch 1989/10000\n",
            "Step 0: Train Loss: 2.0825854818440348e-08, Train Acc: 1.0\n",
            "Epoch 1990/10000\n",
            "Step 0: Train Loss: 2.1970262054082923e-08, Train Acc: 1.0\n",
            "Epoch 1991/10000\n",
            "Step 0: Train Loss: 2.1529190874503e-08, Train Acc: 1.0\n",
            "Epoch 1992/10000\n",
            "Step 0: Train Loss: 1.9574157406054837e-08, Train Acc: 1.0\n",
            "Epoch 1993/10000\n",
            "Step 0: Train Loss: 2.421139733144173e-08, Train Acc: 1.0\n",
            "Epoch 1994/10000\n",
            "Step 0: Train Loss: 2.1445744735615335e-08, Train Acc: 1.0\n",
            "Epoch 1995/10000\n",
            "Step 0: Train Loss: 2.0730487548803467e-08, Train Acc: 1.0\n",
            "Epoch 1996/10000\n",
            "Step 0: Train Loss: 2.0217887808371415e-08, Train Acc: 1.0\n",
            "Epoch 1997/10000\n",
            "Step 0: Train Loss: 2.2065631100076644e-08, Train Acc: 1.0\n",
            "Epoch 1998/10000\n",
            "Step 0: Train Loss: 2.1421902474116905e-08, Train Acc: 1.0\n",
            "Epoch 1999/10000\n",
            "Step 0: Train Loss: 1.9752972590936224e-08, Train Acc: 1.0\n",
            "Epoch 2000/10000\n",
            "Step 0: Train Loss: 2.2554390355367104e-08, Train Acc: 1.0\n",
            "Epoch 2001/10000\n",
            "Step 0: Train Loss: 2.0563595271028134e-08, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 2000 1024 1.3165908833428746\n",
            "Epoch index and hidden dimension and ratio: 2000 20 1.746947799015459\n",
            "Epoch index and hidden dimension and ratio: 2000 20 2.2845852170943295\n",
            "Epoch index and hidden dimension and ratio: 2000 20 3.325447058843852\n",
            "MI(X;T): [12.923884930693088, 10.062324719162845, 7.982637902406978, 4.976193821301706], MI(Y;T): [3.310028848052609, 3.2602668989720236, 3.1836778622606756, 2.7623638847082717]\n",
            "Epoch index and hidden dimension and ratio: 2000 1024 1.3165960229375706\n",
            "Epoch index and hidden dimension and ratio: 2000 20 1.746960816879657\n",
            "Epoch index and hidden dimension and ratio: 2000 20 2.284625336283871\n",
            "Epoch index and hidden dimension and ratio: 2000 20 3.3255148402124157\n",
            "MI(X;T): [12.923884930693088, 10.061937888658415, 7.9822831873838, 4.976407286906914], MI(Y;T): [3.310028848052609, 3.26064238772224, 3.1833721355903037, 2.762737397478409]\n",
            "Epoch index and hidden dimension and ratio: 2000 1024 1.3166011625322667\n",
            "Epoch index and hidden dimension and ratio: 2000 20 1.7469717735820238\n",
            "Epoch index and hidden dimension and ratio: 2000 20 2.2846476811489325\n",
            "Epoch index and hidden dimension and ratio: 2000 20 3.3255582729340194\n",
            "MI(X;T): [12.923784930693088, 10.061784643572505, 7.982518258841578, 4.976766985852267], MI(Y;T): [3.310128848052609, 3.2606303089762303, 3.183466747565058, 2.762483746680341]\n",
            "Epoch index and hidden dimension and ratio: 2000 1024 1.3166063021269625\n",
            "Epoch index and hidden dimension and ratio: 2000 20 1.7469852253750284\n",
            "Epoch index and hidden dimension and ratio: 2000 20 2.2846695181761514\n",
            "Epoch index and hidden dimension and ratio: 2000 20 3.3256092734783267\n",
            "MI(X;T): [12.923484930693089, 10.062028672798567, 7.982364243424527, 4.976372715644745], MI(Y;T): [3.310028848052609, 3.2606303089762303, 3.1835401174416864, 2.762187653196208]\n",
            "Epoch index and hidden dimension and ratio: 2000 1024 1.3166112948760957\n",
            "Epoch index and hidden dimension and ratio: 2000 20 1.746996832970605\n",
            "Epoch index and hidden dimension and ratio: 2000 20 2.284683991554657\n",
            "Epoch index and hidden dimension and ratio: 2000 20 3.3256504029495426\n",
            "MI(X;T): [12.923522675068197, 10.062092247122415, 7.982286828532912, 4.976245267986327], MI(Y;T): [3.310128848052609, 3.2606303089762303, 3.1834044453819015, 2.76178531186033]\n",
            "Epoch index and hidden dimension and ratio: 2000 1024 1.3166161407796662\n",
            "Epoch index and hidden dimension and ratio: 2000 20 1.7470103932458114\n",
            "Epoch index and hidden dimension and ratio: 2000 20 2.2847012580412946\n",
            "Epoch index and hidden dimension and ratio: 2000 20 3.325694164706916\n",
            "MI(X;T): [12.923522675068199, 10.062389765201802, 7.982162290281597, 4.976153394962026], MI(Y;T): [3.310128848052609, 3.2604395371693053, 3.1834124116341673, 2.7617791848073865]\n",
            "Epoch 2002/10000\n",
            "Step 0: Train Loss: 2.0194045546872985e-08, Train Acc: 1.0\n",
            "Epoch 2003/10000\n",
            "Step 0: Train Loss: 2.0492070262889683e-08, Train Acc: 1.0\n",
            "Epoch 2004/10000\n",
            "Step 0: Train Loss: 1.98125782446823e-08, Train Acc: 1.0\n",
            "Epoch 2005/10000\n",
            "Step 0: Train Loss: 2.1004668226964895e-08, Train Acc: 1.0\n",
            "Epoch 2006/10000\n",
            "Step 0: Train Loss: 2.2721282633142437e-08, Train Acc: 1.0\n",
            "Epoch 2007/10000\n",
            "Step 0: Train Loss: 2.1195404542595497e-08, Train Acc: 1.0\n",
            "Epoch 2008/10000\n",
            "Step 0: Train Loss: 1.9657605321299343e-08, Train Acc: 1.0\n",
            "Epoch 2009/10000\n",
            "Step 0: Train Loss: 1.8823140379709002e-08, Train Acc: 1.0\n",
            "Epoch 2010/10000\n",
            "Step 0: Train Loss: 1.939534399753029e-08, Train Acc: 1.0\n",
            "Epoch 2011/10000\n",
            "Step 0: Train Loss: 1.900195378823355e-08, Train Acc: 1.0\n",
            "Epoch 2012/10000\n",
            "Step 0: Train Loss: 1.7142289365779106e-08, Train Acc: 1.0\n",
            "Epoch 2013/10000\n",
            "Step 0: Train Loss: 2.0515912524388114e-08, Train Acc: 1.0\n",
            "Epoch 2014/10000\n",
            "Step 0: Train Loss: 1.9288057373501033e-08, Train Acc: 1.0\n",
            "Epoch 2015/10000\n",
            "Step 0: Train Loss: 1.8870821349992184e-08, Train Acc: 1.0\n",
            "Epoch 2016/10000\n",
            "Step 0: Train Loss: 1.909732283422727e-08, Train Acc: 1.0\n",
            "Epoch 2017/10000\n",
            "Step 0: Train Loss: 1.8763534725962927e-08, Train Acc: 1.0\n",
            "Epoch 2018/10000\n",
            "Step 0: Train Loss: 1.944302852052715e-08, Train Acc: 1.0\n",
            "Epoch 2019/10000\n",
            "Step 0: Train Loss: 1.7511839089934256e-08, Train Acc: 1.0\n",
            "Epoch 2020/10000\n",
            "Step 0: Train Loss: 1.9085401703478055e-08, Train Acc: 1.0\n",
            "Epoch 2021/10000\n",
            "Step 0: Train Loss: 1.9228451719754958e-08, Train Acc: 1.0\n",
            "Epoch 2022/10000\n",
            "Step 0: Train Loss: 2.0027153269097653e-08, Train Acc: 1.0\n",
            "Epoch 2023/10000\n",
            "Step 0: Train Loss: 1.784562542184176e-08, Train Acc: 1.0\n",
            "Epoch 2024/10000\n",
            "Step 0: Train Loss: 1.9359584157996323e-08, Train Acc: 1.0\n",
            "Epoch 2025/10000\n",
            "Step 0: Train Loss: 1.6820425940977657e-08, Train Acc: 1.0\n",
            "Epoch 2026/10000\n",
            "Step 0: Train Loss: 1.7142289365779106e-08, Train Acc: 1.0\n",
            "Epoch 2027/10000\n",
            "Step 0: Train Loss: 1.902579604973198e-08, Train Acc: 1.0\n",
            "Epoch 2028/10000\n",
            "Step 0: Train Loss: 1.983641872982389e-08, Train Acc: 1.0\n",
            "Epoch 2029/10000\n",
            "Step 0: Train Loss: 1.775025815220488e-08, Train Acc: 1.0\n",
            "Epoch 2030/10000\n",
            "Step 0: Train Loss: 1.7690652498458803e-08, Train Acc: 1.0\n",
            "Epoch 2031/10000\n",
            "Step 0: Train Loss: 1.8501275178550713e-08, Train Acc: 1.0\n",
            "Epoch 2032/10000\n",
            "Step 0: Train Loss: 1.713037001138673e-08, Train Acc: 1.0\n",
            "Epoch 2033/10000\n",
            "Step 0: Train Loss: 1.7559523612931116e-08, Train Acc: 1.0\n",
            "Epoch 2034/10000\n",
            "Step 0: Train Loss: 1.8417827263306208e-08, Train Acc: 1.0\n",
            "Epoch 2035/10000\n",
            "Step 0: Train Loss: 1.8823140379709002e-08, Train Acc: 1.0\n",
            "Epoch 2036/10000\n",
            "Step 0: Train Loss: 1.7988675438118662e-08, Train Acc: 1.0\n",
            "Epoch 2037/10000\n",
            "Step 0: Train Loss: 1.7857546552590975e-08, Train Acc: 1.0\n",
            "Epoch 2038/10000\n",
            "Step 0: Train Loss: 1.7237656635415988e-08, Train Acc: 1.0\n",
            "Epoch 2039/10000\n",
            "Step 0: Train Loss: 1.904963653487357e-08, Train Acc: 1.0\n",
            "Epoch 2040/10000\n",
            "Step 0: Train Loss: 1.8322461770026166e-08, Train Acc: 1.0\n",
            "Epoch 2041/10000\n",
            "Step 0: Train Loss: 1.8489355824158338e-08, Train Acc: 1.0\n",
            "Epoch 2042/10000\n",
            "Step 0: Train Loss: 1.738070842804973e-08, Train Acc: 1.0\n",
            "Epoch 2043/10000\n",
            "Step 0: Train Loss: 1.7559521836574277e-08, Train Acc: 1.0\n",
            "Epoch 2044/10000\n",
            "Step 0: Train Loss: 1.941918803538556e-08, Train Acc: 1.0\n",
            "Epoch 2045/10000\n",
            "Step 0: Train Loss: 1.786946590698335e-08, Train Acc: 1.0\n",
            "Epoch 2046/10000\n",
            "Step 0: Train Loss: 1.8393986778164617e-08, Train Acc: 1.0\n",
            "Epoch 2047/10000\n",
            "Step 0: Train Loss: 1.8131727230752404e-08, Train Acc: 1.0\n",
            "Epoch 2048/10000\n",
            "Step 0: Train Loss: 1.7440314081795805e-08, Train Acc: 1.0\n",
            "Epoch 2049/10000\n",
            "Step 0: Train Loss: 1.7631048621069567e-08, Train Acc: 1.0\n",
            "Epoch 2050/10000\n",
            "Step 0: Train Loss: 1.752376199704031e-08, Train Acc: 1.0\n",
            "Epoch 2051/10000\n",
            "Step 0: Train Loss: 1.8203252238890855e-08, Train Acc: 1.0\n",
            "Epoch 2052/10000\n",
            "Step 0: Train Loss: 1.747607569768661e-08, Train Acc: 1.0\n",
            "Epoch 2053/10000\n",
            "Step 0: Train Loss: 1.747607392132977e-08, Train Acc: 1.0\n",
            "Epoch 2054/10000\n",
            "Step 0: Train Loss: 1.7404552465905e-08, Train Acc: 1.0\n",
            "Epoch 2055/10000\n",
            "Step 0: Train Loss: 1.8548959701547574e-08, Train Acc: 1.0\n",
            "Epoch 2056/10000\n",
            "Step 0: Train Loss: 1.634358781643641e-08, Train Acc: 1.0\n",
            "Epoch 2057/10000\n",
            "Step 0: Train Loss: 1.7487996828435826e-08, Train Acc: 1.0\n",
            "Epoch 2058/10000\n",
            "Step 0: Train Loss: 1.6236299416050315e-08, Train Acc: 1.0\n",
            "Epoch 2059/10000\n",
            "Step 0: Train Loss: 1.5723701451975103e-08, Train Acc: 1.0\n",
            "Epoch 2060/10000\n",
            "Step 0: Train Loss: 1.705884500324828e-08, Train Acc: 1.0\n",
            "Epoch 2061/10000\n",
            "Step 0: Train Loss: 1.6438956862430132e-08, Train Acc: 1.0\n",
            "Epoch 2062/10000\n",
            "Step 0: Train Loss: 1.708268548838987e-08, Train Acc: 1.0\n",
            "Epoch 2063/10000\n",
            "Step 0: Train Loss: 1.6093247623416573e-08, Train Acc: 1.0\n",
            "Epoch 2064/10000\n",
            "Step 0: Train Loss: 1.676082206358842e-08, Train Acc: 1.0\n",
            "Epoch 2065/10000\n",
            "Step 0: Train Loss: 1.703500096539301e-08, Train Acc: 1.0\n",
            "Epoch 2066/10000\n",
            "Step 0: Train Loss: 1.5783307105721178e-08, Train Acc: 1.0\n",
            "Epoch 2067/10000\n",
            "Step 0: Train Loss: 1.590251663685649e-08, Train Acc: 1.0\n",
            "Epoch 2068/10000\n",
            "Step 0: Train Loss: 1.6653531886845485e-08, Train Acc: 1.0\n",
            "Epoch 2069/10000\n",
            "Step 0: Train Loss: 1.6605850916562304e-08, Train Acc: 1.0\n",
            "Epoch 2070/10000\n",
            "Step 0: Train Loss: 1.6832345295370033e-08, Train Acc: 1.0\n",
            "Epoch 2071/10000\n",
            "Step 0: Train Loss: 1.62243782853011e-08, Train Acc: 1.0\n",
            "Epoch 2072/10000\n",
            "Step 0: Train Loss: 1.6891950949116108e-08, Train Acc: 1.0\n",
            "Epoch 2073/10000\n",
            "Step 0: Train Loss: 1.6045566653133392e-08, Train Acc: 1.0\n",
            "Epoch 2074/10000\n",
            "Step 0: Train Loss: 1.6582006878707034e-08, Train Acc: 1.0\n",
            "Epoch 2075/10000\n",
            "Step 0: Train Loss: 1.5890595506107275e-08, Train Acc: 1.0\n",
            "Epoch 2076/10000\n",
            "Step 0: Train Loss: 1.5246863327433857e-08, Train Acc: 1.0\n",
            "Epoch 2077/10000\n",
            "Step 0: Train Loss: 1.5914435991248865e-08, Train Acc: 1.0\n",
            "Epoch 2078/10000\n",
            "Step 0: Train Loss: 1.5866753244608844e-08, Train Acc: 1.0\n",
            "Epoch 2079/10000\n",
            "Step 0: Train Loss: 1.546143835184921e-08, Train Acc: 1.0\n",
            "Epoch 2080/10000\n",
            "Step 0: Train Loss: 1.580714759086277e-08, Train Acc: 1.0\n",
            "Epoch 2081/10000\n",
            "Step 0: Train Loss: 1.5819066945255145e-08, Train Acc: 1.0\n",
            "Epoch 2082/10000\n",
            "Step 0: Train Loss: 1.6105170530522628e-08, Train Acc: 1.0\n",
            "Epoch 2083/10000\n",
            "Step 0: Train Loss: 1.3577935220610016e-08, Train Acc: 1.0\n",
            "Epoch 2084/10000\n",
            "Step 0: Train Loss: 1.435279450845428e-08, Train Acc: 1.0\n",
            "Epoch 2085/10000\n",
            "Step 0: Train Loss: 1.6868108687617678e-08, Train Acc: 1.0\n",
            "Epoch 2086/10000\n",
            "Step 0: Train Loss: 1.5282628496038342e-08, Train Acc: 1.0\n",
            "Epoch 2087/10000\n",
            "Step 0: Train Loss: 1.4889237398563182e-08, Train Acc: 1.0\n",
            "Epoch 2088/10000\n",
            "Step 0: Train Loss: 1.620053780015951e-08, Train Acc: 1.0\n",
            "Epoch 2089/10000\n",
            "Step 0: Train Loss: 1.560449369719663e-08, Train Acc: 1.0\n",
            "Epoch 2090/10000\n",
            "Step 0: Train Loss: 1.342296140904864e-08, Train Acc: 1.0\n",
            "Epoch 2091/10000\n",
            "Step 0: Train Loss: 1.4245506108068184e-08, Train Acc: 1.0\n",
            "Epoch 2092/10000\n",
            "Step 0: Train Loss: 1.5056130564516934e-08, Train Acc: 1.0\n",
            "Epoch 2093/10000\n",
            "Step 0: Train Loss: 1.504420943376772e-08, Train Acc: 1.0\n",
            "Epoch 2094/10000\n",
            "Step 0: Train Loss: 1.5223022842292266e-08, Train Acc: 1.0\n",
            "Epoch 2095/10000\n",
            "Step 0: Train Loss: 1.4209744492177379e-08, Train Acc: 1.0\n",
            "Epoch 2096/10000\n",
            "Step 0: Train Loss: 1.4615055832223334e-08, Train Acc: 1.0\n",
            "Epoch 2097/10000\n",
            "Step 0: Train Loss: 1.4042849549866787e-08, Train Acc: 1.0\n",
            "Epoch 2098/10000\n",
            "Step 0: Train Loss: 1.3363356643480984e-08, Train Acc: 1.0\n",
            "Epoch 2099/10000\n",
            "Step 0: Train Loss: 1.562833418233822e-08, Train Acc: 1.0\n",
            "Epoch 2100/10000\n",
            "Step 0: Train Loss: 1.4519686786229613e-08, Train Acc: 1.0\n",
            "Epoch 2101/10000\n",
            "Step 0: Train Loss: 1.624822409951321e-08, Train Acc: 1.0\n",
            "Epoch 2102/10000\n",
            "Step 0: Train Loss: 1.428127038849425e-08, Train Acc: 1.0\n",
            "Epoch 2103/10000\n",
            "Step 0: Train Loss: 1.5079972826015364e-08, Train Acc: 1.0\n",
            "Epoch 2104/10000\n",
            "Step 0: Train Loss: 1.4126297465111293e-08, Train Acc: 1.0\n",
            "Epoch 2105/10000\n",
            "Step 0: Train Loss: 1.4626976962972549e-08, Train Acc: 1.0\n",
            "Epoch 2106/10000\n",
            "Step 0: Train Loss: 1.5199180580793836e-08, Train Acc: 1.0\n",
            "Epoch 2107/10000\n",
            "Step 0: Train Loss: 1.4245506996246604e-08, Train Acc: 1.0\n",
            "Epoch 2108/10000\n",
            "Step 0: Train Loss: 1.3256070019451727e-08, Train Acc: 1.0\n",
            "Epoch 2109/10000\n",
            "Step 0: Train Loss: 1.4626976962972549e-08, Train Acc: 1.0\n",
            "Epoch 2110/10000\n",
            "Step 0: Train Loss: 1.465081833629256e-08, Train Acc: 1.0\n",
            "Epoch 2111/10000\n",
            "Step 0: Train Loss: 1.393556292583753e-08, Train Acc: 1.0\n",
            "Epoch 2112/10000\n",
            "Step 0: Train Loss: 1.3375277774230199e-08, Train Acc: 1.0\n",
            "Epoch 2113/10000\n",
            "Step 0: Train Loss: 1.317262388056406e-08, Train Acc: 1.0\n",
            "Epoch 2114/10000\n",
            "Step 0: Train Loss: 1.4591212682546484e-08, Train Acc: 1.0\n",
            "Epoch 2115/10000\n",
            "Step 0: Train Loss: 1.47938701289263e-08, Train Acc: 1.0\n",
            "Epoch 2116/10000\n",
            "Step 0: Train Loss: 1.4531609693335668e-08, Train Acc: 1.0\n",
            "Epoch 2117/10000\n",
            "Step 0: Train Loss: 1.2850758679405772e-08, Train Acc: 1.0\n",
            "Epoch 2118/10000\n",
            "Step 0: Train Loss: 1.2469286936322987e-08, Train Acc: 1.0\n",
            "Epoch 2119/10000\n",
            "Step 0: Train Loss: 1.2612339617135149e-08, Train Acc: 1.0\n",
            "Epoch 2120/10000\n",
            "Step 0: Train Loss: 1.467465882143415e-08, Train Acc: 1.0\n",
            "Epoch 2121/10000\n",
            "Step 0: Train Loss: 1.425742812699582e-08, Train Acc: 1.0\n",
            "Epoch 2122/10000\n",
            "Step 0: Train Loss: 1.3673300713890058e-08, Train Acc: 1.0\n",
            "Epoch 2123/10000\n",
            "Step 0: Train Loss: 1.3363358419837823e-08, Train Acc: 1.0\n",
            "Epoch 2124/10000\n",
            "Step 0: Train Loss: 1.3780590890632993e-08, Train Acc: 1.0\n",
            "Epoch 2125/10000\n",
            "Step 0: Train Loss: 1.3744828386563768e-08, Train Acc: 1.0\n",
            "Epoch 2126/10000\n",
            "Step 0: Train Loss: 1.4042851326223627e-08, Train Acc: 1.0\n",
            "Epoch 2127/10000\n",
            "Step 0: Train Loss: 1.3136860488316415e-08, Train Acc: 1.0\n",
            "Epoch 2128/10000\n",
            "Step 0: Train Loss: 1.2862679810154987e-08, Train Acc: 1.0\n",
            "Epoch 2129/10000\n",
            "Step 0: Train Loss: 1.2493130974178257e-08, Train Acc: 1.0\n",
            "Epoch 2130/10000\n",
            "Step 0: Train Loss: 1.4197823361428163e-08, Train Acc: 1.0\n",
            "Epoch 2131/10000\n",
            "Step 0: Train Loss: 1.3661380471319262e-08, Train Acc: 1.0\n",
            "Epoch 2132/10000\n",
            "Step 0: Train Loss: 1.3244148888702512e-08, Train Acc: 1.0\n",
            "Epoch 2133/10000\n",
            "Step 0: Train Loss: 1.2373919666686106e-08, Train Acc: 1.0\n",
            "Epoch 2134/10000\n",
            "Step 0: Train Loss: 1.2409683947112171e-08, Train Acc: 1.0\n",
            "Epoch 2135/10000\n",
            "Step 0: Train Loss: 1.4388555236166667e-08, Train Acc: 1.0\n",
            "Epoch 2136/10000\n",
            "Step 0: Train Loss: 1.2850758679405772e-08, Train Acc: 1.0\n",
            "Epoch 2137/10000\n",
            "Step 0: Train Loss: 1.2958047079791868e-08, Train Acc: 1.0\n",
            "Epoch 2138/10000\n",
            "Step 0: Train Loss: 1.349448908172235e-08, Train Acc: 1.0\n",
            "Epoch 2139/10000\n",
            "Step 0: Train Loss: 1.118182879622509e-08, Train Acc: 1.0\n",
            "Epoch 2140/10000\n",
            "Step 0: Train Loss: 1.150369310920496e-08, Train Acc: 1.0\n",
            "Epoch 2141/10000\n",
            "Step 0: Train Loss: 1.1789794918115604e-08, Train Acc: 1.0\n",
            "Epoch 2142/10000\n",
            "Step 0: Train Loss: 1.2719628017521245e-08, Train Acc: 1.0\n",
            "Epoch 2143/10000\n",
            "Step 0: Train Loss: 1.3458725689474704e-08, Train Acc: 1.0\n",
            "Epoch 2144/10000\n",
            "Step 0: Train Loss: 1.2075898503383087e-08, Train Acc: 1.0\n",
            "Epoch 2145/10000\n",
            "Step 0: Train Loss: 1.3065335480177964e-08, Train Acc: 1.0\n",
            "Epoch 2146/10000\n",
            "Step 0: Train Loss: 1.2445447339359816e-08, Train Acc: 1.0\n",
            "Epoch 2147/10000\n",
            "Step 0: Train Loss: 1.2803075044587331e-08, Train Acc: 1.0\n",
            "Epoch 2148/10000\n",
            "Step 0: Train Loss: 1.2648103009382794e-08, Train Acc: 1.0\n",
            "Epoch 2149/10000\n",
            "Step 0: Train Loss: 1.2719627129342825e-08, Train Acc: 1.0\n",
            "Epoch 2150/10000\n",
            "Step 0: Train Loss: 1.1420247858495713e-08, Train Acc: 1.0\n",
            "Epoch 2151/10000\n",
            "Step 0: Train Loss: 1.2218949407838409e-08, Train Acc: 1.0\n",
            "Epoch 2152/10000\n",
            "Step 0: Train Loss: 1.233815893897372e-08, Train Acc: 1.0\n",
            "Epoch 2153/10000\n",
            "Step 0: Train Loss: 1.3005730714610308e-08, Train Acc: 1.0\n",
            "Epoch 2154/10000\n",
            "Step 0: Train Loss: 1.2254711911907634e-08, Train Acc: 1.0\n",
            "Epoch 2155/10000\n",
            "Step 0: Train Loss: 1.2397762816362956e-08, Train Acc: 1.0\n",
            "Epoch 2156/10000\n",
            "Step 0: Train Loss: 1.273154914827046e-08, Train Acc: 1.0\n",
            "Epoch 2157/10000\n",
            "Step 0: Train Loss: 1.189708509485854e-08, Train Acc: 1.0\n",
            "Epoch 2158/10000\n",
            "Step 0: Train Loss: 1.2242790781158419e-08, Train Acc: 1.0\n",
            "Epoch 2159/10000\n",
            "Step 0: Train Loss: 1.187324194518169e-08, Train Acc: 1.0\n",
            "Epoch 2160/10000\n",
            "Step 0: Train Loss: 1.1467930605135734e-08, Train Acc: 1.0\n",
            "Epoch 2161/10000\n",
            "Step 0: Train Loss: 1.2481209843429042e-08, Train Acc: 1.0\n",
            "Epoch 2162/10000\n",
            "Step 0: Train Loss: 1.159906215519868e-08, Train Acc: 1.0\n",
            "Epoch 2163/10000\n",
            "Step 0: Train Loss: 1.120567194590194e-08, Train Acc: 1.0\n",
            "Epoch 2164/10000\n",
            "Step 0: Train Loss: 1.2350079181544515e-08, Train Acc: 1.0\n",
            "Epoch 2165/10000\n",
            "Step 0: Train Loss: 1.312494024574562e-08, Train Acc: 1.0\n",
            "Epoch 2166/10000\n",
            "Step 0: Train Loss: 1.2409683947112171e-08, Train Acc: 1.0\n",
            "Epoch 2167/10000\n",
            "Step 0: Train Loss: 1.1909005337429335e-08, Train Acc: 1.0\n",
            "Epoch 2168/10000\n",
            "Step 0: Train Loss: 1.1658666032587917e-08, Train Acc: 1.0\n",
            "Epoch 2169/10000\n",
            "Step 0: Train Loss: 1.1229512431043531e-08, Train Acc: 1.0\n",
            "Epoch 2170/10000\n",
            "Step 0: Train Loss: 1.0609624290225383e-08, Train Acc: 1.0\n",
            "Epoch 2171/10000\n",
            "Step 0: Train Loss: 1.2457367581930612e-08, Train Acc: 1.0\n",
            "Epoch 2172/10000\n",
            "Step 0: Train Loss: 1.184939968368326e-08, Train Acc: 1.0\n",
            "Epoch 2173/10000\n",
            "Step 0: Train Loss: 1.192092558000013e-08, Train Acc: 1.0\n",
            "Epoch 2174/10000\n",
            "Step 0: Train Loss: 1.1444089231815724e-08, Train Acc: 1.0\n",
            "Epoch 2175/10000\n",
            "Step 0: Train Loss: 1.147985173588495e-08, Train Acc: 1.0\n",
            "Epoch 2176/10000\n",
            "Step 0: Train Loss: 1.2302395546726075e-08, Train Acc: 1.0\n",
            "Epoch 2177/10000\n",
            "Step 0: Train Loss: 1.0883804968386812e-08, Train Acc: 1.0\n",
            "Epoch 2178/10000\n",
            "Step 0: Train Loss: 1.2171263996663129e-08, Train Acc: 1.0\n",
            "Epoch 2179/10000\n",
            "Step 0: Train Loss: 1.1646745790017121e-08, Train Acc: 1.0\n",
            "Epoch 2180/10000\n",
            "Step 0: Train Loss: 1.1336800831429628e-08, Train Acc: 1.0\n",
            "Epoch 2181/10000\n",
            "Step 0: Train Loss: 1.1587139248092626e-08, Train Acc: 1.0\n",
            "Epoch 2182/10000\n",
            "Step 0: Train Loss: 1.0657307925043824e-08, Train Acc: 1.0\n",
            "Epoch 2183/10000\n",
            "Step 0: Train Loss: 1.1324879700680412e-08, Train Acc: 1.0\n",
            "Epoch 2184/10000\n",
            "Step 0: Train Loss: 1.0633466551723814e-08, Train Acc: 1.0\n",
            "Epoch 2185/10000\n",
            "Step 0: Train Loss: 1.1610981509591056e-08, Train Acc: 1.0\n",
            "Epoch 2186/10000\n",
            "Step 0: Train Loss: 1.0859964483245221e-08, Train Acc: 1.0\n",
            "Epoch 2187/10000\n",
            "Step 0: Train Loss: 1.152753448252497e-08, Train Acc: 1.0\n",
            "Epoch 2188/10000\n",
            "Step 0: Train Loss: 1.1384484466248068e-08, Train Acc: 1.0\n",
            "Epoch 2189/10000\n",
            "Step 0: Train Loss: 9.870526618271924e-09, Train Acc: 1.0\n",
            "Epoch 2190/10000\n",
            "Step 0: Train Loss: 1.032352336949316e-08, Train Acc: 1.0\n",
            "Epoch 2191/10000\n",
            "Step 0: Train Loss: 1.0943409733954468e-08, Train Acc: 1.0\n",
            "Epoch 2192/10000\n",
            "Step 0: Train Loss: 1.15513767440234e-08, Train Acc: 1.0\n",
            "Epoch 2193/10000\n",
            "Step 0: Train Loss: 9.596345940110496e-09, Train Acc: 1.0\n",
            "Epoch 2194/10000\n",
            "Step 0: Train Loss: 9.942051626410375e-09, Train Acc: 1.0\n",
            "Epoch 2195/10000\n",
            "Step 0: Train Loss: 1.0061262933902526e-08, Train Acc: 1.0\n",
            "Epoch 2196/10000\n",
            "Step 0: Train Loss: 1.0859963595066802e-08, Train Acc: 1.0\n",
            "Epoch 2197/10000\n",
            "Step 0: Train Loss: 1.0013579299084086e-08, Train Acc: 1.0\n",
            "Epoch 2198/10000\n",
            "Step 0: Train Loss: 1.032352336949316e-08, Train Acc: 1.0\n",
            "Epoch 2199/10000\n",
            "Step 0: Train Loss: 9.751318863493452e-09, Train Acc: 1.0\n",
            "Epoch 2200/10000\n",
            "Step 0: Train Loss: 1.0168550446110203e-08, Train Acc: 1.0\n",
            "Epoch 2201/10000\n",
            "Step 0: Train Loss: 1.0156630203539407e-08, Train Acc: 1.0\n",
            "Epoch 2202/10000\n",
            "Step 0: Train Loss: 1.0061262933902526e-08, Train Acc: 1.0\n",
            "Epoch 2203/10000\n",
            "Step 0: Train Loss: 9.739397732744237e-09, Train Acc: 1.0\n",
            "Epoch 2204/10000\n",
            "Step 0: Train Loss: 1.0335443612063955e-08, Train Acc: 1.0\n",
            "Epoch 2205/10000\n",
            "Step 0: Train Loss: 1.0621546309153018e-08, Train Acc: 1.0\n",
            "Epoch 2206/10000\n",
            "Step 0: Train Loss: 1.1515615128132595e-08, Train Acc: 1.0\n",
            "Epoch 2207/10000\n",
            "Step 0: Train Loss: 9.942052514588795e-09, Train Acc: 1.0\n",
            "Epoch 2208/10000\n",
            "Step 0: Train Loss: 9.822843871631903e-09, Train Acc: 1.0\n",
            "Epoch 2209/10000\n",
            "Step 0: Train Loss: 1.034736385463475e-08, Train Acc: 1.0\n",
            "Epoch 2210/10000\n",
            "Step 0: Train Loss: 9.965894776087225e-09, Train Acc: 1.0\n",
            "Epoch 2211/10000\n",
            "Step 0: Train Loss: 8.893010772226262e-09, Train Acc: 1.0\n",
            "Epoch 2212/10000\n",
            "Step 0: Train Loss: 9.357927766018292e-09, Train Acc: 1.0\n",
            "Epoch 2213/10000\n",
            "Step 0: Train Loss: 1.0204312950179428e-08, Train Acc: 1.0\n",
            "Epoch 2214/10000\n",
            "Step 0: Train Loss: 1.0097025437971752e-08, Train Acc: 1.0\n",
            "Epoch 2215/10000\n",
            "Step 0: Train Loss: 9.608266182681291e-09, Train Acc: 1.0\n",
            "Epoch 2216/10000\n",
            "Step 0: Train Loss: 9.083746199678444e-09, Train Acc: 1.0\n",
            "Epoch 2217/10000\n",
            "Step 0: Train Loss: 9.787081367562678e-09, Train Acc: 1.0\n",
            "Epoch 2218/10000\n",
            "Step 0: Train Loss: 1.0240076342427074e-08, Train Acc: 1.0\n",
            "Epoch 2219/10000\n",
            "Step 0: Train Loss: 8.964537556721552e-09, Train Acc: 1.0\n",
            "Epoch 2220/10000\n",
            "Step 0: Train Loss: 1.0073183176473322e-08, Train Acc: 1.0\n",
            "Epoch 2221/10000\n",
            "Step 0: Train Loss: 9.477137297153604e-09, Train Acc: 1.0\n",
            "Epoch 2222/10000\n",
            "Step 0: Train Loss: 9.679792967176581e-09, Train Acc: 1.0\n",
            "Epoch 2223/10000\n",
            "Step 0: Train Loss: 1.0180470688680998e-08, Train Acc: 1.0\n",
            "Epoch 2224/10000\n",
            "Step 0: Train Loss: 9.107588461176874e-09, Train Acc: 1.0\n",
            "Epoch 2225/10000\n",
            "Step 0: Train Loss: 1.0013579299084086e-08, Train Acc: 1.0\n",
            "Epoch 2226/10000\n",
            "Step 0: Train Loss: 9.310244131199852e-09, Train Acc: 1.0\n",
            "Epoch 2227/10000\n",
            "Step 0: Train Loss: 9.286401869701422e-09, Train Acc: 1.0\n",
            "Epoch 2228/10000\n",
            "Step 0: Train Loss: 9.46521439004755e-09, Train Acc: 1.0\n",
            "Epoch 2229/10000\n",
            "Step 0: Train Loss: 9.1433509652461e-09, Train Acc: 1.0\n",
            "Epoch 2230/10000\n",
            "Step 0: Train Loss: 9.21487686156297e-09, Train Acc: 1.0\n",
            "Epoch 2231/10000\n",
            "Step 0: Train Loss: 9.11950781556925e-09, Train Acc: 1.0\n",
            "Epoch 2232/10000\n",
            "Step 0: Train Loss: 9.071825068929229e-09, Train Acc: 1.0\n",
            "Epoch 2233/10000\n",
            "Step 0: Train Loss: 9.477137297153604e-09, Train Acc: 1.0\n",
            "Epoch 2234/10000\n",
            "Step 0: Train Loss: 1.0037420672404096e-08, Train Acc: 1.0\n",
            "Epoch 2235/10000\n",
            "Step 0: Train Loss: 9.19103460006454e-09, Train Acc: 1.0\n",
            "Epoch 2236/10000\n",
            "Step 0: Train Loss: 9.036062564860003e-09, Train Acc: 1.0\n",
            "Epoch 2237/10000\n",
            "Step 0: Train Loss: 9.417532531585948e-09, Train Acc: 1.0\n",
            "Epoch 2238/10000\n",
            "Step 0: Train Loss: 8.893011660404682e-09, Train Acc: 1.0\n",
            "Epoch 2239/10000\n",
            "Step 0: Train Loss: 8.928774164473907e-09, Train Acc: 1.0\n",
            "Epoch 2240/10000\n",
            "Step 0: Train Loss: 9.632107556001301e-09, Train Acc: 1.0\n",
            "Epoch 2241/10000\n",
            "Step 0: Train Loss: 8.928774164473907e-09, Train Acc: 1.0\n",
            "Epoch 2242/10000\n",
            "Step 0: Train Loss: 9.298323000450637e-09, Train Acc: 1.0\n",
            "Epoch 2243/10000\n",
            "Step 0: Train Loss: 8.702277121130919e-09, Train Acc: 1.0\n",
            "Epoch 2244/10000\n",
            "Step 0: Train Loss: 8.77380212926937e-09, Train Acc: 1.0\n",
            "Epoch 2245/10000\n",
            "Step 0: Train Loss: 8.082388092134352e-09, Train Acc: 1.0\n",
            "Epoch 2246/10000\n",
            "Step 0: Train Loss: 8.559225328497178e-09, Train Acc: 1.0\n",
            "Epoch 2247/10000\n",
            "Step 0: Train Loss: 8.535383955177167e-09, Train Acc: 1.0\n",
            "Epoch 2248/10000\n",
            "Step 0: Train Loss: 8.881090529655467e-09, Train Acc: 1.0\n",
            "Epoch 2249/10000\n",
            "Step 0: Train Loss: 8.463858058860296e-09, Train Acc: 1.0\n",
            "Epoch 2250/10000\n",
            "Step 0: Train Loss: 8.714197363701714e-09, Train Acc: 1.0\n",
            "Epoch 2251/10000\n",
            "Step 0: Train Loss: 9.000300060790778e-09, Train Acc: 1.0\n",
            "Epoch 2252/10000\n",
            "Step 0: Train Loss: 8.30888602365576e-09, Train Acc: 1.0\n",
            "Epoch 2253/10000\n",
            "Step 0: Train Loss: 9.047983695609219e-09, Train Acc: 1.0\n",
            "Epoch 2254/10000\n",
            "Step 0: Train Loss: 9.298322112272217e-09, Train Acc: 1.0\n",
            "Epoch 2255/10000\n",
            "Step 0: Train Loss: 8.261202388837319e-09, Train Acc: 1.0\n",
            "Epoch 2256/10000\n",
            "Step 0: Train Loss: 8.523462824427952e-09, Train Acc: 1.0\n",
            "Epoch 2257/10000\n",
            "Step 0: Train Loss: 7.760522890976063e-09, Train Acc: 1.0\n",
            "Epoch 2258/10000\n",
            "Step 0: Train Loss: 7.748602648405267e-09, Train Acc: 1.0\n",
            "Epoch 2259/10000\n",
            "Step 0: Train Loss: 8.511541693678737e-09, Train Acc: 1.0\n",
            "Epoch 2260/10000\n",
            "Step 0: Train Loss: 8.761880998520155e-09, Train Acc: 1.0\n",
            "Epoch 2261/10000\n",
            "Step 0: Train Loss: 9.131429834496885e-09, Train Acc: 1.0\n",
            "Epoch 2262/10000\n",
            "Step 0: Train Loss: 7.188318829065565e-09, Train Acc: 1.0\n",
            "Epoch 2263/10000\n",
            "Step 0: Train Loss: 8.141992857702007e-09, Train Acc: 1.0\n",
            "Epoch 2264/10000\n",
            "Step 0: Train Loss: 8.77380212926937e-09, Train Acc: 1.0\n",
            "Epoch 2265/10000\n",
            "Step 0: Train Loss: 8.785723260018585e-09, Train Acc: 1.0\n",
            "Epoch 2266/10000\n",
            "Step 0: Train Loss: 7.998941953246685e-09, Train Acc: 1.0\n",
            "Epoch 2267/10000\n",
            "Step 0: Train Loss: 7.677076752088396e-09, Train Acc: 1.0\n",
            "Epoch 2268/10000\n",
            "Step 0: Train Loss: 8.034704457315911e-09, Train Acc: 1.0\n",
            "Epoch 2269/10000\n",
            "Step 0: Train Loss: 8.571146459246393e-09, Train Acc: 1.0\n",
            "Epoch 2270/10000\n",
            "Step 0: Train Loss: 8.547304197747962e-09, Train Acc: 1.0\n",
            "Epoch 2271/10000\n",
            "Step 0: Train Loss: 9.131429834496885e-09, Train Acc: 1.0\n",
            "Epoch 2272/10000\n",
            "Step 0: Train Loss: 8.678434859632489e-09, Train Acc: 1.0\n",
            "Epoch 2273/10000\n",
            "Step 0: Train Loss: 8.40425329329264e-09, Train Acc: 1.0\n",
            "Epoch 2274/10000\n",
            "Step 0: Train Loss: 8.690355990381704e-09, Train Acc: 1.0\n",
            "Epoch 2275/10000\n",
            "Step 0: Train Loss: 8.654592598134059e-09, Train Acc: 1.0\n",
            "Epoch 2276/10000\n",
            "Step 0: Train Loss: 7.59363061320073e-09, Train Acc: 1.0\n",
            "Epoch 2277/10000\n",
            "Step 0: Train Loss: 7.581709482451515e-09, Train Acc: 1.0\n",
            "Epoch 2278/10000\n",
            "Step 0: Train Loss: 7.855891048791364e-09, Train Acc: 1.0\n",
            "Epoch 2279/10000\n",
            "Step 0: Train Loss: 7.605550855771526e-09, Train Acc: 1.0\n",
            "Epoch 2280/10000\n",
            "Step 0: Train Loss: 8.523462824427952e-09, Train Acc: 1.0\n",
            "Epoch 2281/10000\n",
            "Step 0: Train Loss: 7.998941953246685e-09, Train Acc: 1.0\n",
            "Epoch 2282/10000\n",
            "Step 0: Train Loss: 8.153913988451222e-09, Train Acc: 1.0\n",
            "Epoch 2283/10000\n",
            "Step 0: Train Loss: 8.177756249949653e-09, Train Acc: 1.0\n",
            "Epoch 2284/10000\n",
            "Step 0: Train Loss: 7.474421082065419e-09, Train Acc: 1.0\n",
            "Epoch 2285/10000\n",
            "Step 0: Train Loss: 7.808207413972923e-09, Train Acc: 1.0\n",
            "Epoch 2286/10000\n",
            "Step 0: Train Loss: 7.510183586134644e-09, Train Acc: 1.0\n",
            "Epoch 2287/10000\n",
            "Step 0: Train Loss: 7.5697883517023e-09, Train Acc: 1.0\n",
            "Epoch 2288/10000\n",
            "Step 0: Train Loss: 6.985662714953378e-09, Train Acc: 1.0\n",
            "Epoch 2289/10000\n",
            "Step 0: Train Loss: 6.926058393474932e-09, Train Acc: 1.0\n",
            "Epoch 2290/10000\n",
            "Step 0: Train Loss: 7.057188611270249e-09, Train Acc: 1.0\n",
            "Epoch 2291/10000\n",
            "Step 0: Train Loss: 8.0108630839959e-09, Train Acc: 1.0\n",
            "Epoch 2292/10000\n",
            "Step 0: Train Loss: 7.188318829065565e-09, Train Acc: 1.0\n",
            "Epoch 2293/10000\n",
            "Step 0: Train Loss: 7.736681517656052e-09, Train Acc: 1.0\n",
            "Epoch 2294/10000\n",
            "Step 0: Train Loss: 8.058546718814341e-09, Train Acc: 1.0\n",
            "Epoch 2295/10000\n",
            "Step 0: Train Loss: 7.975099691748255e-09, Train Acc: 1.0\n",
            "Epoch 2296/10000\n",
            "Step 0: Train Loss: 7.724760386906837e-09, Train Acc: 1.0\n",
            "Epoch 2297/10000\n",
            "Step 0: Train Loss: 7.009504976451808e-09, Train Acc: 1.0\n",
            "Epoch 2298/10000\n",
            "Step 0: Train Loss: 7.796286283223708e-09, Train Acc: 1.0\n",
            "Epoch 2299/10000\n",
            "Step 0: Train Loss: 7.474421082065419e-09, Train Acc: 1.0\n",
            "Epoch 2300/10000\n",
            "Step 0: Train Loss: 6.806848862339621e-09, Train Acc: 1.0\n",
            "Epoch 2301/10000\n",
            "Step 0: Train Loss: 6.866453627907276e-09, Train Acc: 1.0\n",
            "Epoch 2302/10000\n",
            "Step 0: Train Loss: 7.343290864270102e-09, Train Acc: 1.0\n",
            "Epoch 2303/10000\n",
            "Step 0: Train Loss: 7.164477011656345e-09, Train Acc: 1.0\n",
            "Epoch 2304/10000\n",
            "Step 0: Train Loss: 7.12871406349791e-09, Train Acc: 1.0\n",
            "Epoch 2305/10000\n",
            "Step 0: Train Loss: 6.985662714953378e-09, Train Acc: 1.0\n",
            "Epoch 2306/10000\n",
            "Step 0: Train Loss: 6.508826366768972e-09, Train Acc: 1.0\n",
            "Epoch 2307/10000\n",
            "Step 0: Train Loss: 7.462499951316204e-09, Train Acc: 1.0\n",
            "Epoch 2308/10000\n",
            "Step 0: Train Loss: 7.1167929327486945e-09, Train Acc: 1.0\n",
            "Epoch 2309/10000\n",
            "Step 0: Train Loss: 6.890296333494916e-09, Train Acc: 1.0\n",
            "Epoch 2310/10000\n",
            "Step 0: Train Loss: 5.912780043360044e-09, Train Acc: 1.0\n",
            "Epoch 2311/10000\n",
            "Step 0: Train Loss: 6.580351374907423e-09, Train Acc: 1.0\n",
            "Epoch 2312/10000\n",
            "Step 0: Train Loss: 6.926058393474932e-09, Train Acc: 1.0\n",
            "Epoch 2313/10000\n",
            "Step 0: Train Loss: 7.092950671250264e-09, Train Acc: 1.0\n",
            "Epoch 2314/10000\n",
            "Step 0: Train Loss: 6.80684975051804e-09, Train Acc: 1.0\n",
            "Epoch 2315/10000\n",
            "Step 0: Train Loss: 7.414816316497763e-09, Train Acc: 1.0\n",
            "Epoch 2316/10000\n",
            "Step 0: Train Loss: 7.009504976451808e-09, Train Acc: 1.0\n",
            "Epoch 2317/10000\n",
            "Step 0: Train Loss: 7.224081333134791e-09, Train Acc: 1.0\n",
            "Epoch 2318/10000\n",
            "Step 0: Train Loss: 6.222724113769118e-09, Train Acc: 1.0\n",
            "Epoch 2319/10000\n",
            "Step 0: Train Loss: 7.641314248019171e-09, Train Acc: 1.0\n",
            "Epoch 2320/10000\n",
            "Step 0: Train Loss: 5.888938225950824e-09, Train Acc: 1.0\n",
            "Epoch 2321/10000\n",
            "Step 0: Train Loss: 6.616114767155068e-09, Train Acc: 1.0\n",
            "Epoch 2322/10000\n",
            "Step 0: Train Loss: 6.747244540861175e-09, Train Acc: 1.0\n",
            "Epoch 2323/10000\n",
            "Step 0: Train Loss: 6.818769993088836e-09, Train Acc: 1.0\n",
            "Epoch 2324/10000\n",
            "Step 0: Train Loss: 6.985662714953378e-09, Train Acc: 1.0\n",
            "Epoch 2325/10000\n",
            "Step 0: Train Loss: 7.057188611270249e-09, Train Acc: 1.0\n",
            "Epoch 2326/10000\n",
            "Step 0: Train Loss: 6.234645244518333e-09, Train Acc: 1.0\n",
            "Epoch 2327/10000\n",
            "Step 0: Train Loss: 7.104872246088689e-09, Train Acc: 1.0\n",
            "Epoch 2328/10000\n",
            "Step 0: Train Loss: 7.462499951316204e-09, Train Acc: 1.0\n",
            "Epoch 2329/10000\n",
            "Step 0: Train Loss: 6.473063862699746e-09, Train Acc: 1.0\n",
            "Epoch 2330/10000\n",
            "Step 0: Train Loss: 7.212160646474786e-09, Train Acc: 1.0\n",
            "Epoch 2331/10000\n",
            "Step 0: Train Loss: 6.568430244158208e-09, Train Acc: 1.0\n",
            "Epoch 2332/10000\n",
            "Step 0: Train Loss: 6.806848862339621e-09, Train Acc: 1.0\n",
            "Epoch 2333/10000\n",
            "Step 0: Train Loss: 6.723402279362745e-09, Train Acc: 1.0\n",
            "Epoch 2334/10000\n",
            "Step 0: Train Loss: 6.270407748587559e-09, Train Acc: 1.0\n",
            "Epoch 2335/10000\n",
            "Step 0: Train Loss: 6.377696148973655e-09, Train Acc: 1.0\n",
            "Epoch 2336/10000\n",
            "Step 0: Train Loss: 6.747244984950385e-09, Train Acc: 1.0\n",
            "Epoch 2337/10000\n",
            "Step 0: Train Loss: 5.793570956313943e-09, Train Acc: 1.0\n",
            "Epoch 2338/10000\n",
            "Step 0: Train Loss: 5.793570956313943e-09, Train Acc: 1.0\n",
            "Epoch 2339/10000\n",
            "Step 0: Train Loss: 6.163119348201462e-09, Train Acc: 1.0\n",
            "Epoch 2340/10000\n",
            "Step 0: Train Loss: 6.091593895973801e-09, Train Acc: 1.0\n",
            "Epoch 2341/10000\n",
            "Step 0: Train Loss: 6.2584870619275534e-09, Train Acc: 1.0\n",
            "Epoch 2342/10000\n",
            "Step 0: Train Loss: 6.353854331564435e-09, Train Acc: 1.0\n",
            "Epoch 2343/10000\n",
            "Step 0: Train Loss: 7.021425663111813e-09, Train Acc: 1.0\n",
            "Epoch 2344/10000\n",
            "Step 0: Train Loss: 7.021425663111813e-09, Train Acc: 1.0\n",
            "Epoch 2345/10000\n",
            "Step 0: Train Loss: 6.234645244518333e-09, Train Acc: 1.0\n",
            "Epoch 2346/10000\n",
            "Step 0: Train Loss: 6.115435269293812e-09, Train Acc: 1.0\n",
            "Epoch 2347/10000\n",
            "Step 0: Train Loss: 6.663797957884299e-09, Train Acc: 1.0\n",
            "Epoch 2348/10000\n",
            "Step 0: Train Loss: 5.793570956313943e-09, Train Acc: 1.0\n",
            "Epoch 2349/10000\n",
            "Step 0: Train Loss: 6.4015375222936655e-09, Train Acc: 1.0\n",
            "Epoch 2350/10000\n",
            "Step 0: Train Loss: 6.0200684437461405e-09, Train Acc: 1.0\n",
            "Epoch 2351/10000\n",
            "Step 0: Train Loss: 6.568431132336627e-09, Train Acc: 1.0\n",
            "Epoch 2352/10000\n",
            "Step 0: Train Loss: 6.353853887475225e-09, Train Acc: 1.0\n",
            "Epoch 2353/10000\n",
            "Step 0: Train Loss: 5.912780043360044e-09, Train Acc: 1.0\n",
            "Epoch 2354/10000\n",
            "Step 0: Train Loss: 6.055831391904576e-09, Train Acc: 1.0\n",
            "Epoch 2355/10000\n",
            "Step 0: Train Loss: 5.888938225950824e-09, Train Acc: 1.0\n",
            "Epoch 2356/10000\n",
            "Step 0: Train Loss: 6.461142731950531e-09, Train Acc: 1.0\n",
            "Epoch 2357/10000\n",
            "Step 0: Train Loss: 5.817412773723163e-09, Train Acc: 1.0\n",
            "Epoch 2358/10000\n",
            "Step 0: Train Loss: 5.888938225950824e-09, Train Acc: 1.0\n",
            "Epoch 2359/10000\n",
            "Step 0: Train Loss: 6.079673209313796e-09, Train Acc: 1.0\n",
            "Epoch 2360/10000\n",
            "Step 0: Train Loss: 6.294249565996779e-09, Train Acc: 1.0\n",
            "Epoch 2361/10000\n",
            "Step 0: Train Loss: 5.364417354769557e-09, Train Acc: 1.0\n",
            "Epoch 2362/10000\n",
            "Step 0: Train Loss: 5.888938225950824e-09, Train Acc: 1.0\n",
            "Epoch 2363/10000\n",
            "Step 0: Train Loss: 5.590914842201755e-09, Train Acc: 1.0\n",
            "Epoch 2364/10000\n",
            "Step 0: Train Loss: 5.447863937746433e-09, Train Acc: 1.0\n",
            "Epoch 2365/10000\n",
            "Step 0: Train Loss: 5.924701174109259e-09, Train Acc: 1.0\n",
            "Epoch 2366/10000\n",
            "Step 0: Train Loss: 5.16176168474658e-09, Train Acc: 1.0\n",
            "Epoch 2367/10000\n",
            "Step 0: Train Loss: 5.328654406611122e-09, Train Acc: 1.0\n",
            "Epoch 2368/10000\n",
            "Step 0: Train Loss: 5.841254591132383e-09, Train Acc: 1.0\n",
            "Epoch 2369/10000\n",
            "Step 0: Train Loss: 5.400180302927993e-09, Train Acc: 1.0\n",
            "Epoch 2370/10000\n",
            "Step 0: Train Loss: 5.60283597295097e-09, Train Acc: 1.0\n",
            "Epoch 2371/10000\n",
            "Step 0: Train Loss: 5.984305051498495e-09, Train Acc: 1.0\n",
            "Epoch 2372/10000\n",
            "Step 0: Train Loss: 5.269050085132676e-09, Train Acc: 1.0\n",
            "Epoch 2373/10000\n",
            "Step 0: Train Loss: 5.2928914584526865e-09, Train Acc: 1.0\n",
            "Epoch 2374/10000\n",
            "Step 0: Train Loss: 5.55515233813253e-09, Train Acc: 1.0\n",
            "Epoch 2375/10000\n",
            "Step 0: Train Loss: 5.6147566596109755e-09, Train Acc: 1.0\n",
            "Epoch 2376/10000\n",
            "Step 0: Train Loss: 5.841254591132383e-09, Train Acc: 1.0\n",
            "Epoch 2377/10000\n",
            "Step 0: Train Loss: 5.20944531956502e-09, Train Acc: 1.0\n",
            "Epoch 2378/10000\n",
            "Step 0: Train Loss: 5.447863937746433e-09, Train Acc: 1.0\n",
            "Epoch 2379/10000\n",
            "Step 0: Train Loss: 5.6266777903601906e-09, Train Acc: 1.0\n",
            "Epoch 2380/10000\n",
            "Step 0: Train Loss: 5.567073024792535e-09, Train Acc: 1.0\n",
            "Epoch 2381/10000\n",
            "Step 0: Train Loss: 5.471705755155654e-09, Train Acc: 1.0\n",
            "Epoch 2382/10000\n",
            "Step 0: Train Loss: 5.030631466951263e-09, Train Acc: 1.0\n",
            "Epoch 2383/10000\n",
            "Step 0: Train Loss: 4.911421935815952e-09, Train Acc: 1.0\n",
            "Epoch 2384/10000\n",
            "Step 0: Train Loss: 5.269050085132676e-09, Train Acc: 1.0\n",
            "Epoch 2385/10000\n",
            "Step 0: Train Loss: 5.6982032425878515e-09, Train Acc: 1.0\n",
            "Epoch 2386/10000\n",
            "Step 0: Train Loss: 5.9485429915184795e-09, Train Acc: 1.0\n",
            "Epoch 2387/10000\n",
            "Step 0: Train Loss: 5.912779599270834e-09, Train Acc: 1.0\n",
            "Epoch 2388/10000\n",
            "Step 0: Train Loss: 5.495547572564874e-09, Train Acc: 1.0\n",
            "Epoch 2389/10000\n",
            "Step 0: Train Loss: 5.745886433317082e-09, Train Acc: 1.0\n",
            "Epoch 2390/10000\n",
            "Step 0: Train Loss: 4.994868518792828e-09, Train Acc: 1.0\n",
            "Epoch 2391/10000\n",
            "Step 0: Train Loss: 6.1750404789506774e-09, Train Acc: 1.0\n",
            "Epoch 2392/10000\n",
            "Step 0: Train Loss: 5.793570956313943e-09, Train Acc: 1.0\n",
            "Epoch 2393/10000\n",
            "Step 0: Train Loss: 5.483626441815659e-09, Train Acc: 1.0\n",
            "Epoch 2394/10000\n",
            "Step 0: Train Loss: 5.7816498255647275e-09, Train Acc: 1.0\n",
            "Epoch 2395/10000\n",
            "Step 0: Train Loss: 5.6266777903601906e-09, Train Acc: 1.0\n",
            "Epoch 2396/10000\n",
            "Step 0: Train Loss: 5.269050085132676e-09, Train Acc: 1.0\n",
            "Epoch 2397/10000\n",
            "Step 0: Train Loss: 4.696845135043759e-09, Train Acc: 1.0\n",
            "Epoch 2398/10000\n",
            "Step 0: Train Loss: 5.459784180317229e-09, Train Acc: 1.0\n",
            "Epoch 2399/10000\n",
            "Step 0: Train Loss: 4.327296299067029e-09, Train Acc: 1.0\n",
            "Epoch 2400/10000\n",
            "Step 0: Train Loss: 4.935263753225172e-09, Train Acc: 1.0\n",
            "Epoch 2401/10000\n",
            "Step 0: Train Loss: 4.315375612407024e-09, Train Acc: 1.0\n",
            "Epoch 2402/10000\n",
            "Step 0: Train Loss: 5.3763380414295625e-09, Train Acc: 1.0\n",
            "Epoch 2403/10000\n",
            "Step 0: Train Loss: 5.722045059997072e-09, Train Acc: 1.0\n",
            "Epoch 2404/10000\n",
            "Step 0: Train Loss: 4.637240813565313e-09, Train Acc: 1.0\n",
            "Epoch 2405/10000\n",
            "Step 0: Train Loss: 4.81605466617907e-09, Train Acc: 1.0\n",
            "Epoch 2406/10000\n",
            "Step 0: Train Loss: 5.149840553997365e-09, Train Acc: 1.0\n",
            "Epoch 2407/10000\n",
            "Step 0: Train Loss: 5.078315101769704e-09, Train Acc: 1.0\n",
            "Epoch 2408/10000\n",
            "Step 0: Train Loss: 5.149840553997365e-09, Train Acc: 1.0\n",
            "Epoch 2409/10000\n",
            "Step 0: Train Loss: 4.780291718020635e-09, Train Acc: 1.0\n",
            "Epoch 2410/10000\n",
            "Step 0: Train Loss: 4.923343066565167e-09, Train Acc: 1.0\n",
            "Epoch 2411/10000\n",
            "Step 0: Train Loss: 5.424022120337213e-09, Train Acc: 1.0\n",
            "Epoch 2412/10000\n",
            "Step 0: Train Loss: 5.018710336202048e-09, Train Acc: 1.0\n",
            "Epoch 2413/10000\n",
            "Step 0: Train Loss: 5.125998736588144e-09, Train Acc: 1.0\n",
            "Epoch 2414/10000\n",
            "Step 0: Train Loss: 5.340575093271127e-09, Train Acc: 1.0\n",
            "Epoch 2415/10000\n",
            "Step 0: Train Loss: 4.732608083202194e-09, Train Acc: 1.0\n",
            "Epoch 2416/10000\n",
            "Step 0: Train Loss: 5.20944531956502e-09, Train Acc: 1.0\n",
            "Epoch 2417/10000\n",
            "Step 0: Train Loss: 5.3763380414295625e-09, Train Acc: 1.0\n",
            "Epoch 2418/10000\n",
            "Step 0: Train Loss: 5.257128954383461e-09, Train Acc: 1.0\n",
            "Epoch 2419/10000\n",
            "Step 0: Train Loss: 4.7445292139514095e-09, Train Acc: 1.0\n",
            "Epoch 2420/10000\n",
            "Step 0: Train Loss: 4.827975352839076e-09, Train Acc: 1.0\n",
            "Epoch 2421/10000\n",
            "Step 0: Train Loss: 4.410742882043905e-09, Train Acc: 1.0\n",
            "Epoch 2422/10000\n",
            "Step 0: Train Loss: 4.446505830202341e-09, Train Acc: 1.0\n",
            "Epoch 2423/10000\n",
            "Step 0: Train Loss: 5.2928914584526865e-09, Train Acc: 1.0\n",
            "Epoch 2424/10000\n",
            "Step 0: Train Loss: 5.590914398112545e-09, Train Acc: 1.0\n",
            "Epoch 2425/10000\n",
            "Step 0: Train Loss: 4.79221284876985e-09, Train Acc: 1.0\n",
            "Epoch 2426/10000\n",
            "Step 0: Train Loss: 5.090235788429709e-09, Train Acc: 1.0\n",
            "Epoch 2427/10000\n",
            "Step 0: Train Loss: 5.1856035021558e-09, Train Acc: 1.0\n",
            "Epoch 2428/10000\n",
            "Step 0: Train Loss: 5.042552153611268e-09, Train Acc: 1.0\n",
            "Epoch 2429/10000\n",
            "Step 0: Train Loss: 4.708766265792974e-09, Train Acc: 1.0\n",
            "Epoch 2430/10000\n",
            "Step 0: Train Loss: 4.827975352839076e-09, Train Acc: 1.0\n",
            "Epoch 2431/10000\n",
            "Step 0: Train Loss: 4.482268778360776e-09, Train Acc: 1.0\n",
            "Epoch 2432/10000\n",
            "Step 0: Train Loss: 4.804133535429855e-09, Train Acc: 1.0\n",
            "Epoch 2433/10000\n",
            "Step 0: Train Loss: 3.731249975658102e-09, Train Acc: 1.0\n",
            "Epoch 2434/10000\n",
            "Step 0: Train Loss: 4.887580118406731e-09, Train Acc: 1.0\n",
            "Epoch 2435/10000\n",
            "Step 0: Train Loss: 4.959105570634392e-09, Train Acc: 1.0\n",
            "Epoch 2436/10000\n",
            "Step 0: Train Loss: 4.720686952452979e-09, Train Acc: 1.0\n",
            "Epoch 2437/10000\n",
            "Step 0: Train Loss: 4.541873099839222e-09, Train Acc: 1.0\n",
            "Epoch 2438/10000\n",
            "Step 0: Train Loss: 4.434584699453126e-09, Train Acc: 1.0\n",
            "Epoch 2439/10000\n",
            "Step 0: Train Loss: 4.565714917248442e-09, Train Acc: 1.0\n",
            "Epoch 2440/10000\n",
            "Step 0: Train Loss: 4.79221284876985e-09, Train Acc: 1.0\n",
            "Epoch 2441/10000\n",
            "Step 0: Train Loss: 4.315375612407024e-09, Train Acc: 1.0\n",
            "Epoch 2442/10000\n",
            "Step 0: Train Loss: 4.637240813565313e-09, Train Acc: 1.0\n",
            "Epoch 2443/10000\n",
            "Step 0: Train Loss: 4.553794230588437e-09, Train Acc: 1.0\n",
            "Epoch 2444/10000\n",
            "Step 0: Train Loss: 5.125998736588144e-09, Train Acc: 1.0\n",
            "Epoch 2445/10000\n",
            "Step 0: Train Loss: 4.994868074703618e-09, Train Acc: 1.0\n",
            "Epoch 2446/10000\n",
            "Step 0: Train Loss: 4.327296299067029e-09, Train Acc: 1.0\n",
            "Epoch 2447/10000\n",
            "Step 0: Train Loss: 4.470347647611561e-09, Train Acc: 1.0\n",
            "Epoch 2448/10000\n",
            "Step 0: Train Loss: 4.2557708468393685e-09, Train Acc: 1.0\n",
            "Epoch 2449/10000\n",
            "Step 0: Train Loss: 4.625319682816098e-09, Train Acc: 1.0\n",
            "Epoch 2450/10000\n",
            "Step 0: Train Loss: 4.2557708468393685e-09, Train Acc: 1.0\n",
            "Epoch 2451/10000\n",
            "Step 0: Train Loss: 4.887580118406731e-09, Train Acc: 1.0\n",
            "Epoch 2452/10000\n",
            "Step 0: Train Loss: 4.5776360479976574e-09, Train Acc: 1.0\n",
            "Epoch 2453/10000\n",
            "Step 0: Train Loss: 4.208086767931718e-09, Train Acc: 1.0\n",
            "Epoch 2454/10000\n",
            "Step 0: Train Loss: 4.291533794997804e-09, Train Acc: 1.0\n",
            "Epoch 2455/10000\n",
            "Step 0: Train Loss: 4.124640629044052e-09, Train Acc: 1.0\n",
            "Epoch 2456/10000\n",
            "Step 0: Train Loss: 4.37498037797468e-09, Train Acc: 1.0\n",
            "Epoch 2457/10000\n",
            "Step 0: Train Loss: 4.625319682816098e-09, Train Acc: 1.0\n",
            "Epoch 2458/10000\n",
            "Step 0: Train Loss: 4.136561759793267e-09, Train Acc: 1.0\n",
            "Epoch 2459/10000\n",
            "Step 0: Train Loss: 4.279612664248589e-09, Train Acc: 1.0\n",
            "Epoch 2460/10000\n",
            "Step 0: Train Loss: 4.3511385605654596e-09, Train Acc: 1.0\n",
            "Epoch 2461/10000\n",
            "Step 0: Train Loss: 4.3392174298162445e-09, Train Acc: 1.0\n",
            "Epoch 2462/10000\n",
            "Step 0: Train Loss: 4.5776360479976574e-09, Train Acc: 1.0\n",
            "Epoch 2463/10000\n",
            "Step 0: Train Loss: 4.4941894650207814e-09, Train Acc: 1.0\n",
            "Epoch 2464/10000\n",
            "Step 0: Train Loss: 3.921984959021074e-09, Train Acc: 1.0\n",
            "Epoch 2465/10000\n",
            "Step 0: Train Loss: 4.935263753225172e-09, Train Acc: 1.0\n",
            "Epoch 2466/10000\n",
            "Step 0: Train Loss: 4.148482446453272e-09, Train Acc: 1.0\n",
            "Epoch 2467/10000\n",
            "Step 0: Train Loss: 4.1842453946117075e-09, Train Acc: 1.0\n",
            "Epoch 2468/10000\n",
            "Step 0: Train Loss: 4.303454481657809e-09, Train Acc: 1.0\n",
            "Epoch 2469/10000\n",
            "Step 0: Train Loss: 3.886222010862639e-09, Train Acc: 1.0\n",
            "Epoch 2470/10000\n",
            "Step 0: Train Loss: 4.1723242638624924e-09, Train Acc: 1.0\n",
            "Epoch 2471/10000\n",
            "Step 0: Train Loss: 4.446505830202341e-09, Train Acc: 1.0\n",
            "Epoch 2472/10000\n",
            "Step 0: Train Loss: 4.37498037797468e-09, Train Acc: 1.0\n",
            "Epoch 2473/10000\n",
            "Step 0: Train Loss: 3.802775427885763e-09, Train Acc: 1.0\n",
            "Epoch 2474/10000\n",
            "Step 0: Train Loss: 3.6954872495442714e-09, Train Acc: 1.0\n",
            "Epoch 2475/10000\n",
            "Step 0: Train Loss: 4.5776360479976574e-09, Train Acc: 1.0\n",
            "Epoch 2476/10000\n",
            "Step 0: Train Loss: 4.041194046067176e-09, Train Acc: 1.0\n",
            "Epoch 2477/10000\n",
            "Step 0: Train Loss: 3.6358827060212207e-09, Train Acc: 1.0\n",
            "Epoch 2478/10000\n",
            "Step 0: Train Loss: 4.553794230588437e-09, Train Acc: 1.0\n",
            "Epoch 2479/10000\n",
            "Step 0: Train Loss: 3.8623801934534185e-09, Train Acc: 1.0\n",
            "Epoch 2480/10000\n",
            "Step 0: Train Loss: 4.37498037797468e-09, Train Acc: 1.0\n",
            "Epoch 2481/10000\n",
            "Step 0: Train Loss: 4.291533794997804e-09, Train Acc: 1.0\n",
            "Epoch 2482/10000\n",
            "Step 0: Train Loss: 3.576277940453565e-09, Train Acc: 1.0\n",
            "Epoch 2483/10000\n",
            "Step 0: Train Loss: 4.160403577202487e-09, Train Acc: 1.0\n",
            "Epoch 2484/10000\n",
            "Step 0: Train Loss: 3.910063828271859e-09, Train Acc: 1.0\n",
            "Epoch 2485/10000\n",
            "Step 0: Train Loss: 4.00543154199795e-09, Train Acc: 1.0\n",
            "Epoch 2486/10000\n",
            "Step 0: Train Loss: 3.6001197578627853e-09, Train Acc: 1.0\n",
            "Epoch 2487/10000\n",
            "Step 0: Train Loss: 4.303454481657809e-09, Train Acc: 1.0\n",
            "Epoch 2488/10000\n",
            "Step 0: Train Loss: 3.9339056456810795e-09, Train Acc: 1.0\n",
            "Epoch 2489/10000\n",
            "Step 0: Train Loss: 3.874301324202634e-09, Train Acc: 1.0\n",
            "Epoch 2490/10000\n",
            "Step 0: Train Loss: 3.445148166747458e-09, Train Acc: 1.0\n",
            "Epoch 2491/10000\n",
            "Step 0: Train Loss: 4.00543154199795e-09, Train Acc: 1.0\n",
            "Epoch 2492/10000\n",
            "Step 0: Train Loss: 4.196166081271713e-09, Train Acc: 1.0\n",
            "Epoch 2493/10000\n",
            "Step 0: Train Loss: 4.053115176816391e-09, Train Acc: 1.0\n",
            "Epoch 2494/10000\n",
            "Step 0: Train Loss: 3.6239615752720056e-09, Train Acc: 1.0\n",
            "Epoch 2495/10000\n",
            "Step 0: Train Loss: 3.886222010862639e-09, Train Acc: 1.0\n",
            "Epoch 2496/10000\n",
            "Step 0: Train Loss: 3.3855431791351975e-09, Train Acc: 1.0\n",
            "Epoch 2497/10000\n",
            "Step 0: Train Loss: 4.1723242638624924e-09, Train Acc: 1.0\n",
            "Epoch 2498/10000\n",
            "Step 0: Train Loss: 3.910063828271859e-09, Train Acc: 1.0\n",
            "Epoch 2499/10000\n",
            "Step 0: Train Loss: 3.993510411248735e-09, Train Acc: 1.0\n",
            "Epoch 2500/10000\n",
            "Step 0: Train Loss: 3.6358827060212207e-09, Train Acc: 1.0\n",
            "Epoch 2501/10000\n",
            "Step 0: Train Loss: 3.802775427885763e-09, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 2500 1024 1.3260389268496906\n",
            "Epoch index and hidden dimension and ratio: 2500 20 1.7702113732306282\n",
            "Epoch index and hidden dimension and ratio: 2500 20 2.3270694074675013\n",
            "Epoch index and hidden dimension and ratio: 2500 20 3.428458287271753\n",
            "MI(X;T): [12.926548789667923, 10.050458440844093, 7.992907242643073, 4.950524637087696], MI(Y;T): [3.309814668853642, 3.2633319703259014, 3.1831272235937407, 2.7669055050630402]\n",
            "Epoch index and hidden dimension and ratio: 2500 1024 1.3260380457763141\n",
            "Epoch index and hidden dimension and ratio: 2500 20 1.7702082272467803\n",
            "Epoch index and hidden dimension and ratio: 2500 20 2.3270541723322324\n",
            "Epoch index and hidden dimension and ratio: 2500 20 3.428447429091352\n",
            "MI(X;T): [12.926511045292813, 10.050417440204892, 7.993048951723637, 4.950898946093378], MI(Y;T): [3.309814668853642, 3.263394225950793, 3.1828501532371334, 2.7670837897134826]\n",
            "Epoch index and hidden dimension and ratio: 2500 1024 1.3260377520851887\n",
            "Epoch index and hidden dimension and ratio: 2500 20 1.7702097459976034\n",
            "Epoch index and hidden dimension and ratio: 2500 20 2.3270551880079173\n",
            "Epoch index and hidden dimension and ratio: 2500 20 3.4284688164163843\n",
            "MI(X;T): [12.926330563269092, 10.050600325145531, 7.992984983853843, 4.950993902555627], MI(Y;T): [3.309814668853642, 3.263618400740932, 3.1827499885888093, 2.7670641403541927]\n",
            "Epoch index and hidden dimension and ratio: 2500 1024 1.3260384863130024\n",
            "Epoch index and hidden dimension and ratio: 2500 20 1.7702094205509984\n",
            "Epoch index and hidden dimension and ratio: 2500 20 2.3270595046295766\n",
            "Epoch index and hidden dimension and ratio: 2500 20 3.428482306882943\n",
            "MI(X;T): [12.926478853379272, 10.050531275580251, 7.992574058568383, 4.951957012901035], MI(Y;T): [3.309814668853642, 3.2635319703259014, 3.1829468965419707, 2.7674926954555885]\n",
            "Epoch index and hidden dimension and ratio: 2500 1024 1.3260378989307513\n",
            "Epoch index and hidden dimension and ratio: 2500 20 1.7702032370655043\n",
            "Epoch index and hidden dimension and ratio: 2500 20 2.327048078278125\n",
            "Epoch index and hidden dimension and ratio: 2500 20 3.428471448702542\n",
            "MI(X;T): [12.926478853379272, 10.050862674317926, 7.992438057242558, 4.951912430353415], MI(Y;T): [3.3098146688536416, 3.2633369635994063, 3.18274665579854, 2.7674593174842714]\n",
            "Epoch index and hidden dimension and ratio: 2500 1024 1.326037458394063\n",
            "Epoch index and hidden dimension and ratio: 2500 20 1.7702009589392698\n",
            "Epoch index and hidden dimension and ratio: 2500 20 2.327043253818623\n",
            "Epoch index and hidden dimension and ratio: 2500 20 3.428466842201766\n",
            "MI(X;T): [12.926478853379272, 10.050389315002, 7.9929256593089075, 4.952014618658064], MI(Y;T): [3.3098146688536416, 3.2636543893766863, 3.1829991548214376, 2.766762482615503]\n",
            "Epoch 2502/10000\n",
            "Step 0: Train Loss: 3.95774790717951e-09, Train Acc: 1.0\n",
            "Epoch 2503/10000\n",
            "Step 0: Train Loss: 3.4570690754520683e-09, Train Acc: 1.0\n",
            "Epoch 2504/10000\n",
            "Step 0: Train Loss: 3.731249975658102e-09, Train Acc: 1.0\n",
            "Epoch 2505/10000\n",
            "Step 0: Train Loss: 3.731249975658102e-09, Train Acc: 1.0\n",
            "Epoch 2506/10000\n",
            "Step 0: Train Loss: 4.2557708468393685e-09, Train Acc: 1.0\n",
            "Epoch 2507/10000\n",
            "Step 0: Train Loss: 3.95774790717951e-09, Train Acc: 1.0\n",
            "Epoch 2508/10000\n",
            "Step 0: Train Loss: 3.874301324202634e-09, Train Acc: 1.0\n",
            "Epoch 2509/10000\n",
            "Step 0: Train Loss: 3.504752488225904e-09, Train Acc: 1.0\n",
            "Epoch 2510/10000\n",
            "Step 0: Train Loss: 3.182887287067615e-09, Train Acc: 1.0\n",
            "Epoch 2511/10000\n",
            "Step 0: Train Loss: 3.266334092089096e-09, Train Acc: 1.0\n",
            "Epoch 2512/10000\n",
            "Step 0: Train Loss: 3.6239615752720056e-09, Train Acc: 1.0\n",
            "Epoch 2513/10000\n",
            "Step 0: Train Loss: 3.3617011396813723e-09, Train Acc: 1.0\n",
            "Epoch 2514/10000\n",
            "Step 0: Train Loss: 3.9458267764302946e-09, Train Acc: 1.0\n",
            "Epoch 2515/10000\n",
            "Step 0: Train Loss: 3.743171106407317e-09, Train Acc: 1.0\n",
            "Epoch 2516/10000\n",
            "Step 0: Train Loss: 3.4689899841566785e-09, Train Acc: 1.0\n",
            "Epoch 2517/10000\n",
            "Step 0: Train Loss: 3.5524361230443446e-09, Train Acc: 1.0\n",
            "Epoch 2518/10000\n",
            "Step 0: Train Loss: 3.755091793067322e-09, Train Acc: 1.0\n",
            "Epoch 2519/10000\n",
            "Step 0: Train Loss: 3.993510411248735e-09, Train Acc: 1.0\n",
            "Epoch 2520/10000\n",
            "Step 0: Train Loss: 3.731249975658102e-09, Train Acc: 1.0\n",
            "Epoch 2521/10000\n",
            "Step 0: Train Loss: 3.564357031748955e-09, Train Acc: 1.0\n",
            "Epoch 2522/10000\n",
            "Step 0: Train Loss: 3.6001197578627853e-09, Train Acc: 1.0\n",
            "Epoch 2523/10000\n",
            "Step 0: Train Loss: 4.148482446453272e-09, Train Acc: 1.0\n",
            "Epoch 2524/10000\n",
            "Step 0: Train Loss: 3.6954872495442714e-09, Train Acc: 1.0\n",
            "Epoch 2525/10000\n",
            "Step 0: Train Loss: 3.576277940453565e-09, Train Acc: 1.0\n",
            "Epoch 2526/10000\n",
            "Step 0: Train Loss: 3.3617011396813723e-09, Train Acc: 1.0\n",
            "Epoch 2527/10000\n",
            "Step 0: Train Loss: 3.5285943056351243e-09, Train Acc: 1.0\n",
            "Epoch 2528/10000\n",
            "Step 0: Train Loss: 3.349780230976762e-09, Train Acc: 1.0\n",
            "Epoch 2529/10000\n",
            "Step 0: Train Loss: 3.3855431791351975e-09, Train Acc: 1.0\n",
            "Epoch 2530/10000\n",
            "Step 0: Train Loss: 3.1590454696583947e-09, Train Acc: 1.0\n",
            "Epoch 2531/10000\n",
            "Step 0: Train Loss: 4.0292733594071706e-09, Train Acc: 1.0\n",
            "Epoch 2532/10000\n",
            "Step 0: Train Loss: 3.4570686313628585e-09, Train Acc: 1.0\n",
            "Epoch 2533/10000\n",
            "Step 0: Train Loss: 3.361701583770582e-09, Train Acc: 1.0\n",
            "Epoch 2534/10000\n",
            "Step 0: Train Loss: 3.6358827060212207e-09, Train Acc: 1.0\n",
            "Epoch 2535/10000\n",
            "Step 0: Train Loss: 3.1352038742937793e-09, Train Acc: 1.0\n",
            "Epoch 2536/10000\n",
            "Step 0: Train Loss: 2.8967854781569713e-09, Train Acc: 1.0\n",
            "Epoch 2537/10000\n",
            "Step 0: Train Loss: 3.254412961339881e-09, Train Acc: 1.0\n",
            "Epoch 2538/10000\n",
            "Step 0: Train Loss: 3.4928318015658988e-09, Train Acc: 1.0\n",
            "Epoch 2539/10000\n",
            "Step 0: Train Loss: 3.2901761315429212e-09, Train Acc: 1.0\n",
            "Epoch 2540/10000\n",
            "Step 0: Train Loss: 3.671645432135051e-09, Train Acc: 1.0\n",
            "Epoch 2541/10000\n",
            "Step 0: Train Loss: 3.0875202394753387e-09, Train Acc: 1.0\n",
            "Epoch 2542/10000\n",
            "Step 0: Train Loss: 3.802775427885763e-09, Train Acc: 1.0\n",
            "Epoch 2543/10000\n",
            "Step 0: Train Loss: 3.409384996544418e-09, Train Acc: 1.0\n",
            "Epoch 2544/10000\n",
            "Step 0: Train Loss: 3.421305905249028e-09, Train Acc: 1.0\n",
            "Epoch 2545/10000\n",
            "Step 0: Train Loss: 3.4570686313628585e-09, Train Acc: 1.0\n",
            "Epoch 2546/10000\n",
            "Step 0: Train Loss: 2.7418132209078294e-09, Train Acc: 1.0\n",
            "Epoch 2547/10000\n",
            "Step 0: Train Loss: 3.015994565203073e-09, Train Acc: 1.0\n",
            "Epoch 2548/10000\n",
            "Step 0: Train Loss: 2.777576169066265e-09, Train Acc: 1.0\n",
            "Epoch 2549/10000\n",
            "Step 0: Train Loss: 3.111362056884559e-09, Train Acc: 1.0\n",
            "Epoch 2550/10000\n",
            "Step 0: Train Loss: 3.4689895400674686e-09, Train Acc: 1.0\n",
            "Epoch 2551/10000\n",
            "Step 0: Train Loss: 3.421305905249028e-09, Train Acc: 1.0\n",
            "Epoch 2552/10000\n",
            "Step 0: Train Loss: 3.492831357476689e-09, Train Acc: 1.0\n",
            "Epoch 2553/10000\n",
            "Step 0: Train Loss: 3.111362056884559e-09, Train Acc: 1.0\n",
            "Epoch 2554/10000\n",
            "Step 0: Train Loss: 3.2901756874537114e-09, Train Acc: 1.0\n",
            "Epoch 2555/10000\n",
            "Step 0: Train Loss: 3.1352038742937793e-09, Train Acc: 1.0\n",
            "Epoch 2556/10000\n",
            "Step 0: Train Loss: 3.9339056456810795e-09, Train Acc: 1.0\n",
            "Epoch 2557/10000\n",
            "Step 0: Train Loss: 3.349780675065972e-09, Train Acc: 1.0\n",
            "Epoch 2558/10000\n",
            "Step 0: Train Loss: 3.170966378363005e-09, Train Acc: 1.0\n",
            "Epoch 2559/10000\n",
            "Step 0: Train Loss: 3.1709668224522147e-09, Train Acc: 1.0\n",
            "Epoch 2560/10000\n",
            "Step 0: Train Loss: 3.254413183384486e-09, Train Acc: 1.0\n",
            "Epoch 2561/10000\n",
            "Step 0: Train Loss: 3.421305905249028e-09, Train Acc: 1.0\n",
            "Epoch 2562/10000\n",
            "Step 0: Train Loss: 3.027915695952288e-09, Train Acc: 1.0\n",
            "Epoch 2563/10000\n",
            "Step 0: Train Loss: 3.123282965589169e-09, Train Acc: 1.0\n",
            "Epoch 2564/10000\n",
            "Step 0: Train Loss: 3.2901761315429212e-09, Train Acc: 1.0\n",
            "Epoch 2565/10000\n",
            "Step 0: Train Loss: 3.0875202394753387e-09, Train Acc: 1.0\n",
            "Epoch 2566/10000\n",
            "Step 0: Train Loss: 2.9087063868615814e-09, Train Acc: 1.0\n",
            "Epoch 2567/10000\n",
            "Step 0: Train Loss: 3.1590454696583947e-09, Train Acc: 1.0\n",
            "Epoch 2568/10000\n",
            "Step 0: Train Loss: 2.777576169066265e-09, Train Acc: 1.0\n",
            "Epoch 2569/10000\n",
            "Step 0: Train Loss: 3.1590459137476046e-09, Train Acc: 1.0\n",
            "Epoch 2570/10000\n",
            "Step 0: Train Loss: 3.111362056884559e-09, Train Acc: 1.0\n",
            "Epoch 2571/10000\n",
            "Step 0: Train Loss: 2.7537341296124396e-09, Train Acc: 1.0\n",
            "Epoch 2572/10000\n",
            "Step 0: Train Loss: 2.9802318390892424e-09, Train Acc: 1.0\n",
            "Epoch 2573/10000\n",
            "Step 0: Train Loss: 2.9921527477938525e-09, Train Acc: 1.0\n",
            "Epoch 2574/10000\n",
            "Step 0: Train Loss: 3.027915695952288e-09, Train Acc: 1.0\n",
            "Epoch 2575/10000\n",
            "Step 0: Train Loss: 3.2305709218860557e-09, Train Acc: 1.0\n",
            "Epoch 2576/10000\n",
            "Step 0: Train Loss: 2.8252598038847054e-09, Train Acc: 1.0\n",
            "Epoch 2577/10000\n",
            "Step 0: Train Loss: 2.872943438703146e-09, Train Acc: 1.0\n",
            "Epoch 2578/10000\n",
            "Step 0: Train Loss: 3.254412961339881e-09, Train Acc: 1.0\n",
            "Epoch 2579/10000\n",
            "Step 0: Train Loss: 3.111362056884559e-09, Train Acc: 1.0\n",
            "Epoch 2580/10000\n",
            "Step 0: Train Loss: 3.0755993307707286e-09, Train Acc: 1.0\n",
            "Epoch 2581/10000\n",
            "Step 0: Train Loss: 2.872943438703146e-09, Train Acc: 1.0\n",
            "Epoch 2582/10000\n",
            "Step 0: Train Loss: 3.0875202394753387e-09, Train Acc: 1.0\n",
            "Epoch 2583/10000\n",
            "Step 0: Train Loss: 2.6583668599755583e-09, Train Acc: 1.0\n",
            "Epoch 2584/10000\n",
            "Step 0: Train Loss: 3.0875202394753387e-09, Train Acc: 1.0\n",
            "Epoch 2585/10000\n",
            "Step 0: Train Loss: 2.861022529998536e-09, Train Acc: 1.0\n",
            "Epoch 2586/10000\n",
            "Step 0: Train Loss: 2.932547982226197e-09, Train Acc: 1.0\n",
            "Epoch 2587/10000\n",
            "Step 0: Train Loss: 3.445148166747458e-09, Train Acc: 1.0\n",
            "Epoch 2588/10000\n",
            "Step 0: Train Loss: 2.7298923122032193e-09, Train Acc: 1.0\n",
            "Epoch 2589/10000\n",
            "Step 0: Train Loss: 2.9206268514769818e-09, Train Acc: 1.0\n",
            "Epoch 2590/10000\n",
            "Step 0: Train Loss: 3.0636784220661184e-09, Train Acc: 1.0\n",
            "Epoch 2591/10000\n",
            "Step 0: Train Loss: 3.3855431791351975e-09, Train Acc: 1.0\n",
            "Epoch 2592/10000\n",
            "Step 0: Train Loss: 3.0755993307707286e-09, Train Acc: 1.0\n",
            "Epoch 2593/10000\n",
            "Step 0: Train Loss: 3.0517575133615082e-09, Train Acc: 1.0\n",
            "Epoch 2594/10000\n",
            "Step 0: Train Loss: 2.7656552603616547e-09, Train Acc: 1.0\n",
            "Epoch 2595/10000\n",
            "Step 0: Train Loss: 3.3020965961583215e-09, Train Acc: 1.0\n",
            "Epoch 2596/10000\n",
            "Step 0: Train Loss: 3.2901756874537114e-09, Train Acc: 1.0\n",
            "Epoch 2597/10000\n",
            "Step 0: Train Loss: 2.956389799635417e-09, Train Acc: 1.0\n",
            "Epoch 2598/10000\n",
            "Step 0: Train Loss: 2.8371807125893156e-09, Train Acc: 1.0\n",
            "Epoch 2599/10000\n",
            "Step 0: Train Loss: 2.861022529998536e-09, Train Acc: 1.0\n",
            "Epoch 2600/10000\n",
            "Step 0: Train Loss: 2.8371807125893156e-09, Train Acc: 1.0\n",
            "Epoch 2601/10000\n",
            "Step 0: Train Loss: 2.4437900592033657e-09, Train Acc: 1.0\n",
            "Epoch 2602/10000\n",
            "Step 0: Train Loss: 3.1709668224522147e-09, Train Acc: 1.0\n",
            "Epoch 2603/10000\n",
            "Step 0: Train Loss: 2.8371807125893156e-09, Train Acc: 1.0\n",
            "Epoch 2604/10000\n",
            "Step 0: Train Loss: 2.7298923122032193e-09, Train Acc: 1.0\n",
            "Epoch 2605/10000\n",
            "Step 0: Train Loss: 2.8133388951800953e-09, Train Acc: 1.0\n",
            "Epoch 2606/10000\n",
            "Step 0: Train Loss: 2.694129586089389e-09, Train Acc: 1.0\n",
            "Epoch 2607/10000\n",
            "Step 0: Train Loss: 2.8133388951800953e-09, Train Acc: 1.0\n",
            "Epoch 2608/10000\n",
            "Step 0: Train Loss: 3.266333870044491e-09, Train Acc: 1.0\n",
            "Epoch 2609/10000\n",
            "Step 0: Train Loss: 3.0517570692722984e-09, Train Acc: 1.0\n",
            "Epoch 2610/10000\n",
            "Step 0: Train Loss: 2.4437900592033657e-09, Train Acc: 1.0\n",
            "Epoch 2611/10000\n",
            "Step 0: Train Loss: 2.7418132209078294e-09, Train Acc: 1.0\n",
            "Epoch 2612/10000\n",
            "Step 0: Train Loss: 2.9325482042708018e-09, Train Acc: 1.0\n",
            "Epoch 2613/10000\n",
            "Step 0: Train Loss: 2.801417986475485e-09, Train Acc: 1.0\n",
            "Epoch 2614/10000\n",
            "Step 0: Train Loss: 2.706050494793999e-09, Train Acc: 1.0\n",
            "Epoch 2615/10000\n",
            "Step 0: Train Loss: 2.6106830031125128e-09, Train Acc: 1.0\n",
            "Epoch 2616/10000\n",
            "Step 0: Train Loss: 2.562999368294072e-09, Train Acc: 1.0\n",
            "Epoch 2617/10000\n",
            "Step 0: Train Loss: 2.801417986475485e-09, Train Acc: 1.0\n",
            "Epoch 2618/10000\n",
            "Step 0: Train Loss: 2.706050494793999e-09, Train Acc: 1.0\n",
            "Epoch 2619/10000\n",
            "Step 0: Train Loss: 2.455710967907976e-09, Train Acc: 1.0\n",
            "Epoch 2620/10000\n",
            "Step 0: Train Loss: 2.646445951270948e-09, Train Acc: 1.0\n",
            "Epoch 2621/10000\n",
            "Step 0: Train Loss: 2.872943438703146e-09, Train Acc: 1.0\n",
            "Epoch 2622/10000\n",
            "Step 0: Train Loss: 3.1352036522491744e-09, Train Acc: 1.0\n",
            "Epoch 2623/10000\n",
            "Step 0: Train Loss: 2.706050494793999e-09, Train Acc: 1.0\n",
            "Epoch 2624/10000\n",
            "Step 0: Train Loss: 2.634525042566338e-09, Train Acc: 1.0\n",
            "Epoch 2625/10000\n",
            "Step 0: Train Loss: 3.278254778749101e-09, Train Acc: 1.0\n",
            "Epoch 2626/10000\n",
            "Step 0: Train Loss: 2.312659841408049e-09, Train Acc: 1.0\n",
            "Epoch 2627/10000\n",
            "Step 0: Train Loss: 2.944468668886202e-09, Train Acc: 1.0\n",
            "Epoch 2628/10000\n",
            "Step 0: Train Loss: 2.6822086773847786e-09, Train Acc: 1.0\n",
            "Epoch 2629/10000\n",
            "Step 0: Train Loss: 2.6822086773847786e-09, Train Acc: 1.0\n",
            "Epoch 2630/10000\n",
            "Step 0: Train Loss: 2.646445951270948e-09, Train Acc: 1.0\n",
            "Epoch 2631/10000\n",
            "Step 0: Train Loss: 2.6822086773847786e-09, Train Acc: 1.0\n",
            "Epoch 2632/10000\n",
            "Step 0: Train Loss: 2.6106830031125128e-09, Train Acc: 1.0\n",
            "Epoch 2633/10000\n",
            "Step 0: Train Loss: 2.5868411857032925e-09, Train Acc: 1.0\n",
            "Epoch 2634/10000\n",
            "Step 0: Train Loss: 2.706050494793999e-09, Train Acc: 1.0\n",
            "Epoch 2635/10000\n",
            "Step 0: Train Loss: 2.539157550884852e-09, Train Acc: 1.0\n",
            "Epoch 2636/10000\n",
            "Step 0: Train Loss: 2.5987620944079026e-09, Train Acc: 1.0\n",
            "Epoch 2637/10000\n",
            "Step 0: Train Loss: 2.646445951270948e-09, Train Acc: 1.0\n",
            "Epoch 2638/10000\n",
            "Step 0: Train Loss: 2.7298923122032193e-09, Train Acc: 1.0\n",
            "Epoch 2639/10000\n",
            "Step 0: Train Loss: 2.861022529998536e-09, Train Acc: 1.0\n",
            "Epoch 2640/10000\n",
            "Step 0: Train Loss: 2.5868411857032925e-09, Train Acc: 1.0\n",
            "Epoch 2641/10000\n",
            "Step 0: Train Loss: 2.3722646069757047e-09, Train Acc: 1.0\n",
            "Epoch 2642/10000\n",
            "Step 0: Train Loss: 2.4318691504987555e-09, Train Acc: 1.0\n",
            "Epoch 2643/10000\n",
            "Step 0: Train Loss: 2.5868411857032925e-09, Train Acc: 1.0\n",
            "Epoch 2644/10000\n",
            "Step 0: Train Loss: 2.455710967907976e-09, Train Acc: 1.0\n",
            "Epoch 2645/10000\n",
            "Step 0: Train Loss: 2.956390021680022e-09, Train Acc: 1.0\n",
            "Epoch 2646/10000\n",
            "Step 0: Train Loss: 2.717971403498609e-09, Train Acc: 1.0\n",
            "Epoch 2647/10000\n",
            "Step 0: Train Loss: 2.4318691504987555e-09, Train Acc: 1.0\n",
            "Epoch 2648/10000\n",
            "Step 0: Train Loss: 2.6822086773847786e-09, Train Acc: 1.0\n",
            "Epoch 2649/10000\n",
            "Step 0: Train Loss: 2.3365016588172693e-09, Train Acc: 1.0\n",
            "Epoch 2650/10000\n",
            "Step 0: Train Loss: 2.622603911817123e-09, Train Acc: 1.0\n",
            "Epoch 2651/10000\n",
            "Step 0: Train Loss: 2.241134389180388e-09, Train Acc: 1.0\n",
            "Epoch 2652/10000\n",
            "Step 0: Train Loss: 2.7656552603616547e-09, Train Acc: 1.0\n",
            "Epoch 2653/10000\n",
            "Step 0: Train Loss: 2.6106830031125128e-09, Train Acc: 1.0\n",
            "Epoch 2654/10000\n",
            "Step 0: Train Loss: 2.2530552978849983e-09, Train Acc: 1.0\n",
            "Epoch 2655/10000\n",
            "Step 0: Train Loss: 2.6702877686801685e-09, Train Acc: 1.0\n",
            "Epoch 2656/10000\n",
            "Step 0: Train Loss: 2.8967854781569713e-09, Train Acc: 1.0\n",
            "Epoch 2657/10000\n",
            "Step 0: Train Loss: 2.562999368294072e-09, Train Acc: 1.0\n",
            "Epoch 2658/10000\n",
            "Step 0: Train Loss: 2.646445951270948e-09, Train Acc: 1.0\n",
            "Epoch 2659/10000\n",
            "Step 0: Train Loss: 2.646445951270948e-09, Train Acc: 1.0\n",
            "Epoch 2660/10000\n",
            "Step 0: Train Loss: 2.5272366421802417e-09, Train Acc: 1.0\n",
            "Epoch 2661/10000\n",
            "Step 0: Train Loss: 2.4914736940218063e-09, Train Acc: 1.0\n",
            "Epoch 2662/10000\n",
            "Step 0: Train Loss: 2.4199482417941454e-09, Train Acc: 1.0\n",
            "Epoch 2663/10000\n",
            "Step 0: Train Loss: 2.944469112975412e-09, Train Acc: 1.0\n",
            "Epoch 2664/10000\n",
            "Step 0: Train Loss: 2.8967854781569713e-09, Train Acc: 1.0\n",
            "Epoch 2665/10000\n",
            "Step 0: Train Loss: 2.4199482417941454e-09, Train Acc: 1.0\n",
            "Epoch 2666/10000\n",
            "Step 0: Train Loss: 2.384185515680315e-09, Train Acc: 1.0\n",
            "Epoch 2667/10000\n",
            "Step 0: Train Loss: 2.396106424384925e-09, Train Acc: 1.0\n",
            "Epoch 2668/10000\n",
            "Step 0: Train Loss: 2.562999368294072e-09, Train Acc: 1.0\n",
            "Epoch 2669/10000\n",
            "Step 0: Train Loss: 2.384185515680315e-09, Train Acc: 1.0\n",
            "Epoch 2670/10000\n",
            "Step 0: Train Loss: 2.622603911817123e-09, Train Acc: 1.0\n",
            "Epoch 2671/10000\n",
            "Step 0: Train Loss: 2.4318691504987555e-09, Train Acc: 1.0\n",
            "Epoch 2672/10000\n",
            "Step 0: Train Loss: 2.3365016588172693e-09, Train Acc: 1.0\n",
            "Epoch 2673/10000\n",
            "Step 0: Train Loss: 2.479552785317196e-09, Train Acc: 1.0\n",
            "Epoch 2674/10000\n",
            "Step 0: Train Loss: 2.5868411857032925e-09, Train Acc: 1.0\n",
            "Epoch 2675/10000\n",
            "Step 0: Train Loss: 2.5749202769986823e-09, Train Acc: 1.0\n",
            "Epoch 2676/10000\n",
            "Step 0: Train Loss: 2.229213258431173e-09, Train Acc: 1.0\n",
            "Epoch 2677/10000\n",
            "Step 0: Train Loss: 2.396106424384925e-09, Train Acc: 1.0\n",
            "Epoch 2678/10000\n",
            "Step 0: Train Loss: 1.8596645334767459e-09, Train Acc: 1.0\n",
            "Epoch 2679/10000\n",
            "Step 0: Train Loss: 2.4199482417941454e-09, Train Acc: 1.0\n",
            "Epoch 2680/10000\n",
            "Step 0: Train Loss: 2.2053714410219527e-09, Train Acc: 1.0\n",
            "Epoch 2681/10000\n",
            "Step 0: Train Loss: 2.646445951270948e-09, Train Acc: 1.0\n",
            "Epoch 2682/10000\n",
            "Step 0: Train Loss: 2.694129586089389e-09, Train Acc: 1.0\n",
            "Epoch 2683/10000\n",
            "Step 0: Train Loss: 2.634525042566338e-09, Train Acc: 1.0\n",
            "Epoch 2684/10000\n",
            "Step 0: Train Loss: 2.5749202769986823e-09, Train Acc: 1.0\n",
            "Epoch 2685/10000\n",
            "Step 0: Train Loss: 2.408027333089535e-09, Train Acc: 1.0\n",
            "Epoch 2686/10000\n",
            "Step 0: Train Loss: 2.312659841408049e-09, Train Acc: 1.0\n",
            "Epoch 2687/10000\n",
            "Step 0: Train Loss: 2.5987620944079026e-09, Train Acc: 1.0\n",
            "Epoch 2688/10000\n",
            "Step 0: Train Loss: 2.4080271110449303e-09, Train Acc: 1.0\n",
            "Epoch 2689/10000\n",
            "Step 0: Train Loss: 2.3365016588172693e-09, Train Acc: 1.0\n",
            "Epoch 2690/10000\n",
            "Step 0: Train Loss: 2.3603434762264897e-09, Train Acc: 1.0\n",
            "Epoch 2691/10000\n",
            "Step 0: Train Loss: 1.9550319141359296e-09, Train Acc: 1.0\n",
            "Epoch 2692/10000\n",
            "Step 0: Train Loss: 1.8477436247721357e-09, Train Acc: 1.0\n",
            "Epoch 2693/10000\n",
            "Step 0: Train Loss: 2.396106424384925e-09, Train Acc: 1.0\n",
            "Epoch 2694/10000\n",
            "Step 0: Train Loss: 2.2888180239988287e-09, Train Acc: 1.0\n",
            "Epoch 2695/10000\n",
            "Step 0: Train Loss: 2.1934505323173425e-09, Train Acc: 1.0\n",
            "Epoch 2696/10000\n",
            "Step 0: Train Loss: 2.241134389180388e-09, Train Acc: 1.0\n",
            "Epoch 2697/10000\n",
            "Step 0: Train Loss: 2.2530552978849983e-09, Train Acc: 1.0\n",
            "Epoch 2698/10000\n",
            "Step 0: Train Loss: 2.634525042566338e-09, Train Acc: 1.0\n",
            "Epoch 2699/10000\n",
            "Step 0: Train Loss: 2.5033948247710214e-09, Train Acc: 1.0\n",
            "Epoch 2700/10000\n",
            "Step 0: Train Loss: 2.646445951270948e-09, Train Acc: 1.0\n",
            "Epoch 2701/10000\n",
            "Step 0: Train Loss: 2.7298923122032193e-09, Train Acc: 1.0\n",
            "Epoch 2702/10000\n",
            "Step 0: Train Loss: 2.217292349726563e-09, Train Acc: 1.0\n",
            "Epoch 2703/10000\n",
            "Step 0: Train Loss: 2.2649762065896084e-09, Train Acc: 1.0\n",
            "Epoch 2704/10000\n",
            "Step 0: Train Loss: 2.1338459887942918e-09, Train Acc: 1.0\n",
            "Epoch 2705/10000\n",
            "Step 0: Train Loss: 2.062320314522026e-09, Train Acc: 1.0\n",
            "Epoch 2706/10000\n",
            "Step 0: Train Loss: 2.3722646069757047e-09, Train Acc: 1.0\n",
            "Epoch 2707/10000\n",
            "Step 0: Train Loss: 2.5033946027264165e-09, Train Acc: 1.0\n",
            "Epoch 2708/10000\n",
            "Step 0: Train Loss: 2.384185515680315e-09, Train Acc: 1.0\n",
            "Epoch 2709/10000\n",
            "Step 0: Train Loss: 2.408027333089535e-09, Train Acc: 1.0\n",
            "Epoch 2710/10000\n",
            "Step 0: Train Loss: 2.157687806203512e-09, Train Acc: 1.0\n",
            "Epoch 2711/10000\n",
            "Step 0: Train Loss: 2.229213258431173e-09, Train Acc: 1.0\n",
            "Epoch 2712/10000\n",
            "Step 0: Train Loss: 2.4437900592033657e-09, Train Acc: 1.0\n",
            "Epoch 2713/10000\n",
            "Step 0: Train Loss: 2.2530552978849983e-09, Train Acc: 1.0\n",
            "Epoch 2714/10000\n",
            "Step 0: Train Loss: 2.5749202769986823e-09, Train Acc: 1.0\n",
            "Epoch 2715/10000\n",
            "Step 0: Train Loss: 2.324580750112659e-09, Train Acc: 1.0\n",
            "Epoch 2716/10000\n",
            "Step 0: Train Loss: 2.2888180239988287e-09, Train Acc: 1.0\n",
            "Epoch 2717/10000\n",
            "Step 0: Train Loss: 2.5987620944079026e-09, Train Acc: 1.0\n",
            "Epoch 2718/10000\n",
            "Step 0: Train Loss: 2.1219250800896816e-09, Train Acc: 1.0\n",
            "Epoch 2719/10000\n",
            "Step 0: Train Loss: 2.1696087149081222e-09, Train Acc: 1.0\n",
            "Epoch 2720/10000\n",
            "Step 0: Train Loss: 2.4199482417941454e-09, Train Acc: 1.0\n",
            "Epoch 2721/10000\n",
            "Step 0: Train Loss: 1.9550319141359296e-09, Train Acc: 1.0\n",
            "Epoch 2722/10000\n",
            "Step 0: Train Loss: 2.1696087149081222e-09, Train Acc: 1.0\n",
            "Epoch 2723/10000\n",
            "Step 0: Train Loss: 2.3365016588172693e-09, Train Acc: 1.0\n",
            "Epoch 2724/10000\n",
            "Step 0: Train Loss: 2.324580750112659e-09, Train Acc: 1.0\n",
            "Epoch 2725/10000\n",
            "Step 0: Train Loss: 2.2649762065896084e-09, Train Acc: 1.0\n",
            "Epoch 2726/10000\n",
            "Step 0: Train Loss: 2.217292349726563e-09, Train Acc: 1.0\n",
            "Epoch 2727/10000\n",
            "Step 0: Train Loss: 2.2649762065896084e-09, Train Acc: 1.0\n",
            "Epoch 2728/10000\n",
            "Step 0: Train Loss: 2.145766897498902e-09, Train Acc: 1.0\n",
            "Epoch 2729/10000\n",
            "Step 0: Train Loss: 2.467631876612586e-09, Train Acc: 1.0\n",
            "Epoch 2730/10000\n",
            "Step 0: Train Loss: 2.3603434762264897e-09, Train Acc: 1.0\n",
            "Epoch 2731/10000\n",
            "Step 0: Train Loss: 1.6927717005899012e-09, Train Acc: 1.0\n",
            "Epoch 2732/10000\n",
            "Step 0: Train Loss: 2.145766897498902e-09, Train Acc: 1.0\n",
            "Epoch 2733/10000\n",
            "Step 0: Train Loss: 2.145766897498902e-09, Train Acc: 1.0\n",
            "Epoch 2734/10000\n",
            "Step 0: Train Loss: 2.324580750112659e-09, Train Acc: 1.0\n",
            "Epoch 2735/10000\n",
            "Step 0: Train Loss: 2.3484225675218795e-09, Train Acc: 1.0\n",
            "Epoch 2736/10000\n",
            "Step 0: Train Loss: 2.145766897498902e-09, Train Acc: 1.0\n",
            "Epoch 2737/10000\n",
            "Step 0: Train Loss: 2.324580750112659e-09, Train Acc: 1.0\n",
            "Epoch 2738/10000\n",
            "Step 0: Train Loss: 1.895427370612879e-09, Train Acc: 1.0\n",
            "Epoch 2739/10000\n",
            "Step 0: Train Loss: 2.1338459887942918e-09, Train Acc: 1.0\n",
            "Epoch 2740/10000\n",
            "Step 0: Train Loss: 2.3365016588172693e-09, Train Acc: 1.0\n",
            "Epoch 2741/10000\n",
            "Step 0: Train Loss: 2.2530552978849983e-09, Train Acc: 1.0\n",
            "Epoch 2742/10000\n",
            "Step 0: Train Loss: 2.062320314522026e-09, Train Acc: 1.0\n",
            "Epoch 2743/10000\n",
            "Step 0: Train Loss: 2.1934505323173425e-09, Train Acc: 1.0\n",
            "Epoch 2744/10000\n",
            "Step 0: Train Loss: 2.145766897498902e-09, Train Acc: 1.0\n",
            "Epoch 2745/10000\n",
            "Step 0: Train Loss: 2.2530552978849983e-09, Train Acc: 1.0\n",
            "Epoch 2746/10000\n",
            "Step 0: Train Loss: 2.3722646069757047e-09, Train Acc: 1.0\n",
            "Epoch 2747/10000\n",
            "Step 0: Train Loss: 2.2530552978849983e-09, Train Acc: 1.0\n",
            "Epoch 2748/10000\n",
            "Step 0: Train Loss: 2.7656552603616547e-09, Train Acc: 1.0\n",
            "Epoch 2749/10000\n",
            "Step 0: Train Loss: 1.9669528228405397e-09, Train Acc: 1.0\n",
            "Epoch 2750/10000\n",
            "Step 0: Train Loss: 2.062320314522026e-09, Train Acc: 1.0\n",
            "Epoch 2751/10000\n",
            "Step 0: Train Loss: 1.978873953589755e-09, Train Acc: 1.0\n",
            "Epoch 2752/10000\n",
            "Step 0: Train Loss: 1.8596645334767459e-09, Train Acc: 1.0\n",
            "Epoch 2753/10000\n",
            "Step 0: Train Loss: 2.1696087149081222e-09, Train Acc: 1.0\n",
            "Epoch 2754/10000\n",
            "Step 0: Train Loss: 2.3722646069757047e-09, Train Acc: 1.0\n",
            "Epoch 2755/10000\n",
            "Step 0: Train Loss: 2.1100041713850715e-09, Train Acc: 1.0\n",
            "Epoch 2756/10000\n",
            "Step 0: Train Loss: 1.978873953589755e-09, Train Acc: 1.0\n",
            "Epoch 2757/10000\n",
            "Step 0: Train Loss: 1.9550319141359296e-09, Train Acc: 1.0\n",
            "Epoch 2758/10000\n",
            "Step 0: Train Loss: 2.1219250800896816e-09, Train Acc: 1.0\n",
            "Epoch 2759/10000\n",
            "Step 0: Train Loss: 1.8239018073629154e-09, Train Acc: 1.0\n",
            "Epoch 2760/10000\n",
            "Step 0: Train Loss: 2.0384784971128056e-09, Train Acc: 1.0\n",
            "Epoch 2761/10000\n",
            "Step 0: Train Loss: 1.895427370612879e-09, Train Acc: 1.0\n",
            "Epoch 2762/10000\n",
            "Step 0: Train Loss: 2.4914736940218063e-09, Train Acc: 1.0\n",
            "Epoch 2763/10000\n",
            "Step 0: Train Loss: 2.4318691504987555e-09, Train Acc: 1.0\n",
            "Epoch 2764/10000\n",
            "Step 0: Train Loss: 2.1219250800896816e-09, Train Acc: 1.0\n",
            "Epoch 2765/10000\n",
            "Step 0: Train Loss: 1.9669528228405397e-09, Train Acc: 1.0\n",
            "Epoch 2766/10000\n",
            "Step 0: Train Loss: 1.8239018073629154e-09, Train Acc: 1.0\n",
            "Epoch 2767/10000\n",
            "Step 0: Train Loss: 1.7881389702267825e-09, Train Acc: 1.0\n",
            "Epoch 2768/10000\n",
            "Step 0: Train Loss: 2.0146366797035853e-09, Train Acc: 1.0\n",
            "Epoch 2769/10000\n",
            "Step 0: Train Loss: 1.5377994433407594e-09, Train Acc: 1.0\n",
            "Epoch 2770/10000\n",
            "Step 0: Train Loss: 2.1219250800896816e-09, Train Acc: 1.0\n",
            "Epoch 2771/10000\n",
            "Step 0: Train Loss: 1.895427370612879e-09, Train Acc: 1.0\n",
            "Epoch 2772/10000\n",
            "Step 0: Train Loss: 1.990794862294365e-09, Train Acc: 1.0\n",
            "Epoch 2773/10000\n",
            "Step 0: Train Loss: 1.919269188022099e-09, Train Acc: 1.0\n",
            "Epoch 2774/10000\n",
            "Step 0: Train Loss: 2.0503994058174158e-09, Train Acc: 1.0\n",
            "Epoch 2775/10000\n",
            "Step 0: Train Loss: 2.002715770998975e-09, Train Acc: 1.0\n",
            "Epoch 2776/10000\n",
            "Step 0: Train Loss: 1.7523763551352545e-09, Train Acc: 1.0\n",
            "Epoch 2777/10000\n",
            "Step 0: Train Loss: 2.157687806203512e-09, Train Acc: 1.0\n",
            "Epoch 2778/10000\n",
            "Step 0: Train Loss: 2.0146366797035853e-09, Train Acc: 1.0\n",
            "Epoch 2779/10000\n",
            "Step 0: Train Loss: 2.300738932703439e-09, Train Acc: 1.0\n",
            "Epoch 2780/10000\n",
            "Step 0: Train Loss: 1.990794862294365e-09, Train Acc: 1.0\n",
            "Epoch 2781/10000\n",
            "Step 0: Train Loss: 1.8835064619082686e-09, Train Acc: 1.0\n",
            "Epoch 2782/10000\n",
            "Step 0: Train Loss: 1.8119807876360028e-09, Train Acc: 1.0\n",
            "Epoch 2783/10000\n",
            "Step 0: Train Loss: 2.300738932703439e-09, Train Acc: 1.0\n",
            "Epoch 2784/10000\n",
            "Step 0: Train Loss: 2.4199482417941454e-09, Train Acc: 1.0\n",
            "Epoch 2785/10000\n",
            "Step 0: Train Loss: 2.0861621319312462e-09, Train Acc: 1.0\n",
            "Epoch 2786/10000\n",
            "Step 0: Train Loss: 1.7881389702267825e-09, Train Acc: 1.0\n",
            "Epoch 2787/10000\n",
            "Step 0: Train Loss: 1.8239018073629154e-09, Train Acc: 1.0\n",
            "Epoch 2788/10000\n",
            "Step 0: Train Loss: 2.1934505323173425e-09, Train Acc: 1.0\n",
            "Epoch 2789/10000\n",
            "Step 0: Train Loss: 1.978873953589755e-09, Train Acc: 1.0\n",
            "Epoch 2790/10000\n",
            "Step 0: Train Loss: 1.7881389702267825e-09, Train Acc: 1.0\n",
            "Epoch 2791/10000\n",
            "Step 0: Train Loss: 1.8119807876360028e-09, Train Acc: 1.0\n",
            "Epoch 2792/10000\n",
            "Step 0: Train Loss: 1.752376244112952e-09, Train Acc: 1.0\n",
            "Epoch 2793/10000\n",
            "Step 0: Train Loss: 2.1815296236127324e-09, Train Acc: 1.0\n",
            "Epoch 2794/10000\n",
            "Step 0: Train Loss: 1.919269188022099e-09, Train Acc: 1.0\n",
            "Epoch 2795/10000\n",
            "Step 0: Train Loss: 1.680850791885291e-09, Train Acc: 1.0\n",
            "Epoch 2796/10000\n",
            "Step 0: Train Loss: 1.9550321361805345e-09, Train Acc: 1.0\n",
            "Epoch 2797/10000\n",
            "Step 0: Train Loss: 1.5974043199307175e-09, Train Acc: 1.0\n",
            "Epoch 2798/10000\n",
            "Step 0: Train Loss: 2.062320314522026e-09, Train Acc: 1.0\n",
            "Epoch 2799/10000\n",
            "Step 0: Train Loss: 1.9550319141359296e-09, Train Acc: 1.0\n",
            "Epoch 2800/10000\n",
            "Step 0: Train Loss: 1.9431110054313194e-09, Train Acc: 1.0\n",
            "Epoch 2801/10000\n",
            "Step 0: Train Loss: 1.8239018073629154e-09, Train Acc: 1.0\n",
            "Epoch 2802/10000\n",
            "Step 0: Train Loss: 2.2530552978849983e-09, Train Acc: 1.0\n",
            "Epoch 2803/10000\n",
            "Step 0: Train Loss: 1.9669528228405397e-09, Train Acc: 1.0\n",
            "Epoch 2804/10000\n",
            "Step 0: Train Loss: 1.990794862294365e-09, Train Acc: 1.0\n",
            "Epoch 2805/10000\n",
            "Step 0: Train Loss: 2.241134389180388e-09, Train Acc: 1.0\n",
            "Epoch 2806/10000\n",
            "Step 0: Train Loss: 2.0384784971128056e-09, Train Acc: 1.0\n",
            "Epoch 2807/10000\n",
            "Step 0: Train Loss: 1.8000598789313926e-09, Train Acc: 1.0\n",
            "Epoch 2808/10000\n",
            "Step 0: Train Loss: 1.8477436247721357e-09, Train Acc: 1.0\n",
            "Epoch 2809/10000\n",
            "Step 0: Train Loss: 1.8596645334767459e-09, Train Acc: 1.0\n",
            "Epoch 2810/10000\n",
            "Step 0: Train Loss: 2.2053714410219527e-09, Train Acc: 1.0\n",
            "Epoch 2811/10000\n",
            "Step 0: Train Loss: 1.8835064619082686e-09, Train Acc: 1.0\n",
            "Epoch 2812/10000\n",
            "Step 0: Train Loss: 2.002715770998975e-09, Train Acc: 1.0\n",
            "Epoch 2813/10000\n",
            "Step 0: Train Loss: 1.8477436247721357e-09, Train Acc: 1.0\n",
            "Epoch 2814/10000\n",
            "Step 0: Train Loss: 1.9431110054313194e-09, Train Acc: 1.0\n",
            "Epoch 2815/10000\n",
            "Step 0: Train Loss: 1.990794862294365e-09, Train Acc: 1.0\n",
            "Epoch 2816/10000\n",
            "Step 0: Train Loss: 1.990794862294365e-09, Train Acc: 1.0\n",
            "Epoch 2817/10000\n",
            "Step 0: Train Loss: 1.9431110054313194e-09, Train Acc: 1.0\n",
            "Epoch 2818/10000\n",
            "Step 0: Train Loss: 1.978873953589755e-09, Train Acc: 1.0\n",
            "Epoch 2819/10000\n",
            "Step 0: Train Loss: 2.0503994058174158e-09, Train Acc: 1.0\n",
            "Epoch 2820/10000\n",
            "Step 0: Train Loss: 1.8000601009759976e-09, Train Acc: 1.0\n",
            "Epoch 2821/10000\n",
            "Step 0: Train Loss: 1.668929661136076e-09, Train Acc: 1.0\n",
            "Epoch 2822/10000\n",
            "Step 0: Train Loss: 1.9431110054313194e-09, Train Acc: 1.0\n",
            "Epoch 2823/10000\n",
            "Step 0: Train Loss: 2.3603434762264897e-09, Train Acc: 1.0\n",
            "Epoch 2824/10000\n",
            "Step 0: Train Loss: 1.8715855532036585e-09, Train Acc: 1.0\n",
            "Epoch 2825/10000\n",
            "Step 0: Train Loss: 1.8358227160675256e-09, Train Acc: 1.0\n",
            "Epoch 2826/10000\n",
            "Step 0: Train Loss: 1.8596645334767459e-09, Train Acc: 1.0\n",
            "Epoch 2827/10000\n",
            "Step 0: Train Loss: 1.6331669350222455e-09, Train Acc: 1.0\n",
            "Epoch 2828/10000\n",
            "Step 0: Train Loss: 1.8477436247721357e-09, Train Acc: 1.0\n",
            "Epoch 2829/10000\n",
            "Step 0: Train Loss: 1.9550319141359296e-09, Train Acc: 1.0\n",
            "Epoch 2830/10000\n",
            "Step 0: Train Loss: 1.907348279317489e-09, Train Acc: 1.0\n",
            "Epoch 2831/10000\n",
            "Step 0: Train Loss: 1.8715855532036585e-09, Train Acc: 1.0\n",
            "Epoch 2832/10000\n",
            "Step 0: Train Loss: 1.919269188022099e-09, Train Acc: 1.0\n",
            "Epoch 2833/10000\n",
            "Step 0: Train Loss: 1.919269188022099e-09, Train Acc: 1.0\n",
            "Epoch 2834/10000\n",
            "Step 0: Train Loss: 1.752376244112952e-09, Train Acc: 1.0\n",
            "Epoch 2835/10000\n",
            "Step 0: Train Loss: 2.0861621319312462e-09, Train Acc: 1.0\n",
            "Epoch 2836/10000\n",
            "Step 0: Train Loss: 2.157687806203512e-09, Train Acc: 1.0\n",
            "Epoch 2837/10000\n",
            "Step 0: Train Loss: 1.8477436247721357e-09, Train Acc: 1.0\n",
            "Epoch 2838/10000\n",
            "Step 0: Train Loss: 1.8358227160675256e-09, Train Acc: 1.0\n",
            "Epoch 2839/10000\n",
            "Step 0: Train Loss: 1.668929661136076e-09, Train Acc: 1.0\n",
            "Epoch 2840/10000\n",
            "Step 0: Train Loss: 1.9311900967267093e-09, Train Acc: 1.0\n",
            "Epoch 2841/10000\n",
            "Step 0: Train Loss: 1.8000598789313926e-09, Train Acc: 1.0\n",
            "Epoch 2842/10000\n",
            "Step 0: Train Loss: 1.8358227160675256e-09, Train Acc: 1.0\n",
            "Epoch 2843/10000\n",
            "Step 0: Train Loss: 1.668929883180681e-09, Train Acc: 1.0\n",
            "Epoch 2844/10000\n",
            "Step 0: Train Loss: 1.9550319141359296e-09, Train Acc: 1.0\n",
            "Epoch 2845/10000\n",
            "Step 0: Train Loss: 1.895427370612879e-09, Train Acc: 1.0\n",
            "Epoch 2846/10000\n",
            "Step 0: Train Loss: 1.8119807876360028e-09, Train Acc: 1.0\n",
            "Epoch 2847/10000\n",
            "Step 0: Train Loss: 2.002715770998975e-09, Train Acc: 1.0\n",
            "Epoch 2848/10000\n",
            "Step 0: Train Loss: 1.680850791885291e-09, Train Acc: 1.0\n",
            "Epoch 2849/10000\n",
            "Step 0: Train Loss: 1.907348279317489e-09, Train Acc: 1.0\n",
            "Epoch 2850/10000\n",
            "Step 0: Train Loss: 1.919269188022099e-09, Train Acc: 1.0\n",
            "Epoch 2851/10000\n",
            "Step 0: Train Loss: 1.8477436247721357e-09, Train Acc: 1.0\n",
            "Epoch 2852/10000\n",
            "Step 0: Train Loss: 2.0861621319312462e-09, Train Acc: 1.0\n",
            "Epoch 2853/10000\n",
            "Step 0: Train Loss: 1.704692498272209e-09, Train Acc: 1.0\n",
            "Epoch 2854/10000\n",
            "Step 0: Train Loss: 1.907348279317489e-09, Train Acc: 1.0\n",
            "Epoch 2855/10000\n",
            "Step 0: Train Loss: 2.0146366797035853e-09, Train Acc: 1.0\n",
            "Epoch 2856/10000\n",
            "Step 0: Train Loss: 1.8835064619082686e-09, Train Acc: 1.0\n",
            "Epoch 2857/10000\n",
            "Step 0: Train Loss: 2.1815296236127324e-09, Train Acc: 1.0\n",
            "Epoch 2858/10000\n",
            "Step 0: Train Loss: 1.7642971528175622e-09, Train Acc: 1.0\n",
            "Epoch 2859/10000\n",
            "Step 0: Train Loss: 1.668929661136076e-09, Train Acc: 1.0\n",
            "Epoch 2860/10000\n",
            "Step 0: Train Loss: 1.5377996653853643e-09, Train Acc: 1.0\n",
            "Epoch 2861/10000\n",
            "Step 0: Train Loss: 2.0503994058174158e-09, Train Acc: 1.0\n",
            "Epoch 2862/10000\n",
            "Step 0: Train Loss: 1.7285343156814292e-09, Train Acc: 1.0\n",
            "Epoch 2863/10000\n",
            "Step 0: Train Loss: 1.4543531934307907e-09, Train Acc: 1.0\n",
            "Epoch 2864/10000\n",
            "Step 0: Train Loss: 2.0503994058174158e-09, Train Acc: 1.0\n",
            "Epoch 2865/10000\n",
            "Step 0: Train Loss: 1.8119807876360028e-09, Train Acc: 1.0\n",
            "Epoch 2866/10000\n",
            "Step 0: Train Loss: 1.668929661136076e-09, Train Acc: 1.0\n",
            "Epoch 2867/10000\n",
            "Step 0: Train Loss: 1.8835064619082686e-09, Train Acc: 1.0\n",
            "Epoch 2868/10000\n",
            "Step 0: Train Loss: 1.5020368282492313e-09, Train Acc: 1.0\n",
            "Epoch 2869/10000\n",
            "Step 0: Train Loss: 1.895427370612879e-09, Train Acc: 1.0\n",
            "Epoch 2870/10000\n",
            "Step 0: Train Loss: 1.704692498272209e-09, Train Acc: 1.0\n",
            "Epoch 2871/10000\n",
            "Step 0: Train Loss: 1.680850791885291e-09, Train Acc: 1.0\n",
            "Epoch 2872/10000\n",
            "Step 0: Train Loss: 2.0861621319312462e-09, Train Acc: 1.0\n",
            "Epoch 2873/10000\n",
            "Step 0: Train Loss: 1.633167046044548e-09, Train Acc: 1.0\n",
            "Epoch 2874/10000\n",
            "Step 0: Train Loss: 2.0384784971128056e-09, Train Acc: 1.0\n",
            "Epoch 2875/10000\n",
            "Step 0: Train Loss: 1.4662741021354009e-09, Train Acc: 1.0\n",
            "Epoch 2876/10000\n",
            "Step 0: Train Loss: 2.217292349726563e-09, Train Acc: 1.0\n",
            "Epoch 2877/10000\n",
            "Step 0: Train Loss: 1.7762180615221723e-09, Train Acc: 1.0\n",
            "Epoch 2878/10000\n",
            "Step 0: Train Loss: 1.990794862294365e-09, Train Acc: 1.0\n",
            "Epoch 2879/10000\n",
            "Step 0: Train Loss: 1.7642971528175622e-09, Train Acc: 1.0\n",
            "Epoch 2880/10000\n",
            "Step 0: Train Loss: 2.0861621319312462e-09, Train Acc: 1.0\n",
            "Epoch 2881/10000\n",
            "Step 0: Train Loss: 1.9311900967267093e-09, Train Acc: 1.0\n",
            "Epoch 2882/10000\n",
            "Step 0: Train Loss: 2.0146366797035853e-09, Train Acc: 1.0\n",
            "Epoch 2883/10000\n",
            "Step 0: Train Loss: 1.6450880657714606e-09, Train Acc: 1.0\n",
            "Epoch 2884/10000\n",
            "Step 0: Train Loss: 1.919269188022099e-09, Train Acc: 1.0\n",
            "Epoch 2885/10000\n",
            "Step 0: Train Loss: 1.633167046044548e-09, Train Acc: 1.0\n",
            "Epoch 2886/10000\n",
            "Step 0: Train Loss: 1.7881389702267825e-09, Train Acc: 1.0\n",
            "Epoch 2887/10000\n",
            "Step 0: Train Loss: 1.633167046044548e-09, Train Acc: 1.0\n",
            "Epoch 2888/10000\n",
            "Step 0: Train Loss: 1.6450878437268557e-09, Train Acc: 1.0\n",
            "Epoch 2889/10000\n",
            "Step 0: Train Loss: 1.8239018073629154e-09, Train Acc: 1.0\n",
            "Epoch 2890/10000\n",
            "Step 0: Train Loss: 1.5974043199307175e-09, Train Acc: 1.0\n",
            "Epoch 2891/10000\n",
            "Step 0: Train Loss: 1.7762180615221723e-09, Train Acc: 1.0\n",
            "Epoch 2892/10000\n",
            "Step 0: Train Loss: 1.8835064619082686e-09, Train Acc: 1.0\n",
            "Epoch 2893/10000\n",
            "Step 0: Train Loss: 1.8358227160675256e-09, Train Acc: 1.0\n",
            "Epoch 2894/10000\n",
            "Step 0: Train Loss: 1.9431110054313194e-09, Train Acc: 1.0\n",
            "Epoch 2895/10000\n",
            "Step 0: Train Loss: 1.6212461373399378e-09, Train Acc: 1.0\n",
            "Epoch 2896/10000\n",
            "Step 0: Train Loss: 1.919269188022099e-09, Train Acc: 1.0\n",
            "Epoch 2897/10000\n",
            "Step 0: Train Loss: 1.752376244112952e-09, Train Acc: 1.0\n",
            "Epoch 2898/10000\n",
            "Step 0: Train Loss: 2.0384784971128056e-09, Train Acc: 1.0\n",
            "Epoch 2899/10000\n",
            "Step 0: Train Loss: 1.7046926092945114e-09, Train Acc: 1.0\n",
            "Epoch 2900/10000\n",
            "Step 0: Train Loss: 1.895427370612879e-09, Train Acc: 1.0\n",
            "Epoch 2901/10000\n",
            "Step 0: Train Loss: 1.8358227160675256e-09, Train Acc: 1.0\n",
            "Epoch 2902/10000\n",
            "Step 0: Train Loss: 1.9669528228405397e-09, Train Acc: 1.0\n",
            "Epoch 2903/10000\n",
            "Step 0: Train Loss: 1.6927715895675988e-09, Train Acc: 1.0\n",
            "Epoch 2904/10000\n",
            "Step 0: Train Loss: 1.5377996653853643e-09, Train Acc: 1.0\n",
            "Epoch 2905/10000\n",
            "Step 0: Train Loss: 1.919269188022099e-09, Train Acc: 1.0\n",
            "Epoch 2906/10000\n",
            "Step 0: Train Loss: 1.5974040978861126e-09, Train Acc: 1.0\n",
            "Epoch 2907/10000\n",
            "Step 0: Train Loss: 2.0861621319312462e-09, Train Acc: 1.0\n",
            "Epoch 2908/10000\n",
            "Step 0: Train Loss: 1.5974040978861126e-09, Train Acc: 1.0\n",
            "Epoch 2909/10000\n",
            "Step 0: Train Loss: 1.7642972638398646e-09, Train Acc: 1.0\n",
            "Epoch 2910/10000\n",
            "Step 0: Train Loss: 1.7285345377260342e-09, Train Acc: 1.0\n",
            "Epoch 2911/10000\n",
            "Step 0: Train Loss: 1.7404552243860394e-09, Train Acc: 1.0\n",
            "Epoch 2912/10000\n",
            "Step 0: Train Loss: 1.8239018073629154e-09, Train Acc: 1.0\n",
            "Epoch 2913/10000\n",
            "Step 0: Train Loss: 1.8596645334767459e-09, Train Acc: 1.0\n",
            "Epoch 2914/10000\n",
            "Step 0: Train Loss: 1.8477436247721357e-09, Train Acc: 1.0\n",
            "Epoch 2915/10000\n",
            "Step 0: Train Loss: 1.7285343156814292e-09, Train Acc: 1.0\n",
            "Epoch 2916/10000\n",
            "Step 0: Train Loss: 2.1219250800896816e-09, Train Acc: 1.0\n",
            "Epoch 2917/10000\n",
            "Step 0: Train Loss: 1.668929661136076e-09, Train Acc: 1.0\n",
            "Epoch 2918/10000\n",
            "Step 0: Train Loss: 1.5020368282492313e-09, Train Acc: 1.0\n",
            "Epoch 2919/10000\n",
            "Step 0: Train Loss: 1.8715855532036585e-09, Train Acc: 1.0\n",
            "Epoch 2920/10000\n",
            "Step 0: Train Loss: 1.5854831891815024e-09, Train Acc: 1.0\n",
            "Epoch 2921/10000\n",
            "Step 0: Train Loss: 2.002715770998975e-09, Train Acc: 1.0\n",
            "Epoch 2922/10000\n",
            "Step 0: Train Loss: 1.4662739911130984e-09, Train Acc: 1.0\n",
            "Epoch 2923/10000\n",
            "Step 0: Train Loss: 1.895427370612879e-09, Train Acc: 1.0\n",
            "Epoch 2924/10000\n",
            "Step 0: Train Loss: 1.7881389702267825e-09, Train Acc: 1.0\n",
            "Epoch 2925/10000\n",
            "Step 0: Train Loss: 2.062320314522026e-09, Train Acc: 1.0\n",
            "Epoch 2926/10000\n",
            "Step 0: Train Loss: 1.5497205740899744e-09, Train Acc: 1.0\n",
            "Epoch 2927/10000\n",
            "Step 0: Train Loss: 1.9311900967267093e-09, Train Acc: 1.0\n",
            "Epoch 2928/10000\n",
            "Step 0: Train Loss: 1.752376244112952e-09, Train Acc: 1.0\n",
            "Epoch 2929/10000\n",
            "Step 0: Train Loss: 1.7285345377260342e-09, Train Acc: 1.0\n",
            "Epoch 2930/10000\n",
            "Step 0: Train Loss: 1.8477436247721357e-09, Train Acc: 1.0\n",
            "Epoch 2931/10000\n",
            "Step 0: Train Loss: 1.895427370612879e-09, Train Acc: 1.0\n",
            "Epoch 2932/10000\n",
            "Step 0: Train Loss: 1.5735623914991947e-09, Train Acc: 1.0\n",
            "Epoch 2933/10000\n",
            "Step 0: Train Loss: 1.8358227160675256e-09, Train Acc: 1.0\n",
            "Epoch 2934/10000\n",
            "Step 0: Train Loss: 1.6212461373399378e-09, Train Acc: 1.0\n",
            "Epoch 2935/10000\n",
            "Step 0: Train Loss: 1.633167046044548e-09, Train Acc: 1.0\n",
            "Epoch 2936/10000\n",
            "Step 0: Train Loss: 1.6212460263176354e-09, Train Acc: 1.0\n",
            "Epoch 2937/10000\n",
            "Step 0: Train Loss: 1.668929661136076e-09, Train Acc: 1.0\n",
            "Epoch 2938/10000\n",
            "Step 0: Train Loss: 1.716613406976819e-09, Train Acc: 1.0\n",
            "Epoch 2939/10000\n",
            "Step 0: Train Loss: 1.633167046044548e-09, Train Acc: 1.0\n",
            "Epoch 2940/10000\n",
            "Step 0: Train Loss: 1.7762180615221723e-09, Train Acc: 1.0\n",
            "Epoch 2941/10000\n",
            "Step 0: Train Loss: 1.704692498272209e-09, Train Acc: 1.0\n",
            "Epoch 2942/10000\n",
            "Step 0: Train Loss: 1.633167046044548e-09, Train Acc: 1.0\n",
            "Epoch 2943/10000\n",
            "Step 0: Train Loss: 2.0146366797035853e-09, Train Acc: 1.0\n",
            "Epoch 2944/10000\n",
            "Step 0: Train Loss: 1.6450880657714606e-09, Train Acc: 1.0\n",
            "Epoch 2945/10000\n",
            "Step 0: Train Loss: 1.7762180615221723e-09, Train Acc: 1.0\n",
            "Epoch 2946/10000\n",
            "Step 0: Train Loss: 1.704692498272209e-09, Train Acc: 1.0\n",
            "Epoch 2947/10000\n",
            "Step 0: Train Loss: 2.062320314522026e-09, Train Acc: 1.0\n",
            "Epoch 2948/10000\n",
            "Step 0: Train Loss: 1.8358227160675256e-09, Train Acc: 1.0\n",
            "Epoch 2949/10000\n",
            "Step 0: Train Loss: 1.6570089744760708e-09, Train Acc: 1.0\n",
            "Epoch 2950/10000\n",
            "Step 0: Train Loss: 1.978873953589755e-09, Train Acc: 1.0\n",
            "Epoch 2951/10000\n",
            "Step 0: Train Loss: 1.5974043199307175e-09, Train Acc: 1.0\n",
            "Epoch 2952/10000\n",
            "Step 0: Train Loss: 1.5854834112261074e-09, Train Acc: 1.0\n",
            "Epoch 2953/10000\n",
            "Step 0: Train Loss: 1.668929883180681e-09, Train Acc: 1.0\n",
            "Epoch 2954/10000\n",
            "Step 0: Train Loss: 1.6212460263176354e-09, Train Acc: 1.0\n",
            "Epoch 2955/10000\n",
            "Step 0: Train Loss: 1.9669528228405397e-09, Train Acc: 1.0\n",
            "Epoch 2956/10000\n",
            "Step 0: Train Loss: 1.5974043199307175e-09, Train Acc: 1.0\n",
            "Epoch 2957/10000\n",
            "Step 0: Train Loss: 2.0384784971128056e-09, Train Acc: 1.0\n",
            "Epoch 2958/10000\n",
            "Step 0: Train Loss: 2.0503994058174158e-09, Train Acc: 1.0\n",
            "Epoch 2959/10000\n",
            "Step 0: Train Loss: 1.633167046044548e-09, Train Acc: 1.0\n",
            "Epoch 2960/10000\n",
            "Step 0: Train Loss: 1.6570089744760708e-09, Train Acc: 1.0\n",
            "Epoch 2961/10000\n",
            "Step 0: Train Loss: 2.002715770998975e-09, Train Acc: 1.0\n",
            "Epoch 2962/10000\n",
            "Step 0: Train Loss: 1.4066694475900476e-09, Train Acc: 1.0\n",
            "Epoch 2963/10000\n",
            "Step 0: Train Loss: 2.229213258431173e-09, Train Acc: 1.0\n",
            "Epoch 2964/10000\n",
            "Step 0: Train Loss: 1.8119807876360028e-09, Train Acc: 1.0\n",
            "Epoch 2965/10000\n",
            "Step 0: Train Loss: 1.5497205740899744e-09, Train Acc: 1.0\n",
            "Epoch 2966/10000\n",
            "Step 0: Train Loss: 2.0503994058174158e-09, Train Acc: 1.0\n",
            "Epoch 2967/10000\n",
            "Step 0: Train Loss: 1.633167046044548e-09, Train Acc: 1.0\n",
            "Epoch 2968/10000\n",
            "Step 0: Train Loss: 1.752376244112952e-09, Train Acc: 1.0\n",
            "Epoch 2969/10000\n",
            "Step 0: Train Loss: 1.716613629021424e-09, Train Acc: 1.0\n",
            "Epoch 2970/10000\n",
            "Step 0: Train Loss: 1.8477436247721357e-09, Train Acc: 1.0\n",
            "Epoch 2971/10000\n",
            "Step 0: Train Loss: 1.7881389702267825e-09, Train Acc: 1.0\n",
            "Epoch 2972/10000\n",
            "Step 0: Train Loss: 1.835822827089828e-09, Train Acc: 1.0\n",
            "Epoch 2973/10000\n",
            "Step 0: Train Loss: 2.229213258431173e-09, Train Acc: 1.0\n",
            "Epoch 2974/10000\n",
            "Step 0: Train Loss: 1.4066694475900476e-09, Train Acc: 1.0\n",
            "Epoch 2975/10000\n",
            "Step 0: Train Loss: 2.1338459887942918e-09, Train Acc: 1.0\n",
            "Epoch 2976/10000\n",
            "Step 0: Train Loss: 1.907348279317489e-09, Train Acc: 1.0\n",
            "Epoch 2977/10000\n",
            "Step 0: Train Loss: 1.5616414827945846e-09, Train Acc: 1.0\n",
            "Epoch 2978/10000\n",
            "Step 0: Train Loss: 2.0503994058174158e-09, Train Acc: 1.0\n",
            "Epoch 2979/10000\n",
            "Step 0: Train Loss: 1.6331669350222455e-09, Train Acc: 1.0\n",
            "Epoch 2980/10000\n",
            "Step 0: Train Loss: 1.6927715895675988e-09, Train Acc: 1.0\n",
            "Epoch 2981/10000\n",
            "Step 0: Train Loss: 1.716613406976819e-09, Train Acc: 1.0\n",
            "Epoch 2982/10000\n",
            "Step 0: Train Loss: 1.8119807876360028e-09, Train Acc: 1.0\n",
            "Epoch 2983/10000\n",
            "Step 0: Train Loss: 1.5735623914991947e-09, Train Acc: 1.0\n",
            "Epoch 2984/10000\n",
            "Step 0: Train Loss: 1.8119807876360028e-09, Train Acc: 1.0\n",
            "Epoch 2985/10000\n",
            "Step 0: Train Loss: 1.5616414827945846e-09, Train Acc: 1.0\n",
            "Epoch 2986/10000\n",
            "Step 0: Train Loss: 1.430511264999268e-09, Train Acc: 1.0\n",
            "Epoch 2987/10000\n",
            "Step 0: Train Loss: 2.2768971152942186e-09, Train Acc: 1.0\n",
            "Epoch 2988/10000\n",
            "Step 0: Train Loss: 1.3589857017493046e-09, Train Acc: 1.0\n",
            "Epoch 2989/10000\n",
            "Step 0: Train Loss: 1.6450878437268557e-09, Train Acc: 1.0\n",
            "Epoch 2990/10000\n",
            "Step 0: Train Loss: 1.8596645334767459e-09, Train Acc: 1.0\n",
            "Epoch 2991/10000\n",
            "Step 0: Train Loss: 1.752376244112952e-09, Train Acc: 1.0\n",
            "Epoch 2992/10000\n",
            "Step 0: Train Loss: 1.8239018073629154e-09, Train Acc: 1.0\n",
            "Epoch 2993/10000\n",
            "Step 0: Train Loss: 1.8477436247721357e-09, Train Acc: 1.0\n",
            "Epoch 2994/10000\n",
            "Step 0: Train Loss: 1.7642971528175622e-09, Train Acc: 1.0\n",
            "Epoch 2995/10000\n",
            "Step 0: Train Loss: 1.5497205740899744e-09, Train Acc: 1.0\n",
            "Epoch 2996/10000\n",
            "Step 0: Train Loss: 1.8477436247721357e-09, Train Acc: 1.0\n",
            "Epoch 2997/10000\n",
            "Step 0: Train Loss: 1.7762180615221723e-09, Train Acc: 1.0\n",
            "Epoch 2998/10000\n",
            "Step 0: Train Loss: 1.5497205740899744e-09, Train Acc: 1.0\n",
            "Epoch 2999/10000\n",
            "Step 0: Train Loss: 1.8000601009759976e-09, Train Acc: 1.0\n",
            "Epoch 3000/10000\n",
            "Step 0: Train Loss: 1.6093252286353277e-09, Train Acc: 1.0\n",
            "Epoch 3001/10000\n",
            "Step 0: Train Loss: 2.0503994058174158e-09, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 3000 1024 1.3814403802242787\n",
            "Epoch index and hidden dimension and ratio: 3000 20 1.9315381843247124\n",
            "Epoch index and hidden dimension and ratio: 3000 20 2.5124383453119585\n",
            "Epoch index and hidden dimension and ratio: 3000 20 3.7480750790529003\n",
            "MI(X;T): [12.918239003752069, 9.86142095031577, 8.1182102678285, 4.5414568792053105], MI(Y;T): [3.308046273829889, 3.255422575950641, 3.1534958998161233, 2.6841217171573106]\n",
            "Epoch index and hidden dimension and ratio: 3000 1024 1.381552863925339\n",
            "Epoch index and hidden dimension and ratio: 3000 20 1.9318616782500346\n",
            "Epoch index and hidden dimension and ratio: 3000 20 2.5127717408554275\n",
            "Epoch index and hidden dimension and ratio: 3000 20 3.7486712918676424\n",
            "MI(X;T): [12.91809996779951, 9.86213367104197, 8.11641264636737, 4.541861888032003], MI(Y;T): [3.3081085294547807, 3.2557660455125177, 3.1536383705583697, 2.682614147528434]\n",
            "Epoch index and hidden dimension and ratio: 3000 1024 1.3816628512518327\n",
            "Epoch index and hidden dimension and ratio: 3000 20 1.9321992748615713\n",
            "Epoch index and hidden dimension and ratio: 3000 20 2.513217876399887\n",
            "Epoch index and hidden dimension and ratio: 3000 20 3.749359305662138\n",
            "MI(X;T): [12.917862223424404, 9.860981855654224, 8.116845749392516, 4.542136721024914], MI(Y;T): [3.308046273829889, 3.2561104284963465, 3.153374232632797, 2.682453847071348]\n",
            "Epoch index and hidden dimension and ratio: 3000 1024 1.3817709295860108\n",
            "Epoch index and hidden dimension and ratio: 3000 20 1.9325444652272237\n",
            "Epoch index and hidden dimension and ratio: 3000 20 2.5137010841068337\n",
            "Epoch index and hidden dimension and ratio: 3000 20 3.750095029643244\n",
            "MI(X;T): [12.917681741400681, 9.86268947872391, 8.11605267996461, 4.543443387966825], MI(Y;T): [3.307998405709637, 3.2559382043515566, 3.153358726212427, 2.6820454186417075]\n",
            "Epoch index and hidden dimension and ratio: 3000 1024 1.381880035839128\n",
            "Epoch index and hidden dimension and ratio: 3000 20 1.9329347841887625\n",
            "Epoch index and hidden dimension and ratio: 3000 20 2.514312520868961\n",
            "Epoch index and hidden dimension and ratio: 3000 20 3.7510785175589527\n",
            "MI(X;T): [12.917608112130036, 9.862557968900804, 8.117010744572507, 4.54170081188606], MI(Y;T): [3.3080984057096368, 3.256312828945729, 3.153733248866443, 2.681765297973021]\n",
            "Epoch index and hidden dimension and ratio: 3000 1024 1.3819851772620513\n",
            "Epoch index and hidden dimension and ratio: 3000 20 1.9333181602893958\n",
            "Epoch index and hidden dimension and ratio: 3000 20 2.5149854060100068\n",
            "Epoch index and hidden dimension and ratio: 3000 20 3.7521890132817757\n",
            "MI(X;T): [12.91722171517292, 9.86241897838045, 8.116517799293419, 4.542801190604109], MI(Y;T): [3.3080984057096368, 3.2566015443602483, 3.1536818144552474, 2.6817756866646443]\n",
            "Epoch 3002/10000\n",
            "Step 0: Train Loss: 1.6450880657714606e-09, Train Acc: 1.0\n",
            "Epoch 3003/10000\n",
            "Step 0: Train Loss: 2.1338459887942918e-09, Train Acc: 1.0\n",
            "Epoch 3004/10000\n",
            "Step 0: Train Loss: 1.633167046044548e-09, Train Acc: 1.0\n",
            "Epoch 3005/10000\n",
            "Step 0: Train Loss: 1.8596645334767459e-09, Train Acc: 1.0\n",
            "Epoch 3006/10000\n",
            "Step 0: Train Loss: 1.680850791885291e-09, Train Acc: 1.0\n",
            "Epoch 3007/10000\n",
            "Step 0: Train Loss: 1.8596645334767459e-09, Train Acc: 1.0\n",
            "Epoch 3008/10000\n",
            "Step 0: Train Loss: 1.919269188022099e-09, Train Acc: 1.0\n",
            "Epoch 3009/10000\n",
            "Step 0: Train Loss: 1.7762180615221723e-09, Train Acc: 1.0\n",
            "Epoch 3010/10000\n",
            "Step 0: Train Loss: 1.7762182835667772e-09, Train Acc: 1.0\n",
            "Epoch 3011/10000\n",
            "Step 0: Train Loss: 1.6450880657714606e-09, Train Acc: 1.0\n",
            "Epoch 3012/10000\n",
            "Step 0: Train Loss: 1.8239018073629154e-09, Train Acc: 1.0\n",
            "Epoch 3013/10000\n",
            "Step 0: Train Loss: 1.907348279317489e-09, Train Acc: 1.0\n",
            "Epoch 3014/10000\n",
            "Step 0: Train Loss: 1.7762180615221723e-09, Train Acc: 1.0\n",
            "Epoch 3015/10000\n",
            "Step 0: Train Loss: 1.7642971528175622e-09, Train Acc: 1.0\n",
            "Epoch 3016/10000\n",
            "Step 0: Train Loss: 1.8715855532036585e-09, Train Acc: 1.0\n",
            "Epoch 3017/10000\n",
            "Step 0: Train Loss: 1.7881389702267825e-09, Train Acc: 1.0\n",
            "Epoch 3018/10000\n",
            "Step 0: Train Loss: 2.1696087149081222e-09, Train Acc: 1.0\n",
            "Epoch 3019/10000\n",
            "Step 0: Train Loss: 1.5377996653853643e-09, Train Acc: 1.0\n",
            "Epoch 3020/10000\n",
            "Step 0: Train Loss: 1.513957847976144e-09, Train Acc: 1.0\n",
            "Epoch 3021/10000\n",
            "Step 0: Train Loss: 1.752376244112952e-09, Train Acc: 1.0\n",
            "Epoch 3022/10000\n",
            "Step 0: Train Loss: 1.4424320626815756e-09, Train Acc: 1.0\n",
            "Epoch 3023/10000\n",
            "Step 0: Train Loss: 1.7762180615221723e-09, Train Acc: 1.0\n",
            "Epoch 3024/10000\n",
            "Step 0: Train Loss: 2.0384784971128056e-09, Train Acc: 1.0\n",
            "Epoch 3025/10000\n",
            "Step 0: Train Loss: 1.6927717005899012e-09, Train Acc: 1.0\n",
            "Epoch 3026/10000\n",
            "Step 0: Train Loss: 2.0265575884081954e-09, Train Acc: 1.0\n",
            "Epoch 3027/10000\n",
            "Step 0: Train Loss: 1.6808505698406861e-09, Train Acc: 1.0\n",
            "Epoch 3028/10000\n",
            "Step 0: Train Loss: 1.8835064619082686e-09, Train Acc: 1.0\n",
            "Epoch 3029/10000\n",
            "Step 0: Train Loss: 1.6450878437268557e-09, Train Acc: 1.0\n",
            "Epoch 3030/10000\n",
            "Step 0: Train Loss: 1.752376244112952e-09, Train Acc: 1.0\n",
            "Epoch 3031/10000\n",
            "Step 0: Train Loss: 1.835822827089828e-09, Train Acc: 1.0\n",
            "Epoch 3032/10000\n",
            "Step 0: Train Loss: 1.5616413717722821e-09, Train Acc: 1.0\n",
            "Epoch 3033/10000\n",
            "Step 0: Train Loss: 1.6808505698406861e-09, Train Acc: 1.0\n",
            "Epoch 3034/10000\n",
            "Step 0: Train Loss: 2.002715770998975e-09, Train Acc: 1.0\n",
            "Epoch 3035/10000\n",
            "Step 0: Train Loss: 1.6212460263176354e-09, Train Acc: 1.0\n",
            "Epoch 3036/10000\n",
            "Step 0: Train Loss: 1.919269188022099e-09, Train Acc: 1.0\n",
            "Epoch 3037/10000\n",
            "Step 0: Train Loss: 1.6450880657714606e-09, Train Acc: 1.0\n",
            "Epoch 3038/10000\n",
            "Step 0: Train Loss: 1.7404554464306443e-09, Train Acc: 1.0\n",
            "Epoch 3039/10000\n",
            "Step 0: Train Loss: 1.6927717005899012e-09, Train Acc: 1.0\n",
            "Epoch 3040/10000\n",
            "Step 0: Train Loss: 1.895427370612879e-09, Train Acc: 1.0\n",
            "Epoch 3041/10000\n",
            "Step 0: Train Loss: 1.6331669350222455e-09, Train Acc: 1.0\n",
            "Epoch 3042/10000\n",
            "Step 0: Train Loss: 2.0265575884081954e-09, Train Acc: 1.0\n",
            "Epoch 3043/10000\n",
            "Step 0: Train Loss: 2.1100041713850715e-09, Train Acc: 1.0\n",
            "Epoch 3044/10000\n",
            "Step 0: Train Loss: 1.978873953589755e-09, Train Acc: 1.0\n",
            "Epoch 3045/10000\n",
            "Step 0: Train Loss: 2.0384784971128056e-09, Train Acc: 1.0\n",
            "Epoch 3046/10000\n",
            "Step 0: Train Loss: 1.8715855532036585e-09, Train Acc: 1.0\n",
            "Epoch 3047/10000\n",
            "Step 0: Train Loss: 1.7881389702267825e-09, Train Acc: 1.0\n",
            "Epoch 3048/10000\n",
            "Step 0: Train Loss: 1.704692498272209e-09, Train Acc: 1.0\n",
            "Epoch 3049/10000\n",
            "Step 0: Train Loss: 1.6927715895675988e-09, Train Acc: 1.0\n",
            "Epoch 3050/10000\n",
            "Step 0: Train Loss: 1.919269188022099e-09, Train Acc: 1.0\n",
            "Epoch 3051/10000\n",
            "Step 0: Train Loss: 1.8596645334767459e-09, Train Acc: 1.0\n",
            "Epoch 3052/10000\n",
            "Step 0: Train Loss: 1.7642971528175622e-09, Train Acc: 1.0\n",
            "Epoch 3053/10000\n",
            "Step 0: Train Loss: 2.0503994058174158e-09, Train Acc: 1.0\n",
            "Epoch 3054/10000\n",
            "Step 0: Train Loss: 1.8835064619082686e-09, Train Acc: 1.0\n",
            "Epoch 3055/10000\n",
            "Step 0: Train Loss: 1.513957847976144e-09, Train Acc: 1.0\n",
            "Epoch 3056/10000\n",
            "Step 0: Train Loss: 1.7762180615221723e-09, Train Acc: 1.0\n",
            "Epoch 3057/10000\n",
            "Step 0: Train Loss: 1.7642971528175622e-09, Train Acc: 1.0\n",
            "Epoch 3058/10000\n",
            "Step 0: Train Loss: 1.704692498272209e-09, Train Acc: 1.0\n",
            "Epoch 3059/10000\n",
            "Step 0: Train Loss: 1.7881389702267825e-09, Train Acc: 1.0\n",
            "Epoch 3060/10000\n",
            "Step 0: Train Loss: 1.633167046044548e-09, Train Acc: 1.0\n",
            "Epoch 3061/10000\n",
            "Step 0: Train Loss: 1.7642971528175622e-09, Train Acc: 1.0\n",
            "Epoch 3062/10000\n",
            "Step 0: Train Loss: 1.907348279317489e-09, Train Acc: 1.0\n",
            "Epoch 3063/10000\n",
            "Step 0: Train Loss: 1.7046926092945114e-09, Train Acc: 1.0\n",
            "Epoch 3064/10000\n",
            "Step 0: Train Loss: 2.0146366797035853e-09, Train Acc: 1.0\n",
            "Epoch 3065/10000\n",
            "Step 0: Train Loss: 1.990794862294365e-09, Train Acc: 1.0\n",
            "Epoch 3066/10000\n",
            "Step 0: Train Loss: 1.668929883180681e-09, Train Acc: 1.0\n",
            "Epoch 3067/10000\n",
            "Step 0: Train Loss: 1.8596647555213508e-09, Train Acc: 1.0\n",
            "Epoch 3068/10000\n",
            "Step 0: Train Loss: 2.0384784971128056e-09, Train Acc: 1.0\n",
            "Epoch 3069/10000\n",
            "Step 0: Train Loss: 1.8239018073629154e-09, Train Acc: 1.0\n",
            "Epoch 3070/10000\n",
            "Step 0: Train Loss: 1.9311900967267093e-09, Train Acc: 1.0\n",
            "Epoch 3071/10000\n",
            "Step 0: Train Loss: 2.157687806203512e-09, Train Acc: 1.0\n",
            "Epoch 3072/10000\n",
            "Step 0: Train Loss: 1.8715855532036585e-09, Train Acc: 1.0\n",
            "Epoch 3073/10000\n",
            "Step 0: Train Loss: 1.8000598789313926e-09, Train Acc: 1.0\n",
            "Epoch 3074/10000\n",
            "Step 0: Train Loss: 2.062320314522026e-09, Train Acc: 1.0\n",
            "Epoch 3075/10000\n",
            "Step 0: Train Loss: 1.6212461373399378e-09, Train Acc: 1.0\n",
            "Epoch 3076/10000\n",
            "Step 0: Train Loss: 1.990794862294365e-09, Train Acc: 1.0\n",
            "Epoch 3077/10000\n",
            "Step 0: Train Loss: 1.9311900967267093e-09, Train Acc: 1.0\n",
            "Epoch 3078/10000\n",
            "Step 0: Train Loss: 2.229213258431173e-09, Train Acc: 1.0\n",
            "Epoch 3079/10000\n",
            "Step 0: Train Loss: 1.907348279317489e-09, Train Acc: 1.0\n",
            "Epoch 3080/10000\n",
            "Step 0: Train Loss: 1.9311900967267093e-09, Train Acc: 1.0\n",
            "Epoch 3081/10000\n",
            "Step 0: Train Loss: 1.6093252286353277e-09, Train Acc: 1.0\n",
            "Epoch 3082/10000\n",
            "Step 0: Train Loss: 1.990794862294365e-09, Train Acc: 1.0\n",
            "Epoch 3083/10000\n",
            "Step 0: Train Loss: 2.2888180239988287e-09, Train Acc: 1.0\n",
            "Epoch 3084/10000\n",
            "Step 0: Train Loss: 2.396106424384925e-09, Train Acc: 1.0\n",
            "Epoch 3085/10000\n",
            "Step 0: Train Loss: 1.8477436247721357e-09, Train Acc: 1.0\n",
            "Epoch 3086/10000\n",
            "Step 0: Train Loss: 2.0384784971128056e-09, Train Acc: 1.0\n",
            "Epoch 3087/10000\n",
            "Step 0: Train Loss: 1.8239018073629154e-09, Train Acc: 1.0\n",
            "Epoch 3088/10000\n",
            "Step 0: Train Loss: 1.7762180615221723e-09, Train Acc: 1.0\n",
            "Epoch 3089/10000\n",
            "Step 0: Train Loss: 1.7762180615221723e-09, Train Acc: 1.0\n",
            "Epoch 3090/10000\n",
            "Step 0: Train Loss: 1.990794862294365e-09, Train Acc: 1.0\n",
            "Epoch 3091/10000\n",
            "Step 0: Train Loss: 2.562999368294072e-09, Train Acc: 1.0\n",
            "Epoch 3092/10000\n",
            "Step 0: Train Loss: 2.062320314522026e-09, Train Acc: 1.0\n",
            "Epoch 3093/10000\n",
            "Step 0: Train Loss: 1.9431110054313194e-09, Train Acc: 1.0\n",
            "Epoch 3094/10000\n",
            "Step 0: Train Loss: 1.907348279317489e-09, Train Acc: 1.0\n",
            "Epoch 3095/10000\n",
            "Step 0: Train Loss: 2.1815296236127324e-09, Train Acc: 1.0\n",
            "Epoch 3096/10000\n",
            "Step 0: Train Loss: 1.6331669350222455e-09, Train Acc: 1.0\n",
            "Epoch 3097/10000\n",
            "Step 0: Train Loss: 2.1219250800896816e-09, Train Acc: 1.0\n",
            "Epoch 3098/10000\n",
            "Step 0: Train Loss: 2.0980830406358564e-09, Train Acc: 1.0\n",
            "Epoch 3099/10000\n",
            "Step 0: Train Loss: 2.2649762065896084e-09, Train Acc: 1.0\n",
            "Epoch 3100/10000\n",
            "Step 0: Train Loss: 1.9431110054313194e-09, Train Acc: 1.0\n",
            "Epoch 3101/10000\n",
            "Step 0: Train Loss: 1.9431110054313194e-09, Train Acc: 1.0\n",
            "Epoch 3102/10000\n",
            "Step 0: Train Loss: 2.0384784971128056e-09, Train Acc: 1.0\n",
            "Epoch 3103/10000\n",
            "Step 0: Train Loss: 2.074241223226636e-09, Train Acc: 1.0\n",
            "Epoch 3104/10000\n",
            "Step 0: Train Loss: 2.2768971152942186e-09, Train Acc: 1.0\n",
            "Epoch 3105/10000\n",
            "Step 0: Train Loss: 2.217292349726563e-09, Train Acc: 1.0\n",
            "Epoch 3106/10000\n",
            "Step 0: Train Loss: 2.312659841408049e-09, Train Acc: 1.0\n",
            "Epoch 3107/10000\n",
            "Step 0: Train Loss: 2.145766897498902e-09, Train Acc: 1.0\n",
            "Epoch 3108/10000\n",
            "Step 0: Train Loss: 2.300738932703439e-09, Train Acc: 1.0\n",
            "Epoch 3109/10000\n",
            "Step 0: Train Loss: 1.8000598789313926e-09, Train Acc: 1.0\n",
            "Epoch 3110/10000\n",
            "Step 0: Train Loss: 1.9669528228405397e-09, Train Acc: 1.0\n",
            "Epoch 3111/10000\n",
            "Step 0: Train Loss: 2.944469112975412e-09, Train Acc: 1.0\n",
            "Epoch 3112/10000\n",
            "Step 0: Train Loss: 2.074241223226636e-09, Train Acc: 1.0\n",
            "Epoch 3113/10000\n",
            "Step 0: Train Loss: 2.0861621319312462e-09, Train Acc: 1.0\n",
            "Epoch 3114/10000\n",
            "Step 0: Train Loss: 1.8715855532036585e-09, Train Acc: 1.0\n",
            "Epoch 3115/10000\n",
            "Step 0: Train Loss: 1.9311900967267093e-09, Train Acc: 1.0\n",
            "Epoch 3116/10000\n",
            "Step 0: Train Loss: 2.217292349726563e-09, Train Acc: 1.0\n",
            "Epoch 3117/10000\n",
            "Step 0: Train Loss: 2.0980830406358564e-09, Train Acc: 1.0\n",
            "Epoch 3118/10000\n",
            "Step 0: Train Loss: 2.6106830031125128e-09, Train Acc: 1.0\n",
            "Epoch 3119/10000\n",
            "Step 0: Train Loss: 2.0265575884081954e-09, Train Acc: 1.0\n",
            "Epoch 3120/10000\n",
            "Step 0: Train Loss: 2.300738932703439e-09, Train Acc: 1.0\n",
            "Epoch 3121/10000\n",
            "Step 0: Train Loss: 2.2530552978849983e-09, Train Acc: 1.0\n",
            "Epoch 3122/10000\n",
            "Step 0: Train Loss: 2.312659841408049e-09, Train Acc: 1.0\n",
            "Epoch 3123/10000\n",
            "Step 0: Train Loss: 2.1219250800896816e-09, Train Acc: 1.0\n",
            "Epoch 3124/10000\n",
            "Step 0: Train Loss: 2.8133388951800953e-09, Train Acc: 1.0\n",
            "Epoch 3125/10000\n",
            "Step 0: Train Loss: 2.1338459887942918e-09, Train Acc: 1.0\n",
            "Epoch 3126/10000\n",
            "Step 0: Train Loss: 1.978873953589755e-09, Train Acc: 1.0\n",
            "Epoch 3127/10000\n",
            "Step 0: Train Loss: 2.3365016588172693e-09, Train Acc: 1.0\n",
            "Epoch 3128/10000\n",
            "Step 0: Train Loss: 2.551078459589462e-09, Train Acc: 1.0\n",
            "Epoch 3129/10000\n",
            "Step 0: Train Loss: 2.1815296236127324e-09, Train Acc: 1.0\n",
            "Epoch 3130/10000\n",
            "Step 0: Train Loss: 2.2053714410219527e-09, Train Acc: 1.0\n",
            "Epoch 3131/10000\n",
            "Step 0: Train Loss: 2.789497077770875e-09, Train Acc: 1.0\n",
            "Epoch 3132/10000\n",
            "Step 0: Train Loss: 2.717971403498609e-09, Train Acc: 1.0\n",
            "Epoch 3133/10000\n",
            "Step 0: Train Loss: 2.1815296236127324e-09, Train Acc: 1.0\n",
            "Epoch 3134/10000\n",
            "Step 0: Train Loss: 2.551078459589462e-09, Train Acc: 1.0\n",
            "Epoch 3135/10000\n",
            "Step 0: Train Loss: 2.3603434762264897e-09, Train Acc: 1.0\n",
            "Epoch 3136/10000\n",
            "Step 0: Train Loss: 2.4318691504987555e-09, Train Acc: 1.0\n",
            "Epoch 3137/10000\n",
            "Step 0: Train Loss: 2.312659841408049e-09, Train Acc: 1.0\n",
            "Epoch 3138/10000\n",
            "Step 0: Train Loss: 2.8967854781569713e-09, Train Acc: 1.0\n",
            "Epoch 3139/10000\n",
            "Step 0: Train Loss: 2.241134389180388e-09, Train Acc: 1.0\n",
            "Epoch 3140/10000\n",
            "Step 0: Train Loss: 2.408027333089535e-09, Train Acc: 1.0\n",
            "Epoch 3141/10000\n",
            "Step 0: Train Loss: 2.5153157334756315e-09, Train Acc: 1.0\n",
            "Epoch 3142/10000\n",
            "Step 0: Train Loss: 2.6822084553401737e-09, Train Acc: 1.0\n",
            "Epoch 3143/10000\n",
            "Step 0: Train Loss: 2.5272366421802417e-09, Train Acc: 1.0\n",
            "Epoch 3144/10000\n",
            "Step 0: Train Loss: 2.622603911817123e-09, Train Acc: 1.0\n",
            "Epoch 3145/10000\n",
            "Step 0: Train Loss: 2.551078459589462e-09, Train Acc: 1.0\n",
            "Epoch 3146/10000\n",
            "Step 0: Train Loss: 2.8252598038847054e-09, Train Acc: 1.0\n",
            "Epoch 3147/10000\n",
            "Step 0: Train Loss: 2.646445951270948e-09, Train Acc: 1.0\n",
            "Epoch 3148/10000\n",
            "Step 0: Train Loss: 2.634525042566338e-09, Train Acc: 1.0\n",
            "Epoch 3149/10000\n",
            "Step 0: Train Loss: 2.646445951270948e-09, Train Acc: 1.0\n",
            "Epoch 3150/10000\n",
            "Step 0: Train Loss: 2.5749200549540774e-09, Train Acc: 1.0\n",
            "Epoch 3151/10000\n",
            "Step 0: Train Loss: 2.8491016212939257e-09, Train Acc: 1.0\n",
            "Epoch 3152/10000\n",
            "Step 0: Train Loss: 3.2067291044768353e-09, Train Acc: 1.0\n",
            "Epoch 3153/10000\n",
            "Step 0: Train Loss: 3.4570686313628585e-09, Train Acc: 1.0\n",
            "Epoch 3154/10000\n",
            "Step 0: Train Loss: 3.0755993307707286e-09, Train Acc: 1.0\n",
            "Epoch 3155/10000\n",
            "Step 0: Train Loss: 2.801417986475485e-09, Train Acc: 1.0\n",
            "Epoch 3156/10000\n",
            "Step 0: Train Loss: 3.588198849158175e-09, Train Acc: 1.0\n",
            "Epoch 3157/10000\n",
            "Step 0: Train Loss: 4.386900620545475e-09, Train Acc: 1.0\n",
            "Epoch 3158/10000\n",
            "Step 0: Train Loss: 208.3566131591797, Train Acc: 0.12790000438690186\n",
            "Epoch 3159/10000\n",
            "Step 0: Train Loss: 10.81152057647705, Train Acc: 0.37689998745918274\n",
            "Epoch 3160/10000\n",
            "Step 0: Train Loss: 1.7955529689788818, Train Acc: 0.6004999876022339\n",
            "Epoch 3161/10000\n",
            "Step 0: Train Loss: 0.9470059275627136, Train Acc: 0.7189000248908997\n",
            "Epoch 3162/10000\n",
            "Step 0: Train Loss: 0.6568492650985718, Train Acc: 0.8025000095367432\n",
            "Epoch 3163/10000\n",
            "Step 0: Train Loss: 0.45010772347450256, Train Acc: 0.8725000023841858\n",
            "Epoch 3164/10000\n",
            "Step 0: Train Loss: 0.3551658093929291, Train Acc: 0.9032999873161316\n",
            "Epoch 3165/10000\n",
            "Step 0: Train Loss: 0.3081664443016052, Train Acc: 0.9185000061988831\n",
            "Epoch 3166/10000\n",
            "Step 0: Train Loss: 0.2693142294883728, Train Acc: 0.9283999800682068\n",
            "Epoch 3167/10000\n",
            "Step 0: Train Loss: 0.24767908453941345, Train Acc: 0.9354000091552734\n",
            "Epoch 3168/10000\n",
            "Step 0: Train Loss: 0.21171364188194275, Train Acc: 0.9430999755859375\n",
            "Epoch 3169/10000\n",
            "Step 0: Train Loss: 0.20066604018211365, Train Acc: 0.9441999793052673\n",
            "Epoch 3170/10000\n",
            "Step 0: Train Loss: 0.19006681442260742, Train Acc: 0.9491999745368958\n",
            "Epoch 3171/10000\n",
            "Step 0: Train Loss: 0.18740367889404297, Train Acc: 0.9485999941825867\n",
            "Epoch 3172/10000\n",
            "Step 0: Train Loss: 0.1701039969921112, Train Acc: 0.9539999961853027\n",
            "Epoch 3173/10000\n",
            "Step 0: Train Loss: 0.15123456716537476, Train Acc: 0.9564999938011169\n",
            "Epoch 3174/10000\n",
            "Step 0: Train Loss: 0.15126769244670868, Train Acc: 0.9584000110626221\n",
            "Epoch 3175/10000\n",
            "Step 0: Train Loss: 0.15527157485485077, Train Acc: 0.9577000141143799\n",
            "Epoch 3176/10000\n",
            "Step 0: Train Loss: 0.13792504370212555, Train Acc: 0.9614999890327454\n",
            "Epoch 3177/10000\n",
            "Step 0: Train Loss: 0.1275402307510376, Train Acc: 0.9649999737739563\n",
            "Epoch 3178/10000\n",
            "Step 0: Train Loss: 0.13049329817295074, Train Acc: 0.963699996471405\n",
            "Epoch 3179/10000\n",
            "Step 0: Train Loss: 0.12766149640083313, Train Acc: 0.9641000032424927\n",
            "Epoch 3180/10000\n",
            "Step 0: Train Loss: 0.1116393655538559, Train Acc: 0.9674999713897705\n",
            "Epoch 3181/10000\n",
            "Step 0: Train Loss: 0.11285356432199478, Train Acc: 0.9695000052452087\n",
            "Epoch 3182/10000\n",
            "Step 0: Train Loss: 0.10760947316884995, Train Acc: 0.9710999727249146\n",
            "Epoch 3183/10000\n",
            "Step 0: Train Loss: 0.1021028384566307, Train Acc: 0.9713000059127808\n",
            "Epoch 3184/10000\n",
            "Step 0: Train Loss: 0.10158489644527435, Train Acc: 0.9714999794960022\n",
            "Epoch 3185/10000\n",
            "Step 0: Train Loss: 0.10295221954584122, Train Acc: 0.9707000255584717\n",
            "Epoch 3186/10000\n",
            "Step 0: Train Loss: 0.09312301874160767, Train Acc: 0.974399983882904\n",
            "Epoch 3187/10000\n",
            "Step 0: Train Loss: 0.08850686997175217, Train Acc: 0.9757000207901001\n",
            "Epoch 3188/10000\n",
            "Step 0: Train Loss: 0.08950182795524597, Train Acc: 0.9754999876022339\n",
            "Epoch 3189/10000\n",
            "Step 0: Train Loss: 0.07856296002864838, Train Acc: 0.9782000184059143\n",
            "Epoch 3190/10000\n",
            "Step 0: Train Loss: 0.08191770315170288, Train Acc: 0.9768999814987183\n",
            "Epoch 3191/10000\n",
            "Step 0: Train Loss: 0.08137638866901398, Train Acc: 0.9765999913215637\n",
            "Epoch 3192/10000\n",
            "Step 0: Train Loss: 0.08443088084459305, Train Acc: 0.9761999845504761\n",
            "Epoch 3193/10000\n",
            "Step 0: Train Loss: 0.08052453398704529, Train Acc: 0.9761999845504761\n",
            "Epoch 3194/10000\n",
            "Step 0: Train Loss: 0.0735834389925003, Train Acc: 0.979200005531311\n",
            "Epoch 3195/10000\n",
            "Step 0: Train Loss: 0.07048401236534119, Train Acc: 0.9800999760627747\n",
            "Epoch 3196/10000\n",
            "Step 0: Train Loss: 0.06478102505207062, Train Acc: 0.9824000000953674\n",
            "Epoch 3197/10000\n",
            "Step 0: Train Loss: 0.06658542901277542, Train Acc: 0.9811000227928162\n",
            "Epoch 3198/10000\n",
            "Step 0: Train Loss: 0.07659021764993668, Train Acc: 0.9781000018119812\n",
            "Epoch 3199/10000\n",
            "Step 0: Train Loss: 0.07221447676420212, Train Acc: 0.9779000282287598\n",
            "Epoch 3200/10000\n",
            "Step 0: Train Loss: 0.06004483625292778, Train Acc: 0.9832000136375427\n",
            "Epoch 3201/10000\n",
            "Step 0: Train Loss: 0.06144390255212784, Train Acc: 0.9824000000953674\n",
            "Epoch 3202/10000\n",
            "Step 0: Train Loss: 0.0628228560090065, Train Acc: 0.9829999804496765\n",
            "Epoch 3203/10000\n",
            "Step 0: Train Loss: 0.06065557897090912, Train Acc: 0.9836999773979187\n",
            "Epoch 3204/10000\n",
            "Step 0: Train Loss: 0.06294488161802292, Train Acc: 0.983299970626831\n",
            "Epoch 3205/10000\n",
            "Step 0: Train Loss: 0.06102561205625534, Train Acc: 0.9839000105857849\n",
            "Epoch 3206/10000\n",
            "Step 0: Train Loss: 0.049468982964754105, Train Acc: 0.9882000088691711\n",
            "Epoch 3207/10000\n",
            "Step 0: Train Loss: 0.05371011793613434, Train Acc: 0.9855999946594238\n",
            "Epoch 3208/10000\n",
            "Step 0: Train Loss: 0.054887205362319946, Train Acc: 0.9861999750137329\n",
            "Epoch 3209/10000\n",
            "Step 0: Train Loss: 0.0553005188703537, Train Acc: 0.9848999977111816\n",
            "Epoch 3210/10000\n",
            "Step 0: Train Loss: 0.05100645869970322, Train Acc: 0.9872000217437744\n",
            "Epoch 3211/10000\n",
            "Step 0: Train Loss: 0.04366956651210785, Train Acc: 0.9896000027656555\n",
            "Epoch 3212/10000\n",
            "Step 0: Train Loss: 0.04517098888754845, Train Acc: 0.9884999990463257\n",
            "Epoch 3213/10000\n",
            "Step 0: Train Loss: 0.045055706053972244, Train Acc: 0.9890000224113464\n",
            "Epoch 3214/10000\n",
            "Step 0: Train Loss: 0.047985102981328964, Train Acc: 0.9878000020980835\n",
            "Epoch 3215/10000\n",
            "Step 0: Train Loss: 0.04346056282520294, Train Acc: 0.9894999861717224\n",
            "Epoch 3216/10000\n",
            "Step 0: Train Loss: 0.04089956730604172, Train Acc: 0.9890999794006348\n",
            "Epoch 3217/10000\n",
            "Step 0: Train Loss: 0.042416878044605255, Train Acc: 0.9894999861717224\n",
            "Epoch 3218/10000\n",
            "Step 0: Train Loss: 0.043434273451566696, Train Acc: 0.988099992275238\n",
            "Epoch 3219/10000\n",
            "Step 0: Train Loss: 0.03712388873100281, Train Acc: 0.9916999936103821\n",
            "Epoch 3220/10000\n",
            "Step 0: Train Loss: 0.037578098475933075, Train Acc: 0.9904999732971191\n",
            "Epoch 3221/10000\n",
            "Step 0: Train Loss: 0.03809437155723572, Train Acc: 0.9907000064849854\n",
            "Epoch 3222/10000\n",
            "Step 0: Train Loss: 0.03566852957010269, Train Acc: 0.9914000034332275\n",
            "Epoch 3223/10000\n",
            "Step 0: Train Loss: 0.036935485899448395, Train Acc: 0.9915000200271606\n",
            "Epoch 3224/10000\n",
            "Step 0: Train Loss: 0.03296218439936638, Train Acc: 0.9915000200271606\n",
            "Epoch 3225/10000\n",
            "Step 0: Train Loss: 0.0339292548596859, Train Acc: 0.9922999739646912\n",
            "Epoch 3226/10000\n",
            "Step 0: Train Loss: 0.034526947885751724, Train Acc: 0.9919000267982483\n",
            "Epoch 3227/10000\n",
            "Step 0: Train Loss: 0.029697643592953682, Train Acc: 0.9933000206947327\n",
            "Epoch 3228/10000\n",
            "Step 0: Train Loss: 0.029093289747834206, Train Acc: 0.9932000041007996\n",
            "Epoch 3229/10000\n",
            "Step 0: Train Loss: 0.0302767064422369, Train Acc: 0.9930999875068665\n",
            "Epoch 3230/10000\n",
            "Step 0: Train Loss: 0.027597198262810707, Train Acc: 0.9940999746322632\n",
            "Epoch 3231/10000\n",
            "Step 0: Train Loss: 0.028856974095106125, Train Acc: 0.9936000108718872\n",
            "Epoch 3232/10000\n",
            "Step 0: Train Loss: 0.028637677431106567, Train Acc: 0.992900013923645\n",
            "Epoch 3233/10000\n",
            "Step 0: Train Loss: 0.03095298446714878, Train Acc: 0.993399977684021\n",
            "Epoch 3234/10000\n",
            "Step 0: Train Loss: 0.026829631999135017, Train Acc: 0.9933000206947327\n",
            "Epoch 3235/10000\n",
            "Step 0: Train Loss: 0.03101126104593277, Train Acc: 0.9927999973297119\n",
            "Epoch 3236/10000\n",
            "Step 0: Train Loss: 0.025188343599438667, Train Acc: 0.9943000078201294\n",
            "Epoch 3237/10000\n",
            "Step 0: Train Loss: 0.028753189370036125, Train Acc: 0.9933000206947327\n",
            "Epoch 3238/10000\n",
            "Step 0: Train Loss: 0.026093915104866028, Train Acc: 0.9940000176429749\n",
            "Epoch 3239/10000\n",
            "Step 0: Train Loss: 0.026461487635970116, Train Acc: 0.9940000176429749\n",
            "Epoch 3240/10000\n",
            "Step 0: Train Loss: 0.020774180069565773, Train Acc: 0.9958999752998352\n",
            "Epoch 3241/10000\n",
            "Step 0: Train Loss: 0.02801932767033577, Train Acc: 0.9933000206947327\n",
            "Epoch 3242/10000\n",
            "Step 0: Train Loss: 0.0255945585668087, Train Acc: 0.9943000078201294\n",
            "Epoch 3243/10000\n",
            "Step 0: Train Loss: 0.021884439513087273, Train Acc: 0.9955000281333923\n",
            "Epoch 3244/10000\n",
            "Step 0: Train Loss: 0.027330977842211723, Train Acc: 0.9932000041007996\n",
            "Epoch 3245/10000\n",
            "Step 0: Train Loss: 0.023408817127346992, Train Acc: 0.9951000213623047\n",
            "Epoch 3246/10000\n",
            "Step 0: Train Loss: 0.019309790804982185, Train Acc: 0.9957000017166138\n",
            "Epoch 3247/10000\n",
            "Step 0: Train Loss: 0.02070452831685543, Train Acc: 0.9958000183105469\n",
            "Epoch 3248/10000\n",
            "Step 0: Train Loss: 0.02380044013261795, Train Acc: 0.9957000017166138\n",
            "Epoch 3249/10000\n",
            "Step 0: Train Loss: 0.021138111129403114, Train Acc: 0.9952999949455261\n",
            "Epoch 3250/10000\n",
            "Step 0: Train Loss: 0.02132837474346161, Train Acc: 0.995199978351593\n",
            "Epoch 3251/10000\n",
            "Step 0: Train Loss: 0.01572156697511673, Train Acc: 0.9968000054359436\n",
            "Epoch 3252/10000\n",
            "Step 0: Train Loss: 0.017588121816515923, Train Acc: 0.9965000152587891\n",
            "Epoch 3253/10000\n",
            "Step 0: Train Loss: 0.017477156594395638, Train Acc: 0.9959999918937683\n",
            "Epoch 3254/10000\n",
            "Step 0: Train Loss: 0.01786431297659874, Train Acc: 0.9959999918937683\n",
            "Epoch 3255/10000\n",
            "Step 0: Train Loss: 0.01862615905702114, Train Acc: 0.996399998664856\n",
            "Epoch 3256/10000\n",
            "Step 0: Train Loss: 0.019126685336232185, Train Acc: 0.9965000152587891\n",
            "Epoch 3257/10000\n",
            "Step 0: Train Loss: 0.018091004341840744, Train Acc: 0.9955999851226807\n",
            "Epoch 3258/10000\n",
            "Step 0: Train Loss: 0.012965909205377102, Train Acc: 0.9973999857902527\n",
            "Epoch 3259/10000\n",
            "Step 0: Train Loss: 0.01641484908759594, Train Acc: 0.9961000084877014\n",
            "Epoch 3260/10000\n",
            "Step 0: Train Loss: 0.015643605962395668, Train Acc: 0.9968000054359436\n",
            "Epoch 3261/10000\n",
            "Step 0: Train Loss: 0.01710861548781395, Train Acc: 0.9966999888420105\n",
            "Epoch 3262/10000\n",
            "Step 0: Train Loss: 0.01517951674759388, Train Acc: 0.9966999888420105\n",
            "Epoch 3263/10000\n",
            "Step 0: Train Loss: 0.018113428726792336, Train Acc: 0.9962000250816345\n",
            "Epoch 3264/10000\n",
            "Step 0: Train Loss: 0.014069934375584126, Train Acc: 0.9976000189781189\n",
            "Epoch 3265/10000\n",
            "Step 0: Train Loss: 0.014390895143151283, Train Acc: 0.9965999722480774\n",
            "Epoch 3266/10000\n",
            "Step 0: Train Loss: 0.014069844037294388, Train Acc: 0.9973999857902527\n",
            "Epoch 3267/10000\n",
            "Step 0: Train Loss: 0.014715498313307762, Train Acc: 0.996999979019165\n",
            "Epoch 3268/10000\n",
            "Step 0: Train Loss: 0.011210352182388306, Train Acc: 0.9977999925613403\n",
            "Epoch 3269/10000\n",
            "Step 0: Train Loss: 0.011183138936758041, Train Acc: 0.9983999729156494\n",
            "Epoch 3270/10000\n",
            "Step 0: Train Loss: 0.014796264469623566, Train Acc: 0.9969000220298767\n",
            "Epoch 3271/10000\n",
            "Step 0: Train Loss: 0.010126888751983643, Train Acc: 0.9983999729156494\n",
            "Epoch 3272/10000\n",
            "Step 0: Train Loss: 0.010102836415171623, Train Acc: 0.9983999729156494\n",
            "Epoch 3273/10000\n",
            "Step 0: Train Loss: 0.012659525498747826, Train Acc: 0.9973999857902527\n",
            "Epoch 3274/10000\n",
            "Step 0: Train Loss: 0.013654305599629879, Train Acc: 0.9973999857902527\n",
            "Epoch 3275/10000\n",
            "Step 0: Train Loss: 0.010519192554056644, Train Acc: 0.9986000061035156\n",
            "Epoch 3276/10000\n",
            "Step 0: Train Loss: 0.014344380237162113, Train Acc: 0.9972000122070312\n",
            "Epoch 3277/10000\n",
            "Step 0: Train Loss: 0.013530614785850048, Train Acc: 0.9976000189781189\n",
            "Epoch 3278/10000\n",
            "Step 0: Train Loss: 0.009897982701659203, Train Acc: 0.9979000091552734\n",
            "Epoch 3279/10000\n",
            "Step 0: Train Loss: 0.009714657440781593, Train Acc: 0.9976999759674072\n",
            "Epoch 3280/10000\n",
            "Step 0: Train Loss: 0.01106248702853918, Train Acc: 0.9976000189781189\n",
            "Epoch 3281/10000\n",
            "Step 0: Train Loss: 0.01196358073502779, Train Acc: 0.9976000189781189\n",
            "Epoch 3282/10000\n",
            "Step 0: Train Loss: 0.013027125969529152, Train Acc: 0.9976000189781189\n",
            "Epoch 3283/10000\n",
            "Step 0: Train Loss: 0.010883809998631477, Train Acc: 0.9980999827384949\n",
            "Epoch 3284/10000\n",
            "Step 0: Train Loss: 0.009381791576743126, Train Acc: 0.9983999729156494\n",
            "Epoch 3285/10000\n",
            "Step 0: Train Loss: 0.011019537225365639, Train Acc: 0.9983999729156494\n",
            "Epoch 3286/10000\n",
            "Step 0: Train Loss: 0.008920117281377316, Train Acc: 0.9987000226974487\n",
            "Epoch 3287/10000\n",
            "Step 0: Train Loss: 0.009248913265764713, Train Acc: 0.9987999796867371\n",
            "Epoch 3288/10000\n",
            "Step 0: Train Loss: 0.00828109122812748, Train Acc: 0.9984999895095825\n",
            "Epoch 3289/10000\n",
            "Step 0: Train Loss: 0.008589737117290497, Train Acc: 0.9984999895095825\n",
            "Epoch 3290/10000\n",
            "Step 0: Train Loss: 0.00904110912233591, Train Acc: 0.9986000061035156\n",
            "Epoch 3291/10000\n",
            "Step 0: Train Loss: 0.008962777443230152, Train Acc: 0.9983000159263611\n",
            "Epoch 3292/10000\n",
            "Step 0: Train Loss: 0.009443456307053566, Train Acc: 0.9983000159263611\n",
            "Epoch 3293/10000\n",
            "Step 0: Train Loss: 0.009788370691239834, Train Acc: 0.9983000159263611\n",
            "Epoch 3294/10000\n",
            "Step 0: Train Loss: 0.007346103899180889, Train Acc: 0.9990000128746033\n",
            "Epoch 3295/10000\n",
            "Step 0: Train Loss: 0.00923251174390316, Train Acc: 0.9983000159263611\n",
            "Epoch 3296/10000\n",
            "Step 0: Train Loss: 0.007293769624084234, Train Acc: 0.9988999962806702\n",
            "Epoch 3297/10000\n",
            "Step 0: Train Loss: 0.0084617231041193, Train Acc: 0.9987999796867371\n",
            "Epoch 3298/10000\n",
            "Step 0: Train Loss: 0.006896392907947302, Train Acc: 0.9988999962806702\n",
            "Epoch 3299/10000\n",
            "Step 0: Train Loss: 0.0077669029124081135, Train Acc: 0.9988999962806702\n",
            "Epoch 3300/10000\n",
            "Step 0: Train Loss: 0.007862179540097713, Train Acc: 0.9987999796867371\n",
            "Epoch 3301/10000\n",
            "Step 0: Train Loss: 0.008009832352399826, Train Acc: 0.9987999796867371\n",
            "Epoch 3302/10000\n",
            "Step 0: Train Loss: 0.008998841047286987, Train Acc: 0.9986000061035156\n",
            "Epoch 3303/10000\n",
            "Step 0: Train Loss: 0.0064819734543561935, Train Acc: 0.9991999864578247\n",
            "Epoch 3304/10000\n",
            "Step 0: Train Loss: 0.005407511256635189, Train Acc: 0.9994000196456909\n",
            "Epoch 3305/10000\n",
            "Step 0: Train Loss: 0.006462778430432081, Train Acc: 0.9993000030517578\n",
            "Epoch 3306/10000\n",
            "Step 0: Train Loss: 0.007592321839183569, Train Acc: 0.9987999796867371\n",
            "Epoch 3307/10000\n",
            "Step 0: Train Loss: 0.006682498846203089, Train Acc: 0.9983000159263611\n",
            "Epoch 3308/10000\n",
            "Step 0: Train Loss: 0.0055649238638579845, Train Acc: 0.9993000030517578\n",
            "Epoch 3309/10000\n",
            "Step 0: Train Loss: 0.007329106330871582, Train Acc: 0.9988999962806702\n",
            "Epoch 3310/10000\n",
            "Step 0: Train Loss: 0.006475049536675215, Train Acc: 0.9990000128746033\n",
            "Epoch 3311/10000\n",
            "Step 0: Train Loss: 0.006627458147704601, Train Acc: 0.9991000294685364\n",
            "Epoch 3312/10000\n",
            "Step 0: Train Loss: 0.0078519806265831, Train Acc: 0.9983999729156494\n",
            "Epoch 3313/10000\n",
            "Step 0: Train Loss: 0.006927461829036474, Train Acc: 0.9987999796867371\n",
            "Epoch 3314/10000\n",
            "Step 0: Train Loss: 0.0043950737453997135, Train Acc: 0.9994000196456909\n",
            "Epoch 3315/10000\n",
            "Step 0: Train Loss: 0.006486563012003899, Train Acc: 0.9990000128746033\n",
            "Epoch 3316/10000\n",
            "Step 0: Train Loss: 0.0058668977580964565, Train Acc: 0.9990000128746033\n",
            "Epoch 3317/10000\n",
            "Step 0: Train Loss: 0.004631898831576109, Train Acc: 0.9994000196456909\n",
            "Epoch 3318/10000\n",
            "Step 0: Train Loss: 0.0052013397216796875, Train Acc: 0.9991999864578247\n",
            "Epoch 3319/10000\n",
            "Step 0: Train Loss: 0.007425361778587103, Train Acc: 0.9988999962806702\n",
            "Epoch 3320/10000\n",
            "Step 0: Train Loss: 0.007233384530991316, Train Acc: 0.9987999796867371\n",
            "Epoch 3321/10000\n",
            "Step 0: Train Loss: 0.00809498317539692, Train Acc: 0.9983999729156494\n",
            "Epoch 3322/10000\n",
            "Step 0: Train Loss: 0.005587357562035322, Train Acc: 0.9987999796867371\n",
            "Epoch 3323/10000\n",
            "Step 0: Train Loss: 0.00649476470425725, Train Acc: 0.9986000061035156\n",
            "Epoch 3324/10000\n",
            "Step 0: Train Loss: 0.004992777947336435, Train Acc: 0.9991999864578247\n",
            "Epoch 3325/10000\n",
            "Step 0: Train Loss: 0.004417412914335728, Train Acc: 0.9993000030517578\n",
            "Epoch 3326/10000\n",
            "Step 0: Train Loss: 0.00552368676289916, Train Acc: 0.9990000128746033\n",
            "Epoch 3327/10000\n",
            "Step 0: Train Loss: 0.004777892958372831, Train Acc: 0.9991000294685364\n",
            "Epoch 3328/10000\n",
            "Step 0: Train Loss: 0.0038524153642356396, Train Acc: 0.9991999864578247\n",
            "Epoch 3329/10000\n",
            "Step 0: Train Loss: 0.005868630949407816, Train Acc: 0.9990000128746033\n",
            "Epoch 3330/10000\n",
            "Step 0: Train Loss: 0.0056292153894901276, Train Acc: 0.9987999796867371\n",
            "Epoch 3331/10000\n",
            "Step 0: Train Loss: 0.0035863022785633802, Train Acc: 0.9995999932289124\n",
            "Epoch 3332/10000\n",
            "Step 0: Train Loss: 0.00399835454300046, Train Acc: 0.9993000030517578\n",
            "Epoch 3333/10000\n",
            "Step 0: Train Loss: 0.0040993522852659225, Train Acc: 0.9993000030517578\n",
            "Epoch 3334/10000\n",
            "Step 0: Train Loss: 0.00508834095671773, Train Acc: 0.9991000294685364\n",
            "Epoch 3335/10000\n",
            "Step 0: Train Loss: 0.004983843769878149, Train Acc: 0.9991000294685364\n",
            "Epoch 3336/10000\n",
            "Step 0: Train Loss: 0.005435104947537184, Train Acc: 0.9987000226974487\n",
            "Epoch 3337/10000\n",
            "Step 0: Train Loss: 0.006653151009231806, Train Acc: 0.9984999895095825\n",
            "Epoch 3338/10000\n",
            "Step 0: Train Loss: 0.004099166952073574, Train Acc: 0.9994999766349792\n",
            "Epoch 3339/10000\n",
            "Step 0: Train Loss: 0.005256288684904575, Train Acc: 0.9987999796867371\n",
            "Epoch 3340/10000\n",
            "Step 0: Train Loss: 0.004594326950609684, Train Acc: 0.9990000128746033\n",
            "Epoch 3341/10000\n",
            "Step 0: Train Loss: 0.0035227497573941946, Train Acc: 0.9994999766349792\n",
            "Epoch 3342/10000\n",
            "Step 0: Train Loss: 0.003013178240507841, Train Acc: 0.9997000098228455\n",
            "Epoch 3343/10000\n",
            "Step 0: Train Loss: 0.003973370883613825, Train Acc: 0.9991000294685364\n",
            "Epoch 3344/10000\n",
            "Step 0: Train Loss: 0.003969826735556126, Train Acc: 0.9991999864578247\n",
            "Epoch 3345/10000\n",
            "Step 0: Train Loss: 0.004970747046172619, Train Acc: 0.9990000128746033\n",
            "Epoch 3346/10000\n",
            "Step 0: Train Loss: 0.004018943756818771, Train Acc: 0.9991000294685364\n",
            "Epoch 3347/10000\n",
            "Step 0: Train Loss: 0.003353334730491042, Train Acc: 0.9994000196456909\n",
            "Epoch 3348/10000\n",
            "Step 0: Train Loss: 0.0036633724812418222, Train Acc: 0.9994000196456909\n",
            "Epoch 3349/10000\n",
            "Step 0: Train Loss: 0.004771356470882893, Train Acc: 0.9991999864578247\n",
            "Epoch 3350/10000\n",
            "Step 0: Train Loss: 0.004464528989046812, Train Acc: 0.9991999864578247\n",
            "Epoch 3351/10000\n",
            "Step 0: Train Loss: 0.0026439244393259287, Train Acc: 0.9994999766349792\n",
            "Epoch 3352/10000\n",
            "Step 0: Train Loss: 0.0032419986091554165, Train Acc: 0.9994999766349792\n",
            "Epoch 3353/10000\n",
            "Step 0: Train Loss: 0.003201690735295415, Train Acc: 0.9993000030517578\n",
            "Epoch 3354/10000\n",
            "Step 0: Train Loss: 0.0024717894848436117, Train Acc: 0.9995999932289124\n",
            "Epoch 3355/10000\n",
            "Step 0: Train Loss: 0.004107402637600899, Train Acc: 0.9991999864578247\n",
            "Epoch 3356/10000\n",
            "Step 0: Train Loss: 0.0033419120591133833, Train Acc: 0.9994000196456909\n",
            "Epoch 3357/10000\n",
            "Step 0: Train Loss: 0.0035630520433187485, Train Acc: 0.9994000196456909\n",
            "Epoch 3358/10000\n",
            "Step 0: Train Loss: 0.0033138964790850878, Train Acc: 0.9993000030517578\n",
            "Epoch 3359/10000\n",
            "Step 0: Train Loss: 0.003457730868831277, Train Acc: 0.9995999932289124\n",
            "Epoch 3360/10000\n",
            "Step 0: Train Loss: 0.0031008629593998194, Train Acc: 0.9994000196456909\n",
            "Epoch 3361/10000\n",
            "Step 0: Train Loss: 0.0025192725006490946, Train Acc: 0.9994999766349792\n",
            "Epoch 3362/10000\n",
            "Step 0: Train Loss: 0.0029811894055455923, Train Acc: 0.9995999932289124\n",
            "Epoch 3363/10000\n",
            "Step 0: Train Loss: 0.003498199861496687, Train Acc: 0.9994000196456909\n",
            "Epoch 3364/10000\n",
            "Step 0: Train Loss: 0.003233969211578369, Train Acc: 0.9994000196456909\n",
            "Epoch 3365/10000\n",
            "Step 0: Train Loss: 0.0031909840181469917, Train Acc: 0.9994999766349792\n",
            "Epoch 3366/10000\n",
            "Step 0: Train Loss: 0.004182743839919567, Train Acc: 0.9991000294685364\n",
            "Epoch 3367/10000\n",
            "Step 0: Train Loss: 0.003448148025199771, Train Acc: 0.9994000196456909\n",
            "Epoch 3368/10000\n",
            "Step 0: Train Loss: 0.003308451734483242, Train Acc: 0.9994999766349792\n",
            "Epoch 3369/10000\n",
            "Step 0: Train Loss: 0.002699237549677491, Train Acc: 0.9994000196456909\n",
            "Epoch 3370/10000\n",
            "Step 0: Train Loss: 0.0020832917653024197, Train Acc: 0.9998999834060669\n",
            "Epoch 3371/10000\n",
            "Step 0: Train Loss: 0.002762590069323778, Train Acc: 0.9995999932289124\n",
            "Epoch 3372/10000\n",
            "Step 0: Train Loss: 0.0033590872772037983, Train Acc: 0.9994000196456909\n",
            "Epoch 3373/10000\n",
            "Step 0: Train Loss: 0.004190589301288128, Train Acc: 0.9991000294685364\n",
            "Epoch 3374/10000\n",
            "Step 0: Train Loss: 0.002603313187137246, Train Acc: 0.9998000264167786\n",
            "Epoch 3375/10000\n",
            "Step 0: Train Loss: 0.002397482516244054, Train Acc: 0.9997000098228455\n",
            "Epoch 3376/10000\n",
            "Step 0: Train Loss: 0.0028034208808094263, Train Acc: 0.9995999932289124\n",
            "Epoch 3377/10000\n",
            "Step 0: Train Loss: 0.001732785371132195, Train Acc: 0.9998999834060669\n",
            "Epoch 3378/10000\n",
            "Step 0: Train Loss: 0.0016531223664060235, Train Acc: 0.9998999834060669\n",
            "Epoch 3379/10000\n",
            "Step 0: Train Loss: 0.002126257633790374, Train Acc: 0.9997000098228455\n",
            "Epoch 3380/10000\n",
            "Step 0: Train Loss: 0.0033134985715150833, Train Acc: 0.9993000030517578\n",
            "Epoch 3381/10000\n",
            "Step 0: Train Loss: 0.0018771190661936998, Train Acc: 0.9998999834060669\n",
            "Epoch 3382/10000\n",
            "Step 0: Train Loss: 0.0022693348582834005, Train Acc: 0.9998000264167786\n",
            "Epoch 3383/10000\n",
            "Step 0: Train Loss: 0.0022632109466940165, Train Acc: 0.9997000098228455\n",
            "Epoch 3384/10000\n",
            "Step 0: Train Loss: 0.0021089985966682434, Train Acc: 0.9998000264167786\n",
            "Epoch 3385/10000\n",
            "Step 0: Train Loss: 0.0022654766216874123, Train Acc: 0.9998000264167786\n",
            "Epoch 3386/10000\n",
            "Step 0: Train Loss: 0.0022810238879173994, Train Acc: 0.9997000098228455\n",
            "Epoch 3387/10000\n",
            "Step 0: Train Loss: 0.0031257146038115025, Train Acc: 0.9994999766349792\n",
            "Epoch 3388/10000\n",
            "Step 0: Train Loss: 0.002681900281459093, Train Acc: 0.9995999932289124\n",
            "Epoch 3389/10000\n",
            "Step 0: Train Loss: 0.002015553181990981, Train Acc: 0.9998000264167786\n",
            "Epoch 3390/10000\n",
            "Step 0: Train Loss: 0.0017413576133549213, Train Acc: 0.9998999834060669\n",
            "Epoch 3391/10000\n",
            "Step 0: Train Loss: 0.002316173631697893, Train Acc: 0.9995999932289124\n",
            "Epoch 3392/10000\n",
            "Step 0: Train Loss: 0.003059188835322857, Train Acc: 0.9993000030517578\n",
            "Epoch 3393/10000\n",
            "Step 0: Train Loss: 0.001403540838509798, Train Acc: 0.9998000264167786\n",
            "Epoch 3394/10000\n",
            "Step 0: Train Loss: 0.0017609819769859314, Train Acc: 0.9998999834060669\n",
            "Epoch 3395/10000\n",
            "Step 0: Train Loss: 0.0020094243809580803, Train Acc: 0.9998000264167786\n",
            "Epoch 3396/10000\n",
            "Step 0: Train Loss: 0.00188550166785717, Train Acc: 0.9997000098228455\n",
            "Epoch 3397/10000\n",
            "Step 0: Train Loss: 0.0019395080162212253, Train Acc: 0.9998000264167786\n",
            "Epoch 3398/10000\n",
            "Step 0: Train Loss: 0.001288423198275268, Train Acc: 0.9998999834060669\n",
            "Epoch 3399/10000\n",
            "Step 0: Train Loss: 0.0021189076360315084, Train Acc: 0.9998000264167786\n",
            "Epoch 3400/10000\n",
            "Step 0: Train Loss: 0.0016352306120097637, Train Acc: 0.9998000264167786\n",
            "Epoch 3401/10000\n",
            "Step 0: Train Loss: 0.0016477941535413265, Train Acc: 0.9998000264167786\n",
            "Epoch 3402/10000\n",
            "Step 0: Train Loss: 0.002010953612625599, Train Acc: 0.9995999932289124\n",
            "Epoch 3403/10000\n",
            "Step 0: Train Loss: 0.0011306650703772902, Train Acc: 0.9998999834060669\n",
            "Epoch 3404/10000\n",
            "Step 0: Train Loss: 0.0011462209513410926, Train Acc: 0.9998999834060669\n",
            "Epoch 3405/10000\n",
            "Step 0: Train Loss: 0.0013574863551184535, Train Acc: 0.9998999834060669\n",
            "Epoch 3406/10000\n",
            "Step 0: Train Loss: 0.0015909497160464525, Train Acc: 0.9997000098228455\n",
            "Epoch 3407/10000\n",
            "Step 0: Train Loss: 0.0011136961402371526, Train Acc: 0.9998000264167786\n",
            "Epoch 3408/10000\n",
            "Step 0: Train Loss: 0.0011243853950873017, Train Acc: 0.9998000264167786\n",
            "Epoch 3409/10000\n",
            "Step 0: Train Loss: 0.0011255559511482716, Train Acc: 1.0\n",
            "Epoch 3410/10000\n",
            "Step 0: Train Loss: 0.0012265287805348635, Train Acc: 0.9998999834060669\n",
            "Epoch 3411/10000\n",
            "Step 0: Train Loss: 0.0012026504846289754, Train Acc: 1.0\n",
            "Epoch 3412/10000\n",
            "Step 0: Train Loss: 0.0009525876957923174, Train Acc: 1.0\n",
            "Epoch 3413/10000\n",
            "Step 0: Train Loss: 0.0017733573913574219, Train Acc: 0.9998000264167786\n",
            "Epoch 3414/10000\n",
            "Step 0: Train Loss: 0.0022408224176615477, Train Acc: 0.9997000098228455\n",
            "Epoch 3415/10000\n",
            "Step 0: Train Loss: 0.0014793272130191326, Train Acc: 0.9998999834060669\n",
            "Epoch 3416/10000\n",
            "Step 0: Train Loss: 0.0015723686665296555, Train Acc: 0.9998999834060669\n",
            "Epoch 3417/10000\n",
            "Step 0: Train Loss: 0.0015529540833085775, Train Acc: 0.9998000264167786\n",
            "Epoch 3418/10000\n",
            "Step 0: Train Loss: 0.000877709942869842, Train Acc: 1.0\n",
            "Epoch 3419/10000\n",
            "Step 0: Train Loss: 0.0012371089542284608, Train Acc: 0.9998000264167786\n",
            "Epoch 3420/10000\n",
            "Step 0: Train Loss: 0.0009203085792250931, Train Acc: 0.9998999834060669\n",
            "Epoch 3421/10000\n",
            "Step 0: Train Loss: 0.0013164493720978498, Train Acc: 0.9998999834060669\n",
            "Epoch 3422/10000\n",
            "Step 0: Train Loss: 0.0012851537903770804, Train Acc: 0.9998999834060669\n",
            "Epoch 3423/10000\n",
            "Step 0: Train Loss: 0.0009669081773608923, Train Acc: 0.9998999834060669\n",
            "Epoch 3424/10000\n",
            "Step 0: Train Loss: 0.0009194269077852368, Train Acc: 0.9998999834060669\n",
            "Epoch 3425/10000\n",
            "Step 0: Train Loss: 0.0008461343823000789, Train Acc: 0.9998999834060669\n",
            "Epoch 3426/10000\n",
            "Step 0: Train Loss: 0.0008989640045911074, Train Acc: 1.0\n",
            "Epoch 3427/10000\n",
            "Step 0: Train Loss: 0.000987011007964611, Train Acc: 0.9998999834060669\n",
            "Epoch 3428/10000\n",
            "Step 0: Train Loss: 0.001265007071197033, Train Acc: 0.9998999834060669\n",
            "Epoch 3429/10000\n",
            "Step 0: Train Loss: 0.0010985024273395538, Train Acc: 0.9998999834060669\n",
            "Epoch 3430/10000\n",
            "Step 0: Train Loss: 0.001346689066849649, Train Acc: 0.9998999834060669\n",
            "Epoch 3431/10000\n",
            "Step 0: Train Loss: 0.0008635511621832848, Train Acc: 1.0\n",
            "Epoch 3432/10000\n",
            "Step 0: Train Loss: 0.0007805383647792041, Train Acc: 1.0\n",
            "Epoch 3433/10000\n",
            "Step 0: Train Loss: 0.001099920249544084, Train Acc: 0.9998999834060669\n",
            "Epoch 3434/10000\n",
            "Step 0: Train Loss: 0.0022291552741080523, Train Acc: 0.9997000098228455\n",
            "Epoch 3435/10000\n",
            "Step 0: Train Loss: 0.0008716842858120799, Train Acc: 1.0\n",
            "Epoch 3436/10000\n",
            "Step 0: Train Loss: 0.0009476040140725672, Train Acc: 1.0\n",
            "Epoch 3437/10000\n",
            "Step 0: Train Loss: 0.0018133962294086814, Train Acc: 0.9997000098228455\n",
            "Epoch 3438/10000\n",
            "Step 0: Train Loss: 0.0012282599927857518, Train Acc: 0.9998000264167786\n",
            "Epoch 3439/10000\n",
            "Step 0: Train Loss: 0.0011296463198959827, Train Acc: 0.9998999834060669\n",
            "Epoch 3440/10000\n",
            "Step 0: Train Loss: 0.0014532749773934484, Train Acc: 0.9998000264167786\n",
            "Epoch 3441/10000\n",
            "Step 0: Train Loss: 0.0007958247442729771, Train Acc: 1.0\n",
            "Epoch 3442/10000\n",
            "Step 0: Train Loss: 0.0005936233792454004, Train Acc: 1.0\n",
            "Epoch 3443/10000\n",
            "Step 0: Train Loss: 0.0008444470586255193, Train Acc: 0.9998999834060669\n",
            "Epoch 3444/10000\n",
            "Step 0: Train Loss: 0.0006779969553463161, Train Acc: 1.0\n",
            "Epoch 3445/10000\n",
            "Step 0: Train Loss: 0.001353519968688488, Train Acc: 0.9998999834060669\n",
            "Epoch 3446/10000\n",
            "Step 0: Train Loss: 0.0009540359606035054, Train Acc: 0.9998999834060669\n",
            "Epoch 3447/10000\n",
            "Step 0: Train Loss: 0.0011264466447755694, Train Acc: 0.9998999834060669\n",
            "Epoch 3448/10000\n",
            "Step 0: Train Loss: 0.0008683979976922274, Train Acc: 0.9998999834060669\n",
            "Epoch 3449/10000\n",
            "Step 0: Train Loss: 0.0010881939670071006, Train Acc: 0.9998999834060669\n",
            "Epoch 3450/10000\n",
            "Step 0: Train Loss: 0.0013195113278925419, Train Acc: 0.9998000264167786\n",
            "Epoch 3451/10000\n",
            "Step 0: Train Loss: 0.001075586536899209, Train Acc: 0.9998999834060669\n",
            "Epoch 3452/10000\n",
            "Step 0: Train Loss: 0.0011119726113975048, Train Acc: 0.9998999834060669\n",
            "Epoch 3453/10000\n",
            "Step 0: Train Loss: 0.00113761518150568, Train Acc: 0.9998999834060669\n",
            "Epoch 3454/10000\n",
            "Step 0: Train Loss: 0.0009170091361738741, Train Acc: 0.9998999834060669\n",
            "Epoch 3455/10000\n",
            "Step 0: Train Loss: 0.0006264183321036398, Train Acc: 1.0\n",
            "Epoch 3456/10000\n",
            "Step 0: Train Loss: 0.0005934362416155636, Train Acc: 1.0\n",
            "Epoch 3457/10000\n",
            "Step 0: Train Loss: 0.001481696148402989, Train Acc: 0.9998000264167786\n",
            "Epoch 3458/10000\n",
            "Step 0: Train Loss: 0.0010931098368018866, Train Acc: 0.9998999834060669\n",
            "Epoch 3459/10000\n",
            "Step 0: Train Loss: 0.0010987238492816687, Train Acc: 0.9998999834060669\n",
            "Epoch 3460/10000\n",
            "Step 0: Train Loss: 0.0006040956941433251, Train Acc: 1.0\n",
            "Epoch 3461/10000\n",
            "Step 0: Train Loss: 0.0005923729040659964, Train Acc: 1.0\n",
            "Epoch 3462/10000\n",
            "Step 0: Train Loss: 0.0009696758934296668, Train Acc: 0.9998999834060669\n",
            "Epoch 3463/10000\n",
            "Step 0: Train Loss: 0.0005871669854968786, Train Acc: 1.0\n",
            "Epoch 3464/10000\n",
            "Step 0: Train Loss: 0.0007766744820401073, Train Acc: 0.9998999834060669\n",
            "Epoch 3465/10000\n",
            "Step 0: Train Loss: 0.001198016107082367, Train Acc: 0.9998000264167786\n",
            "Epoch 3466/10000\n",
            "Step 0: Train Loss: 0.0005723733920603991, Train Acc: 1.0\n",
            "Epoch 3467/10000\n",
            "Step 0: Train Loss: 0.0005725714727304876, Train Acc: 1.0\n",
            "Epoch 3468/10000\n",
            "Step 0: Train Loss: 0.0011610122164711356, Train Acc: 0.9998000264167786\n",
            "Epoch 3469/10000\n",
            "Step 0: Train Loss: 0.0013557550264522433, Train Acc: 0.9997000098228455\n",
            "Epoch 3470/10000\n",
            "Step 0: Train Loss: 0.0005195135599933565, Train Acc: 1.0\n",
            "Epoch 3471/10000\n",
            "Step 0: Train Loss: 0.000552986457478255, Train Acc: 1.0\n",
            "Epoch 3472/10000\n",
            "Step 0: Train Loss: 0.0005409165169112384, Train Acc: 1.0\n",
            "Epoch 3473/10000\n",
            "Step 0: Train Loss: 0.0009066453785635531, Train Acc: 0.9998999834060669\n",
            "Epoch 3474/10000\n",
            "Step 0: Train Loss: 0.0007237870013341308, Train Acc: 0.9998999834060669\n",
            "Epoch 3475/10000\n",
            "Step 0: Train Loss: 0.001427480368874967, Train Acc: 0.9997000098228455\n",
            "Epoch 3476/10000\n",
            "Step 0: Train Loss: 0.000498441222589463, Train Acc: 1.0\n",
            "Epoch 3477/10000\n",
            "Step 0: Train Loss: 0.00041623591096140444, Train Acc: 1.0\n",
            "Epoch 3478/10000\n",
            "Step 0: Train Loss: 0.000908984278794378, Train Acc: 0.9998999834060669\n",
            "Epoch 3479/10000\n",
            "Step 0: Train Loss: 0.0011229144874960184, Train Acc: 0.9998000264167786\n",
            "Epoch 3480/10000\n",
            "Step 0: Train Loss: 0.0005008596344850957, Train Acc: 1.0\n",
            "Epoch 3481/10000\n",
            "Step 0: Train Loss: 0.0009934816043823957, Train Acc: 0.9998000264167786\n",
            "Epoch 3482/10000\n",
            "Step 0: Train Loss: 0.0007897875620983541, Train Acc: 0.9998999834060669\n",
            "Epoch 3483/10000\n",
            "Step 0: Train Loss: 0.0008703408529981971, Train Acc: 0.9998999834060669\n",
            "Epoch 3484/10000\n",
            "Step 0: Train Loss: 0.001407018513418734, Train Acc: 0.9997000098228455\n",
            "Epoch 3485/10000\n",
            "Step 0: Train Loss: 0.0009218630730174482, Train Acc: 0.9998999834060669\n",
            "Epoch 3486/10000\n",
            "Step 0: Train Loss: 0.0011001999955624342, Train Acc: 0.9998000264167786\n",
            "Epoch 3487/10000\n",
            "Step 0: Train Loss: 0.0010144694242626429, Train Acc: 0.9998000264167786\n",
            "Epoch 3488/10000\n",
            "Step 0: Train Loss: 0.0003706644638441503, Train Acc: 1.0\n",
            "Epoch 3489/10000\n",
            "Step 0: Train Loss: 0.0004653200739994645, Train Acc: 1.0\n",
            "Epoch 3490/10000\n",
            "Step 0: Train Loss: 0.00046972083509899676, Train Acc: 1.0\n",
            "Epoch 3491/10000\n",
            "Step 0: Train Loss: 0.000838644802570343, Train Acc: 0.9998999834060669\n",
            "Epoch 3492/10000\n",
            "Step 0: Train Loss: 0.0007434965809807181, Train Acc: 0.9998999834060669\n",
            "Epoch 3493/10000\n",
            "Step 0: Train Loss: 0.0008180946460925043, Train Acc: 0.9998999834060669\n",
            "Epoch 3494/10000\n",
            "Step 0: Train Loss: 0.0011206238996237516, Train Acc: 0.9998000264167786\n",
            "Epoch 3495/10000\n",
            "Step 0: Train Loss: 0.0003561068733688444, Train Acc: 1.0\n",
            "Epoch 3496/10000\n",
            "Step 0: Train Loss: 0.0010506053222343326, Train Acc: 0.9998000264167786\n",
            "Epoch 3497/10000\n",
            "Step 0: Train Loss: 0.0013078090269118547, Train Acc: 0.9997000098228455\n",
            "Epoch 3498/10000\n",
            "Step 0: Train Loss: 0.0008162630838342011, Train Acc: 0.9998999834060669\n",
            "Epoch 3499/10000\n",
            "Step 0: Train Loss: 0.0010593730257824063, Train Acc: 0.9998000264167786\n",
            "Epoch 3500/10000\n",
            "Step 0: Train Loss: 0.0010535742621868849, Train Acc: 0.9998000264167786\n",
            "Epoch 3501/10000\n",
            "Step 0: Train Loss: 0.0011852496536448598, Train Acc: 0.9997000098228455\n",
            "Epoch index and hidden dimension and ratio: 3500 1024 0.9564475150830585\n",
            "Epoch index and hidden dimension and ratio: 3500 20 0.7905833543608001\n",
            "Epoch index and hidden dimension and ratio: 3500 20 1.0228893307078117\n",
            "Epoch index and hidden dimension and ratio: 3500 20 2.071856641089345\n",
            "MI(X;T): [10.566331108882277, 7.491775418826863, 5.216966457354684, 2.529002103890541], MI(Y;T): [2.627606348758481, 3.169442397500249, 3.195081382215807, 2.9709529815273585]\n",
            "Epoch index and hidden dimension and ratio: 3500 1024 0.9564674860795913\n",
            "Epoch index and hidden dimension and ratio: 3500 20 0.7906112342866243\n",
            "Epoch index and hidden dimension and ratio: 3500 20 1.0230735488851055\n",
            "Epoch index and hidden dimension and ratio: 3500 20 2.0727251310035353\n",
            "MI(X;T): [10.567017748523567, 7.492001444930377, 5.215838448960655, 2.5281507641263543], MI(Y;T): [2.6274316913122338, 3.169254988243525, 3.195180768223842, 2.9710419584223238]\n",
            "Epoch index and hidden dimension and ratio: 3500 1024 0.9564904674101604\n",
            "Epoch index and hidden dimension and ratio: 3500 20 0.7906596715896613\n",
            "Epoch index and hidden dimension and ratio: 3500 20 1.023305630779036\n",
            "Epoch index and hidden dimension and ratio: 3500 20 2.073676537931696\n",
            "MI(X;T): [10.56494257319831, 7.494278013620995, 5.214196867394031, 2.5293758311143764], MI(Y;T): [2.627072458262495, 3.1691632290295604, 3.1954773416433886, 2.971097591077875]\n",
            "Epoch index and hidden dimension and ratio: 3500 1024 0.9565039772019327\n",
            "Epoch index and hidden dimension and ratio: 3500 20 0.7907088682681098\n",
            "Epoch index and hidden dimension and ratio: 3500 20 1.0234530941924933\n",
            "Epoch index and hidden dimension and ratio: 3500 20 2.0743852809796843\n",
            "MI(X;T): [10.566616525152199, 7.494860625673001, 5.214934712997229, 2.529662688302304], MI(Y;T): [2.6268247646137475, 3.1688688569050885, 3.1949676971365, 2.971158907172772]\n",
            "Epoch index and hidden dimension and ratio: 3500 1024 0.956502655591868\n",
            "Epoch index and hidden dimension and ratio: 3500 20 0.7907287747521128\n",
            "Epoch index and hidden dimension and ratio: 3500 20 1.0235246358485273\n",
            "Epoch index and hidden dimension and ratio: 3500 20 2.074876860419654\n",
            "MI(X;T): [10.565654865819603, 7.496254491564123, 5.213889480144058, 2.5285821227911485], MI(Y;T): [2.626953868891457, 3.168856499086112, 3.1951010787862657, 2.971089394642653]\n",
            "Epoch index and hidden dimension and ratio: 3500 1024 0.9565139627001992\n",
            "Epoch index and hidden dimension and ratio: 3500 20 0.7907660383883798\n",
            "Epoch index and hidden dimension and ratio: 3500 20 1.023589956490993\n",
            "Epoch index and hidden dimension and ratio: 3500 20 2.0752310674257632\n",
            "MI(X;T): [10.564397850601463, 7.49596745465774, 5.213355919834439, 2.5293363081673106], MI(Y;T): [2.626818973615117, 3.1684513407944785, 3.1953212740351633, 2.971224007349401]\n",
            "Epoch 3502/10000\n",
            "Step 0: Train Loss: 0.0007250057533383369, Train Acc: 0.9998999834060669\n",
            "Epoch 3503/10000\n",
            "Step 0: Train Loss: 0.0006979354075156152, Train Acc: 0.9998999834060669\n",
            "Epoch 3504/10000\n",
            "Step 0: Train Loss: 0.00042429202585481107, Train Acc: 1.0\n",
            "Epoch 3505/10000\n",
            "Step 0: Train Loss: 0.00040582174551673234, Train Acc: 1.0\n",
            "Epoch 3506/10000\n",
            "Step 0: Train Loss: 0.00032522986293770373, Train Acc: 1.0\n",
            "Epoch 3507/10000\n",
            "Step 0: Train Loss: 0.0003614853194449097, Train Acc: 1.0\n",
            "Epoch 3508/10000\n",
            "Step 0: Train Loss: 0.0003984292270615697, Train Acc: 1.0\n",
            "Epoch 3509/10000\n",
            "Step 0: Train Loss: 0.0006904843030497432, Train Acc: 0.9998000264167786\n",
            "Epoch 3510/10000\n",
            "Step 0: Train Loss: 0.00035609546466730535, Train Acc: 1.0\n",
            "Epoch 3511/10000\n",
            "Step 0: Train Loss: 0.0003718879306688905, Train Acc: 1.0\n",
            "Epoch 3512/10000\n",
            "Step 0: Train Loss: 0.0005673067062161863, Train Acc: 0.9998999834060669\n",
            "Epoch 3513/10000\n",
            "Step 0: Train Loss: 0.0006330473115667701, Train Acc: 0.9998999834060669\n",
            "Epoch 3514/10000\n",
            "Step 0: Train Loss: 0.0007283227168954909, Train Acc: 0.9998999834060669\n",
            "Epoch 3515/10000\n",
            "Step 0: Train Loss: 0.0005313206929713488, Train Acc: 0.9998999834060669\n",
            "Epoch 3516/10000\n",
            "Step 0: Train Loss: 0.000388973654480651, Train Acc: 1.0\n",
            "Epoch 3517/10000\n",
            "Step 0: Train Loss: 0.0006913581164553761, Train Acc: 0.9998999834060669\n",
            "Epoch 3518/10000\n",
            "Step 0: Train Loss: 0.00033390059252269566, Train Acc: 1.0\n",
            "Epoch 3519/10000\n",
            "Step 0: Train Loss: 0.0003709699085447937, Train Acc: 1.0\n",
            "Epoch 3520/10000\n",
            "Step 0: Train Loss: 0.0003275006602052599, Train Acc: 1.0\n",
            "Epoch 3521/10000\n",
            "Step 0: Train Loss: 0.00045713907456956804, Train Acc: 0.9998999834060669\n",
            "Epoch 3522/10000\n",
            "Step 0: Train Loss: 0.000381648977054283, Train Acc: 1.0\n",
            "Epoch 3523/10000\n",
            "Step 0: Train Loss: 0.0007059736526571214, Train Acc: 0.9998999834060669\n",
            "Epoch 3524/10000\n",
            "Step 0: Train Loss: 0.00038988838787190616, Train Acc: 1.0\n",
            "Epoch 3525/10000\n",
            "Step 0: Train Loss: 0.0005634339177049696, Train Acc: 0.9998999834060669\n",
            "Epoch 3526/10000\n",
            "Step 0: Train Loss: 0.000883833912666887, Train Acc: 0.9998000264167786\n",
            "Epoch 3527/10000\n",
            "Step 0: Train Loss: 0.00033856116351671517, Train Acc: 1.0\n",
            "Epoch 3528/10000\n",
            "Step 0: Train Loss: 0.00025980101781897247, Train Acc: 1.0\n",
            "Epoch 3529/10000\n",
            "Step 0: Train Loss: 0.0007270133937709033, Train Acc: 0.9998999834060669\n",
            "Epoch 3530/10000\n",
            "Step 0: Train Loss: 0.0005763945518992841, Train Acc: 0.9998999834060669\n",
            "Epoch 3531/10000\n",
            "Step 0: Train Loss: 0.0005005324492231011, Train Acc: 0.9998999834060669\n",
            "Epoch 3532/10000\n",
            "Step 0: Train Loss: 0.0002932240895461291, Train Acc: 1.0\n",
            "Epoch 3533/10000\n",
            "Step 0: Train Loss: 0.00032299000304192305, Train Acc: 1.0\n",
            "Epoch 3534/10000\n",
            "Step 0: Train Loss: 0.0005137155530974269, Train Acc: 0.9998999834060669\n",
            "Epoch 3535/10000\n",
            "Step 0: Train Loss: 0.00033982901368290186, Train Acc: 1.0\n",
            "Epoch 3536/10000\n",
            "Step 0: Train Loss: 0.0006501246243715286, Train Acc: 0.9998999834060669\n",
            "Epoch 3537/10000\n",
            "Step 0: Train Loss: 0.0005274568102322519, Train Acc: 0.9998999834060669\n",
            "Epoch 3538/10000\n",
            "Step 0: Train Loss: 0.00039558776188641787, Train Acc: 0.9998999834060669\n",
            "Epoch 3539/10000\n",
            "Step 0: Train Loss: 0.0004042181826662272, Train Acc: 0.9998999834060669\n",
            "Epoch 3540/10000\n",
            "Step 0: Train Loss: 0.00024432712234556675, Train Acc: 1.0\n",
            "Epoch 3541/10000\n",
            "Step 0: Train Loss: 0.0003051261883229017, Train Acc: 1.0\n",
            "Epoch 3542/10000\n",
            "Step 0: Train Loss: 0.000320659251883626, Train Acc: 1.0\n",
            "Epoch 3543/10000\n",
            "Step 0: Train Loss: 0.0004984423867426813, Train Acc: 0.9998999834060669\n",
            "Epoch 3544/10000\n",
            "Step 0: Train Loss: 0.0003173559671267867, Train Acc: 1.0\n",
            "Epoch 3545/10000\n",
            "Step 0: Train Loss: 0.0003950049285776913, Train Acc: 0.9998999834060669\n",
            "Epoch 3546/10000\n",
            "Step 0: Train Loss: 0.00028001502505503595, Train Acc: 1.0\n",
            "Epoch 3547/10000\n",
            "Step 0: Train Loss: 0.0002545936731621623, Train Acc: 1.0\n",
            "Epoch 3548/10000\n",
            "Step 0: Train Loss: 0.0002571304212324321, Train Acc: 1.0\n",
            "Epoch 3549/10000\n",
            "Step 0: Train Loss: 0.0004218319372739643, Train Acc: 0.9998999834060669\n",
            "Epoch 3550/10000\n",
            "Step 0: Train Loss: 0.000299427512800321, Train Acc: 1.0\n",
            "Epoch 3551/10000\n",
            "Step 0: Train Loss: 0.00029729827656410635, Train Acc: 1.0\n",
            "Epoch 3552/10000\n",
            "Step 0: Train Loss: 0.00030393057386390865, Train Acc: 1.0\n",
            "Epoch 3553/10000\n",
            "Step 0: Train Loss: 0.00029676995472982526, Train Acc: 1.0\n",
            "Epoch 3554/10000\n",
            "Step 0: Train Loss: 0.00029228991479612887, Train Acc: 1.0\n",
            "Epoch 3555/10000\n",
            "Step 0: Train Loss: 0.0002641383616719395, Train Acc: 1.0\n",
            "Epoch 3556/10000\n",
            "Step 0: Train Loss: 0.0003144785878248513, Train Acc: 1.0\n",
            "Epoch 3557/10000\n",
            "Step 0: Train Loss: 0.00036138869472779334, Train Acc: 0.9998999834060669\n",
            "Epoch 3558/10000\n",
            "Step 0: Train Loss: 0.00023485670681111515, Train Acc: 1.0\n",
            "Epoch 3559/10000\n",
            "Step 0: Train Loss: 0.0002240112517029047, Train Acc: 1.0\n",
            "Epoch 3560/10000\n",
            "Step 0: Train Loss: 0.00025277730310335755, Train Acc: 1.0\n",
            "Epoch 3561/10000\n",
            "Step 0: Train Loss: 0.0005953088402748108, Train Acc: 0.9998999834060669\n",
            "Epoch 3562/10000\n",
            "Step 0: Train Loss: 0.0005802824744023383, Train Acc: 0.9998999834060669\n",
            "Epoch 3563/10000\n",
            "Step 0: Train Loss: 0.0002677557058632374, Train Acc: 1.0\n",
            "Epoch 3564/10000\n",
            "Step 0: Train Loss: 0.0005977240507490933, Train Acc: 0.9998999834060669\n",
            "Epoch 3565/10000\n",
            "Step 0: Train Loss: 0.0008208178333006799, Train Acc: 0.9998000264167786\n",
            "Epoch 3566/10000\n",
            "Step 0: Train Loss: 0.0003800184349529445, Train Acc: 0.9998999834060669\n",
            "Epoch 3567/10000\n",
            "Step 0: Train Loss: 0.0003684700350277126, Train Acc: 0.9998999834060669\n",
            "Epoch 3568/10000\n",
            "Step 0: Train Loss: 0.0002424932026769966, Train Acc: 1.0\n",
            "Epoch 3569/10000\n",
            "Step 0: Train Loss: 0.00026604367303662, Train Acc: 1.0\n",
            "Epoch 3570/10000\n",
            "Step 0: Train Loss: 0.0002610209339763969, Train Acc: 1.0\n",
            "Epoch 3571/10000\n",
            "Step 0: Train Loss: 0.00046186818508431315, Train Acc: 0.9998999834060669\n",
            "Epoch 3572/10000\n",
            "Step 0: Train Loss: 0.00047797622391954064, Train Acc: 0.9998999834060669\n",
            "Epoch 3573/10000\n",
            "Step 0: Train Loss: 0.0004996868665330112, Train Acc: 0.9998999834060669\n",
            "Epoch 3574/10000\n",
            "Step 0: Train Loss: 0.00022621116659138352, Train Acc: 1.0\n",
            "Epoch 3575/10000\n",
            "Step 0: Train Loss: 0.0005827742861583829, Train Acc: 0.9998999834060669\n",
            "Epoch 3576/10000\n",
            "Step 0: Train Loss: 0.0003808373003266752, Train Acc: 0.9998999834060669\n",
            "Epoch 3577/10000\n",
            "Step 0: Train Loss: 0.00019668774621095508, Train Acc: 1.0\n",
            "Epoch 3578/10000\n",
            "Step 0: Train Loss: 0.0002019807870965451, Train Acc: 1.0\n",
            "Epoch 3579/10000\n",
            "Step 0: Train Loss: 0.00030809183954261243, Train Acc: 0.9998999834060669\n",
            "Epoch 3580/10000\n",
            "Step 0: Train Loss: 0.0004279295972082764, Train Acc: 0.9998999834060669\n",
            "Epoch 3581/10000\n",
            "Step 0: Train Loss: 0.00040202346281148493, Train Acc: 0.9998999834060669\n",
            "Epoch 3582/10000\n",
            "Step 0: Train Loss: 0.00019882345804944634, Train Acc: 1.0\n",
            "Epoch 3583/10000\n",
            "Step 0: Train Loss: 0.00024243607185781002, Train Acc: 1.0\n",
            "Epoch 3584/10000\n",
            "Step 0: Train Loss: 0.0001849184336606413, Train Acc: 1.0\n",
            "Epoch 3585/10000\n",
            "Step 0: Train Loss: 0.00022078349138610065, Train Acc: 1.0\n",
            "Epoch 3586/10000\n",
            "Step 0: Train Loss: 0.00042944803135469556, Train Acc: 0.9998999834060669\n",
            "Epoch 3587/10000\n",
            "Step 0: Train Loss: 0.0006636675097979605, Train Acc: 0.9998000264167786\n",
            "Epoch 3588/10000\n",
            "Step 0: Train Loss: 0.0005175881087779999, Train Acc: 0.9998999834060669\n",
            "Epoch 3589/10000\n",
            "Step 0: Train Loss: 0.00046150502748787403, Train Acc: 0.9998999834060669\n",
            "Epoch 3590/10000\n",
            "Step 0: Train Loss: 0.0002784423704724759, Train Acc: 0.9998999834060669\n",
            "Epoch 3591/10000\n",
            "Step 0: Train Loss: 0.00020681085879914463, Train Acc: 1.0\n",
            "Epoch 3592/10000\n",
            "Step 0: Train Loss: 0.0005443934933282435, Train Acc: 0.9998999834060669\n",
            "Epoch 3593/10000\n",
            "Step 0: Train Loss: 0.0002453835913911462, Train Acc: 1.0\n",
            "Epoch 3594/10000\n",
            "Step 0: Train Loss: 0.00021067867055535316, Train Acc: 1.0\n",
            "Epoch 3595/10000\n",
            "Step 0: Train Loss: 0.0004047310212627053, Train Acc: 0.9998999834060669\n",
            "Epoch 3596/10000\n",
            "Step 0: Train Loss: 0.0004337077552918345, Train Acc: 0.9998999834060669\n",
            "Epoch 3597/10000\n",
            "Step 0: Train Loss: 0.00041574230999685824, Train Acc: 0.9998999834060669\n",
            "Epoch 3598/10000\n",
            "Step 0: Train Loss: 0.00016385229537263513, Train Acc: 1.0\n",
            "Epoch 3599/10000\n",
            "Step 0: Train Loss: 0.00016133273311425, Train Acc: 1.0\n",
            "Epoch 3600/10000\n",
            "Step 0: Train Loss: 0.00017491666949354112, Train Acc: 1.0\n",
            "Epoch 3601/10000\n",
            "Step 0: Train Loss: 0.00019353321113158017, Train Acc: 1.0\n",
            "Epoch 3602/10000\n",
            "Step 0: Train Loss: 0.00020662281895056367, Train Acc: 1.0\n",
            "Epoch 3603/10000\n",
            "Step 0: Train Loss: 0.0003727795265149325, Train Acc: 0.9998999834060669\n",
            "Epoch 3604/10000\n",
            "Step 0: Train Loss: 0.0002766286488622427, Train Acc: 0.9998999834060669\n",
            "Epoch 3605/10000\n",
            "Step 0: Train Loss: 0.00019047403475269675, Train Acc: 1.0\n",
            "Epoch 3606/10000\n",
            "Step 0: Train Loss: 0.00019452194101177156, Train Acc: 1.0\n",
            "Epoch 3607/10000\n",
            "Step 0: Train Loss: 0.00021516687411349267, Train Acc: 1.0\n",
            "Epoch 3608/10000\n",
            "Step 0: Train Loss: 0.0001564069971209392, Train Acc: 1.0\n",
            "Epoch 3609/10000\n",
            "Step 0: Train Loss: 0.00017136475071310997, Train Acc: 1.0\n",
            "Epoch 3610/10000\n",
            "Step 0: Train Loss: 0.00047213808284141123, Train Acc: 0.9998999834060669\n",
            "Epoch 3611/10000\n",
            "Step 0: Train Loss: 0.0004930066061206162, Train Acc: 0.9998999834060669\n",
            "Epoch 3612/10000\n",
            "Step 0: Train Loss: 0.0001457606558687985, Train Acc: 1.0\n",
            "Epoch 3613/10000\n",
            "Step 0: Train Loss: 0.0002545190218370408, Train Acc: 0.9998999834060669\n",
            "Epoch 3614/10000\n",
            "Step 0: Train Loss: 0.0005375981563702226, Train Acc: 0.9998999834060669\n",
            "Epoch 3615/10000\n",
            "Step 0: Train Loss: 0.00014694374112877995, Train Acc: 1.0\n",
            "Epoch 3616/10000\n",
            "Step 0: Train Loss: 0.0004604661953635514, Train Acc: 0.9998999834060669\n",
            "Epoch 3617/10000\n",
            "Step 0: Train Loss: 0.00014621170703321695, Train Acc: 1.0\n",
            "Epoch 3618/10000\n",
            "Step 0: Train Loss: 0.00039252237183973193, Train Acc: 0.9998999834060669\n",
            "Epoch 3619/10000\n",
            "Step 0: Train Loss: 0.00015778174565639347, Train Acc: 1.0\n",
            "Epoch 3620/10000\n",
            "Step 0: Train Loss: 0.0006678307545371354, Train Acc: 0.9998000264167786\n",
            "Epoch 3621/10000\n",
            "Step 0: Train Loss: 0.00047672734945081174, Train Acc: 0.9998999834060669\n",
            "Epoch 3622/10000\n",
            "Step 0: Train Loss: 0.0001557730429340154, Train Acc: 1.0\n",
            "Epoch 3623/10000\n",
            "Step 0: Train Loss: 0.00013091106666252017, Train Acc: 1.0\n",
            "Epoch 3624/10000\n",
            "Step 0: Train Loss: 0.00024971377570182085, Train Acc: 1.0\n",
            "Epoch 3625/10000\n",
            "Step 0: Train Loss: 0.0002301107597304508, Train Acc: 1.0\n",
            "Epoch 3626/10000\n",
            "Step 0: Train Loss: 0.00022268907923717052, Train Acc: 1.0\n",
            "Epoch 3627/10000\n",
            "Step 0: Train Loss: 0.0003687309508677572, Train Acc: 0.9998999834060669\n",
            "Epoch 3628/10000\n",
            "Step 0: Train Loss: 0.00036134166293777525, Train Acc: 0.9998999834060669\n",
            "Epoch 3629/10000\n",
            "Step 0: Train Loss: 0.00013725151075050235, Train Acc: 1.0\n",
            "Epoch 3630/10000\n",
            "Step 0: Train Loss: 0.00012240574869792908, Train Acc: 1.0\n",
            "Epoch 3631/10000\n",
            "Step 0: Train Loss: 0.00016170524759218097, Train Acc: 1.0\n",
            "Epoch 3632/10000\n",
            "Step 0: Train Loss: 0.00042771262815222144, Train Acc: 0.9998999834060669\n",
            "Epoch 3633/10000\n",
            "Step 0: Train Loss: 0.00014806410763412714, Train Acc: 1.0\n",
            "Epoch 3634/10000\n",
            "Step 0: Train Loss: 0.0004379938472993672, Train Acc: 0.9998999834060669\n",
            "Epoch 3635/10000\n",
            "Step 0: Train Loss: 0.000156692010932602, Train Acc: 1.0\n",
            "Epoch 3636/10000\n",
            "Step 0: Train Loss: 0.00021651125280186534, Train Acc: 1.0\n",
            "Epoch 3637/10000\n",
            "Step 0: Train Loss: 0.00012531096581369638, Train Acc: 1.0\n",
            "Epoch 3638/10000\n",
            "Step 0: Train Loss: 0.00015021288709249347, Train Acc: 1.0\n",
            "Epoch 3639/10000\n",
            "Step 0: Train Loss: 0.00015722139505669475, Train Acc: 1.0\n",
            "Epoch 3640/10000\n",
            "Step 0: Train Loss: 0.00015345009160228074, Train Acc: 1.0\n",
            "Epoch 3641/10000\n",
            "Step 0: Train Loss: 0.00015490541409235448, Train Acc: 1.0\n",
            "Epoch 3642/10000\n",
            "Step 0: Train Loss: 0.0004162781115155667, Train Acc: 0.9998999834060669\n",
            "Epoch 3643/10000\n",
            "Step 0: Train Loss: 0.0001331603853031993, Train Acc: 1.0\n",
            "Epoch 3644/10000\n",
            "Step 0: Train Loss: 0.0003487219219096005, Train Acc: 0.9998999834060669\n",
            "Epoch 3645/10000\n",
            "Step 0: Train Loss: 0.00016602929099462926, Train Acc: 1.0\n",
            "Epoch 3646/10000\n",
            "Step 0: Train Loss: 0.00012511170643847436, Train Acc: 1.0\n",
            "Epoch 3647/10000\n",
            "Step 0: Train Loss: 0.00017213536193594337, Train Acc: 1.0\n",
            "Epoch 3648/10000\n",
            "Step 0: Train Loss: 0.00011461388203315437, Train Acc: 1.0\n",
            "Epoch 3649/10000\n",
            "Step 0: Train Loss: 0.0002248474193038419, Train Acc: 1.0\n",
            "Epoch 3650/10000\n",
            "Step 0: Train Loss: 0.0004405185754876584, Train Acc: 0.9998999834060669\n",
            "Epoch 3651/10000\n",
            "Step 0: Train Loss: 0.00016016580048017204, Train Acc: 1.0\n",
            "Epoch 3652/10000\n",
            "Step 0: Train Loss: 0.00012952699034940451, Train Acc: 1.0\n",
            "Epoch 3653/10000\n",
            "Step 0: Train Loss: 0.0001338061993010342, Train Acc: 1.0\n",
            "Epoch 3654/10000\n",
            "Step 0: Train Loss: 0.00038497793138958514, Train Acc: 0.9998999834060669\n",
            "Epoch 3655/10000\n",
            "Step 0: Train Loss: 0.00042580332956276834, Train Acc: 0.9998999834060669\n",
            "Epoch 3656/10000\n",
            "Step 0: Train Loss: 0.0003154281585011631, Train Acc: 0.9998999834060669\n",
            "Epoch 3657/10000\n",
            "Step 0: Train Loss: 0.00045606307685375214, Train Acc: 0.9998999834060669\n",
            "Epoch 3658/10000\n",
            "Step 0: Train Loss: 0.0001957151835085824, Train Acc: 1.0\n",
            "Epoch 3659/10000\n",
            "Step 0: Train Loss: 0.00012844166485592723, Train Acc: 1.0\n",
            "Epoch 3660/10000\n",
            "Step 0: Train Loss: 0.00028112687868997455, Train Acc: 0.9998999834060669\n",
            "Epoch 3661/10000\n",
            "Step 0: Train Loss: 0.00012827294995076954, Train Acc: 1.0\n",
            "Epoch 3662/10000\n",
            "Step 0: Train Loss: 0.00019545845862012357, Train Acc: 1.0\n",
            "Epoch 3663/10000\n",
            "Step 0: Train Loss: 0.0001170825635199435, Train Acc: 1.0\n",
            "Epoch 3664/10000\n",
            "Step 0: Train Loss: 0.00011209402146050707, Train Acc: 1.0\n",
            "Epoch 3665/10000\n",
            "Step 0: Train Loss: 0.00012748125300277025, Train Acc: 1.0\n",
            "Epoch 3666/10000\n",
            "Step 0: Train Loss: 0.00010242731514154002, Train Acc: 1.0\n",
            "Epoch 3667/10000\n",
            "Step 0: Train Loss: 0.00010738224955275655, Train Acc: 1.0\n",
            "Epoch 3668/10000\n",
            "Step 0: Train Loss: 0.00012606880045495927, Train Acc: 1.0\n",
            "Epoch 3669/10000\n",
            "Step 0: Train Loss: 0.0001339713198831305, Train Acc: 1.0\n",
            "Epoch 3670/10000\n",
            "Step 0: Train Loss: 9.793275239644572e-05, Train Acc: 1.0\n",
            "Epoch 3671/10000\n",
            "Step 0: Train Loss: 0.00011088986502727494, Train Acc: 1.0\n",
            "Epoch 3672/10000\n",
            "Step 0: Train Loss: 0.00017284892965108156, Train Acc: 1.0\n",
            "Epoch 3673/10000\n",
            "Step 0: Train Loss: 0.00011099368566647172, Train Acc: 1.0\n",
            "Epoch 3674/10000\n",
            "Step 0: Train Loss: 0.00014588591875508428, Train Acc: 1.0\n",
            "Epoch 3675/10000\n",
            "Step 0: Train Loss: 0.00041475833859294653, Train Acc: 0.9998999834060669\n",
            "Epoch 3676/10000\n",
            "Step 0: Train Loss: 0.0001056716064340435, Train Acc: 1.0\n",
            "Epoch 3677/10000\n",
            "Step 0: Train Loss: 0.00011055166396545246, Train Acc: 1.0\n",
            "Epoch 3678/10000\n",
            "Step 0: Train Loss: 0.0001713826204650104, Train Acc: 1.0\n",
            "Epoch 3679/10000\n",
            "Step 0: Train Loss: 0.00011079393880208954, Train Acc: 1.0\n",
            "Epoch 3680/10000\n",
            "Step 0: Train Loss: 0.0001243823644472286, Train Acc: 1.0\n",
            "Epoch 3681/10000\n",
            "Step 0: Train Loss: 0.000355336262146011, Train Acc: 0.9998999834060669\n",
            "Epoch 3682/10000\n",
            "Step 0: Train Loss: 0.00017035631753969938, Train Acc: 1.0\n",
            "Epoch 3683/10000\n",
            "Step 0: Train Loss: 9.855980897555128e-05, Train Acc: 1.0\n",
            "Epoch 3684/10000\n",
            "Step 0: Train Loss: 0.00035183216095902026, Train Acc: 0.9998999834060669\n",
            "Epoch 3685/10000\n",
            "Step 0: Train Loss: 0.0001262369187315926, Train Acc: 1.0\n",
            "Epoch 3686/10000\n",
            "Step 0: Train Loss: 0.00010530708823353052, Train Acc: 1.0\n",
            "Epoch 3687/10000\n",
            "Step 0: Train Loss: 9.776656224858016e-05, Train Acc: 1.0\n",
            "Epoch 3688/10000\n",
            "Step 0: Train Loss: 0.0004509420250542462, Train Acc: 0.9998999834060669\n",
            "Epoch 3689/10000\n",
            "Step 0: Train Loss: 9.029341163113713e-05, Train Acc: 1.0\n",
            "Epoch 3690/10000\n",
            "Step 0: Train Loss: 0.00010444293002365157, Train Acc: 1.0\n",
            "Epoch 3691/10000\n",
            "Step 0: Train Loss: 0.0002060448459815234, Train Acc: 1.0\n",
            "Epoch 3692/10000\n",
            "Step 0: Train Loss: 0.00011782023648265749, Train Acc: 1.0\n",
            "Epoch 3693/10000\n",
            "Step 0: Train Loss: 0.00010202125122305006, Train Acc: 1.0\n",
            "Epoch 3694/10000\n",
            "Step 0: Train Loss: 0.00010457060125190765, Train Acc: 1.0\n",
            "Epoch 3695/10000\n",
            "Step 0: Train Loss: 9.862636215984821e-05, Train Acc: 1.0\n",
            "Epoch 3696/10000\n",
            "Step 0: Train Loss: 0.00011705541692208499, Train Acc: 1.0\n",
            "Epoch 3697/10000\n",
            "Step 0: Train Loss: 0.00016968557611107826, Train Acc: 1.0\n",
            "Epoch 3698/10000\n",
            "Step 0: Train Loss: 0.0002089859772240743, Train Acc: 1.0\n",
            "Epoch 3699/10000\n",
            "Step 0: Train Loss: 0.00010428801761008799, Train Acc: 1.0\n",
            "Epoch 3700/10000\n",
            "Step 0: Train Loss: 0.0001830926485126838, Train Acc: 1.0\n",
            "Epoch 3701/10000\n",
            "Step 0: Train Loss: 0.00010503408702788875, Train Acc: 1.0\n",
            "Epoch 3702/10000\n",
            "Step 0: Train Loss: 0.00011203923349967226, Train Acc: 1.0\n",
            "Epoch 3703/10000\n",
            "Step 0: Train Loss: 9.261530794901773e-05, Train Acc: 1.0\n",
            "Epoch 3704/10000\n",
            "Step 0: Train Loss: 0.00010994351760018617, Train Acc: 1.0\n",
            "Epoch 3705/10000\n",
            "Step 0: Train Loss: 8.32558362162672e-05, Train Acc: 1.0\n",
            "Epoch 3706/10000\n",
            "Step 0: Train Loss: 0.00013341073645278811, Train Acc: 1.0\n",
            "Epoch 3707/10000\n",
            "Step 0: Train Loss: 0.0001358754961984232, Train Acc: 1.0\n",
            "Epoch 3708/10000\n",
            "Step 0: Train Loss: 7.775065751047805e-05, Train Acc: 1.0\n",
            "Epoch 3709/10000\n",
            "Step 0: Train Loss: 0.00014919033856131136, Train Acc: 1.0\n",
            "Epoch 3710/10000\n",
            "Step 0: Train Loss: 8.802420052234083e-05, Train Acc: 1.0\n",
            "Epoch 3711/10000\n",
            "Step 0: Train Loss: 0.00037709373282268643, Train Acc: 0.9998999834060669\n",
            "Epoch 3712/10000\n",
            "Step 0: Train Loss: 0.00010099054634338245, Train Acc: 1.0\n",
            "Epoch 3713/10000\n",
            "Step 0: Train Loss: 8.428993896814063e-05, Train Acc: 1.0\n",
            "Epoch 3714/10000\n",
            "Step 0: Train Loss: 0.0002490268961992115, Train Acc: 0.9998999834060669\n",
            "Epoch 3715/10000\n",
            "Step 0: Train Loss: 9.123958443524316e-05, Train Acc: 1.0\n",
            "Epoch 3716/10000\n",
            "Step 0: Train Loss: 0.0003021290176548064, Train Acc: 0.9998999834060669\n",
            "Epoch 3717/10000\n",
            "Step 0: Train Loss: 9.838941332418472e-05, Train Acc: 1.0\n",
            "Epoch 3718/10000\n",
            "Step 0: Train Loss: 9.561208571540192e-05, Train Acc: 1.0\n",
            "Epoch 3719/10000\n",
            "Step 0: Train Loss: 8.63191598909907e-05, Train Acc: 1.0\n",
            "Epoch 3720/10000\n",
            "Step 0: Train Loss: 0.00023974869691301137, Train Acc: 0.9998999834060669\n",
            "Epoch 3721/10000\n",
            "Step 0: Train Loss: 0.00023752808920107782, Train Acc: 0.9998999834060669\n",
            "Epoch 3722/10000\n",
            "Step 0: Train Loss: 0.00010998251673299819, Train Acc: 1.0\n",
            "Epoch 3723/10000\n",
            "Step 0: Train Loss: 8.444024570053443e-05, Train Acc: 1.0\n",
            "Epoch 3724/10000\n",
            "Step 0: Train Loss: 9.967172081815079e-05, Train Acc: 1.0\n",
            "Epoch 3725/10000\n",
            "Step 0: Train Loss: 0.00017383777594659477, Train Acc: 1.0\n",
            "Epoch 3726/10000\n",
            "Step 0: Train Loss: 0.00013716633839067072, Train Acc: 1.0\n",
            "Epoch 3727/10000\n",
            "Step 0: Train Loss: 0.00012573215644806623, Train Acc: 1.0\n",
            "Epoch 3728/10000\n",
            "Step 0: Train Loss: 0.00015972772962413728, Train Acc: 1.0\n",
            "Epoch 3729/10000\n",
            "Step 0: Train Loss: 7.443744107149541e-05, Train Acc: 1.0\n",
            "Epoch 3730/10000\n",
            "Step 0: Train Loss: 8.164221071638167e-05, Train Acc: 1.0\n",
            "Epoch 3731/10000\n",
            "Step 0: Train Loss: 0.0001254492235602811, Train Acc: 1.0\n",
            "Epoch 3732/10000\n",
            "Step 0: Train Loss: 9.738544031279162e-05, Train Acc: 1.0\n",
            "Epoch 3733/10000\n",
            "Step 0: Train Loss: 0.00010819101589731872, Train Acc: 1.0\n",
            "Epoch 3734/10000\n",
            "Step 0: Train Loss: 7.336919952649623e-05, Train Acc: 1.0\n",
            "Epoch 3735/10000\n",
            "Step 0: Train Loss: 7.681190618313849e-05, Train Acc: 1.0\n",
            "Epoch 3736/10000\n",
            "Step 0: Train Loss: 7.826225191820413e-05, Train Acc: 1.0\n",
            "Epoch 3737/10000\n",
            "Step 0: Train Loss: 0.00010893611761275679, Train Acc: 1.0\n",
            "Epoch 3738/10000\n",
            "Step 0: Train Loss: 6.574357394129038e-05, Train Acc: 1.0\n",
            "Epoch 3739/10000\n",
            "Step 0: Train Loss: 8.468391752103344e-05, Train Acc: 1.0\n",
            "Epoch 3740/10000\n",
            "Step 0: Train Loss: 7.853632268961519e-05, Train Acc: 1.0\n",
            "Epoch 3741/10000\n",
            "Step 0: Train Loss: 8.676737343193963e-05, Train Acc: 1.0\n",
            "Epoch 3742/10000\n",
            "Step 0: Train Loss: 0.00010947713599307463, Train Acc: 1.0\n",
            "Epoch 3743/10000\n",
            "Step 0: Train Loss: 0.00010924072557827458, Train Acc: 1.0\n",
            "Epoch 3744/10000\n",
            "Step 0: Train Loss: 8.22230358608067e-05, Train Acc: 1.0\n",
            "Epoch 3745/10000\n",
            "Step 0: Train Loss: 8.407658606301993e-05, Train Acc: 1.0\n",
            "Epoch 3746/10000\n",
            "Step 0: Train Loss: 0.00011416140478104353, Train Acc: 1.0\n",
            "Epoch 3747/10000\n",
            "Step 0: Train Loss: 9.953510743798688e-05, Train Acc: 1.0\n",
            "Epoch 3748/10000\n",
            "Step 0: Train Loss: 7.276307587744668e-05, Train Acc: 1.0\n",
            "Epoch 3749/10000\n",
            "Step 0: Train Loss: 6.52569651720114e-05, Train Acc: 1.0\n",
            "Epoch 3750/10000\n",
            "Step 0: Train Loss: 6.082284380681813e-05, Train Acc: 1.0\n",
            "Epoch 3751/10000\n",
            "Step 0: Train Loss: 8.083558350335807e-05, Train Acc: 1.0\n",
            "Epoch 3752/10000\n",
            "Step 0: Train Loss: 0.00010908148396993056, Train Acc: 1.0\n",
            "Epoch 3753/10000\n",
            "Step 0: Train Loss: 8.349047129740939e-05, Train Acc: 1.0\n",
            "Epoch 3754/10000\n",
            "Step 0: Train Loss: 7.814428681740537e-05, Train Acc: 1.0\n",
            "Epoch 3755/10000\n",
            "Step 0: Train Loss: 9.070309897651896e-05, Train Acc: 1.0\n",
            "Epoch 3756/10000\n",
            "Step 0: Train Loss: 9.113667329074815e-05, Train Acc: 1.0\n",
            "Epoch 3757/10000\n",
            "Step 0: Train Loss: 6.334152567433193e-05, Train Acc: 1.0\n",
            "Epoch 3758/10000\n",
            "Step 0: Train Loss: 6.552025297423825e-05, Train Acc: 1.0\n",
            "Epoch 3759/10000\n",
            "Step 0: Train Loss: 8.014397462829947e-05, Train Acc: 1.0\n",
            "Epoch 3760/10000\n",
            "Step 0: Train Loss: 6.196409231051803e-05, Train Acc: 1.0\n",
            "Epoch 3761/10000\n",
            "Step 0: Train Loss: 0.00011022386752301827, Train Acc: 1.0\n",
            "Epoch 3762/10000\n",
            "Step 0: Train Loss: 8.869841985870153e-05, Train Acc: 1.0\n",
            "Epoch 3763/10000\n",
            "Step 0: Train Loss: 7.515026663895696e-05, Train Acc: 1.0\n",
            "Epoch 3764/10000\n",
            "Step 0: Train Loss: 6.625328387599438e-05, Train Acc: 1.0\n",
            "Epoch 3765/10000\n",
            "Step 0: Train Loss: 6.742258847225457e-05, Train Acc: 1.0\n",
            "Epoch 3766/10000\n",
            "Step 0: Train Loss: 6.070795643609017e-05, Train Acc: 1.0\n",
            "Epoch 3767/10000\n",
            "Step 0: Train Loss: 8.342570072272792e-05, Train Acc: 1.0\n",
            "Epoch 3768/10000\n",
            "Step 0: Train Loss: 6.877705891383812e-05, Train Acc: 1.0\n",
            "Epoch 3769/10000\n",
            "Step 0: Train Loss: 6.447224586736411e-05, Train Acc: 1.0\n",
            "Epoch 3770/10000\n",
            "Step 0: Train Loss: 6.116990698501468e-05, Train Acc: 1.0\n",
            "Epoch 3771/10000\n",
            "Step 0: Train Loss: 6.994315481279045e-05, Train Acc: 1.0\n",
            "Epoch 3772/10000\n",
            "Step 0: Train Loss: 5.670908649335615e-05, Train Acc: 1.0\n",
            "Epoch 3773/10000\n",
            "Step 0: Train Loss: 5.560100544244051e-05, Train Acc: 1.0\n",
            "Epoch 3774/10000\n",
            "Step 0: Train Loss: 5.126263931742869e-05, Train Acc: 1.0\n",
            "Epoch 3775/10000\n",
            "Step 0: Train Loss: 5.408657671068795e-05, Train Acc: 1.0\n",
            "Epoch 3776/10000\n",
            "Step 0: Train Loss: 5.361668445402756e-05, Train Acc: 1.0\n",
            "Epoch 3777/10000\n",
            "Step 0: Train Loss: 5.8539088058751076e-05, Train Acc: 1.0\n",
            "Epoch 3778/10000\n",
            "Step 0: Train Loss: 5.117490218253806e-05, Train Acc: 1.0\n",
            "Epoch 3779/10000\n",
            "Step 0: Train Loss: 6.386422319337726e-05, Train Acc: 1.0\n",
            "Epoch 3780/10000\n",
            "Step 0: Train Loss: 6.363091233652085e-05, Train Acc: 1.0\n",
            "Epoch 3781/10000\n",
            "Step 0: Train Loss: 5.4281110351439565e-05, Train Acc: 1.0\n",
            "Epoch 3782/10000\n",
            "Step 0: Train Loss: 6.201597716426477e-05, Train Acc: 1.0\n",
            "Epoch 3783/10000\n",
            "Step 0: Train Loss: 5.797008270747028e-05, Train Acc: 1.0\n",
            "Epoch 3784/10000\n",
            "Step 0: Train Loss: 5.8768378949025646e-05, Train Acc: 1.0\n",
            "Epoch 3785/10000\n",
            "Step 0: Train Loss: 8.136633550748229e-05, Train Acc: 1.0\n",
            "Epoch 3786/10000\n",
            "Step 0: Train Loss: 6.375504744937643e-05, Train Acc: 1.0\n",
            "Epoch 3787/10000\n",
            "Step 0: Train Loss: 8.984297164715827e-05, Train Acc: 1.0\n",
            "Epoch 3788/10000\n",
            "Step 0: Train Loss: 5.3808907978236675e-05, Train Acc: 1.0\n",
            "Epoch 3789/10000\n",
            "Step 0: Train Loss: 5.464424975798465e-05, Train Acc: 1.0\n",
            "Epoch 3790/10000\n",
            "Step 0: Train Loss: 6.225680408533663e-05, Train Acc: 1.0\n",
            "Epoch 3791/10000\n",
            "Step 0: Train Loss: 5.23933231306728e-05, Train Acc: 1.0\n",
            "Epoch 3792/10000\n",
            "Step 0: Train Loss: 5.347546175471507e-05, Train Acc: 1.0\n",
            "Epoch 3793/10000\n",
            "Step 0: Train Loss: 7.836998702259734e-05, Train Acc: 1.0\n",
            "Epoch 3794/10000\n",
            "Step 0: Train Loss: 5.7114601077046245e-05, Train Acc: 1.0\n",
            "Epoch 3795/10000\n",
            "Step 0: Train Loss: 6.048544673831202e-05, Train Acc: 1.0\n",
            "Epoch 3796/10000\n",
            "Step 0: Train Loss: 5.493127173394896e-05, Train Acc: 1.0\n",
            "Epoch 3797/10000\n",
            "Step 0: Train Loss: 5.48210518900305e-05, Train Acc: 1.0\n",
            "Epoch 3798/10000\n",
            "Step 0: Train Loss: 5.112640428706072e-05, Train Acc: 1.0\n",
            "Epoch 3799/10000\n",
            "Step 0: Train Loss: 7.834385905880481e-05, Train Acc: 1.0\n",
            "Epoch 3800/10000\n",
            "Step 0: Train Loss: 4.7688590711914e-05, Train Acc: 1.0\n",
            "Epoch 3801/10000\n",
            "Step 0: Train Loss: 4.693704977398738e-05, Train Acc: 1.0\n",
            "Epoch 3802/10000\n",
            "Step 0: Train Loss: 5.61008564545773e-05, Train Acc: 1.0\n",
            "Epoch 3803/10000\n",
            "Step 0: Train Loss: 5.104429874336347e-05, Train Acc: 1.0\n",
            "Epoch 3804/10000\n",
            "Step 0: Train Loss: 7.256444951053709e-05, Train Acc: 1.0\n",
            "Epoch 3805/10000\n",
            "Step 0: Train Loss: 5.695583968190476e-05, Train Acc: 1.0\n",
            "Epoch 3806/10000\n",
            "Step 0: Train Loss: 5.2787057938985527e-05, Train Acc: 1.0\n",
            "Epoch 3807/10000\n",
            "Step 0: Train Loss: 4.3844283936778083e-05, Train Acc: 1.0\n",
            "Epoch 3808/10000\n",
            "Step 0: Train Loss: 4.564475602819584e-05, Train Acc: 1.0\n",
            "Epoch 3809/10000\n",
            "Step 0: Train Loss: 5.305609738570638e-05, Train Acc: 1.0\n",
            "Epoch 3810/10000\n",
            "Step 0: Train Loss: 6.689672591164708e-05, Train Acc: 1.0\n",
            "Epoch 3811/10000\n",
            "Step 0: Train Loss: 4.924486347590573e-05, Train Acc: 1.0\n",
            "Epoch 3812/10000\n",
            "Step 0: Train Loss: 6.570421101059765e-05, Train Acc: 1.0\n",
            "Epoch 3813/10000\n",
            "Step 0: Train Loss: 5.616753696813248e-05, Train Acc: 1.0\n",
            "Epoch 3814/10000\n",
            "Step 0: Train Loss: 8.537748362869024e-05, Train Acc: 1.0\n",
            "Epoch 3815/10000\n",
            "Step 0: Train Loss: 5.060316470917314e-05, Train Acc: 1.0\n",
            "Epoch 3816/10000\n",
            "Step 0: Train Loss: 6.229223072296008e-05, Train Acc: 1.0\n",
            "Epoch 3817/10000\n",
            "Step 0: Train Loss: 5.050937397754751e-05, Train Acc: 1.0\n",
            "Epoch 3818/10000\n",
            "Step 0: Train Loss: 4.5657223381567746e-05, Train Acc: 1.0\n",
            "Epoch 3819/10000\n",
            "Step 0: Train Loss: 4.466944301384501e-05, Train Acc: 1.0\n",
            "Epoch 3820/10000\n",
            "Step 0: Train Loss: 3.9850419852882624e-05, Train Acc: 1.0\n",
            "Epoch 3821/10000\n",
            "Step 0: Train Loss: 4.611091935657896e-05, Train Acc: 1.0\n",
            "Epoch 3822/10000\n",
            "Step 0: Train Loss: 5.143489397596568e-05, Train Acc: 1.0\n",
            "Epoch 3823/10000\n",
            "Step 0: Train Loss: 4.539509245660156e-05, Train Acc: 1.0\n",
            "Epoch 3824/10000\n",
            "Step 0: Train Loss: 6.643110828008503e-05, Train Acc: 1.0\n",
            "Epoch 3825/10000\n",
            "Step 0: Train Loss: 5.102601062390022e-05, Train Acc: 1.0\n",
            "Epoch 3826/10000\n",
            "Step 0: Train Loss: 4.0671508031664416e-05, Train Acc: 1.0\n",
            "Epoch 3827/10000\n",
            "Step 0: Train Loss: 4.1389012039871886e-05, Train Acc: 1.0\n",
            "Epoch 3828/10000\n",
            "Step 0: Train Loss: 4.988942600903101e-05, Train Acc: 1.0\n",
            "Epoch 3829/10000\n",
            "Step 0: Train Loss: 4.0916998841566965e-05, Train Acc: 1.0\n",
            "Epoch 3830/10000\n",
            "Step 0: Train Loss: 4.59994662378449e-05, Train Acc: 1.0\n",
            "Epoch 3831/10000\n",
            "Step 0: Train Loss: 4.24895879405085e-05, Train Acc: 1.0\n",
            "Epoch 3832/10000\n",
            "Step 0: Train Loss: 3.927664874936454e-05, Train Acc: 1.0\n",
            "Epoch 3833/10000\n",
            "Step 0: Train Loss: 4.4623018766287714e-05, Train Acc: 1.0\n",
            "Epoch 3834/10000\n",
            "Step 0: Train Loss: 4.620631443685852e-05, Train Acc: 1.0\n",
            "Epoch 3835/10000\n",
            "Step 0: Train Loss: 4.556408021016978e-05, Train Acc: 1.0\n",
            "Epoch 3836/10000\n",
            "Step 0: Train Loss: 4.162287586950697e-05, Train Acc: 1.0\n",
            "Epoch 3837/10000\n",
            "Step 0: Train Loss: 4.30122745456174e-05, Train Acc: 1.0\n",
            "Epoch 3838/10000\n",
            "Step 0: Train Loss: 4.399298632051796e-05, Train Acc: 1.0\n",
            "Epoch 3839/10000\n",
            "Step 0: Train Loss: 6.0463498812168837e-05, Train Acc: 1.0\n",
            "Epoch 3840/10000\n",
            "Step 0: Train Loss: 4.440174961928278e-05, Train Acc: 1.0\n",
            "Epoch 3841/10000\n",
            "Step 0: Train Loss: 5.102336945128627e-05, Train Acc: 1.0\n",
            "Epoch 3842/10000\n",
            "Step 0: Train Loss: 3.85851672035642e-05, Train Acc: 1.0\n",
            "Epoch 3843/10000\n",
            "Step 0: Train Loss: 4.022893335786648e-05, Train Acc: 1.0\n",
            "Epoch 3844/10000\n",
            "Step 0: Train Loss: 6.913235847605392e-05, Train Acc: 1.0\n",
            "Epoch 3845/10000\n",
            "Step 0: Train Loss: 4.216769229969941e-05, Train Acc: 1.0\n",
            "Epoch 3846/10000\n",
            "Step 0: Train Loss: 4.5609991502715275e-05, Train Acc: 1.0\n",
            "Epoch 3847/10000\n",
            "Step 0: Train Loss: 7.333731628023088e-05, Train Acc: 1.0\n",
            "Epoch 3848/10000\n",
            "Step 0: Train Loss: 3.5489840229274705e-05, Train Acc: 1.0\n",
            "Epoch 3849/10000\n",
            "Step 0: Train Loss: 6.737323565175757e-05, Train Acc: 1.0\n",
            "Epoch 3850/10000\n",
            "Step 0: Train Loss: 4.453587098396383e-05, Train Acc: 1.0\n",
            "Epoch 3851/10000\n",
            "Step 0: Train Loss: 3.2233583624474704e-05, Train Acc: 1.0\n",
            "Epoch 3852/10000\n",
            "Step 0: Train Loss: 4.373280535219237e-05, Train Acc: 1.0\n",
            "Epoch 3853/10000\n",
            "Step 0: Train Loss: 3.447090057306923e-05, Train Acc: 1.0\n",
            "Epoch 3854/10000\n",
            "Step 0: Train Loss: 5.514215081348084e-05, Train Acc: 1.0\n",
            "Epoch 3855/10000\n",
            "Step 0: Train Loss: 3.9791662857169285e-05, Train Acc: 1.0\n",
            "Epoch 3856/10000\n",
            "Step 0: Train Loss: 4.5124928874429315e-05, Train Acc: 1.0\n",
            "Epoch 3857/10000\n",
            "Step 0: Train Loss: 3.950122481910512e-05, Train Acc: 1.0\n",
            "Epoch 3858/10000\n",
            "Step 0: Train Loss: 5.445041824714281e-05, Train Acc: 1.0\n",
            "Epoch 3859/10000\n",
            "Step 0: Train Loss: 3.2892745366552845e-05, Train Acc: 1.0\n",
            "Epoch 3860/10000\n",
            "Step 0: Train Loss: 3.601661228458397e-05, Train Acc: 1.0\n",
            "Epoch 3861/10000\n",
            "Step 0: Train Loss: 3.772916898014955e-05, Train Acc: 1.0\n",
            "Epoch 3862/10000\n",
            "Step 0: Train Loss: 4.6404336899286136e-05, Train Acc: 1.0\n",
            "Epoch 3863/10000\n",
            "Step 0: Train Loss: 3.104973438894376e-05, Train Acc: 1.0\n",
            "Epoch 3864/10000\n",
            "Step 0: Train Loss: 3.7335696106310934e-05, Train Acc: 1.0\n",
            "Epoch 3865/10000\n",
            "Step 0: Train Loss: 5.148288619238883e-05, Train Acc: 1.0\n",
            "Epoch 3866/10000\n",
            "Step 0: Train Loss: 3.6130189982941374e-05, Train Acc: 1.0\n",
            "Epoch 3867/10000\n",
            "Step 0: Train Loss: 3.6871973861707374e-05, Train Acc: 1.0\n",
            "Epoch 3868/10000\n",
            "Step 0: Train Loss: 3.773270145757124e-05, Train Acc: 1.0\n",
            "Epoch 3869/10000\n",
            "Step 0: Train Loss: 4.905438981950283e-05, Train Acc: 1.0\n",
            "Epoch 3870/10000\n",
            "Step 0: Train Loss: 3.787978857872076e-05, Train Acc: 1.0\n",
            "Epoch 3871/10000\n",
            "Step 0: Train Loss: 3.4243406844325364e-05, Train Acc: 1.0\n",
            "Epoch 3872/10000\n",
            "Step 0: Train Loss: 4.937018093187362e-05, Train Acc: 1.0\n",
            "Epoch 3873/10000\n",
            "Step 0: Train Loss: 3.068883961532265e-05, Train Acc: 1.0\n",
            "Epoch 3874/10000\n",
            "Step 0: Train Loss: 2.994703572767321e-05, Train Acc: 1.0\n",
            "Epoch 3875/10000\n",
            "Step 0: Train Loss: 2.9541457479353994e-05, Train Acc: 1.0\n",
            "Epoch 3876/10000\n",
            "Step 0: Train Loss: 2.7196752853342332e-05, Train Acc: 1.0\n",
            "Epoch 3877/10000\n",
            "Step 0: Train Loss: 3.2291773095494136e-05, Train Acc: 1.0\n",
            "Epoch 3878/10000\n",
            "Step 0: Train Loss: 4.5588851207867265e-05, Train Acc: 1.0\n",
            "Epoch 3879/10000\n",
            "Step 0: Train Loss: 3.532991831889376e-05, Train Acc: 1.0\n",
            "Epoch 3880/10000\n",
            "Step 0: Train Loss: 4.652659481507726e-05, Train Acc: 1.0\n",
            "Epoch 3881/10000\n",
            "Step 0: Train Loss: 3.235496114939451e-05, Train Acc: 1.0\n",
            "Epoch 3882/10000\n",
            "Step 0: Train Loss: 3.464177279965952e-05, Train Acc: 1.0\n",
            "Epoch 3883/10000\n",
            "Step 0: Train Loss: 4.004467336926609e-05, Train Acc: 1.0\n",
            "Epoch 3884/10000\n",
            "Step 0: Train Loss: 3.267221472924575e-05, Train Acc: 1.0\n",
            "Epoch 3885/10000\n",
            "Step 0: Train Loss: 3.098188972217031e-05, Train Acc: 1.0\n",
            "Epoch 3886/10000\n",
            "Step 0: Train Loss: 3.309483508928679e-05, Train Acc: 1.0\n",
            "Epoch 3887/10000\n",
            "Step 0: Train Loss: 3.738283703569323e-05, Train Acc: 1.0\n",
            "Epoch 3888/10000\n",
            "Step 0: Train Loss: 4.5435292122419924e-05, Train Acc: 1.0\n",
            "Epoch 3889/10000\n",
            "Step 0: Train Loss: 3.2092830224428326e-05, Train Acc: 1.0\n",
            "Epoch 3890/10000\n",
            "Step 0: Train Loss: 3.5591561754699796e-05, Train Acc: 1.0\n",
            "Epoch 3891/10000\n",
            "Step 0: Train Loss: 3.6360943340696394e-05, Train Acc: 1.0\n",
            "Epoch 3892/10000\n",
            "Step 0: Train Loss: 4.4965450797462836e-05, Train Acc: 1.0\n",
            "Epoch 3893/10000\n",
            "Step 0: Train Loss: 3.753192868316546e-05, Train Acc: 1.0\n",
            "Epoch 3894/10000\n",
            "Step 0: Train Loss: 4.361178434919566e-05, Train Acc: 1.0\n",
            "Epoch 3895/10000\n",
            "Step 0: Train Loss: 2.7788395527750254e-05, Train Acc: 1.0\n",
            "Epoch 3896/10000\n",
            "Step 0: Train Loss: 3.9344904507743195e-05, Train Acc: 1.0\n",
            "Epoch 3897/10000\n",
            "Step 0: Train Loss: 3.7798188714077696e-05, Train Acc: 1.0\n",
            "Epoch 3898/10000\n",
            "Step 0: Train Loss: 3.914780609193258e-05, Train Acc: 1.0\n",
            "Epoch 3899/10000\n",
            "Step 0: Train Loss: 2.8059264877811074e-05, Train Acc: 1.0\n",
            "Epoch 3900/10000\n",
            "Step 0: Train Loss: 3.7298719689715654e-05, Train Acc: 1.0\n",
            "Epoch 3901/10000\n",
            "Step 0: Train Loss: 3.2717216527089477e-05, Train Acc: 1.0\n",
            "Epoch 3902/10000\n",
            "Step 0: Train Loss: 3.679253495647572e-05, Train Acc: 1.0\n",
            "Epoch 3903/10000\n",
            "Step 0: Train Loss: 2.9453443858074024e-05, Train Acc: 1.0\n",
            "Epoch 3904/10000\n",
            "Step 0: Train Loss: 3.0366942155524157e-05, Train Acc: 1.0\n",
            "Epoch 3905/10000\n",
            "Step 0: Train Loss: 3.5649143683258444e-05, Train Acc: 1.0\n",
            "Epoch 3906/10000\n",
            "Step 0: Train Loss: 3.0226095987018198e-05, Train Acc: 1.0\n",
            "Epoch 3907/10000\n",
            "Step 0: Train Loss: 2.6202649678452872e-05, Train Acc: 1.0\n",
            "Epoch 3908/10000\n",
            "Step 0: Train Loss: 3.055310298805125e-05, Train Acc: 1.0\n",
            "Epoch 3909/10000\n",
            "Step 0: Train Loss: 2.9046816052868962e-05, Train Acc: 1.0\n",
            "Epoch 3910/10000\n",
            "Step 0: Train Loss: 3.302496770629659e-05, Train Acc: 1.0\n",
            "Epoch 3911/10000\n",
            "Step 0: Train Loss: 3.098512752330862e-05, Train Acc: 1.0\n",
            "Epoch 3912/10000\n",
            "Step 0: Train Loss: 2.493720421625767e-05, Train Acc: 1.0\n",
            "Epoch 3913/10000\n",
            "Step 0: Train Loss: 3.088544690399431e-05, Train Acc: 1.0\n",
            "Epoch 3914/10000\n",
            "Step 0: Train Loss: 3.8187496102182195e-05, Train Acc: 1.0\n",
            "Epoch 3915/10000\n",
            "Step 0: Train Loss: 3.33936586685013e-05, Train Acc: 1.0\n",
            "Epoch 3916/10000\n",
            "Step 0: Train Loss: 3.039110561076086e-05, Train Acc: 1.0\n",
            "Epoch 3917/10000\n",
            "Step 0: Train Loss: 3.064666816499084e-05, Train Acc: 1.0\n",
            "Epoch 3918/10000\n",
            "Step 0: Train Loss: 2.672938171599526e-05, Train Acc: 1.0\n",
            "Epoch 3919/10000\n",
            "Step 0: Train Loss: 2.558899541327264e-05, Train Acc: 1.0\n",
            "Epoch 3920/10000\n",
            "Step 0: Train Loss: 2.6907751816906966e-05, Train Acc: 1.0\n",
            "Epoch 3921/10000\n",
            "Step 0: Train Loss: 2.664580097189173e-05, Train Acc: 1.0\n",
            "Epoch 3922/10000\n",
            "Step 0: Train Loss: 2.5529345293762162e-05, Train Acc: 1.0\n",
            "Epoch 3923/10000\n",
            "Step 0: Train Loss: 2.6804407752933912e-05, Train Acc: 1.0\n",
            "Epoch 3924/10000\n",
            "Step 0: Train Loss: 2.6526731744525023e-05, Train Acc: 1.0\n",
            "Epoch 3925/10000\n",
            "Step 0: Train Loss: 2.5980207283282652e-05, Train Acc: 1.0\n",
            "Epoch 3926/10000\n",
            "Step 0: Train Loss: 2.8420836315490305e-05, Train Acc: 1.0\n",
            "Epoch 3927/10000\n",
            "Step 0: Train Loss: 2.7760326702264138e-05, Train Acc: 1.0\n",
            "Epoch 3928/10000\n",
            "Step 0: Train Loss: 2.8534715966088697e-05, Train Acc: 1.0\n",
            "Epoch 3929/10000\n",
            "Step 0: Train Loss: 2.27314649237087e-05, Train Acc: 1.0\n",
            "Epoch 3930/10000\n",
            "Step 0: Train Loss: 2.60646193055436e-05, Train Acc: 1.0\n",
            "Epoch 3931/10000\n",
            "Step 0: Train Loss: 2.5162538804579526e-05, Train Acc: 1.0\n",
            "Epoch 3932/10000\n",
            "Step 0: Train Loss: 2.8255159122636542e-05, Train Acc: 1.0\n",
            "Epoch 3933/10000\n",
            "Step 0: Train Loss: 2.3927241272758693e-05, Train Acc: 1.0\n",
            "Epoch 3934/10000\n",
            "Step 0: Train Loss: 2.533345832489431e-05, Train Acc: 1.0\n",
            "Epoch 3935/10000\n",
            "Step 0: Train Loss: 3.1396455597132444e-05, Train Acc: 1.0\n",
            "Epoch 3936/10000\n",
            "Step 0: Train Loss: 2.857007348211482e-05, Train Acc: 1.0\n",
            "Epoch 3937/10000\n",
            "Step 0: Train Loss: 3.1698786187916994e-05, Train Acc: 1.0\n",
            "Epoch 3938/10000\n",
            "Step 0: Train Loss: 2.601143751235213e-05, Train Acc: 1.0\n",
            "Epoch 3939/10000\n",
            "Step 0: Train Loss: 3.458778519416228e-05, Train Acc: 1.0\n",
            "Epoch 3940/10000\n",
            "Step 0: Train Loss: 2.195809611293953e-05, Train Acc: 1.0\n",
            "Epoch 3941/10000\n",
            "Step 0: Train Loss: 2.2116120817372575e-05, Train Acc: 1.0\n",
            "Epoch 3942/10000\n",
            "Step 0: Train Loss: 2.4627075617900118e-05, Train Acc: 1.0\n",
            "Epoch 3943/10000\n",
            "Step 0: Train Loss: 3.461724554654211e-05, Train Acc: 1.0\n",
            "Epoch 3944/10000\n",
            "Step 0: Train Loss: 2.2514679585583508e-05, Train Acc: 1.0\n",
            "Epoch 3945/10000\n",
            "Step 0: Train Loss: 2.613190736155957e-05, Train Acc: 1.0\n",
            "Epoch 3946/10000\n",
            "Step 0: Train Loss: 3.155709055135958e-05, Train Acc: 1.0\n",
            "Epoch 3947/10000\n",
            "Step 0: Train Loss: 3.202186053385958e-05, Train Acc: 1.0\n",
            "Epoch 3948/10000\n",
            "Step 0: Train Loss: 2.0267083527869545e-05, Train Acc: 1.0\n",
            "Epoch 3949/10000\n",
            "Step 0: Train Loss: 2.3204449462355115e-05, Train Acc: 1.0\n",
            "Epoch 3950/10000\n",
            "Step 0: Train Loss: 2.5810850274865516e-05, Train Acc: 1.0\n",
            "Epoch 3951/10000\n",
            "Step 0: Train Loss: 2.283656613144558e-05, Train Acc: 1.0\n",
            "Epoch 3952/10000\n",
            "Step 0: Train Loss: 2.3209826395031996e-05, Train Acc: 1.0\n",
            "Epoch 3953/10000\n",
            "Step 0: Train Loss: 2.2853217160445638e-05, Train Acc: 1.0\n",
            "Epoch 3954/10000\n",
            "Step 0: Train Loss: 2.02836818061769e-05, Train Acc: 1.0\n",
            "Epoch 3955/10000\n",
            "Step 0: Train Loss: 3.4389322536299005e-05, Train Acc: 1.0\n",
            "Epoch 3956/10000\n",
            "Step 0: Train Loss: 2.72729157586582e-05, Train Acc: 1.0\n",
            "Epoch 3957/10000\n",
            "Step 0: Train Loss: 2.7552008759812452e-05, Train Acc: 1.0\n",
            "Epoch 3958/10000\n",
            "Step 0: Train Loss: 2.5883264243020676e-05, Train Acc: 1.0\n",
            "Epoch 3959/10000\n",
            "Step 0: Train Loss: 2.5453577109146863e-05, Train Acc: 1.0\n",
            "Epoch 3960/10000\n",
            "Step 0: Train Loss: 2.3190090360003524e-05, Train Acc: 1.0\n",
            "Epoch 3961/10000\n",
            "Step 0: Train Loss: 3.171034040860832e-05, Train Acc: 1.0\n",
            "Epoch 3962/10000\n",
            "Step 0: Train Loss: 2.208141449955292e-05, Train Acc: 1.0\n",
            "Epoch 3963/10000\n",
            "Step 0: Train Loss: 2.9675593395950273e-05, Train Acc: 1.0\n",
            "Epoch 3964/10000\n",
            "Step 0: Train Loss: 2.3774275177856907e-05, Train Acc: 1.0\n",
            "Epoch 3965/10000\n",
            "Step 0: Train Loss: 2.5796374757192098e-05, Train Acc: 1.0\n",
            "Epoch 3966/10000\n",
            "Step 0: Train Loss: 2.336863508389797e-05, Train Acc: 1.0\n",
            "Epoch 3967/10000\n",
            "Step 0: Train Loss: 2.6793026336235926e-05, Train Acc: 1.0\n",
            "Epoch 3968/10000\n",
            "Step 0: Train Loss: 2.9699443985009566e-05, Train Acc: 1.0\n",
            "Epoch 3969/10000\n",
            "Step 0: Train Loss: 2.3791777493897825e-05, Train Acc: 1.0\n",
            "Epoch 3970/10000\n",
            "Step 0: Train Loss: 2.325480272702407e-05, Train Acc: 1.0\n",
            "Epoch 3971/10000\n",
            "Step 0: Train Loss: 2.2199716113391332e-05, Train Acc: 1.0\n",
            "Epoch 3972/10000\n",
            "Step 0: Train Loss: 2.9682680178666487e-05, Train Acc: 1.0\n",
            "Epoch 3973/10000\n",
            "Step 0: Train Loss: 3.018815186806023e-05, Train Acc: 1.0\n",
            "Epoch 3974/10000\n",
            "Step 0: Train Loss: 2.28628414333798e-05, Train Acc: 1.0\n",
            "Epoch 3975/10000\n",
            "Step 0: Train Loss: 2.0610996216419153e-05, Train Acc: 1.0\n",
            "Epoch 3976/10000\n",
            "Step 0: Train Loss: 2.9960638130432926e-05, Train Acc: 1.0\n",
            "Epoch 3977/10000\n",
            "Step 0: Train Loss: 2.7425512598711066e-05, Train Acc: 1.0\n",
            "Epoch 3978/10000\n",
            "Step 0: Train Loss: 2.538456283218693e-05, Train Acc: 1.0\n",
            "Epoch 3979/10000\n",
            "Step 0: Train Loss: 1.9425209757173434e-05, Train Acc: 1.0\n",
            "Epoch 3980/10000\n",
            "Step 0: Train Loss: 2.4388087695115246e-05, Train Acc: 1.0\n",
            "Epoch 3981/10000\n",
            "Step 0: Train Loss: 2.3098875317373313e-05, Train Acc: 1.0\n",
            "Epoch 3982/10000\n",
            "Step 0: Train Loss: 2.822860551532358e-05, Train Acc: 1.0\n",
            "Epoch 3983/10000\n",
            "Step 0: Train Loss: 2.3992737624212168e-05, Train Acc: 1.0\n",
            "Epoch 3984/10000\n",
            "Step 0: Train Loss: 1.9594039258663543e-05, Train Acc: 1.0\n",
            "Epoch 3985/10000\n",
            "Step 0: Train Loss: 2.028456401603762e-05, Train Acc: 1.0\n",
            "Epoch 3986/10000\n",
            "Step 0: Train Loss: 1.9905070075765252e-05, Train Acc: 1.0\n",
            "Epoch 3987/10000\n",
            "Step 0: Train Loss: 2.113428490702063e-05, Train Acc: 1.0\n",
            "Epoch 3988/10000\n",
            "Step 0: Train Loss: 1.897605397971347e-05, Train Acc: 1.0\n",
            "Epoch 3989/10000\n",
            "Step 0: Train Loss: 2.1317368009476922e-05, Train Acc: 1.0\n",
            "Epoch 3990/10000\n",
            "Step 0: Train Loss: 2.096692696795799e-05, Train Acc: 1.0\n",
            "Epoch 3991/10000\n",
            "Step 0: Train Loss: 2.195172601204831e-05, Train Acc: 1.0\n",
            "Epoch 3992/10000\n",
            "Step 0: Train Loss: 2.099368612107355e-05, Train Acc: 1.0\n",
            "Epoch 3993/10000\n",
            "Step 0: Train Loss: 1.8632039427757263e-05, Train Acc: 1.0\n",
            "Epoch 3994/10000\n",
            "Step 0: Train Loss: 2.421285171294585e-05, Train Acc: 1.0\n",
            "Epoch 3995/10000\n",
            "Step 0: Train Loss: 2.173242137359921e-05, Train Acc: 1.0\n",
            "Epoch 3996/10000\n",
            "Step 0: Train Loss: 1.8452268705004826e-05, Train Acc: 1.0\n",
            "Epoch 3997/10000\n",
            "Step 0: Train Loss: 1.9016022633877583e-05, Train Acc: 1.0\n",
            "Epoch 3998/10000\n",
            "Step 0: Train Loss: 1.7541568013257347e-05, Train Acc: 1.0\n",
            "Epoch 3999/10000\n",
            "Step 0: Train Loss: 1.687904841674026e-05, Train Acc: 1.0\n",
            "Epoch 4000/10000\n",
            "Step 0: Train Loss: 1.9784072719630785e-05, Train Acc: 1.0\n",
            "Epoch 4001/10000\n",
            "Step 0: Train Loss: 2.238655179098714e-05, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 4000 1024 0.9822005556489248\n",
            "Epoch index and hidden dimension and ratio: 4000 20 0.8722498405854047\n",
            "Epoch index and hidden dimension and ratio: 4000 20 1.2240545011572355\n",
            "Epoch index and hidden dimension and ratio: 4000 20 2.768520127899494\n",
            "MI(X;T): [10.57416956725694, 7.639226922270236, 5.375134810322756, 2.6239776002967057], MI(Y;T): [2.6300412189387123, 3.1802930051524956, 3.195982338371244, 2.9682687307593274]\n",
            "Epoch index and hidden dimension and ratio: 4000 1024 0.9822040065196493\n",
            "Epoch index and hidden dimension and ratio: 4000 20 0.8722828191747065\n",
            "Epoch index and hidden dimension and ratio: 4000 20 1.2241528947391809\n",
            "Epoch index and hidden dimension and ratio: 4000 20 2.7688073761264644\n",
            "MI(X;T): [10.57436725219695, 7.639138789545889, 5.374717893838201, 2.62424190445434], MI(Y;T): [2.6302250697633394, 3.180259812869641, 3.1960474770856946, 2.9686211400921207]\n",
            "Epoch index and hidden dimension and ratio: 4000 1024 0.9822107614155354\n",
            "Epoch index and hidden dimension and ratio: 4000 20 0.8723051122671457\n",
            "Epoch index and hidden dimension and ratio: 4000 20 1.2242115500099664\n",
            "Epoch index and hidden dimension and ratio: 4000 20 2.7688863447111984\n",
            "MI(X;T): [10.574218981562431, 7.639092139891042, 5.3746766163303334, 2.624654004778715], MI(Y;T): [2.6302220212941507, 3.180346990416864, 3.1961182087671656, 2.9686839185924345]\n",
            "Epoch index and hidden dimension and ratio: 4000 1024 0.9822194253037372\n",
            "Epoch index and hidden dimension and ratio: 4000 20 0.8723342397382888\n",
            "Epoch index and hidden dimension and ratio: 4000 20 1.2242840438619544\n",
            "Epoch index and hidden dimension and ratio: 4000 20 2.7689926232648197\n",
            "MI(X;T): [10.574310371793052, 7.63902508531643, 5.375070821232431, 2.6243325092834136], MI(Y;T): [2.630213189093789, 3.180232690797613, 3.196360676903237, 2.9692446618063046]\n",
            "Epoch index and hidden dimension and ratio: 4000 1024 0.9822264004679674\n",
            "Epoch index and hidden dimension and ratio: 4000 20 0.8723517596138555\n",
            "Epoch index and hidden dimension and ratio: 4000 20 1.224309816632451\n",
            "Epoch index and hidden dimension and ratio: 4000 20 2.769001836266372\n",
            "MI(X;T): [10.574541330468243, 7.639369890032061, 5.374596630118707, 2.62410402852465], MI(Y;T): [2.63007437099622, 3.180008137780005, 3.1962668572788777, 2.9692175369409846]\n",
            "Epoch index and hidden dimension and ratio: 4000 1024 0.9822330085182909\n",
            "Epoch index and hidden dimension and ratio: 4000 20 0.8723468779147812\n",
            "Epoch index and hidden dimension and ratio: 4000 20 1.2242566206184704\n",
            "Epoch index and hidden dimension and ratio: 4000 20 2.7687685499056367\n",
            "MI(X;T): [10.574883567222997, 7.639150064908813, 5.374742999918023, 2.6234528189018738], MI(Y;T): [2.6299569076915814, 3.179785089121285, 3.196351622691492, 2.9691735737628915]\n",
            "Epoch 4002/10000\n",
            "Step 0: Train Loss: 2.0313376808189787e-05, Train Acc: 1.0\n",
            "Epoch 4003/10000\n",
            "Step 0: Train Loss: 2.3941242034197785e-05, Train Acc: 1.0\n",
            "Epoch 4004/10000\n",
            "Step 0: Train Loss: 2.5106295652221888e-05, Train Acc: 1.0\n",
            "Epoch 4005/10000\n",
            "Step 0: Train Loss: 1.6318186681019142e-05, Train Acc: 1.0\n",
            "Epoch 4006/10000\n",
            "Step 0: Train Loss: 2.272024539706763e-05, Train Acc: 1.0\n",
            "Epoch 4007/10000\n",
            "Step 0: Train Loss: 1.54075569298584e-05, Train Acc: 1.0\n",
            "Epoch 4008/10000\n",
            "Step 0: Train Loss: 1.8041740986518562e-05, Train Acc: 1.0\n",
            "Epoch 4009/10000\n",
            "Step 0: Train Loss: 2.1550411474891007e-05, Train Acc: 1.0\n",
            "Epoch 4010/10000\n",
            "Step 0: Train Loss: 2.1128917069290765e-05, Train Acc: 1.0\n",
            "Epoch 4011/10000\n",
            "Step 0: Train Loss: 1.9993951354990713e-05, Train Acc: 1.0\n",
            "Epoch 4012/10000\n",
            "Step 0: Train Loss: 1.9577466446207836e-05, Train Acc: 1.0\n",
            "Epoch 4013/10000\n",
            "Step 0: Train Loss: 1.994051672227215e-05, Train Acc: 1.0\n",
            "Epoch 4014/10000\n",
            "Step 0: Train Loss: 2.0838097043451853e-05, Train Acc: 1.0\n",
            "Epoch 4015/10000\n",
            "Step 0: Train Loss: 1.7730731997289695e-05, Train Acc: 1.0\n",
            "Epoch 4016/10000\n",
            "Step 0: Train Loss: 2.1108273358549923e-05, Train Acc: 1.0\n",
            "Epoch 4017/10000\n",
            "Step 0: Train Loss: 1.729394352878444e-05, Train Acc: 1.0\n",
            "Epoch 4018/10000\n",
            "Step 0: Train Loss: 1.816124131437391e-05, Train Acc: 1.0\n",
            "Epoch 4019/10000\n",
            "Step 0: Train Loss: 1.4948310308682267e-05, Train Acc: 1.0\n",
            "Epoch 4020/10000\n",
            "Step 0: Train Loss: 1.5439036360476166e-05, Train Acc: 1.0\n",
            "Epoch 4021/10000\n",
            "Step 0: Train Loss: 1.90639966604067e-05, Train Acc: 1.0\n",
            "Epoch 4022/10000\n",
            "Step 0: Train Loss: 1.845485166995786e-05, Train Acc: 1.0\n",
            "Epoch 4023/10000\n",
            "Step 0: Train Loss: 1.8072425518766977e-05, Train Acc: 1.0\n",
            "Epoch 4024/10000\n",
            "Step 0: Train Loss: 1.96726068679709e-05, Train Acc: 1.0\n",
            "Epoch 4025/10000\n",
            "Step 0: Train Loss: 2.0802855942747556e-05, Train Acc: 1.0\n",
            "Epoch 4026/10000\n",
            "Step 0: Train Loss: 1.550228444102686e-05, Train Acc: 1.0\n",
            "Epoch 4027/10000\n",
            "Step 0: Train Loss: 1.987734685826581e-05, Train Acc: 1.0\n",
            "Epoch 4028/10000\n",
            "Step 0: Train Loss: 1.6941337889875285e-05, Train Acc: 1.0\n",
            "Epoch 4029/10000\n",
            "Step 0: Train Loss: 1.7817577827372588e-05, Train Acc: 1.0\n",
            "Epoch 4030/10000\n",
            "Step 0: Train Loss: 1.8593787899590097e-05, Train Acc: 1.0\n",
            "Epoch 4031/10000\n",
            "Step 0: Train Loss: 1.9189312297385186e-05, Train Acc: 1.0\n",
            "Epoch 4032/10000\n",
            "Step 0: Train Loss: 1.5239958884194493e-05, Train Acc: 1.0\n",
            "Epoch 4033/10000\n",
            "Step 0: Train Loss: 2.0548213797155768e-05, Train Acc: 1.0\n",
            "Epoch 4034/10000\n",
            "Step 0: Train Loss: 1.6330417565768585e-05, Train Acc: 1.0\n",
            "Epoch 4035/10000\n",
            "Step 0: Train Loss: 1.8078628272633068e-05, Train Acc: 1.0\n",
            "Epoch 4036/10000\n",
            "Step 0: Train Loss: 1.8530736269894987e-05, Train Acc: 1.0\n",
            "Epoch 4037/10000\n",
            "Step 0: Train Loss: 2.182242678827606e-05, Train Acc: 1.0\n",
            "Epoch 4038/10000\n",
            "Step 0: Train Loss: 1.7003321772790514e-05, Train Acc: 1.0\n",
            "Epoch 4039/10000\n",
            "Step 0: Train Loss: 1.504560077592032e-05, Train Acc: 1.0\n",
            "Epoch 4040/10000\n",
            "Step 0: Train Loss: 1.4508947970170993e-05, Train Acc: 1.0\n",
            "Epoch 4041/10000\n",
            "Step 0: Train Loss: 1.8718896171776578e-05, Train Acc: 1.0\n",
            "Epoch 4042/10000\n",
            "Step 0: Train Loss: 1.7166192264994606e-05, Train Acc: 1.0\n",
            "Epoch 4043/10000\n",
            "Step 0: Train Loss: 1.5772742699482478e-05, Train Acc: 1.0\n",
            "Epoch 4044/10000\n",
            "Step 0: Train Loss: 1.4872152860334609e-05, Train Acc: 1.0\n",
            "Epoch 4045/10000\n",
            "Step 0: Train Loss: 1.6674906873959117e-05, Train Acc: 1.0\n",
            "Epoch 4046/10000\n",
            "Step 0: Train Loss: 1.658457586017903e-05, Train Acc: 1.0\n",
            "Epoch 4047/10000\n",
            "Step 0: Train Loss: 1.4507825653709006e-05, Train Acc: 1.0\n",
            "Epoch 4048/10000\n",
            "Step 0: Train Loss: 1.4533577996189706e-05, Train Acc: 1.0\n",
            "Epoch 4049/10000\n",
            "Step 0: Train Loss: 1.7152509826701134e-05, Train Acc: 1.0\n",
            "Epoch 4050/10000\n",
            "Step 0: Train Loss: 1.3549959476222284e-05, Train Acc: 1.0\n",
            "Epoch 4051/10000\n",
            "Step 0: Train Loss: 1.7260925233131275e-05, Train Acc: 1.0\n",
            "Epoch 4052/10000\n",
            "Step 0: Train Loss: 1.6553249224671163e-05, Train Acc: 1.0\n",
            "Epoch 4053/10000\n",
            "Step 0: Train Loss: 1.3871246665075887e-05, Train Acc: 1.0\n",
            "Epoch 4054/10000\n",
            "Step 0: Train Loss: 1.545289887872059e-05, Train Acc: 1.0\n",
            "Epoch 4055/10000\n",
            "Step 0: Train Loss: 1.7970689441426657e-05, Train Acc: 1.0\n",
            "Epoch 4056/10000\n",
            "Step 0: Train Loss: 1.8111453755409457e-05, Train Acc: 1.0\n",
            "Epoch 4057/10000\n",
            "Step 0: Train Loss: 1.8457865735399537e-05, Train Acc: 1.0\n",
            "Epoch 4058/10000\n",
            "Step 0: Train Loss: 1.1918688869627658e-05, Train Acc: 1.0\n",
            "Epoch 4059/10000\n",
            "Step 0: Train Loss: 1.7320315237157047e-05, Train Acc: 1.0\n",
            "Epoch 4060/10000\n",
            "Step 0: Train Loss: 1.424778292857809e-05, Train Acc: 1.0\n",
            "Epoch 4061/10000\n",
            "Step 0: Train Loss: 1.5526269635302015e-05, Train Acc: 1.0\n",
            "Epoch 4062/10000\n",
            "Step 0: Train Loss: 1.560081545903813e-05, Train Acc: 1.0\n",
            "Epoch 4063/10000\n",
            "Step 0: Train Loss: 1.6620559108559974e-05, Train Acc: 1.0\n",
            "Epoch 4064/10000\n",
            "Step 0: Train Loss: 1.701207111182157e-05, Train Acc: 1.0\n",
            "Epoch 4065/10000\n",
            "Step 0: Train Loss: 1.512220023869304e-05, Train Acc: 1.0\n",
            "Epoch 4066/10000\n",
            "Step 0: Train Loss: 1.5865009117987938e-05, Train Acc: 1.0\n",
            "Epoch 4067/10000\n",
            "Step 0: Train Loss: 1.3347439562494401e-05, Train Acc: 1.0\n",
            "Epoch 4068/10000\n",
            "Step 0: Train Loss: 1.4756024938833434e-05, Train Acc: 1.0\n",
            "Epoch 4069/10000\n",
            "Step 0: Train Loss: 1.4019924492458813e-05, Train Acc: 1.0\n",
            "Epoch 4070/10000\n",
            "Step 0: Train Loss: 1.807767512218561e-05, Train Acc: 1.0\n",
            "Epoch 4071/10000\n",
            "Step 0: Train Loss: 1.6987693015835248e-05, Train Acc: 1.0\n",
            "Epoch 4072/10000\n",
            "Step 0: Train Loss: 1.5539344531134702e-05, Train Acc: 1.0\n",
            "Epoch 4073/10000\n",
            "Step 0: Train Loss: 1.338463607680751e-05, Train Acc: 1.0\n",
            "Epoch 4074/10000\n",
            "Step 0: Train Loss: 1.2674408935708925e-05, Train Acc: 1.0\n",
            "Epoch 4075/10000\n",
            "Step 0: Train Loss: 1.5305980923585594e-05, Train Acc: 1.0\n",
            "Epoch 4076/10000\n",
            "Step 0: Train Loss: 1.5852712749619968e-05, Train Acc: 1.0\n",
            "Epoch 4077/10000\n",
            "Step 0: Train Loss: 1.1813078344857786e-05, Train Acc: 1.0\n",
            "Epoch 4078/10000\n",
            "Step 0: Train Loss: 1.4037455912330188e-05, Train Acc: 1.0\n",
            "Epoch 4079/10000\n",
            "Step 0: Train Loss: 1.1950667612836696e-05, Train Acc: 1.0\n",
            "Epoch 4080/10000\n",
            "Step 0: Train Loss: 1.3383933946897741e-05, Train Acc: 1.0\n",
            "Epoch 4081/10000\n",
            "Step 0: Train Loss: 1.2599882211361546e-05, Train Acc: 1.0\n",
            "Epoch 4082/10000\n",
            "Step 0: Train Loss: 1.3629831300931983e-05, Train Acc: 1.0\n",
            "Epoch 4083/10000\n",
            "Step 0: Train Loss: 1.3992092135595158e-05, Train Acc: 1.0\n",
            "Epoch 4084/10000\n",
            "Step 0: Train Loss: 1.3717044566874392e-05, Train Acc: 1.0\n",
            "Epoch 4085/10000\n",
            "Step 0: Train Loss: 1.1118334441562183e-05, Train Acc: 1.0\n",
            "Epoch 4086/10000\n",
            "Step 0: Train Loss: 1.3508647498383652e-05, Train Acc: 1.0\n",
            "Epoch 4087/10000\n",
            "Step 0: Train Loss: 1.3167616998543963e-05, Train Acc: 1.0\n",
            "Epoch 4088/10000\n",
            "Step 0: Train Loss: 1.6147592759807594e-05, Train Acc: 1.0\n",
            "Epoch 4089/10000\n",
            "Step 0: Train Loss: 1.4127473150438163e-05, Train Acc: 1.0\n",
            "Epoch 4090/10000\n",
            "Step 0: Train Loss: 1.465568220737623e-05, Train Acc: 1.0\n",
            "Epoch 4091/10000\n",
            "Step 0: Train Loss: 1.5515621271333657e-05, Train Acc: 1.0\n",
            "Epoch 4092/10000\n",
            "Step 0: Train Loss: 1.4621001355408225e-05, Train Acc: 1.0\n",
            "Epoch 4093/10000\n",
            "Step 0: Train Loss: 1.3156378372514155e-05, Train Acc: 1.0\n",
            "Epoch 4094/10000\n",
            "Step 0: Train Loss: 1.3837268852512352e-05, Train Acc: 1.0\n",
            "Epoch 4095/10000\n",
            "Step 0: Train Loss: 1.5965235434123315e-05, Train Acc: 1.0\n",
            "Epoch 4096/10000\n",
            "Step 0: Train Loss: 1.2375956430332735e-05, Train Acc: 1.0\n",
            "Epoch 4097/10000\n",
            "Step 0: Train Loss: 1.4629968973167706e-05, Train Acc: 1.0\n",
            "Epoch 4098/10000\n",
            "Step 0: Train Loss: 1.4096052836976014e-05, Train Acc: 1.0\n",
            "Epoch 4099/10000\n",
            "Step 0: Train Loss: 1.585325662745163e-05, Train Acc: 1.0\n",
            "Epoch 4100/10000\n",
            "Step 0: Train Loss: 1.547938518342562e-05, Train Acc: 1.0\n",
            "Epoch 4101/10000\n",
            "Step 0: Train Loss: 1.4891548744344618e-05, Train Acc: 1.0\n",
            "Epoch 4102/10000\n",
            "Step 0: Train Loss: 1.1885125786648132e-05, Train Acc: 1.0\n",
            "Epoch 4103/10000\n",
            "Step 0: Train Loss: 1.2738907571474556e-05, Train Acc: 1.0\n",
            "Epoch 4104/10000\n",
            "Step 0: Train Loss: 1.2490522749430966e-05, Train Acc: 1.0\n",
            "Epoch 4105/10000\n",
            "Step 0: Train Loss: 1.0785747690533753e-05, Train Acc: 1.0\n",
            "Epoch 4106/10000\n",
            "Step 0: Train Loss: 1.4662728972325567e-05, Train Acc: 1.0\n",
            "Epoch 4107/10000\n",
            "Step 0: Train Loss: 1.1400778930692468e-05, Train Acc: 1.0\n",
            "Epoch 4108/10000\n",
            "Step 0: Train Loss: 1.4384038877324201e-05, Train Acc: 1.0\n",
            "Epoch 4109/10000\n",
            "Step 0: Train Loss: 1.0235698027827311e-05, Train Acc: 1.0\n",
            "Epoch 4110/10000\n",
            "Step 0: Train Loss: 1.1721017472154927e-05, Train Acc: 1.0\n",
            "Epoch 4111/10000\n",
            "Step 0: Train Loss: 1.1176729458384216e-05, Train Acc: 1.0\n",
            "Epoch 4112/10000\n",
            "Step 0: Train Loss: 1.2690533367276657e-05, Train Acc: 1.0\n",
            "Epoch 4113/10000\n",
            "Step 0: Train Loss: 1.1826620720967185e-05, Train Acc: 1.0\n",
            "Epoch 4114/10000\n",
            "Step 0: Train Loss: 1.0963377462758217e-05, Train Acc: 1.0\n",
            "Epoch 4115/10000\n",
            "Step 0: Train Loss: 1.55799716594629e-05, Train Acc: 1.0\n",
            "Epoch 4116/10000\n",
            "Step 0: Train Loss: 1.2319794223003555e-05, Train Acc: 1.0\n",
            "Epoch 4117/10000\n",
            "Step 0: Train Loss: 1.3822194887325168e-05, Train Acc: 1.0\n",
            "Epoch 4118/10000\n",
            "Step 0: Train Loss: 1.0057256986328866e-05, Train Acc: 1.0\n",
            "Epoch 4119/10000\n",
            "Step 0: Train Loss: 1.3798901818518061e-05, Train Acc: 1.0\n",
            "Epoch 4120/10000\n",
            "Step 0: Train Loss: 1.1926797014893964e-05, Train Acc: 1.0\n",
            "Epoch 4121/10000\n",
            "Step 0: Train Loss: 1.4726181689184159e-05, Train Acc: 1.0\n",
            "Epoch 4122/10000\n",
            "Step 0: Train Loss: 1.0689646842365619e-05, Train Acc: 1.0\n",
            "Epoch 4123/10000\n",
            "Step 0: Train Loss: 1.1844492291857023e-05, Train Acc: 1.0\n",
            "Epoch 4124/10000\n",
            "Step 0: Train Loss: 1.3848344678990543e-05, Train Acc: 1.0\n",
            "Epoch 4125/10000\n",
            "Step 0: Train Loss: 1.4244116755435243e-05, Train Acc: 1.0\n",
            "Epoch 4126/10000\n",
            "Step 0: Train Loss: 1.3442394447338302e-05, Train Acc: 1.0\n",
            "Epoch 4127/10000\n",
            "Step 0: Train Loss: 1.46038546517957e-05, Train Acc: 1.0\n",
            "Epoch 4128/10000\n",
            "Step 0: Train Loss: 1.772105497366283e-05, Train Acc: 1.0\n",
            "Epoch 4129/10000\n",
            "Step 0: Train Loss: 9.79990181804169e-06, Train Acc: 1.0\n",
            "Epoch 4130/10000\n",
            "Step 0: Train Loss: 1.2963560038770083e-05, Train Acc: 1.0\n",
            "Epoch 4131/10000\n",
            "Step 0: Train Loss: 1.2270455044927076e-05, Train Acc: 1.0\n",
            "Epoch 4132/10000\n",
            "Step 0: Train Loss: 1.0493001354916487e-05, Train Acc: 1.0\n",
            "Epoch 4133/10000\n",
            "Step 0: Train Loss: 1.0102918167831376e-05, Train Acc: 1.0\n",
            "Epoch 4134/10000\n",
            "Step 0: Train Loss: 1.1799339517892804e-05, Train Acc: 1.0\n",
            "Epoch 4135/10000\n",
            "Step 0: Train Loss: 9.743489499669522e-06, Train Acc: 1.0\n",
            "Epoch 4136/10000\n",
            "Step 0: Train Loss: 1.1168803212058265e-05, Train Acc: 1.0\n",
            "Epoch 4137/10000\n",
            "Step 0: Train Loss: 1.149828403868014e-05, Train Acc: 1.0\n",
            "Epoch 4138/10000\n",
            "Step 0: Train Loss: 1.3106719052302651e-05, Train Acc: 1.0\n",
            "Epoch 4139/10000\n",
            "Step 0: Train Loss: 9.337018127553165e-06, Train Acc: 1.0\n",
            "Epoch 4140/10000\n",
            "Step 0: Train Loss: 1.0852695595531259e-05, Train Acc: 1.0\n",
            "Epoch 4141/10000\n",
            "Step 0: Train Loss: 1.0978694263030775e-05, Train Acc: 1.0\n",
            "Epoch 4142/10000\n",
            "Step 0: Train Loss: 1.1221185559406877e-05, Train Acc: 1.0\n",
            "Epoch 4143/10000\n",
            "Step 0: Train Loss: 1.3459725778375287e-05, Train Acc: 1.0\n",
            "Epoch 4144/10000\n",
            "Step 0: Train Loss: 1.1816453479696065e-05, Train Acc: 1.0\n",
            "Epoch 4145/10000\n",
            "Step 0: Train Loss: 1.3967499398859218e-05, Train Acc: 1.0\n",
            "Epoch 4146/10000\n",
            "Step 0: Train Loss: 1.338207130174851e-05, Train Acc: 1.0\n",
            "Epoch 4147/10000\n",
            "Step 0: Train Loss: 1.1438998626545072e-05, Train Acc: 1.0\n",
            "Epoch 4148/10000\n",
            "Step 0: Train Loss: 9.959409908333328e-06, Train Acc: 1.0\n",
            "Epoch 4149/10000\n",
            "Step 0: Train Loss: 9.777749255590606e-06, Train Acc: 1.0\n",
            "Epoch 4150/10000\n",
            "Step 0: Train Loss: 8.440043529844843e-06, Train Acc: 1.0\n",
            "Epoch 4151/10000\n",
            "Step 0: Train Loss: 9.696141205495223e-06, Train Acc: 1.0\n",
            "Epoch 4152/10000\n",
            "Step 0: Train Loss: 1.0733556337072514e-05, Train Acc: 1.0\n",
            "Epoch 4153/10000\n",
            "Step 0: Train Loss: 9.269183465221431e-06, Train Acc: 1.0\n",
            "Epoch 4154/10000\n",
            "Step 0: Train Loss: 1.1243176231801044e-05, Train Acc: 1.0\n",
            "Epoch 4155/10000\n",
            "Step 0: Train Loss: 7.770601769152563e-06, Train Acc: 1.0\n",
            "Epoch 4156/10000\n",
            "Step 0: Train Loss: 1.0237768037768546e-05, Train Acc: 1.0\n",
            "Epoch 4157/10000\n",
            "Step 0: Train Loss: 1.1143353731313255e-05, Train Acc: 1.0\n",
            "Epoch 4158/10000\n",
            "Step 0: Train Loss: 1.119950320571661e-05, Train Acc: 1.0\n",
            "Epoch 4159/10000\n",
            "Step 0: Train Loss: 9.799055987969041e-06, Train Acc: 1.0\n",
            "Epoch 4160/10000\n",
            "Step 0: Train Loss: 9.032471098180395e-06, Train Acc: 1.0\n",
            "Epoch 4161/10000\n",
            "Step 0: Train Loss: 9.97822735371301e-06, Train Acc: 1.0\n",
            "Epoch 4162/10000\n",
            "Step 0: Train Loss: 8.360352694580797e-06, Train Acc: 1.0\n",
            "Epoch 4163/10000\n",
            "Step 0: Train Loss: 1.3179238521843217e-05, Train Acc: 1.0\n",
            "Epoch 4164/10000\n",
            "Step 0: Train Loss: 1.1428191101003904e-05, Train Acc: 1.0\n",
            "Epoch 4165/10000\n",
            "Step 0: Train Loss: 1.0525266588956583e-05, Train Acc: 1.0\n",
            "Epoch 4166/10000\n",
            "Step 0: Train Loss: 9.015549039759208e-06, Train Acc: 1.0\n",
            "Epoch 4167/10000\n",
            "Step 0: Train Loss: 8.383221029362176e-06, Train Acc: 1.0\n",
            "Epoch 4168/10000\n",
            "Step 0: Train Loss: 9.120252798311412e-06, Train Acc: 1.0\n",
            "Epoch 4169/10000\n",
            "Step 0: Train Loss: 1.061670536728343e-05, Train Acc: 1.0\n",
            "Epoch 4170/10000\n",
            "Step 0: Train Loss: 1.0497663424757775e-05, Train Acc: 1.0\n",
            "Epoch 4171/10000\n",
            "Step 0: Train Loss: 9.206901268044021e-06, Train Acc: 1.0\n",
            "Epoch 4172/10000\n",
            "Step 0: Train Loss: 8.421528946200851e-06, Train Acc: 1.0\n",
            "Epoch 4173/10000\n",
            "Step 0: Train Loss: 8.596451152698137e-06, Train Acc: 1.0\n",
            "Epoch 4174/10000\n",
            "Step 0: Train Loss: 1.1724562682502437e-05, Train Acc: 1.0\n",
            "Epoch 4175/10000\n",
            "Step 0: Train Loss: 9.676664376456756e-06, Train Acc: 1.0\n",
            "Epoch 4176/10000\n",
            "Step 0: Train Loss: 9.760810826264787e-06, Train Acc: 1.0\n",
            "Epoch 4177/10000\n",
            "Step 0: Train Loss: 1.1629016626102384e-05, Train Acc: 1.0\n",
            "Epoch 4178/10000\n",
            "Step 0: Train Loss: 7.998411092557944e-06, Train Acc: 1.0\n",
            "Epoch 4179/10000\n",
            "Step 0: Train Loss: 7.873031790950336e-06, Train Acc: 1.0\n",
            "Epoch 4180/10000\n",
            "Step 0: Train Loss: 8.03328748588683e-06, Train Acc: 1.0\n",
            "Epoch 4181/10000\n",
            "Step 0: Train Loss: 8.676640391058754e-06, Train Acc: 1.0\n",
            "Epoch 4182/10000\n",
            "Step 0: Train Loss: 8.981751307146624e-06, Train Acc: 1.0\n",
            "Epoch 4183/10000\n",
            "Step 0: Train Loss: 8.511110536346678e-06, Train Acc: 1.0\n",
            "Epoch 4184/10000\n",
            "Step 0: Train Loss: 1.05389717646176e-05, Train Acc: 1.0\n",
            "Epoch 4185/10000\n",
            "Step 0: Train Loss: 1.000153770291945e-05, Train Acc: 1.0\n",
            "Epoch 4186/10000\n",
            "Step 0: Train Loss: 9.019201570481528e-06, Train Acc: 1.0\n",
            "Epoch 4187/10000\n",
            "Step 0: Train Loss: 7.325450951611856e-06, Train Acc: 1.0\n",
            "Epoch 4188/10000\n",
            "Step 0: Train Loss: 9.042054443852976e-06, Train Acc: 1.0\n",
            "Epoch 4189/10000\n",
            "Step 0: Train Loss: 1.1030330824723933e-05, Train Acc: 1.0\n",
            "Epoch 4190/10000\n",
            "Step 0: Train Loss: 1.0055668099084869e-05, Train Acc: 1.0\n",
            "Epoch 4191/10000\n",
            "Step 0: Train Loss: 1.0156525604543276e-05, Train Acc: 1.0\n",
            "Epoch 4192/10000\n",
            "Step 0: Train Loss: 7.722274858679157e-06, Train Acc: 1.0\n",
            "Epoch 4193/10000\n",
            "Step 0: Train Loss: 8.795601388555951e-06, Train Acc: 1.0\n",
            "Epoch 4194/10000\n",
            "Step 0: Train Loss: 9.12175437406404e-06, Train Acc: 1.0\n",
            "Epoch 4195/10000\n",
            "Step 0: Train Loss: 8.229834747908171e-06, Train Acc: 1.0\n",
            "Epoch 4196/10000\n",
            "Step 0: Train Loss: 8.223189979617018e-06, Train Acc: 1.0\n",
            "Epoch 4197/10000\n",
            "Step 0: Train Loss: 9.13794974621851e-06, Train Acc: 1.0\n",
            "Epoch 4198/10000\n",
            "Step 0: Train Loss: 9.049203072208911e-06, Train Acc: 1.0\n",
            "Epoch 4199/10000\n",
            "Step 0: Train Loss: 9.20719321584329e-06, Train Acc: 1.0\n",
            "Epoch 4200/10000\n",
            "Step 0: Train Loss: 8.843181603879202e-06, Train Acc: 1.0\n",
            "Epoch 4201/10000\n",
            "Step 0: Train Loss: 6.749600743205519e-06, Train Acc: 1.0\n",
            "Epoch 4202/10000\n",
            "Step 0: Train Loss: 9.081258212972898e-06, Train Acc: 1.0\n",
            "Epoch 4203/10000\n",
            "Step 0: Train Loss: 9.671878615336027e-06, Train Acc: 1.0\n",
            "Epoch 4204/10000\n",
            "Step 0: Train Loss: 7.980344889801927e-06, Train Acc: 1.0\n",
            "Epoch 4205/10000\n",
            "Step 0: Train Loss: 8.39827953313943e-06, Train Acc: 1.0\n",
            "Epoch 4206/10000\n",
            "Step 0: Train Loss: 8.562949915358331e-06, Train Acc: 1.0\n",
            "Epoch 4207/10000\n",
            "Step 0: Train Loss: 6.341076641547261e-06, Train Acc: 1.0\n",
            "Epoch 4208/10000\n",
            "Step 0: Train Loss: 7.556252967333421e-06, Train Acc: 1.0\n",
            "Epoch 4209/10000\n",
            "Step 0: Train Loss: 8.202365279430524e-06, Train Acc: 1.0\n",
            "Epoch 4210/10000\n",
            "Step 0: Train Loss: 9.523218977847137e-06, Train Acc: 1.0\n",
            "Epoch 4211/10000\n",
            "Step 0: Train Loss: 9.776722436072305e-06, Train Acc: 1.0\n",
            "Epoch 4212/10000\n",
            "Step 0: Train Loss: 8.4178600445739e-06, Train Acc: 1.0\n",
            "Epoch 4213/10000\n",
            "Step 0: Train Loss: 8.28054817247903e-06, Train Acc: 1.0\n",
            "Epoch 4214/10000\n",
            "Step 0: Train Loss: 8.09764151199488e-06, Train Acc: 1.0\n",
            "Epoch 4215/10000\n",
            "Step 0: Train Loss: 7.703917617618572e-06, Train Acc: 1.0\n",
            "Epoch 4216/10000\n",
            "Step 0: Train Loss: 7.799629202054348e-06, Train Acc: 1.0\n",
            "Epoch 4217/10000\n",
            "Step 0: Train Loss: 7.751508746878244e-06, Train Acc: 1.0\n",
            "Epoch 4218/10000\n",
            "Step 0: Train Loss: 8.118082405417226e-06, Train Acc: 1.0\n",
            "Epoch 4219/10000\n",
            "Step 0: Train Loss: 8.467663974442985e-06, Train Acc: 1.0\n",
            "Epoch 4220/10000\n",
            "Step 0: Train Loss: 8.945236004365142e-06, Train Acc: 1.0\n",
            "Epoch 4221/10000\n",
            "Step 0: Train Loss: 8.129475645546336e-06, Train Acc: 1.0\n",
            "Epoch 4222/10000\n",
            "Step 0: Train Loss: 7.236229066620581e-06, Train Acc: 1.0\n",
            "Epoch 4223/10000\n",
            "Step 0: Train Loss: 7.559237019449938e-06, Train Acc: 1.0\n",
            "Epoch 4224/10000\n",
            "Step 0: Train Loss: 7.429889592458494e-06, Train Acc: 1.0\n",
            "Epoch 4225/10000\n",
            "Step 0: Train Loss: 8.126363354676869e-06, Train Acc: 1.0\n",
            "Epoch 4226/10000\n",
            "Step 0: Train Loss: 7.664436452614609e-06, Train Acc: 1.0\n",
            "Epoch 4227/10000\n",
            "Step 0: Train Loss: 7.752425517537631e-06, Train Acc: 1.0\n",
            "Epoch 4228/10000\n",
            "Step 0: Train Loss: 9.393189429829363e-06, Train Acc: 1.0\n",
            "Epoch 4229/10000\n",
            "Step 0: Train Loss: 7.438939064741135e-06, Train Acc: 1.0\n",
            "Epoch 4230/10000\n",
            "Step 0: Train Loss: 8.598503882240038e-06, Train Acc: 1.0\n",
            "Epoch 4231/10000\n",
            "Step 0: Train Loss: 6.681803824903909e-06, Train Acc: 1.0\n",
            "Epoch 4232/10000\n",
            "Step 0: Train Loss: 6.460378244810272e-06, Train Acc: 1.0\n",
            "Epoch 4233/10000\n",
            "Step 0: Train Loss: 8.025757779250853e-06, Train Acc: 1.0\n",
            "Epoch 4234/10000\n",
            "Step 0: Train Loss: 7.346538495767163e-06, Train Acc: 1.0\n",
            "Epoch 4235/10000\n",
            "Step 0: Train Loss: 8.30923454486765e-06, Train Acc: 1.0\n",
            "Epoch 4236/10000\n",
            "Step 0: Train Loss: 7.552973784186179e-06, Train Acc: 1.0\n",
            "Epoch 4237/10000\n",
            "Step 0: Train Loss: 8.374580829695333e-06, Train Acc: 1.0\n",
            "Epoch 4238/10000\n",
            "Step 0: Train Loss: 6.893164027133025e-06, Train Acc: 1.0\n",
            "Epoch 4239/10000\n",
            "Step 0: Train Loss: 7.217062830022769e-06, Train Acc: 1.0\n",
            "Epoch 4240/10000\n",
            "Step 0: Train Loss: 8.822094969218597e-06, Train Acc: 1.0\n",
            "Epoch 4241/10000\n",
            "Step 0: Train Loss: 8.528953003406059e-06, Train Acc: 1.0\n",
            "Epoch 4242/10000\n",
            "Step 0: Train Loss: 7.373757853201823e-06, Train Acc: 1.0\n",
            "Epoch 4243/10000\n",
            "Step 0: Train Loss: 7.799292689014692e-06, Train Acc: 1.0\n",
            "Epoch 4244/10000\n",
            "Step 0: Train Loss: 7.661060408281628e-06, Train Acc: 1.0\n",
            "Epoch 4245/10000\n",
            "Step 0: Train Loss: 7.881540113885421e-06, Train Acc: 1.0\n",
            "Epoch 4246/10000\n",
            "Step 0: Train Loss: 8.117947800201364e-06, Train Acc: 1.0\n",
            "Epoch 4247/10000\n",
            "Step 0: Train Loss: 7.0539722401008476e-06, Train Acc: 1.0\n",
            "Epoch 4248/10000\n",
            "Step 0: Train Loss: 6.227686753845774e-06, Train Acc: 1.0\n",
            "Epoch 4249/10000\n",
            "Step 0: Train Loss: 6.057804512238363e-06, Train Acc: 1.0\n",
            "Epoch 4250/10000\n",
            "Step 0: Train Loss: 5.893137313250918e-06, Train Acc: 1.0\n",
            "Epoch 4251/10000\n",
            "Step 0: Train Loss: 6.420778390747728e-06, Train Acc: 1.0\n",
            "Epoch 4252/10000\n",
            "Step 0: Train Loss: 7.734310202067718e-06, Train Acc: 1.0\n",
            "Epoch 4253/10000\n",
            "Step 0: Train Loss: 7.480116892111255e-06, Train Acc: 1.0\n",
            "Epoch 4254/10000\n",
            "Step 0: Train Loss: 7.3398114182055e-06, Train Acc: 1.0\n",
            "Epoch 4255/10000\n",
            "Step 0: Train Loss: 6.733568625350017e-06, Train Acc: 1.0\n",
            "Epoch 4256/10000\n",
            "Step 0: Train Loss: 5.53281915927073e-06, Train Acc: 1.0\n",
            "Epoch 4257/10000\n",
            "Step 0: Train Loss: 7.809519956936128e-06, Train Acc: 1.0\n",
            "Epoch 4258/10000\n",
            "Step 0: Train Loss: 6.907199349370785e-06, Train Acc: 1.0\n",
            "Epoch 4259/10000\n",
            "Step 0: Train Loss: 6.860794655949576e-06, Train Acc: 1.0\n",
            "Epoch 4260/10000\n",
            "Step 0: Train Loss: 6.0373427004378755e-06, Train Acc: 1.0\n",
            "Epoch 4261/10000\n",
            "Step 0: Train Loss: 6.563217993971193e-06, Train Acc: 1.0\n",
            "Epoch 4262/10000\n",
            "Step 0: Train Loss: 6.106238743086578e-06, Train Acc: 1.0\n",
            "Epoch 4263/10000\n",
            "Step 0: Train Loss: 5.846138719789451e-06, Train Acc: 1.0\n",
            "Epoch 4264/10000\n",
            "Step 0: Train Loss: 6.518119789689081e-06, Train Acc: 1.0\n",
            "Epoch 4265/10000\n",
            "Step 0: Train Loss: 7.251717306644423e-06, Train Acc: 1.0\n",
            "Epoch 4266/10000\n",
            "Step 0: Train Loss: 7.865456609579269e-06, Train Acc: 1.0\n",
            "Epoch 4267/10000\n",
            "Step 0: Train Loss: 7.3977207648567855e-06, Train Acc: 1.0\n",
            "Epoch 4268/10000\n",
            "Step 0: Train Loss: 6.1952537180332e-06, Train Acc: 1.0\n",
            "Epoch 4269/10000\n",
            "Step 0: Train Loss: 7.729667231615167e-06, Train Acc: 1.0\n",
            "Epoch 4270/10000\n",
            "Step 0: Train Loss: 6.400221536750905e-06, Train Acc: 1.0\n",
            "Epoch 4271/10000\n",
            "Step 0: Train Loss: 8.415163392783143e-06, Train Acc: 1.0\n",
            "Epoch 4272/10000\n",
            "Step 0: Train Loss: 4.929361239192076e-06, Train Acc: 1.0\n",
            "Epoch 4273/10000\n",
            "Step 0: Train Loss: 6.843407390988432e-06, Train Acc: 1.0\n",
            "Epoch 4274/10000\n",
            "Step 0: Train Loss: 6.04887691224576e-06, Train Acc: 1.0\n",
            "Epoch 4275/10000\n",
            "Step 0: Train Loss: 6.898788342368789e-06, Train Acc: 1.0\n",
            "Epoch 4276/10000\n",
            "Step 0: Train Loss: 6.992630460445071e-06, Train Acc: 1.0\n",
            "Epoch 4277/10000\n",
            "Step 0: Train Loss: 5.414718089014059e-06, Train Acc: 1.0\n",
            "Epoch 4278/10000\n",
            "Step 0: Train Loss: 7.2362222454103176e-06, Train Acc: 1.0\n",
            "Epoch 4279/10000\n",
            "Step 0: Train Loss: 5.28555301571032e-06, Train Acc: 1.0\n",
            "Epoch 4280/10000\n",
            "Step 0: Train Loss: 6.6493312260718085e-06, Train Acc: 1.0\n",
            "Epoch 4281/10000\n",
            "Step 0: Train Loss: 5.83015844313195e-06, Train Acc: 1.0\n",
            "Epoch 4282/10000\n",
            "Step 0: Train Loss: 6.319560725387419e-06, Train Acc: 1.0\n",
            "Epoch 4283/10000\n",
            "Step 0: Train Loss: 5.988960765535012e-06, Train Acc: 1.0\n",
            "Epoch 4284/10000\n",
            "Step 0: Train Loss: 6.208070772117935e-06, Train Acc: 1.0\n",
            "Epoch 4285/10000\n",
            "Step 0: Train Loss: 5.7506540542817675e-06, Train Acc: 1.0\n",
            "Epoch 4286/10000\n",
            "Step 0: Train Loss: 6.032529654476093e-06, Train Acc: 1.0\n",
            "Epoch 4287/10000\n",
            "Step 0: Train Loss: 6.760937594663119e-06, Train Acc: 1.0\n",
            "Epoch 4288/10000\n",
            "Step 0: Train Loss: 6.072954874980496e-06, Train Acc: 1.0\n",
            "Epoch 4289/10000\n",
            "Step 0: Train Loss: 6.318567557173083e-06, Train Acc: 1.0\n",
            "Epoch 4290/10000\n",
            "Step 0: Train Loss: 6.56458587400266e-06, Train Acc: 1.0\n",
            "Epoch 4291/10000\n",
            "Step 0: Train Loss: 5.887034603802022e-06, Train Acc: 1.0\n",
            "Epoch 4292/10000\n",
            "Step 0: Train Loss: 6.174735517561203e-06, Train Acc: 1.0\n",
            "Epoch 4293/10000\n",
            "Step 0: Train Loss: 6.804972144891508e-06, Train Acc: 1.0\n",
            "Epoch 4294/10000\n",
            "Step 0: Train Loss: 5.735150807595346e-06, Train Acc: 1.0\n",
            "Epoch 4295/10000\n",
            "Step 0: Train Loss: 6.281033165578265e-06, Train Acc: 1.0\n",
            "Epoch 4296/10000\n",
            "Step 0: Train Loss: 6.051294349163072e-06, Train Acc: 1.0\n",
            "Epoch 4297/10000\n",
            "Step 0: Train Loss: 6.256216693145689e-06, Train Acc: 1.0\n",
            "Epoch 4298/10000\n",
            "Step 0: Train Loss: 6.927241429366404e-06, Train Acc: 1.0\n",
            "Epoch 4299/10000\n",
            "Step 0: Train Loss: 5.889982276130468e-06, Train Acc: 1.0\n",
            "Epoch 4300/10000\n",
            "Step 0: Train Loss: 5.0919352361233905e-06, Train Acc: 1.0\n",
            "Epoch 4301/10000\n",
            "Step 0: Train Loss: 5.441340817924356e-06, Train Acc: 1.0\n",
            "Epoch 4302/10000\n",
            "Step 0: Train Loss: 6.278938144532731e-06, Train Acc: 1.0\n",
            "Epoch 4303/10000\n",
            "Step 0: Train Loss: 6.333219971565995e-06, Train Acc: 1.0\n",
            "Epoch 4304/10000\n",
            "Step 0: Train Loss: 5.826491815241752e-06, Train Acc: 1.0\n",
            "Epoch 4305/10000\n",
            "Step 0: Train Loss: 5.52383153262781e-06, Train Acc: 1.0\n",
            "Epoch 4306/10000\n",
            "Step 0: Train Loss: 5.244631211098749e-06, Train Acc: 1.0\n",
            "Epoch 4307/10000\n",
            "Step 0: Train Loss: 6.4923106037895195e-06, Train Acc: 1.0\n",
            "Epoch 4308/10000\n",
            "Step 0: Train Loss: 5.427078122011153e-06, Train Acc: 1.0\n",
            "Epoch 4309/10000\n",
            "Step 0: Train Loss: 5.023032827011775e-06, Train Acc: 1.0\n",
            "Epoch 4310/10000\n",
            "Step 0: Train Loss: 5.434464128484251e-06, Train Acc: 1.0\n",
            "Epoch 4311/10000\n",
            "Step 0: Train Loss: 4.929918304696912e-06, Train Acc: 1.0\n",
            "Epoch 4312/10000\n",
            "Step 0: Train Loss: 6.1155906223575585e-06, Train Acc: 1.0\n",
            "Epoch 4313/10000\n",
            "Step 0: Train Loss: 5.717821295547765e-06, Train Acc: 1.0\n",
            "Epoch 4314/10000\n",
            "Step 0: Train Loss: 4.762483968079323e-06, Train Acc: 1.0\n",
            "Epoch 4315/10000\n",
            "Step 0: Train Loss: 6.280708475969732e-06, Train Acc: 1.0\n",
            "Epoch 4316/10000\n",
            "Step 0: Train Loss: 5.524588232219685e-06, Train Acc: 1.0\n",
            "Epoch 4317/10000\n",
            "Step 0: Train Loss: 4.988616183254635e-06, Train Acc: 1.0\n",
            "Epoch 4318/10000\n",
            "Step 0: Train Loss: 7.015189567027846e-06, Train Acc: 1.0\n",
            "Epoch 4319/10000\n",
            "Step 0: Train Loss: 5.476142632687697e-06, Train Acc: 1.0\n",
            "Epoch 4320/10000\n",
            "Step 0: Train Loss: 4.180475116299931e-06, Train Acc: 1.0\n",
            "Epoch 4321/10000\n",
            "Step 0: Train Loss: 4.701970738096861e-06, Train Acc: 1.0\n",
            "Epoch 4322/10000\n",
            "Step 0: Train Loss: 5.1009074013563804e-06, Train Acc: 1.0\n",
            "Epoch 4323/10000\n",
            "Step 0: Train Loss: 4.78324318464729e-06, Train Acc: 1.0\n",
            "Epoch 4324/10000\n",
            "Step 0: Train Loss: 5.516686087503331e-06, Train Acc: 1.0\n",
            "Epoch 4325/10000\n",
            "Step 0: Train Loss: 5.939054972259328e-06, Train Acc: 1.0\n",
            "Epoch 4326/10000\n",
            "Step 0: Train Loss: 5.457011411635904e-06, Train Acc: 1.0\n",
            "Epoch 4327/10000\n",
            "Step 0: Train Loss: 6.561312147823628e-06, Train Acc: 1.0\n",
            "Epoch 4328/10000\n",
            "Step 0: Train Loss: 5.7404322433285415e-06, Train Acc: 1.0\n",
            "Epoch 4329/10000\n",
            "Step 0: Train Loss: 5.2696109378302936e-06, Train Acc: 1.0\n",
            "Epoch 4330/10000\n",
            "Step 0: Train Loss: 7.501992513425648e-06, Train Acc: 1.0\n",
            "Epoch 4331/10000\n",
            "Step 0: Train Loss: 5.808175501442747e-06, Train Acc: 1.0\n",
            "Epoch 4332/10000\n",
            "Step 0: Train Loss: 5.059253453509882e-06, Train Acc: 1.0\n",
            "Epoch 4333/10000\n",
            "Step 0: Train Loss: 4.690555215347558e-06, Train Acc: 1.0\n",
            "Epoch 4334/10000\n",
            "Step 0: Train Loss: 4.3611166802293155e-06, Train Acc: 1.0\n",
            "Epoch 4335/10000\n",
            "Step 0: Train Loss: 4.605043159244815e-06, Train Acc: 1.0\n",
            "Epoch 4336/10000\n",
            "Step 0: Train Loss: 5.418980890681269e-06, Train Acc: 1.0\n",
            "Epoch 4337/10000\n",
            "Step 0: Train Loss: 5.103075181978056e-06, Train Acc: 1.0\n",
            "Epoch 4338/10000\n",
            "Step 0: Train Loss: 6.079312242945889e-06, Train Acc: 1.0\n",
            "Epoch 4339/10000\n",
            "Step 0: Train Loss: 4.466166046768194e-06, Train Acc: 1.0\n",
            "Epoch 4340/10000\n",
            "Step 0: Train Loss: 5.36609240953112e-06, Train Acc: 1.0\n",
            "Epoch 4341/10000\n",
            "Step 0: Train Loss: 4.818180059373844e-06, Train Acc: 1.0\n",
            "Epoch 4342/10000\n",
            "Step 0: Train Loss: 3.9062088035279885e-06, Train Acc: 1.0\n",
            "Epoch 4343/10000\n",
            "Step 0: Train Loss: 4.264654762664577e-06, Train Acc: 1.0\n",
            "Epoch 4344/10000\n",
            "Step 0: Train Loss: 4.816497948922915e-06, Train Acc: 1.0\n",
            "Epoch 4345/10000\n",
            "Step 0: Train Loss: 4.044617526233196e-06, Train Acc: 1.0\n",
            "Epoch 4346/10000\n",
            "Step 0: Train Loss: 5.331654847395839e-06, Train Acc: 1.0\n",
            "Epoch 4347/10000\n",
            "Step 0: Train Loss: 4.973179329681443e-06, Train Acc: 1.0\n",
            "Epoch 4348/10000\n",
            "Step 0: Train Loss: 4.755109785037348e-06, Train Acc: 1.0\n",
            "Epoch 4349/10000\n",
            "Step 0: Train Loss: 4.779289156431332e-06, Train Acc: 1.0\n",
            "Epoch 4350/10000\n",
            "Step 0: Train Loss: 4.772505690198159e-06, Train Acc: 1.0\n",
            "Epoch 4351/10000\n",
            "Step 0: Train Loss: 5.282338861434255e-06, Train Acc: 1.0\n",
            "Epoch 4352/10000\n",
            "Step 0: Train Loss: 4.4243888623896055e-06, Train Acc: 1.0\n",
            "Epoch 4353/10000\n",
            "Step 0: Train Loss: 5.281116955302423e-06, Train Acc: 1.0\n",
            "Epoch 4354/10000\n",
            "Step 0: Train Loss: 4.977106073056348e-06, Train Acc: 1.0\n",
            "Epoch 4355/10000\n",
            "Step 0: Train Loss: 5.788944235973759e-06, Train Acc: 1.0\n",
            "Epoch 4356/10000\n",
            "Step 0: Train Loss: 3.7389381759567186e-06, Train Acc: 1.0\n",
            "Epoch 4357/10000\n",
            "Step 0: Train Loss: 4.503019681578735e-06, Train Acc: 1.0\n",
            "Epoch 4358/10000\n",
            "Step 0: Train Loss: 4.5000574573350605e-06, Train Acc: 1.0\n",
            "Epoch 4359/10000\n",
            "Step 0: Train Loss: 4.825848918699194e-06, Train Acc: 1.0\n",
            "Epoch 4360/10000\n",
            "Step 0: Train Loss: 4.2865017348958645e-06, Train Acc: 1.0\n",
            "Epoch 4361/10000\n",
            "Step 0: Train Loss: 4.755062946060207e-06, Train Acc: 1.0\n",
            "Epoch 4362/10000\n",
            "Step 0: Train Loss: 4.449439984455239e-06, Train Acc: 1.0\n",
            "Epoch 4363/10000\n",
            "Step 0: Train Loss: 4.91218315801234e-06, Train Acc: 1.0\n",
            "Epoch 4364/10000\n",
            "Step 0: Train Loss: 4.1286639316240326e-06, Train Acc: 1.0\n",
            "Epoch 4365/10000\n",
            "Step 0: Train Loss: 4.960017122357385e-06, Train Acc: 1.0\n",
            "Epoch 4366/10000\n",
            "Step 0: Train Loss: 3.7848685678909533e-06, Train Acc: 1.0\n",
            "Epoch 4367/10000\n",
            "Step 0: Train Loss: 3.814525825873716e-06, Train Acc: 1.0\n",
            "Epoch 4368/10000\n",
            "Step 0: Train Loss: 4.262312813807512e-06, Train Acc: 1.0\n",
            "Epoch 4369/10000\n",
            "Step 0: Train Loss: 4.0386644286627416e-06, Train Acc: 1.0\n",
            "Epoch 4370/10000\n",
            "Step 0: Train Loss: 3.836087216768647e-06, Train Acc: 1.0\n",
            "Epoch 4371/10000\n",
            "Step 0: Train Loss: 3.979095708928071e-06, Train Acc: 1.0\n",
            "Epoch 4372/10000\n",
            "Step 0: Train Loss: 4.2855444917222485e-06, Train Acc: 1.0\n",
            "Epoch 4373/10000\n",
            "Step 0: Train Loss: 4.413941042002989e-06, Train Acc: 1.0\n",
            "Epoch 4374/10000\n",
            "Step 0: Train Loss: 3.6929332054569386e-06, Train Acc: 1.0\n",
            "Epoch 4375/10000\n",
            "Step 0: Train Loss: 4.431300112628378e-06, Train Acc: 1.0\n",
            "Epoch 4376/10000\n",
            "Step 0: Train Loss: 5.248621164355427e-06, Train Acc: 1.0\n",
            "Epoch 4377/10000\n",
            "Step 0: Train Loss: 4.276806521374965e-06, Train Acc: 1.0\n",
            "Epoch 4378/10000\n",
            "Step 0: Train Loss: 5.059012892161263e-06, Train Acc: 1.0\n",
            "Epoch 4379/10000\n",
            "Step 0: Train Loss: 3.984761860920116e-06, Train Acc: 1.0\n",
            "Epoch 4380/10000\n",
            "Step 0: Train Loss: 3.7399358916445635e-06, Train Acc: 1.0\n",
            "Epoch 4381/10000\n",
            "Step 0: Train Loss: 3.829007255262695e-06, Train Acc: 1.0\n",
            "Epoch 4382/10000\n",
            "Step 0: Train Loss: 4.0215377339336555e-06, Train Acc: 1.0\n",
            "Epoch 4383/10000\n",
            "Step 0: Train Loss: 3.5742380077863345e-06, Train Acc: 1.0\n",
            "Epoch 4384/10000\n",
            "Step 0: Train Loss: 3.3047572287614457e-06, Train Acc: 1.0\n",
            "Epoch 4385/10000\n",
            "Step 0: Train Loss: 3.8310809031827375e-06, Train Acc: 1.0\n",
            "Epoch 4386/10000\n",
            "Step 0: Train Loss: 3.7541904021054506e-06, Train Acc: 1.0\n",
            "Epoch 4387/10000\n",
            "Step 0: Train Loss: 4.969872406945797e-06, Train Acc: 1.0\n",
            "Epoch 4388/10000\n",
            "Step 0: Train Loss: 3.958826255257009e-06, Train Acc: 1.0\n",
            "Epoch 4389/10000\n",
            "Step 0: Train Loss: 4.3053305489593185e-06, Train Acc: 1.0\n",
            "Epoch 4390/10000\n",
            "Step 0: Train Loss: 4.239631834934698e-06, Train Acc: 1.0\n",
            "Epoch 4391/10000\n",
            "Step 0: Train Loss: 4.62490379504743e-06, Train Acc: 1.0\n",
            "Epoch 4392/10000\n",
            "Step 0: Train Loss: 4.4004132178088184e-06, Train Acc: 1.0\n",
            "Epoch 4393/10000\n",
            "Step 0: Train Loss: 4.001131856057327e-06, Train Acc: 1.0\n",
            "Epoch 4394/10000\n",
            "Step 0: Train Loss: 4.3291261135891546e-06, Train Acc: 1.0\n",
            "Epoch 4395/10000\n",
            "Step 0: Train Loss: 3.6473002182901837e-06, Train Acc: 1.0\n",
            "Epoch 4396/10000\n",
            "Step 0: Train Loss: 3.816212938545505e-06, Train Acc: 1.0\n",
            "Epoch 4397/10000\n",
            "Step 0: Train Loss: 3.935105269192718e-06, Train Acc: 1.0\n",
            "Epoch 4398/10000\n",
            "Step 0: Train Loss: 3.4027900710498216e-06, Train Acc: 1.0\n",
            "Epoch 4399/10000\n",
            "Step 0: Train Loss: 4.647699825000018e-06, Train Acc: 1.0\n",
            "Epoch 4400/10000\n",
            "Step 0: Train Loss: 4.1162356865243055e-06, Train Acc: 1.0\n",
            "Epoch 4401/10000\n",
            "Step 0: Train Loss: 4.240052476234268e-06, Train Acc: 1.0\n",
            "Epoch 4402/10000\n",
            "Step 0: Train Loss: 4.4286825868766755e-06, Train Acc: 1.0\n",
            "Epoch 4403/10000\n",
            "Step 0: Train Loss: 4.538390840025386e-06, Train Acc: 1.0\n",
            "Epoch 4404/10000\n",
            "Step 0: Train Loss: 3.447521066846093e-06, Train Acc: 1.0\n",
            "Epoch 4405/10000\n",
            "Step 0: Train Loss: 3.430954393479624e-06, Train Acc: 1.0\n",
            "Epoch 4406/10000\n",
            "Step 0: Train Loss: 3.4899358070106246e-06, Train Acc: 1.0\n",
            "Epoch 4407/10000\n",
            "Step 0: Train Loss: 4.0887111936172005e-06, Train Acc: 1.0\n",
            "Epoch 4408/10000\n",
            "Step 0: Train Loss: 3.0717897061549593e-06, Train Acc: 1.0\n",
            "Epoch 4409/10000\n",
            "Step 0: Train Loss: 3.444630692683859e-06, Train Acc: 1.0\n",
            "Epoch 4410/10000\n",
            "Step 0: Train Loss: 2.6634832011041e-06, Train Acc: 1.0\n",
            "Epoch 4411/10000\n",
            "Step 0: Train Loss: 4.333301149017643e-06, Train Acc: 1.0\n",
            "Epoch 4412/10000\n",
            "Step 0: Train Loss: 4.412994712765794e-06, Train Acc: 1.0\n",
            "Epoch 4413/10000\n",
            "Step 0: Train Loss: 3.2917666885623476e-06, Train Acc: 1.0\n",
            "Epoch 4414/10000\n",
            "Step 0: Train Loss: 3.1500592285738094e-06, Train Acc: 1.0\n",
            "Epoch 4415/10000\n",
            "Step 0: Train Loss: 3.576207063815673e-06, Train Acc: 1.0\n",
            "Epoch 4416/10000\n",
            "Step 0: Train Loss: 3.937014298571739e-06, Train Acc: 1.0\n",
            "Epoch 4417/10000\n",
            "Step 0: Train Loss: 3.035077497770544e-06, Train Acc: 1.0\n",
            "Epoch 4418/10000\n",
            "Step 0: Train Loss: 3.1143479191086954e-06, Train Acc: 1.0\n",
            "Epoch 4419/10000\n",
            "Step 0: Train Loss: 4.045703462907113e-06, Train Acc: 1.0\n",
            "Epoch 4420/10000\n",
            "Step 0: Train Loss: 3.772305717575364e-06, Train Acc: 1.0\n",
            "Epoch 4421/10000\n",
            "Step 0: Train Loss: 3.5060338632320054e-06, Train Acc: 1.0\n",
            "Epoch 4422/10000\n",
            "Step 0: Train Loss: 3.7611291645589517e-06, Train Acc: 1.0\n",
            "Epoch 4423/10000\n",
            "Step 0: Train Loss: 3.6669921428256202e-06, Train Acc: 1.0\n",
            "Epoch 4424/10000\n",
            "Step 0: Train Loss: 3.4876447898568586e-06, Train Acc: 1.0\n",
            "Epoch 4425/10000\n",
            "Step 0: Train Loss: 4.015314516436774e-06, Train Acc: 1.0\n",
            "Epoch 4426/10000\n",
            "Step 0: Train Loss: 3.605731308198301e-06, Train Acc: 1.0\n",
            "Epoch 4427/10000\n",
            "Step 0: Train Loss: 3.107137672486715e-06, Train Acc: 1.0\n",
            "Epoch 4428/10000\n",
            "Step 0: Train Loss: 3.566272653188207e-06, Train Acc: 1.0\n",
            "Epoch 4429/10000\n",
            "Step 0: Train Loss: 2.9419393285934348e-06, Train Acc: 1.0\n",
            "Epoch 4430/10000\n",
            "Step 0: Train Loss: 3.306476855868823e-06, Train Acc: 1.0\n",
            "Epoch 4431/10000\n",
            "Step 0: Train Loss: 3.889427716785576e-06, Train Acc: 1.0\n",
            "Epoch 4432/10000\n",
            "Step 0: Train Loss: 2.9910017929069e-06, Train Acc: 1.0\n",
            "Epoch 4433/10000\n",
            "Step 0: Train Loss: 3.837003077933332e-06, Train Acc: 1.0\n",
            "Epoch 4434/10000\n",
            "Step 0: Train Loss: 3.3213852930202847e-06, Train Acc: 1.0\n",
            "Epoch 4435/10000\n",
            "Step 0: Train Loss: 3.1143731575866695e-06, Train Acc: 1.0\n",
            "Epoch 4436/10000\n",
            "Step 0: Train Loss: 3.299932131994865e-06, Train Acc: 1.0\n",
            "Epoch 4437/10000\n",
            "Step 0: Train Loss: 3.5807970562018454e-06, Train Acc: 1.0\n",
            "Epoch 4438/10000\n",
            "Step 0: Train Loss: 2.8859528811153723e-06, Train Acc: 1.0\n",
            "Epoch 4439/10000\n",
            "Step 0: Train Loss: 3.423885573283769e-06, Train Acc: 1.0\n",
            "Epoch 4440/10000\n",
            "Step 0: Train Loss: 3.2900593396334443e-06, Train Acc: 1.0\n",
            "Epoch 4441/10000\n",
            "Step 0: Train Loss: 3.647081939561758e-06, Train Acc: 1.0\n",
            "Epoch 4442/10000\n",
            "Step 0: Train Loss: 3.313317165520857e-06, Train Acc: 1.0\n",
            "Epoch 4443/10000\n",
            "Step 0: Train Loss: 3.0650578537461115e-06, Train Acc: 1.0\n",
            "Epoch 4444/10000\n",
            "Step 0: Train Loss: 3.3689723295537988e-06, Train Acc: 1.0\n",
            "Epoch 4445/10000\n",
            "Step 0: Train Loss: 3.1725276130600832e-06, Train Acc: 1.0\n",
            "Epoch 4446/10000\n",
            "Step 0: Train Loss: 2.8528804705274524e-06, Train Acc: 1.0\n",
            "Epoch 4447/10000\n",
            "Step 0: Train Loss: 3.0523310670105275e-06, Train Acc: 1.0\n",
            "Epoch 4448/10000\n",
            "Step 0: Train Loss: 3.1653669338993495e-06, Train Acc: 1.0\n",
            "Epoch 4449/10000\n",
            "Step 0: Train Loss: 3.0831765798211563e-06, Train Acc: 1.0\n",
            "Epoch 4450/10000\n",
            "Step 0: Train Loss: 3.228296236557071e-06, Train Acc: 1.0\n",
            "Epoch 4451/10000\n",
            "Step 0: Train Loss: 3.54450799022743e-06, Train Acc: 1.0\n",
            "Epoch 4452/10000\n",
            "Step 0: Train Loss: 3.1684089663031045e-06, Train Acc: 1.0\n",
            "Epoch 4453/10000\n",
            "Step 0: Train Loss: 3.7832492125744466e-06, Train Acc: 1.0\n",
            "Epoch 4454/10000\n",
            "Step 0: Train Loss: 3.625944145824178e-06, Train Acc: 1.0\n",
            "Epoch 4455/10000\n",
            "Step 0: Train Loss: 3.5098869375360664e-06, Train Acc: 1.0\n",
            "Epoch 4456/10000\n",
            "Step 0: Train Loss: 3.480193754512584e-06, Train Acc: 1.0\n",
            "Epoch 4457/10000\n",
            "Step 0: Train Loss: 3.3205828913196456e-06, Train Acc: 1.0\n",
            "Epoch 4458/10000\n",
            "Step 0: Train Loss: 3.1275696983357193e-06, Train Acc: 1.0\n",
            "Epoch 4459/10000\n",
            "Step 0: Train Loss: 3.594119789340766e-06, Train Acc: 1.0\n",
            "Epoch 4460/10000\n",
            "Step 0: Train Loss: 3.5524080885807052e-06, Train Acc: 1.0\n",
            "Epoch 4461/10000\n",
            "Step 0: Train Loss: 3.643270247266628e-06, Train Acc: 1.0\n",
            "Epoch 4462/10000\n",
            "Step 0: Train Loss: 2.9561679184553213e-06, Train Acc: 1.0\n",
            "Epoch 4463/10000\n",
            "Step 0: Train Loss: 3.944001946365461e-06, Train Acc: 1.0\n",
            "Epoch 4464/10000\n",
            "Step 0: Train Loss: 3.2253831250272924e-06, Train Acc: 1.0\n",
            "Epoch 4465/10000\n",
            "Step 0: Train Loss: 3.2669890970282722e-06, Train Acc: 1.0\n",
            "Epoch 4466/10000\n",
            "Step 0: Train Loss: 3.090156724283588e-06, Train Acc: 1.0\n",
            "Epoch 4467/10000\n",
            "Step 0: Train Loss: 2.7145122203364735e-06, Train Acc: 1.0\n",
            "Epoch 4468/10000\n",
            "Step 0: Train Loss: 3.8597672755713575e-06, Train Acc: 1.0\n",
            "Epoch 4469/10000\n",
            "Step 0: Train Loss: 3.161052973155165e-06, Train Acc: 1.0\n",
            "Epoch 4470/10000\n",
            "Step 0: Train Loss: 2.9543843993451446e-06, Train Acc: 1.0\n",
            "Epoch 4471/10000\n",
            "Step 0: Train Loss: 3.3117928524006857e-06, Train Acc: 1.0\n",
            "Epoch 4472/10000\n",
            "Step 0: Train Loss: 3.5094592476525577e-06, Train Acc: 1.0\n",
            "Epoch 4473/10000\n",
            "Step 0: Train Loss: 2.848376198016922e-06, Train Acc: 1.0\n",
            "Epoch 4474/10000\n",
            "Step 0: Train Loss: 2.637812258399208e-06, Train Acc: 1.0\n",
            "Epoch 4475/10000\n",
            "Step 0: Train Loss: 2.8866984393971507e-06, Train Acc: 1.0\n",
            "Epoch 4476/10000\n",
            "Step 0: Train Loss: 2.888536755563109e-06, Train Acc: 1.0\n",
            "Epoch 4477/10000\n",
            "Step 0: Train Loss: 3.2160025966732064e-06, Train Acc: 1.0\n",
            "Epoch 4478/10000\n",
            "Step 0: Train Loss: 2.4006683361221803e-06, Train Acc: 1.0\n",
            "Epoch 4479/10000\n",
            "Step 0: Train Loss: 3.1197389489534544e-06, Train Acc: 1.0\n",
            "Epoch 4480/10000\n",
            "Step 0: Train Loss: 2.677384372873348e-06, Train Acc: 1.0\n",
            "Epoch 4481/10000\n",
            "Step 0: Train Loss: 2.543959226386505e-06, Train Acc: 1.0\n",
            "Epoch 4482/10000\n",
            "Step 0: Train Loss: 2.555668743298156e-06, Train Acc: 1.0\n",
            "Epoch 4483/10000\n",
            "Step 0: Train Loss: 2.806695192703046e-06, Train Acc: 1.0\n",
            "Epoch 4484/10000\n",
            "Step 0: Train Loss: 2.6120783331862185e-06, Train Acc: 1.0\n",
            "Epoch 4485/10000\n",
            "Step 0: Train Loss: 3.3162925774377072e-06, Train Acc: 1.0\n",
            "Epoch 4486/10000\n",
            "Step 0: Train Loss: 2.913548541982891e-06, Train Acc: 1.0\n",
            "Epoch 4487/10000\n",
            "Step 0: Train Loss: 2.629394657560624e-06, Train Acc: 1.0\n",
            "Epoch 4488/10000\n",
            "Step 0: Train Loss: 2.9327088668651413e-06, Train Acc: 1.0\n",
            "Epoch 4489/10000\n",
            "Step 0: Train Loss: 2.4547048269596417e-06, Train Acc: 1.0\n",
            "Epoch 4490/10000\n",
            "Step 0: Train Loss: 2.658782250364311e-06, Train Acc: 1.0\n",
            "Epoch 4491/10000\n",
            "Step 0: Train Loss: 2.8589167868631193e-06, Train Acc: 1.0\n",
            "Epoch 4492/10000\n",
            "Step 0: Train Loss: 2.8466224648582283e-06, Train Acc: 1.0\n",
            "Epoch 4493/10000\n",
            "Step 0: Train Loss: 2.4856196887412807e-06, Train Acc: 1.0\n",
            "Epoch 4494/10000\n",
            "Step 0: Train Loss: 3.0958722163632046e-06, Train Acc: 1.0\n",
            "Epoch 4495/10000\n",
            "Step 0: Train Loss: 2.9263205760798883e-06, Train Acc: 1.0\n",
            "Epoch 4496/10000\n",
            "Step 0: Train Loss: 3.030199877684936e-06, Train Acc: 1.0\n",
            "Epoch 4497/10000\n",
            "Step 0: Train Loss: 2.4552336981287226e-06, Train Acc: 1.0\n",
            "Epoch 4498/10000\n",
            "Step 0: Train Loss: 2.903751919802744e-06, Train Acc: 1.0\n",
            "Epoch 4499/10000\n",
            "Step 0: Train Loss: 2.3867094114393694e-06, Train Acc: 1.0\n",
            "Epoch 4500/10000\n",
            "Step 0: Train Loss: 2.239734385511838e-06, Train Acc: 1.0\n",
            "Epoch 4501/10000\n",
            "Step 0: Train Loss: 2.5583240130799823e-06, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 4500 1024 0.9965048553383979\n",
            "Epoch index and hidden dimension and ratio: 4500 20 0.9161589880433088\n",
            "Epoch index and hidden dimension and ratio: 4500 20 1.3365551835643361\n",
            "Epoch index and hidden dimension and ratio: 4500 20 3.1818525585780324\n",
            "MI(X;T): [10.602841034994123, 7.710899528198874, 5.411890057195177, 2.909280201376439], MI(Y;T): [2.6245262397587505, 3.181449984441758, 3.197200137009558, 2.9696188191476587]\n",
            "Epoch index and hidden dimension and ratio: 4500 1024 0.9965129318443487\n",
            "Epoch index and hidden dimension and ratio: 4500 20 0.9161704871566837\n",
            "Epoch index and hidden dimension and ratio: 4500 20 1.336610157010765\n",
            "Epoch index and hidden dimension and ratio: 4500 20 3.182206601066257\n",
            "MI(X;T): [10.602878779369231, 7.712093336377984, 5.41221850024643, 2.909230366131517], MI(Y;T): [2.624409467265406, 3.1817063907963314, 3.1972093336842162, 2.9695944990040735]\n",
            "Epoch index and hidden dimension and ratio: 4500 1024 0.9965224768059269\n",
            "Epoch index and hidden dimension and ratio: 4500 20 0.9161856746649147\n",
            "Epoch index and hidden dimension and ratio: 4500 20 1.3366770646464878\n",
            "Epoch index and hidden dimension and ratio: 4500 20 3.182595850381842\n",
            "MI(X;T): [10.602931556525377, 7.712247421741445, 5.4119510307605445, 2.908788425994833], MI(Y;T): [2.6243642452279126, 3.181046712264818, 3.197248904199877, 2.9690866637991573]\n",
            "Epoch index and hidden dimension and ratio: 4500 1024 0.9965293051245944\n",
            "Epoch index and hidden dimension and ratio: 4500 20 0.9161986925291128\n",
            "Epoch index and hidden dimension and ratio: 4500 20 1.3367053766061958\n",
            "Epoch index and hidden dimension and ratio: 4500 20 3.182803142916769\n",
            "MI(X;T): [10.602898474949942, 7.71237375463755, 5.412001157619294, 2.908473861846624], MI(Y;T): [2.6242125177925284, 3.1812592017371046, 3.1972147594788245, 2.9688050176703413]\n",
            "Epoch index and hidden dimension and ratio: 4500 1024 0.996531581230817\n",
            "Epoch index and hidden dimension and ratio: 4500 20 0.9162146394127555\n",
            "Epoch index and hidden dimension and ratio: 4500 20 1.3367476541065673\n",
            "Epoch index and hidden dimension and ratio: 4500 20 3.1829377185465866\n",
            "MI(X;T): [10.602898474949942, 7.712513973872559, 5.412031971468348, 2.9084066246191194], MI(Y;T): [2.624265087083077, 3.1812791587743656, 3.197015021894389, 2.9688449616115635]\n",
            "Epoch index and hidden dimension and ratio: 4500 1024 0.9965345915648531\n",
            "Epoch index and hidden dimension and ratio: 4500 20 0.9162326474582295\n",
            "Epoch index and hidden dimension and ratio: 4500 20 1.336791582079926\n",
            "Epoch index and hidden dimension and ratio: 4500 20 3.1830308356694186\n",
            "MI(X;T): [10.602931556525377, 7.712077958267253, 5.4124713987276944, 2.9092007989807933], MI(Y;T): [2.624410157641236, 3.1814061409050365, 3.1972422688829134, 2.9688673975613606]\n",
            "Epoch 4502/10000\n",
            "Step 0: Train Loss: 2.916370476668817e-06, Train Acc: 1.0\n",
            "Epoch 4503/10000\n",
            "Step 0: Train Loss: 2.999001480930019e-06, Train Acc: 1.0\n",
            "Epoch 4504/10000\n",
            "Step 0: Train Loss: 2.436717522869003e-06, Train Acc: 1.0\n",
            "Epoch 4505/10000\n",
            "Step 0: Train Loss: 3.468705017439788e-06, Train Acc: 1.0\n",
            "Epoch 4506/10000\n",
            "Step 0: Train Loss: 2.7609512471826747e-06, Train Acc: 1.0\n",
            "Epoch 4507/10000\n",
            "Step 0: Train Loss: 2.603329448902514e-06, Train Acc: 1.0\n",
            "Epoch 4508/10000\n",
            "Step 0: Train Loss: 2.150681893908768e-06, Train Acc: 1.0\n",
            "Epoch 4509/10000\n",
            "Step 0: Train Loss: 2.0322277123341337e-06, Train Acc: 1.0\n",
            "Epoch 4510/10000\n",
            "Step 0: Train Loss: 3.000889137183549e-06, Train Acc: 1.0\n",
            "Epoch 4511/10000\n",
            "Step 0: Train Loss: 2.843716629286064e-06, Train Acc: 1.0\n",
            "Epoch 4512/10000\n",
            "Step 0: Train Loss: 2.1149792246433208e-06, Train Acc: 1.0\n",
            "Epoch 4513/10000\n",
            "Step 0: Train Loss: 1.9938586319767637e-06, Train Acc: 1.0\n",
            "Epoch 4514/10000\n",
            "Step 0: Train Loss: 2.1664268388121855e-06, Train Acc: 1.0\n",
            "Epoch 4515/10000\n",
            "Step 0: Train Loss: 2.226805008831434e-06, Train Acc: 1.0\n",
            "Epoch 4516/10000\n",
            "Step 0: Train Loss: 2.1062937776150648e-06, Train Acc: 1.0\n",
            "Epoch 4517/10000\n",
            "Step 0: Train Loss: 2.69987481260614e-06, Train Acc: 1.0\n",
            "Epoch 4518/10000\n",
            "Step 0: Train Loss: 2.4917296741477912e-06, Train Acc: 1.0\n",
            "Epoch 4519/10000\n",
            "Step 0: Train Loss: 2.9895350053266156e-06, Train Acc: 1.0\n",
            "Epoch 4520/10000\n",
            "Step 0: Train Loss: 2.4718283384572715e-06, Train Acc: 1.0\n",
            "Epoch 4521/10000\n",
            "Step 0: Train Loss: 2.854634203686146e-06, Train Acc: 1.0\n",
            "Epoch 4522/10000\n",
            "Step 0: Train Loss: 2.1928458409092855e-06, Train Acc: 1.0\n",
            "Epoch 4523/10000\n",
            "Step 0: Train Loss: 2.6060215532197617e-06, Train Acc: 1.0\n",
            "Epoch 4524/10000\n",
            "Step 0: Train Loss: 2.3087245608621743e-06, Train Acc: 1.0\n",
            "Epoch 4525/10000\n",
            "Step 0: Train Loss: 2.850583086910774e-06, Train Acc: 1.0\n",
            "Epoch 4526/10000\n",
            "Step 0: Train Loss: 2.271841367473826e-06, Train Acc: 1.0\n",
            "Epoch 4527/10000\n",
            "Step 0: Train Loss: 2.2394192455976736e-06, Train Acc: 1.0\n",
            "Epoch 4528/10000\n",
            "Step 0: Train Loss: 2.3745556063659023e-06, Train Acc: 1.0\n",
            "Epoch 4529/10000\n",
            "Step 0: Train Loss: 2.635503733472433e-06, Train Acc: 1.0\n",
            "Epoch 4530/10000\n",
            "Step 0: Train Loss: 2.3198774670163402e-06, Train Acc: 1.0\n",
            "Epoch 4531/10000\n",
            "Step 0: Train Loss: 2.3401084945362527e-06, Train Acc: 1.0\n",
            "Epoch 4532/10000\n",
            "Step 0: Train Loss: 2.4167779884010088e-06, Train Acc: 1.0\n",
            "Epoch 4533/10000\n",
            "Step 0: Train Loss: 2.261317376905936e-06, Train Acc: 1.0\n",
            "Epoch 4534/10000\n",
            "Step 0: Train Loss: 2.287149527546717e-06, Train Acc: 1.0\n",
            "Epoch 4535/10000\n",
            "Step 0: Train Loss: 2.407099600532092e-06, Train Acc: 1.0\n",
            "Epoch 4536/10000\n",
            "Step 0: Train Loss: 2.1601933894999092e-06, Train Acc: 1.0\n",
            "Epoch 4537/10000\n",
            "Step 0: Train Loss: 1.9840915683744242e-06, Train Acc: 1.0\n",
            "Epoch 4538/10000\n",
            "Step 0: Train Loss: 2.2191134121385403e-06, Train Acc: 1.0\n",
            "Epoch 4539/10000\n",
            "Step 0: Train Loss: 2.5689425910968566e-06, Train Acc: 1.0\n",
            "Epoch 4540/10000\n",
            "Step 0: Train Loss: 2.2326996713672997e-06, Train Acc: 1.0\n",
            "Epoch 4541/10000\n",
            "Step 0: Train Loss: 2.163162434953847e-06, Train Acc: 1.0\n",
            "Epoch 4542/10000\n",
            "Step 0: Train Loss: 2.3945617613208015e-06, Train Acc: 1.0\n",
            "Epoch 4543/10000\n",
            "Step 0: Train Loss: 2.2989624994806945e-06, Train Acc: 1.0\n",
            "Epoch 4544/10000\n",
            "Step 0: Train Loss: 2.540735522416071e-06, Train Acc: 1.0\n",
            "Epoch 4545/10000\n",
            "Step 0: Train Loss: 2.10063558370166e-06, Train Acc: 1.0\n",
            "Epoch 4546/10000\n",
            "Step 0: Train Loss: 1.9694052753038704e-06, Train Acc: 1.0\n",
            "Epoch 4547/10000\n",
            "Step 0: Train Loss: 2.11958035833959e-06, Train Acc: 1.0\n",
            "Epoch 4548/10000\n",
            "Step 0: Train Loss: 1.890747967081552e-06, Train Acc: 1.0\n",
            "Epoch 4549/10000\n",
            "Step 0: Train Loss: 1.8938266066470533e-06, Train Acc: 1.0\n",
            "Epoch 4550/10000\n",
            "Step 0: Train Loss: 2.0039685750816716e-06, Train Acc: 1.0\n",
            "Epoch 4551/10000\n",
            "Step 0: Train Loss: 2.0269360447855433e-06, Train Acc: 1.0\n",
            "Epoch 4552/10000\n",
            "Step 0: Train Loss: 2.265383955091238e-06, Train Acc: 1.0\n",
            "Epoch 4553/10000\n",
            "Step 0: Train Loss: 2.419811380605097e-06, Train Acc: 1.0\n",
            "Epoch 4554/10000\n",
            "Step 0: Train Loss: 2.1808623387187254e-06, Train Acc: 1.0\n",
            "Epoch 4555/10000\n",
            "Step 0: Train Loss: 2.316062818863429e-06, Train Acc: 1.0\n",
            "Epoch 4556/10000\n",
            "Step 0: Train Loss: 2.2205581444723066e-06, Train Acc: 1.0\n",
            "Epoch 4557/10000\n",
            "Step 0: Train Loss: 2.0666770979005378e-06, Train Acc: 1.0\n",
            "Epoch 4558/10000\n",
            "Step 0: Train Loss: 2.1693304006475955e-06, Train Acc: 1.0\n",
            "Epoch 4559/10000\n",
            "Step 0: Train Loss: 1.9706974399014143e-06, Train Acc: 1.0\n",
            "Epoch 4560/10000\n",
            "Step 0: Train Loss: 2.1126259071024833e-06, Train Acc: 1.0\n",
            "Epoch 4561/10000\n",
            "Step 0: Train Loss: 2.095153831760399e-06, Train Acc: 1.0\n",
            "Epoch 4562/10000\n",
            "Step 0: Train Loss: 2.0414261143741896e-06, Train Acc: 1.0\n",
            "Epoch 4563/10000\n",
            "Step 0: Train Loss: 2.5025317427207483e-06, Train Acc: 1.0\n",
            "Epoch 4564/10000\n",
            "Step 0: Train Loss: 2.2133779111754848e-06, Train Acc: 1.0\n",
            "Epoch 4565/10000\n",
            "Step 0: Train Loss: 1.8503710634831805e-06, Train Acc: 1.0\n",
            "Epoch 4566/10000\n",
            "Step 0: Train Loss: 1.9966582840424962e-06, Train Acc: 1.0\n",
            "Epoch 4567/10000\n",
            "Step 0: Train Loss: 2.250564193673199e-06, Train Acc: 1.0\n",
            "Epoch 4568/10000\n",
            "Step 0: Train Loss: 1.6788218317742576e-06, Train Acc: 1.0\n",
            "Epoch 4569/10000\n",
            "Step 0: Train Loss: 2.1391249447333394e-06, Train Acc: 1.0\n",
            "Epoch 4570/10000\n",
            "Step 0: Train Loss: 1.6704976815162809e-06, Train Acc: 1.0\n",
            "Epoch 4571/10000\n",
            "Step 0: Train Loss: 1.798532139218878e-06, Train Acc: 1.0\n",
            "Epoch 4572/10000\n",
            "Step 0: Train Loss: 1.6548842722841073e-06, Train Acc: 1.0\n",
            "Epoch 4573/10000\n",
            "Step 0: Train Loss: 1.7639620182308136e-06, Train Acc: 1.0\n",
            "Epoch 4574/10000\n",
            "Step 0: Train Loss: 2.043609583779471e-06, Train Acc: 1.0\n",
            "Epoch 4575/10000\n",
            "Step 0: Train Loss: 2.20292849917314e-06, Train Acc: 1.0\n",
            "Epoch 4576/10000\n",
            "Step 0: Train Loss: 1.9514841369527858e-06, Train Acc: 1.0\n",
            "Epoch 4577/10000\n",
            "Step 0: Train Loss: 2.1981388726999285e-06, Train Acc: 1.0\n",
            "Epoch 4578/10000\n",
            "Step 0: Train Loss: 1.7521543895782088e-06, Train Acc: 1.0\n",
            "Epoch 4579/10000\n",
            "Step 0: Train Loss: 1.980969955184264e-06, Train Acc: 1.0\n",
            "Epoch 4580/10000\n",
            "Step 0: Train Loss: 2.2113908926257864e-06, Train Acc: 1.0\n",
            "Epoch 4581/10000\n",
            "Step 0: Train Loss: 1.9545516352081904e-06, Train Acc: 1.0\n",
            "Epoch 4582/10000\n",
            "Step 0: Train Loss: 1.977061401703395e-06, Train Acc: 1.0\n",
            "Epoch 4583/10000\n",
            "Step 0: Train Loss: 1.8516154796088813e-06, Train Acc: 1.0\n",
            "Epoch 4584/10000\n",
            "Step 0: Train Loss: 1.8478827996659675e-06, Train Acc: 1.0\n",
            "Epoch 4585/10000\n",
            "Step 0: Train Loss: 1.8309125380255864e-06, Train Acc: 1.0\n",
            "Epoch 4586/10000\n",
            "Step 0: Train Loss: 1.7350992038700497e-06, Train Acc: 1.0\n",
            "Epoch 4587/10000\n",
            "Step 0: Train Loss: 1.8349411448070896e-06, Train Acc: 1.0\n",
            "Epoch 4588/10000\n",
            "Step 0: Train Loss: 2.1018915958848083e-06, Train Acc: 1.0\n",
            "Epoch 4589/10000\n",
            "Step 0: Train Loss: 2.1318476228771033e-06, Train Acc: 1.0\n",
            "Epoch 4590/10000\n",
            "Step 0: Train Loss: 1.7596363477423438e-06, Train Acc: 1.0\n",
            "Epoch 4591/10000\n",
            "Step 0: Train Loss: 1.7914966292664758e-06, Train Acc: 1.0\n",
            "Epoch 4592/10000\n",
            "Step 0: Train Loss: 1.542528480058536e-06, Train Acc: 1.0\n",
            "Epoch 4593/10000\n",
            "Step 0: Train Loss: 1.8991567003467935e-06, Train Acc: 1.0\n",
            "Epoch 4594/10000\n",
            "Step 0: Train Loss: 2.0455145204323344e-06, Train Acc: 1.0\n",
            "Epoch 4595/10000\n",
            "Step 0: Train Loss: 1.5314062693505548e-06, Train Acc: 1.0\n",
            "Epoch 4596/10000\n",
            "Step 0: Train Loss: 1.9704636997630587e-06, Train Acc: 1.0\n",
            "Epoch 4597/10000\n",
            "Step 0: Train Loss: 2.118739530487801e-06, Train Acc: 1.0\n",
            "Epoch 4598/10000\n",
            "Step 0: Train Loss: 1.880649051599903e-06, Train Acc: 1.0\n",
            "Epoch 4599/10000\n",
            "Step 0: Train Loss: 1.8574047544461791e-06, Train Acc: 1.0\n",
            "Epoch 4600/10000\n",
            "Step 0: Train Loss: 1.4525458027492277e-06, Train Acc: 1.0\n",
            "Epoch 4601/10000\n",
            "Step 0: Train Loss: 1.7544509773870232e-06, Train Acc: 1.0\n",
            "Epoch 4602/10000\n",
            "Step 0: Train Loss: 1.8474858052286436e-06, Train Acc: 1.0\n",
            "Epoch 4603/10000\n",
            "Step 0: Train Loss: 1.5842077800698462e-06, Train Acc: 1.0\n",
            "Epoch 4604/10000\n",
            "Step 0: Train Loss: 1.8773286001305678e-06, Train Acc: 1.0\n",
            "Epoch 4605/10000\n",
            "Step 0: Train Loss: 1.5124812762223883e-06, Train Acc: 1.0\n",
            "Epoch 4606/10000\n",
            "Step 0: Train Loss: 1.7252598354389193e-06, Train Acc: 1.0\n",
            "Epoch 4607/10000\n",
            "Step 0: Train Loss: 1.441796257495298e-06, Train Acc: 1.0\n",
            "Epoch 4608/10000\n",
            "Step 0: Train Loss: 1.6396319324485376e-06, Train Acc: 1.0\n",
            "Epoch 4609/10000\n",
            "Step 0: Train Loss: 1.6490340613017906e-06, Train Acc: 1.0\n",
            "Epoch 4610/10000\n",
            "Step 0: Train Loss: 1.846139639383182e-06, Train Acc: 1.0\n",
            "Epoch 4611/10000\n",
            "Step 0: Train Loss: 1.8054793144983705e-06, Train Acc: 1.0\n",
            "Epoch 4612/10000\n",
            "Step 0: Train Loss: 1.6894641703402158e-06, Train Acc: 1.0\n",
            "Epoch 4613/10000\n",
            "Step 0: Train Loss: 1.859033432083379e-06, Train Acc: 1.0\n",
            "Epoch 4614/10000\n",
            "Step 0: Train Loss: 1.7016255924318102e-06, Train Acc: 1.0\n",
            "Epoch 4615/10000\n",
            "Step 0: Train Loss: 1.6527181969649973e-06, Train Acc: 1.0\n",
            "Epoch 4616/10000\n",
            "Step 0: Train Loss: 1.8481347296983586e-06, Train Acc: 1.0\n",
            "Epoch 4617/10000\n",
            "Step 0: Train Loss: 1.9264705315436004e-06, Train Acc: 1.0\n",
            "Epoch 4618/10000\n",
            "Step 0: Train Loss: 1.5700003359597758e-06, Train Acc: 1.0\n",
            "Epoch 4619/10000\n",
            "Step 0: Train Loss: 1.6585839830440818e-06, Train Acc: 1.0\n",
            "Epoch 4620/10000\n",
            "Step 0: Train Loss: 1.6704154859326081e-06, Train Acc: 1.0\n",
            "Epoch 4621/10000\n",
            "Step 0: Train Loss: 1.6951214547589188e-06, Train Acc: 1.0\n",
            "Epoch 4622/10000\n",
            "Step 0: Train Loss: 1.6536142766199191e-06, Train Acc: 1.0\n",
            "Epoch 4623/10000\n",
            "Step 0: Train Loss: 1.5281381138265715e-06, Train Acc: 1.0\n",
            "Epoch 4624/10000\n",
            "Step 0: Train Loss: 1.5968666957633104e-06, Train Acc: 1.0\n",
            "Epoch 4625/10000\n",
            "Step 0: Train Loss: 1.5872400354055571e-06, Train Acc: 1.0\n",
            "Epoch 4626/10000\n",
            "Step 0: Train Loss: 1.5033796216812334e-06, Train Acc: 1.0\n",
            "Epoch 4627/10000\n",
            "Step 0: Train Loss: 1.8323270296605187e-06, Train Acc: 1.0\n",
            "Epoch 4628/10000\n",
            "Step 0: Train Loss: 1.665906211201218e-06, Train Acc: 1.0\n",
            "Epoch 4629/10000\n",
            "Step 0: Train Loss: 1.448970351702883e-06, Train Acc: 1.0\n",
            "Epoch 4630/10000\n",
            "Step 0: Train Loss: 1.8348365529163857e-06, Train Acc: 1.0\n",
            "Epoch 4631/10000\n",
            "Step 0: Train Loss: 1.366600599794765e-06, Train Acc: 1.0\n",
            "Epoch 4632/10000\n",
            "Step 0: Train Loss: 1.5148635839068447e-06, Train Acc: 1.0\n",
            "Epoch 4633/10000\n",
            "Step 0: Train Loss: 1.7265069800487254e-06, Train Acc: 1.0\n",
            "Epoch 4634/10000\n",
            "Step 0: Train Loss: 1.7090862911572913e-06, Train Acc: 1.0\n",
            "Epoch 4635/10000\n",
            "Step 0: Train Loss: 1.7255499642487848e-06, Train Acc: 1.0\n",
            "Epoch 4636/10000\n",
            "Step 0: Train Loss: 1.7337788449367508e-06, Train Acc: 1.0\n",
            "Epoch 4637/10000\n",
            "Step 0: Train Loss: 1.6338374280167045e-06, Train Acc: 1.0\n",
            "Epoch 4638/10000\n",
            "Step 0: Train Loss: 1.3547239632316632e-06, Train Acc: 1.0\n",
            "Epoch 4639/10000\n",
            "Step 0: Train Loss: 1.345736791336094e-06, Train Acc: 1.0\n",
            "Epoch 4640/10000\n",
            "Step 0: Train Loss: 1.574873522258713e-06, Train Acc: 1.0\n",
            "Epoch 4641/10000\n",
            "Step 0: Train Loss: 1.7169561488117324e-06, Train Acc: 1.0\n",
            "Epoch 4642/10000\n",
            "Step 0: Train Loss: 1.4606637250835774e-06, Train Acc: 1.0\n",
            "Epoch 4643/10000\n",
            "Step 0: Train Loss: 1.2501334367698291e-06, Train Acc: 1.0\n",
            "Epoch 4644/10000\n",
            "Step 0: Train Loss: 1.4205688785295933e-06, Train Acc: 1.0\n",
            "Epoch 4645/10000\n",
            "Step 0: Train Loss: 1.399138795932231e-06, Train Acc: 1.0\n",
            "Epoch 4646/10000\n",
            "Step 0: Train Loss: 1.431713371857768e-06, Train Acc: 1.0\n",
            "Epoch 4647/10000\n",
            "Step 0: Train Loss: 1.6363261465812684e-06, Train Acc: 1.0\n",
            "Epoch 4648/10000\n",
            "Step 0: Train Loss: 1.120639012697211e-06, Train Acc: 1.0\n",
            "Epoch 4649/10000\n",
            "Step 0: Train Loss: 1.4274737623054534e-06, Train Acc: 1.0\n",
            "Epoch 4650/10000\n",
            "Step 0: Train Loss: 1.5874095424806e-06, Train Acc: 1.0\n",
            "Epoch 4651/10000\n",
            "Step 0: Train Loss: 1.418901206307055e-06, Train Acc: 1.0\n",
            "Epoch 4652/10000\n",
            "Step 0: Train Loss: 1.51949734572554e-06, Train Acc: 1.0\n",
            "Epoch 4653/10000\n",
            "Step 0: Train Loss: 1.3922738162364112e-06, Train Acc: 1.0\n",
            "Epoch 4654/10000\n",
            "Step 0: Train Loss: 1.5230577901093056e-06, Train Acc: 1.0\n",
            "Epoch 4655/10000\n",
            "Step 0: Train Loss: 1.4020057506058947e-06, Train Acc: 1.0\n",
            "Epoch 4656/10000\n",
            "Step 0: Train Loss: 1.6908593352127355e-06, Train Acc: 1.0\n",
            "Epoch 4657/10000\n",
            "Step 0: Train Loss: 1.2231495247760904e-06, Train Acc: 1.0\n",
            "Epoch 4658/10000\n",
            "Step 0: Train Loss: 1.5273784583769157e-06, Train Acc: 1.0\n",
            "Epoch 4659/10000\n",
            "Step 0: Train Loss: 1.291054559260374e-06, Train Acc: 1.0\n",
            "Epoch 4660/10000\n",
            "Step 0: Train Loss: 1.5258790426742053e-06, Train Acc: 1.0\n",
            "Epoch 4661/10000\n",
            "Step 0: Train Loss: 1.3270395129438839e-06, Train Acc: 1.0\n",
            "Epoch 4662/10000\n",
            "Step 0: Train Loss: 1.3680316897080047e-06, Train Acc: 1.0\n",
            "Epoch 4663/10000\n",
            "Step 0: Train Loss: 1.4749161891813856e-06, Train Acc: 1.0\n",
            "Epoch 4664/10000\n",
            "Step 0: Train Loss: 1.337078288088378e-06, Train Acc: 1.0\n",
            "Epoch 4665/10000\n",
            "Step 0: Train Loss: 1.436900220141979e-06, Train Acc: 1.0\n",
            "Epoch 4666/10000\n",
            "Step 0: Train Loss: 1.389489170833258e-06, Train Acc: 1.0\n",
            "Epoch 4667/10000\n",
            "Step 0: Train Loss: 1.4053263157620677e-06, Train Acc: 1.0\n",
            "Epoch 4668/10000\n",
            "Step 0: Train Loss: 1.4230411125026876e-06, Train Acc: 1.0\n",
            "Epoch 4669/10000\n",
            "Step 0: Train Loss: 1.398744302605337e-06, Train Acc: 1.0\n",
            "Epoch 4670/10000\n",
            "Step 0: Train Loss: 1.3331984973774524e-06, Train Acc: 1.0\n",
            "Epoch 4671/10000\n",
            "Step 0: Train Loss: 1.3551569963965449e-06, Train Acc: 1.0\n",
            "Epoch 4672/10000\n",
            "Step 0: Train Loss: 1.5497757885896135e-06, Train Acc: 1.0\n",
            "Epoch 4673/10000\n",
            "Step 0: Train Loss: 1.5290437431758619e-06, Train Acc: 1.0\n",
            "Epoch 4674/10000\n",
            "Step 0: Train Loss: 1.4213802614904125e-06, Train Acc: 1.0\n",
            "Epoch 4675/10000\n",
            "Step 0: Train Loss: 1.3464068615576252e-06, Train Acc: 1.0\n",
            "Epoch 4676/10000\n",
            "Step 0: Train Loss: 1.5677347846576595e-06, Train Acc: 1.0\n",
            "Epoch 4677/10000\n",
            "Step 0: Train Loss: 1.2627592695935164e-06, Train Acc: 1.0\n",
            "Epoch 4678/10000\n",
            "Step 0: Train Loss: 1.5141166613830137e-06, Train Acc: 1.0\n",
            "Epoch 4679/10000\n",
            "Step 0: Train Loss: 1.393577463204565e-06, Train Acc: 1.0\n",
            "Epoch 4680/10000\n",
            "Step 0: Train Loss: 1.2062730547768297e-06, Train Acc: 1.0\n",
            "Epoch 4681/10000\n",
            "Step 0: Train Loss: 1.2450543636077782e-06, Train Acc: 1.0\n",
            "Epoch 4682/10000\n",
            "Step 0: Train Loss: 1.2049246151946136e-06, Train Acc: 1.0\n",
            "Epoch 4683/10000\n",
            "Step 0: Train Loss: 1.4928551763659925e-06, Train Acc: 1.0\n",
            "Epoch 4684/10000\n",
            "Step 0: Train Loss: 1.246729198101093e-06, Train Acc: 1.0\n",
            "Epoch 4685/10000\n",
            "Step 0: Train Loss: 1.3502817637345288e-06, Train Acc: 1.0\n",
            "Epoch 4686/10000\n",
            "Step 0: Train Loss: 1.2738518080368522e-06, Train Acc: 1.0\n",
            "Epoch 4687/10000\n",
            "Step 0: Train Loss: 1.471918039896991e-06, Train Acc: 1.0\n",
            "Epoch 4688/10000\n",
            "Step 0: Train Loss: 1.2820765959986602e-06, Train Acc: 1.0\n",
            "Epoch 4689/10000\n",
            "Step 0: Train Loss: 1.1190375062142266e-06, Train Acc: 1.0\n",
            "Epoch 4690/10000\n",
            "Step 0: Train Loss: 1.5578320926579181e-06, Train Acc: 1.0\n",
            "Epoch 4691/10000\n",
            "Step 0: Train Loss: 1.3325487770998734e-06, Train Acc: 1.0\n",
            "Epoch 4692/10000\n",
            "Step 0: Train Loss: 1.3527986766348477e-06, Train Acc: 1.0\n",
            "Epoch 4693/10000\n",
            "Step 0: Train Loss: 1.6823025816847803e-06, Train Acc: 1.0\n",
            "Epoch 4694/10000\n",
            "Step 0: Train Loss: 1.311546725446533e-06, Train Acc: 1.0\n",
            "Epoch 4695/10000\n",
            "Step 0: Train Loss: 1.2011972785330727e-06, Train Acc: 1.0\n",
            "Epoch 4696/10000\n",
            "Step 0: Train Loss: 1.295580659643747e-06, Train Acc: 1.0\n",
            "Epoch 4697/10000\n",
            "Step 0: Train Loss: 1.2054687203999492e-06, Train Acc: 1.0\n",
            "Epoch 4698/10000\n",
            "Step 0: Train Loss: 1.2151871260357439e-06, Train Acc: 1.0\n",
            "Epoch 4699/10000\n",
            "Step 0: Train Loss: 1.4272404769144487e-06, Train Acc: 1.0\n",
            "Epoch 4700/10000\n",
            "Step 0: Train Loss: 1.0892608770518564e-06, Train Acc: 1.0\n",
            "Epoch 4701/10000\n",
            "Step 0: Train Loss: 9.604721071809763e-07, Train Acc: 1.0\n",
            "Epoch 4702/10000\n",
            "Step 0: Train Loss: 1.10368671357719e-06, Train Acc: 1.0\n",
            "Epoch 4703/10000\n",
            "Step 0: Train Loss: 1.15524733246275e-06, Train Acc: 1.0\n",
            "Epoch 4704/10000\n",
            "Step 0: Train Loss: 1.2029661320411833e-06, Train Acc: 1.0\n",
            "Epoch 4705/10000\n",
            "Step 0: Train Loss: 1.35466973461007e-06, Train Acc: 1.0\n",
            "Epoch 4706/10000\n",
            "Step 0: Train Loss: 1.3472268847181113e-06, Train Acc: 1.0\n",
            "Epoch 4707/10000\n",
            "Step 0: Train Loss: 1.0888585393331596e-06, Train Acc: 1.0\n",
            "Epoch 4708/10000\n",
            "Step 0: Train Loss: 1.0489601436347584e-06, Train Acc: 1.0\n",
            "Epoch 4709/10000\n",
            "Step 0: Train Loss: 1.1001587836290128e-06, Train Acc: 1.0\n",
            "Epoch 4710/10000\n",
            "Step 0: Train Loss: 1.2883459703516564e-06, Train Acc: 1.0\n",
            "Epoch 4711/10000\n",
            "Step 0: Train Loss: 1.0404518206996727e-06, Train Acc: 1.0\n",
            "Epoch 4712/10000\n",
            "Step 0: Train Loss: 1.0718034673118382e-06, Train Acc: 1.0\n",
            "Epoch 4713/10000\n",
            "Step 0: Train Loss: 1.1900758636329556e-06, Train Acc: 1.0\n",
            "Epoch 4714/10000\n",
            "Step 0: Train Loss: 1.216733608089271e-06, Train Acc: 1.0\n",
            "Epoch 4715/10000\n",
            "Step 0: Train Loss: 1.2631705885723932e-06, Train Acc: 1.0\n",
            "Epoch 4716/10000\n",
            "Step 0: Train Loss: 1.0970804851240246e-06, Train Acc: 1.0\n",
            "Epoch 4717/10000\n",
            "Step 0: Train Loss: 1.3241118494988768e-06, Train Acc: 1.0\n",
            "Epoch 4718/10000\n",
            "Step 0: Train Loss: 1.041588234329538e-06, Train Acc: 1.0\n",
            "Epoch 4719/10000\n",
            "Step 0: Train Loss: 1.3741670272793272e-06, Train Acc: 1.0\n",
            "Epoch 4720/10000\n",
            "Step 0: Train Loss: 1.0426174412714317e-06, Train Acc: 1.0\n",
            "Epoch 4721/10000\n",
            "Step 0: Train Loss: 1.2197892829135526e-06, Train Acc: 1.0\n",
            "Epoch 4722/10000\n",
            "Step 0: Train Loss: 9.75766965893854e-07, Train Acc: 1.0\n",
            "Epoch 4723/10000\n",
            "Step 0: Train Loss: 1.373397253701114e-06, Train Acc: 1.0\n",
            "Epoch 4724/10000\n",
            "Step 0: Train Loss: 1.116939870371425e-06, Train Acc: 1.0\n",
            "Epoch 4725/10000\n",
            "Step 0: Train Loss: 1.144137286246405e-06, Train Acc: 1.0\n",
            "Epoch 4726/10000\n",
            "Step 0: Train Loss: 1.0043847851193277e-06, Train Acc: 1.0\n",
            "Epoch 4727/10000\n",
            "Step 0: Train Loss: 1.1073967698393972e-06, Train Acc: 1.0\n",
            "Epoch 4728/10000\n",
            "Step 0: Train Loss: 1.0715414191508899e-06, Train Acc: 1.0\n",
            "Epoch 4729/10000\n",
            "Step 0: Train Loss: 1.1928058256671648e-06, Train Acc: 1.0\n",
            "Epoch 4730/10000\n",
            "Step 0: Train Loss: 1.2605777328644763e-06, Train Acc: 1.0\n",
            "Epoch 4731/10000\n",
            "Step 0: Train Loss: 1.1025933872588212e-06, Train Acc: 1.0\n",
            "Epoch 4732/10000\n",
            "Step 0: Train Loss: 9.087165153687238e-07, Train Acc: 1.0\n",
            "Epoch 4733/10000\n",
            "Step 0: Train Loss: 1.2143875665060477e-06, Train Acc: 1.0\n",
            "Epoch 4734/10000\n",
            "Step 0: Train Loss: 1.0044450391433202e-06, Train Acc: 1.0\n",
            "Epoch 4735/10000\n",
            "Step 0: Train Loss: 1.046112288349832e-06, Train Acc: 1.0\n",
            "Epoch 4736/10000\n",
            "Step 0: Train Loss: 1.207806008096668e-06, Train Acc: 1.0\n",
            "Epoch 4737/10000\n",
            "Step 0: Train Loss: 9.566250582793145e-07, Train Acc: 1.0\n",
            "Epoch 4738/10000\n",
            "Step 0: Train Loss: 1.2176159316368285e-06, Train Acc: 1.0\n",
            "Epoch 4739/10000\n",
            "Step 0: Train Loss: 1.0934945748886093e-06, Train Acc: 1.0\n",
            "Epoch 4740/10000\n",
            "Step 0: Train Loss: 1.200272549795045e-06, Train Acc: 1.0\n",
            "Epoch 4741/10000\n",
            "Step 0: Train Loss: 9.700323744255002e-07, Train Acc: 1.0\n",
            "Epoch 4742/10000\n",
            "Step 0: Train Loss: 1.1290495649518562e-06, Train Acc: 1.0\n",
            "Epoch 4743/10000\n",
            "Step 0: Train Loss: 1.0788102144942968e-06, Train Acc: 1.0\n",
            "Epoch 4744/10000\n",
            "Step 0: Train Loss: 1.3007831967115635e-06, Train Acc: 1.0\n",
            "Epoch 4745/10000\n",
            "Step 0: Train Loss: 9.843121233643615e-07, Train Acc: 1.0\n",
            "Epoch 4746/10000\n",
            "Step 0: Train Loss: 1.0236918797090766e-06, Train Acc: 1.0\n",
            "Epoch 4747/10000\n",
            "Step 0: Train Loss: 9.561158549331594e-07, Train Acc: 1.0\n",
            "Epoch 4748/10000\n",
            "Step 0: Train Loss: 1.141940970228461e-06, Train Acc: 1.0\n",
            "Epoch 4749/10000\n",
            "Step 0: Train Loss: 1.10645305539947e-06, Train Acc: 1.0\n",
            "Epoch 4750/10000\n",
            "Step 0: Train Loss: 9.575929880156764e-07, Train Acc: 1.0\n",
            "Epoch 4751/10000\n",
            "Step 0: Train Loss: 1.0010768392021419e-06, Train Acc: 1.0\n",
            "Epoch 4752/10000\n",
            "Step 0: Train Loss: 1.0202794555880246e-06, Train Acc: 1.0\n",
            "Epoch 4753/10000\n",
            "Step 0: Train Loss: 9.269916176890547e-07, Train Acc: 1.0\n",
            "Epoch 4754/10000\n",
            "Step 0: Train Loss: 1.0899102562689222e-06, Train Acc: 1.0\n",
            "Epoch 4755/10000\n",
            "Step 0: Train Loss: 1.2318276958467322e-06, Train Acc: 1.0\n",
            "Epoch 4756/10000\n",
            "Step 0: Train Loss: 1.0748871090981993e-06, Train Acc: 1.0\n",
            "Epoch 4757/10000\n",
            "Step 0: Train Loss: 9.9475585102482e-07, Train Acc: 1.0\n",
            "Epoch 4758/10000\n",
            "Step 0: Train Loss: 1.0980830893458915e-06, Train Acc: 1.0\n",
            "Epoch 4759/10000\n",
            "Step 0: Train Loss: 8.273920002466184e-07, Train Acc: 1.0\n",
            "Epoch 4760/10000\n",
            "Step 0: Train Loss: 8.330124501298997e-07, Train Acc: 1.0\n",
            "Epoch 4761/10000\n",
            "Step 0: Train Loss: 1.0244699524264433e-06, Train Acc: 1.0\n",
            "Epoch 4762/10000\n",
            "Step 0: Train Loss: 1.0177586773352232e-06, Train Acc: 1.0\n",
            "Epoch 4763/10000\n",
            "Step 0: Train Loss: 1.0384019333287142e-06, Train Acc: 1.0\n",
            "Epoch 4764/10000\n",
            "Step 0: Train Loss: 1.0791635531859356e-06, Train Acc: 1.0\n",
            "Epoch 4765/10000\n",
            "Step 0: Train Loss: 9.470697364122316e-07, Train Acc: 1.0\n",
            "Epoch 4766/10000\n",
            "Step 0: Train Loss: 8.500239800923737e-07, Train Acc: 1.0\n",
            "Epoch 4767/10000\n",
            "Step 0: Train Loss: 9.448457376493025e-07, Train Acc: 1.0\n",
            "Epoch 4768/10000\n",
            "Step 0: Train Loss: 9.417735213901324e-07, Train Acc: 1.0\n",
            "Epoch 4769/10000\n",
            "Step 0: Train Loss: 9.975518651117454e-07, Train Acc: 1.0\n",
            "Epoch 4770/10000\n",
            "Step 0: Train Loss: 1.0563840078248177e-06, Train Acc: 1.0\n",
            "Epoch 4771/10000\n",
            "Step 0: Train Loss: 8.888626439329528e-07, Train Acc: 1.0\n",
            "Epoch 4772/10000\n",
            "Step 0: Train Loss: 9.22835795336141e-07, Train Acc: 1.0\n",
            "Epoch 4773/10000\n",
            "Step 0: Train Loss: 8.898425107872754e-07, Train Acc: 1.0\n",
            "Epoch 4774/10000\n",
            "Step 0: Train Loss: 9.769195230546757e-07, Train Acc: 1.0\n",
            "Epoch 4775/10000\n",
            "Step 0: Train Loss: 1.0438654953759396e-06, Train Acc: 1.0\n",
            "Epoch 4776/10000\n",
            "Step 0: Train Loss: 8.053642659433535e-07, Train Acc: 1.0\n",
            "Epoch 4777/10000\n",
            "Step 0: Train Loss: 1.1035320994778886e-06, Train Acc: 1.0\n",
            "Epoch 4778/10000\n",
            "Step 0: Train Loss: 1.0756948540802114e-06, Train Acc: 1.0\n",
            "Epoch 4779/10000\n",
            "Step 0: Train Loss: 1.1101274139946327e-06, Train Acc: 1.0\n",
            "Epoch 4780/10000\n",
            "Step 0: Train Loss: 8.303448453261808e-07, Train Acc: 1.0\n",
            "Epoch 4781/10000\n",
            "Step 0: Train Loss: 8.906275184017431e-07, Train Acc: 1.0\n",
            "Epoch 4782/10000\n",
            "Step 0: Train Loss: 9.724305982672377e-07, Train Acc: 1.0\n",
            "Epoch 4783/10000\n",
            "Step 0: Train Loss: 7.770402135065524e-07, Train Acc: 1.0\n",
            "Epoch 4784/10000\n",
            "Step 0: Train Loss: 8.541667853023682e-07, Train Acc: 1.0\n",
            "Epoch 4785/10000\n",
            "Step 0: Train Loss: 8.218156040129543e-07, Train Acc: 1.0\n",
            "Epoch 4786/10000\n",
            "Step 0: Train Loss: 7.963103030306229e-07, Train Acc: 1.0\n",
            "Epoch 4787/10000\n",
            "Step 0: Train Loss: 8.835247058414097e-07, Train Acc: 1.0\n",
            "Epoch 4788/10000\n",
            "Step 0: Train Loss: 7.708335374445596e-07, Train Acc: 1.0\n",
            "Epoch 4789/10000\n",
            "Step 0: Train Loss: 8.359523349099618e-07, Train Acc: 1.0\n",
            "Epoch 4790/10000\n",
            "Step 0: Train Loss: 8.683148848831479e-07, Train Acc: 1.0\n",
            "Epoch 4791/10000\n",
            "Step 0: Train Loss: 8.08058985057869e-07, Train Acc: 1.0\n",
            "Epoch 4792/10000\n",
            "Step 0: Train Loss: 9.937012919181143e-07, Train Acc: 1.0\n",
            "Epoch 4793/10000\n",
            "Step 0: Train Loss: 9.648096011005691e-07, Train Acc: 1.0\n",
            "Epoch 4794/10000\n",
            "Step 0: Train Loss: 9.009349355437735e-07, Train Acc: 1.0\n",
            "Epoch 4795/10000\n",
            "Step 0: Train Loss: 7.318454322557955e-07, Train Acc: 1.0\n",
            "Epoch 4796/10000\n",
            "Step 0: Train Loss: 8.677225764586183e-07, Train Acc: 1.0\n",
            "Epoch 4797/10000\n",
            "Step 0: Train Loss: 6.751845376129495e-07, Train Acc: 1.0\n",
            "Epoch 4798/10000\n",
            "Step 0: Train Loss: 8.690618642503978e-07, Train Acc: 1.0\n",
            "Epoch 4799/10000\n",
            "Step 0: Train Loss: 9.02586634765612e-07, Train Acc: 1.0\n",
            "Epoch 4800/10000\n",
            "Step 0: Train Loss: 8.962929314293433e-07, Train Acc: 1.0\n",
            "Epoch 4801/10000\n",
            "Step 0: Train Loss: 7.127495678105333e-07, Train Acc: 1.0\n",
            "Epoch 4802/10000\n",
            "Step 0: Train Loss: 6.762016937500448e-07, Train Acc: 1.0\n",
            "Epoch 4803/10000\n",
            "Step 0: Train Loss: 8.513237617080449e-07, Train Acc: 1.0\n",
            "Epoch 4804/10000\n",
            "Step 0: Train Loss: 8.379577707273711e-07, Train Acc: 1.0\n",
            "Epoch 4805/10000\n",
            "Step 0: Train Loss: 8.987213391264959e-07, Train Acc: 1.0\n",
            "Epoch 4806/10000\n",
            "Step 0: Train Loss: 8.014647505660832e-07, Train Acc: 1.0\n",
            "Epoch 4807/10000\n",
            "Step 0: Train Loss: 9.146830279860296e-07, Train Acc: 1.0\n",
            "Epoch 4808/10000\n",
            "Step 0: Train Loss: 8.296823921227769e-07, Train Acc: 1.0\n",
            "Epoch 4809/10000\n",
            "Step 0: Train Loss: 6.957903337934113e-07, Train Acc: 1.0\n",
            "Epoch 4810/10000\n",
            "Step 0: Train Loss: 7.707131430834124e-07, Train Acc: 1.0\n",
            "Epoch 4811/10000\n",
            "Step 0: Train Loss: 1.0052789320980082e-06, Train Acc: 1.0\n",
            "Epoch 4812/10000\n",
            "Step 0: Train Loss: 8.425109285781218e-07, Train Acc: 1.0\n",
            "Epoch 4813/10000\n",
            "Step 0: Train Loss: 8.140491445374209e-07, Train Acc: 1.0\n",
            "Epoch 4814/10000\n",
            "Step 0: Train Loss: 7.759141453789198e-07, Train Acc: 1.0\n",
            "Epoch 4815/10000\n",
            "Step 0: Train Loss: 7.782525699440157e-07, Train Acc: 1.0\n",
            "Epoch 4816/10000\n",
            "Step 0: Train Loss: 8.773709510023764e-07, Train Acc: 1.0\n",
            "Epoch 4817/10000\n",
            "Step 0: Train Loss: 6.783924959563592e-07, Train Acc: 1.0\n",
            "Epoch 4818/10000\n",
            "Step 0: Train Loss: 7.646675044270523e-07, Train Acc: 1.0\n",
            "Epoch 4819/10000\n",
            "Step 0: Train Loss: 8.856156910042046e-07, Train Acc: 1.0\n",
            "Epoch 4820/10000\n",
            "Step 0: Train Loss: 7.281927310032188e-07, Train Acc: 1.0\n",
            "Epoch 4821/10000\n",
            "Step 0: Train Loss: 7.790947051944386e-07, Train Acc: 1.0\n",
            "Epoch 4822/10000\n",
            "Step 0: Train Loss: 9.477174671701505e-07, Train Acc: 1.0\n",
            "Epoch 4823/10000\n",
            "Step 0: Train Loss: 7.325929800572339e-07, Train Acc: 1.0\n",
            "Epoch 4824/10000\n",
            "Step 0: Train Loss: 7.9414786569032e-07, Train Acc: 1.0\n",
            "Epoch 4825/10000\n",
            "Step 0: Train Loss: 8.542912155462545e-07, Train Acc: 1.0\n",
            "Epoch 4826/10000\n",
            "Step 0: Train Loss: 7.718440429016482e-07, Train Acc: 1.0\n",
            "Epoch 4827/10000\n",
            "Step 0: Train Loss: 8.345210744664655e-07, Train Acc: 1.0\n",
            "Epoch 4828/10000\n",
            "Step 0: Train Loss: 7.848254313103098e-07, Train Acc: 1.0\n",
            "Epoch 4829/10000\n",
            "Step 0: Train Loss: 9.123878612626868e-07, Train Acc: 1.0\n",
            "Epoch 4830/10000\n",
            "Step 0: Train Loss: 8.162183462445682e-07, Train Acc: 1.0\n",
            "Epoch 4831/10000\n",
            "Step 0: Train Loss: 7.461204631908913e-07, Train Acc: 1.0\n",
            "Epoch 4832/10000\n",
            "Step 0: Train Loss: 7.015272558419383e-07, Train Acc: 1.0\n",
            "Epoch 4833/10000\n",
            "Step 0: Train Loss: 9.5992254500743e-07, Train Acc: 1.0\n",
            "Epoch 4834/10000\n",
            "Step 0: Train Loss: 7.273577011801535e-07, Train Acc: 1.0\n",
            "Epoch 4835/10000\n",
            "Step 0: Train Loss: 7.116170763765695e-07, Train Acc: 1.0\n",
            "Epoch 4836/10000\n",
            "Step 0: Train Loss: 7.865367592785333e-07, Train Acc: 1.0\n",
            "Epoch 4837/10000\n",
            "Step 0: Train Loss: 8.022837505450298e-07, Train Acc: 1.0\n",
            "Epoch 4838/10000\n",
            "Step 0: Train Loss: 7.130080916795123e-07, Train Acc: 1.0\n",
            "Epoch 4839/10000\n",
            "Step 0: Train Loss: 7.052145178931823e-07, Train Acc: 1.0\n",
            "Epoch 4840/10000\n",
            "Step 0: Train Loss: 8.162750191331725e-07, Train Acc: 1.0\n",
            "Epoch 4841/10000\n",
            "Step 0: Train Loss: 8.361685104318894e-07, Train Acc: 1.0\n",
            "Epoch 4842/10000\n",
            "Step 0: Train Loss: 7.895357043707918e-07, Train Acc: 1.0\n",
            "Epoch 4843/10000\n",
            "Step 0: Train Loss: 6.617856342927553e-07, Train Acc: 1.0\n",
            "Epoch 4844/10000\n",
            "Step 0: Train Loss: 7.018490464361093e-07, Train Acc: 1.0\n",
            "Epoch 4845/10000\n",
            "Step 0: Train Loss: 7.36575486826041e-07, Train Acc: 1.0\n",
            "Epoch 4846/10000\n",
            "Step 0: Train Loss: 7.017259235908568e-07, Train Acc: 1.0\n",
            "Epoch 4847/10000\n",
            "Step 0: Train Loss: 7.563573944935342e-07, Train Acc: 1.0\n",
            "Epoch 4848/10000\n",
            "Step 0: Train Loss: 6.921417252669926e-07, Train Acc: 1.0\n",
            "Epoch 4849/10000\n",
            "Step 0: Train Loss: 7.520879989897367e-07, Train Acc: 1.0\n",
            "Epoch 4850/10000\n",
            "Step 0: Train Loss: 7.630466711816553e-07, Train Acc: 1.0\n",
            "Epoch 4851/10000\n",
            "Step 0: Train Loss: 7.818863991815306e-07, Train Acc: 1.0\n",
            "Epoch 4852/10000\n",
            "Step 0: Train Loss: 7.599433615723683e-07, Train Acc: 1.0\n",
            "Epoch 4853/10000\n",
            "Step 0: Train Loss: 7.029843800410163e-07, Train Acc: 1.0\n",
            "Epoch 4854/10000\n",
            "Step 0: Train Loss: 7.316131132029113e-07, Train Acc: 1.0\n",
            "Epoch 4855/10000\n",
            "Step 0: Train Loss: 8.177353265637066e-07, Train Acc: 1.0\n",
            "Epoch 4856/10000\n",
            "Step 0: Train Loss: 6.540358299389482e-07, Train Acc: 1.0\n",
            "Epoch 4857/10000\n",
            "Step 0: Train Loss: 7.643632784493093e-07, Train Acc: 1.0\n",
            "Epoch 4858/10000\n",
            "Step 0: Train Loss: 6.385944857356662e-07, Train Acc: 1.0\n",
            "Epoch 4859/10000\n",
            "Step 0: Train Loss: 6.715715699101565e-07, Train Acc: 1.0\n",
            "Epoch 4860/10000\n",
            "Step 0: Train Loss: 7.5253342401993e-07, Train Acc: 1.0\n",
            "Epoch 4861/10000\n",
            "Step 0: Train Loss: 6.459546284531825e-07, Train Acc: 1.0\n",
            "Epoch 4862/10000\n",
            "Step 0: Train Loss: 7.975609719323984e-07, Train Acc: 1.0\n",
            "Epoch 4863/10000\n",
            "Step 0: Train Loss: 6.669890808552736e-07, Train Acc: 1.0\n",
            "Epoch 4864/10000\n",
            "Step 0: Train Loss: 7.031184736661089e-07, Train Acc: 1.0\n",
            "Epoch 4865/10000\n",
            "Step 0: Train Loss: 6.745821110598627e-07, Train Acc: 1.0\n",
            "Epoch 4866/10000\n",
            "Step 0: Train Loss: 7.917906259535812e-07, Train Acc: 1.0\n",
            "Epoch 4867/10000\n",
            "Step 0: Train Loss: 7.230819960568624e-07, Train Acc: 1.0\n",
            "Epoch 4868/10000\n",
            "Step 0: Train Loss: 8.294471740555309e-07, Train Acc: 1.0\n",
            "Epoch 4869/10000\n",
            "Step 0: Train Loss: 6.254880986489297e-07, Train Acc: 1.0\n",
            "Epoch 4870/10000\n",
            "Step 0: Train Loss: 6.167987294247723e-07, Train Acc: 1.0\n",
            "Epoch 4871/10000\n",
            "Step 0: Train Loss: 6.199395556905074e-07, Train Acc: 1.0\n",
            "Epoch 4872/10000\n",
            "Step 0: Train Loss: 6.146909754534136e-07, Train Acc: 1.0\n",
            "Epoch 4873/10000\n",
            "Step 0: Train Loss: 6.239802701202279e-07, Train Acc: 1.0\n",
            "Epoch 4874/10000\n",
            "Step 0: Train Loss: 6.494523745459446e-07, Train Acc: 1.0\n",
            "Epoch 4875/10000\n",
            "Step 0: Train Loss: 5.332319688022835e-07, Train Acc: 1.0\n",
            "Epoch 4876/10000\n",
            "Step 0: Train Loss: 6.963549594729557e-07, Train Acc: 1.0\n",
            "Epoch 4877/10000\n",
            "Step 0: Train Loss: 5.925427899455826e-07, Train Acc: 1.0\n",
            "Epoch 4878/10000\n",
            "Step 0: Train Loss: 5.955447477390408e-07, Train Acc: 1.0\n",
            "Epoch 4879/10000\n",
            "Step 0: Train Loss: 6.009224762237864e-07, Train Acc: 1.0\n",
            "Epoch 4880/10000\n",
            "Step 0: Train Loss: 6.34964067103283e-07, Train Acc: 1.0\n",
            "Epoch 4881/10000\n",
            "Step 0: Train Loss: 6.343440190903493e-07, Train Acc: 1.0\n",
            "Epoch 4882/10000\n",
            "Step 0: Train Loss: 5.886217309125641e-07, Train Acc: 1.0\n",
            "Epoch 4883/10000\n",
            "Step 0: Train Loss: 5.97724579165515e-07, Train Acc: 1.0\n",
            "Epoch 4884/10000\n",
            "Step 0: Train Loss: 6.430258849832171e-07, Train Acc: 1.0\n",
            "Epoch 4885/10000\n",
            "Step 0: Train Loss: 6.423371701202996e-07, Train Acc: 1.0\n",
            "Epoch 4886/10000\n",
            "Step 0: Train Loss: 6.452834213632741e-07, Train Acc: 1.0\n",
            "Epoch 4887/10000\n",
            "Step 0: Train Loss: 7.462724056495063e-07, Train Acc: 1.0\n",
            "Epoch 4888/10000\n",
            "Step 0: Train Loss: 6.927503477527353e-07, Train Acc: 1.0\n",
            "Epoch 4889/10000\n",
            "Step 0: Train Loss: 6.807193813074264e-07, Train Acc: 1.0\n",
            "Epoch 4890/10000\n",
            "Step 0: Train Loss: 5.551559638661274e-07, Train Acc: 1.0\n",
            "Epoch 4891/10000\n",
            "Step 0: Train Loss: 5.617879992314556e-07, Train Acc: 1.0\n",
            "Epoch 4892/10000\n",
            "Step 0: Train Loss: 6.517267934214033e-07, Train Acc: 1.0\n",
            "Epoch 4893/10000\n",
            "Step 0: Train Loss: 6.538093089147878e-07, Train Acc: 1.0\n",
            "Epoch 4894/10000\n",
            "Step 0: Train Loss: 6.192968271534482e-07, Train Acc: 1.0\n",
            "Epoch 4895/10000\n",
            "Step 0: Train Loss: 5.080981395622075e-07, Train Acc: 1.0\n",
            "Epoch 4896/10000\n",
            "Step 0: Train Loss: 7.083813784447557e-07, Train Acc: 1.0\n",
            "Epoch 4897/10000\n",
            "Step 0: Train Loss: 6.299032975221053e-07, Train Acc: 1.0\n",
            "Epoch 4898/10000\n",
            "Step 0: Train Loss: 5.86712189942773e-07, Train Acc: 1.0\n",
            "Epoch 4899/10000\n",
            "Step 0: Train Loss: 7.111388526936935e-07, Train Acc: 1.0\n",
            "Epoch 4900/10000\n",
            "Step 0: Train Loss: 6.826282401561912e-07, Train Acc: 1.0\n",
            "Epoch 4901/10000\n",
            "Step 0: Train Loss: 6.084898700464692e-07, Train Acc: 1.0\n",
            "Epoch 4902/10000\n",
            "Step 0: Train Loss: 5.737700803365442e-07, Train Acc: 1.0\n",
            "Epoch 4903/10000\n",
            "Step 0: Train Loss: 4.8195295221376e-07, Train Acc: 1.0\n",
            "Epoch 4904/10000\n",
            "Step 0: Train Loss: 6.67878794047283e-07, Train Acc: 1.0\n",
            "Epoch 4905/10000\n",
            "Step 0: Train Loss: 6.112736627983395e-07, Train Acc: 1.0\n",
            "Epoch 4906/10000\n",
            "Step 0: Train Loss: 6.84786584770336e-07, Train Acc: 1.0\n",
            "Epoch 4907/10000\n",
            "Step 0: Train Loss: 6.626495974160207e-07, Train Acc: 1.0\n",
            "Epoch 4908/10000\n",
            "Step 0: Train Loss: 6.830007919234049e-07, Train Acc: 1.0\n",
            "Epoch 4909/10000\n",
            "Step 0: Train Loss: 6.254674644878833e-07, Train Acc: 1.0\n",
            "Epoch 4910/10000\n",
            "Step 0: Train Loss: 5.462156877911184e-07, Train Acc: 1.0\n",
            "Epoch 4911/10000\n",
            "Step 0: Train Loss: 5.774151645709935e-07, Train Acc: 1.0\n",
            "Epoch 4912/10000\n",
            "Step 0: Train Loss: 6.583432536899636e-07, Train Acc: 1.0\n",
            "Epoch 4913/10000\n",
            "Step 0: Train Loss: 5.389505304265185e-07, Train Acc: 1.0\n",
            "Epoch 4914/10000\n",
            "Step 0: Train Loss: 6.169113362375356e-07, Train Acc: 1.0\n",
            "Epoch 4915/10000\n",
            "Step 0: Train Loss: 5.246469072517357e-07, Train Acc: 1.0\n",
            "Epoch 4916/10000\n",
            "Step 0: Train Loss: 5.651455126098881e-07, Train Acc: 1.0\n",
            "Epoch 4917/10000\n",
            "Step 0: Train Loss: 5.391589184000622e-07, Train Acc: 1.0\n",
            "Epoch 4918/10000\n",
            "Step 0: Train Loss: 6.061760018383211e-07, Train Acc: 1.0\n",
            "Epoch 4919/10000\n",
            "Step 0: Train Loss: 6.395675882231444e-07, Train Acc: 1.0\n",
            "Epoch 4920/10000\n",
            "Step 0: Train Loss: 5.563209697356797e-07, Train Acc: 1.0\n",
            "Epoch 4921/10000\n",
            "Step 0: Train Loss: 5.556991595767613e-07, Train Acc: 1.0\n",
            "Epoch 4922/10000\n",
            "Step 0: Train Loss: 5.64209528874926e-07, Train Acc: 1.0\n",
            "Epoch 4923/10000\n",
            "Step 0: Train Loss: 5.287519684316067e-07, Train Acc: 1.0\n",
            "Epoch 4924/10000\n",
            "Step 0: Train Loss: 6.722578973494819e-07, Train Acc: 1.0\n",
            "Epoch 4925/10000\n",
            "Step 0: Train Loss: 5.297663960845966e-07, Train Acc: 1.0\n",
            "Epoch 4926/10000\n",
            "Step 0: Train Loss: 5.017143394070445e-07, Train Acc: 1.0\n",
            "Epoch 4927/10000\n",
            "Step 0: Train Loss: 5.788369890069589e-07, Train Acc: 1.0\n",
            "Epoch 4928/10000\n",
            "Step 0: Train Loss: 5.485799192683771e-07, Train Acc: 1.0\n",
            "Epoch 4929/10000\n",
            "Step 0: Train Loss: 5.789829629065935e-07, Train Acc: 1.0\n",
            "Epoch 4930/10000\n",
            "Step 0: Train Loss: 5.414673296399997e-07, Train Acc: 1.0\n",
            "Epoch 4931/10000\n",
            "Step 0: Train Loss: 5.928179120928689e-07, Train Acc: 1.0\n",
            "Epoch 4932/10000\n",
            "Step 0: Train Loss: 5.299224312693696e-07, Train Acc: 1.0\n",
            "Epoch 4933/10000\n",
            "Step 0: Train Loss: 6.346272130031139e-07, Train Acc: 1.0\n",
            "Epoch 4934/10000\n",
            "Step 0: Train Loss: 6.59051238471875e-07, Train Acc: 1.0\n",
            "Epoch 4935/10000\n",
            "Step 0: Train Loss: 5.428544227470411e-07, Train Acc: 1.0\n",
            "Epoch 4936/10000\n",
            "Step 0: Train Loss: 4.918774720863439e-07, Train Acc: 1.0\n",
            "Epoch 4937/10000\n",
            "Step 0: Train Loss: 5.691691171705315e-07, Train Acc: 1.0\n",
            "Epoch 4938/10000\n",
            "Step 0: Train Loss: 5.445892270472541e-07, Train Acc: 1.0\n",
            "Epoch 4939/10000\n",
            "Step 0: Train Loss: 5.078871367913962e-07, Train Acc: 1.0\n",
            "Epoch 4940/10000\n",
            "Step 0: Train Loss: 4.980706194146478e-07, Train Acc: 1.0\n",
            "Epoch 4941/10000\n",
            "Step 0: Train Loss: 5.42874488473899e-07, Train Acc: 1.0\n",
            "Epoch 4942/10000\n",
            "Step 0: Train Loss: 5.415201371761214e-07, Train Acc: 1.0\n",
            "Epoch 4943/10000\n",
            "Step 0: Train Loss: 5.337808488548035e-07, Train Acc: 1.0\n",
            "Epoch 4944/10000\n",
            "Step 0: Train Loss: 5.345330578165886e-07, Train Acc: 1.0\n",
            "Epoch 4945/10000\n",
            "Step 0: Train Loss: 4.957437340635806e-07, Train Acc: 1.0\n",
            "Epoch 4946/10000\n",
            "Step 0: Train Loss: 4.946220428792003e-07, Train Acc: 1.0\n",
            "Epoch 4947/10000\n",
            "Step 0: Train Loss: 5.364480557545903e-07, Train Acc: 1.0\n",
            "Epoch 4948/10000\n",
            "Step 0: Train Loss: 4.840239853365347e-07, Train Acc: 1.0\n",
            "Epoch 4949/10000\n",
            "Step 0: Train Loss: 5.470319592859596e-07, Train Acc: 1.0\n",
            "Epoch 4950/10000\n",
            "Step 0: Train Loss: 4.818656407223898e-07, Train Acc: 1.0\n",
            "Epoch 4951/10000\n",
            "Step 0: Train Loss: 4.86864962567779e-07, Train Acc: 1.0\n",
            "Epoch 4952/10000\n",
            "Step 0: Train Loss: 4.4173131641400687e-07, Train Acc: 1.0\n",
            "Epoch 4953/10000\n",
            "Step 0: Train Loss: 4.99285079058609e-07, Train Acc: 1.0\n",
            "Epoch 4954/10000\n",
            "Step 0: Train Loss: 4.068674570589792e-07, Train Acc: 1.0\n",
            "Epoch 4955/10000\n",
            "Step 0: Train Loss: 4.775095021614106e-07, Train Acc: 1.0\n",
            "Epoch 4956/10000\n",
            "Step 0: Train Loss: 5.407341063801141e-07, Train Acc: 1.0\n",
            "Epoch 4957/10000\n",
            "Step 0: Train Loss: 4.782326641361578e-07, Train Acc: 1.0\n",
            "Epoch 4958/10000\n",
            "Step 0: Train Loss: 4.8650315420673e-07, Train Acc: 1.0\n",
            "Epoch 4959/10000\n",
            "Step 0: Train Loss: 4.925981897940801e-07, Train Acc: 1.0\n",
            "Epoch 4960/10000\n",
            "Step 0: Train Loss: 5.592887077909836e-07, Train Acc: 1.0\n",
            "Epoch 4961/10000\n",
            "Step 0: Train Loss: 5.311388804329908e-07, Train Acc: 1.0\n",
            "Epoch 4962/10000\n",
            "Step 0: Train Loss: 4.65971965013523e-07, Train Acc: 1.0\n",
            "Epoch 4963/10000\n",
            "Step 0: Train Loss: 5.126066753291525e-07, Train Acc: 1.0\n",
            "Epoch 4964/10000\n",
            "Step 0: Train Loss: 5.350297556105943e-07, Train Acc: 1.0\n",
            "Epoch 4965/10000\n",
            "Step 0: Train Loss: 5.964001843494771e-07, Train Acc: 1.0\n",
            "Epoch 4966/10000\n",
            "Step 0: Train Loss: 4.81042661704123e-07, Train Acc: 1.0\n",
            "Epoch 4967/10000\n",
            "Step 0: Train Loss: 5.340021402844286e-07, Train Acc: 1.0\n",
            "Epoch 4968/10000\n",
            "Step 0: Train Loss: 5.316913984643179e-07, Train Acc: 1.0\n",
            "Epoch 4969/10000\n",
            "Step 0: Train Loss: 4.833960929317982e-07, Train Acc: 1.0\n",
            "Epoch 4970/10000\n",
            "Step 0: Train Loss: 4.840836140829197e-07, Train Acc: 1.0\n",
            "Epoch 4971/10000\n",
            "Step 0: Train Loss: 4.6042040935390105e-07, Train Acc: 1.0\n",
            "Epoch 4972/10000\n",
            "Step 0: Train Loss: 5.080618166175555e-07, Train Acc: 1.0\n",
            "Epoch 4973/10000\n",
            "Step 0: Train Loss: 3.988316166214645e-07, Train Acc: 1.0\n",
            "Epoch 4974/10000\n",
            "Step 0: Train Loss: 4.6360361238839687e-07, Train Acc: 1.0\n",
            "Epoch 4975/10000\n",
            "Step 0: Train Loss: 4.1641536085990083e-07, Train Acc: 1.0\n",
            "Epoch 4976/10000\n",
            "Step 0: Train Loss: 4.539841427231295e-07, Train Acc: 1.0\n",
            "Epoch 4977/10000\n",
            "Step 0: Train Loss: 4.7172457584565564e-07, Train Acc: 1.0\n",
            "Epoch 4978/10000\n",
            "Step 0: Train Loss: 4.802529929293087e-07, Train Acc: 1.0\n",
            "Epoch 4979/10000\n",
            "Step 0: Train Loss: 5.114490022606333e-07, Train Acc: 1.0\n",
            "Epoch 4980/10000\n",
            "Step 0: Train Loss: 4.52093729563785e-07, Train Acc: 1.0\n",
            "Epoch 4981/10000\n",
            "Step 0: Train Loss: 4.5654229552383185e-07, Train Acc: 1.0\n",
            "Epoch 4982/10000\n",
            "Step 0: Train Loss: 4.223960843319219e-07, Train Acc: 1.0\n",
            "Epoch 4983/10000\n",
            "Step 0: Train Loss: 4.674191131925909e-07, Train Acc: 1.0\n",
            "Epoch 4984/10000\n",
            "Step 0: Train Loss: 5.068873747404723e-07, Train Acc: 1.0\n",
            "Epoch 4985/10000\n",
            "Step 0: Train Loss: 4.167858094206167e-07, Train Acc: 1.0\n",
            "Epoch 4986/10000\n",
            "Step 0: Train Loss: 4.969418796463287e-07, Train Acc: 1.0\n",
            "Epoch 4987/10000\n",
            "Step 0: Train Loss: 3.83033778916797e-07, Train Acc: 1.0\n",
            "Epoch 4988/10000\n",
            "Step 0: Train Loss: 5.315150133355928e-07, Train Acc: 1.0\n",
            "Epoch 4989/10000\n",
            "Step 0: Train Loss: 4.4741761939803837e-07, Train Acc: 1.0\n",
            "Epoch 4990/10000\n",
            "Step 0: Train Loss: 4.5870643816670054e-07, Train Acc: 1.0\n",
            "Epoch 4991/10000\n",
            "Step 0: Train Loss: 4.189272431176505e-07, Train Acc: 1.0\n",
            "Epoch 4992/10000\n",
            "Step 0: Train Loss: 4.3774164737442334e-07, Train Acc: 1.0\n",
            "Epoch 4993/10000\n",
            "Step 0: Train Loss: 4.6036626599743613e-07, Train Acc: 1.0\n",
            "Epoch 4994/10000\n",
            "Step 0: Train Loss: 4.072449257819244e-07, Train Acc: 1.0\n",
            "Epoch 4995/10000\n",
            "Step 0: Train Loss: 4.6997968183859484e-07, Train Acc: 1.0\n",
            "Epoch 4996/10000\n",
            "Step 0: Train Loss: 5.343671318769339e-07, Train Acc: 1.0\n",
            "Epoch 4997/10000\n",
            "Step 0: Train Loss: 4.68610323878238e-07, Train Acc: 1.0\n",
            "Epoch 4998/10000\n",
            "Step 0: Train Loss: 4.7509510636700725e-07, Train Acc: 1.0\n",
            "Epoch 4999/10000\n",
            "Step 0: Train Loss: 3.8036785099393455e-07, Train Acc: 1.0\n",
            "Epoch 5000/10000\n",
            "Step 0: Train Loss: 5.006483547731477e-07, Train Acc: 1.0\n",
            "Epoch 5001/10000\n",
            "Step 0: Train Loss: 3.8596701301685243e-07, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 5000 1024 1.0073732625508351\n",
            "Epoch index and hidden dimension and ratio: 5000 20 0.9512170726685385\n",
            "Epoch index and hidden dimension and ratio: 5000 20 1.432827526842404\n",
            "Epoch index and hidden dimension and ratio: 5000 20 3.5449705114030223\n",
            "MI(X;T): [10.62388244298057, 7.797802877118986, 5.439452036257919, 3.0007999636360037], MI(Y;T): [2.624254697466119, 3.18373114780215, 3.1954663832709445, 2.9670835472608603]\n",
            "Epoch index and hidden dimension and ratio: 5000 1024 1.0073721612091147\n",
            "Epoch index and hidden dimension and ratio: 5000 20 0.9512121909694642\n",
            "Epoch index and hidden dimension and ratio: 5000 20 1.4327630314364321\n",
            "Epoch index and hidden dimension and ratio: 5000 20 3.5446977407499203\n",
            "MI(X;T): [10.624013620763893, 7.797315988129183, 5.439877069606504, 3.000585001019983], MI(Y;T): [2.624134516919004, 3.1836177954621725, 3.195404045578815, 2.9674209510041396]\n",
            "Epoch index and hidden dimension and ratio: 5000 1024 1.007368857183953\n",
            "Epoch index and hidden dimension and ratio: 5000 20 0.9512164217753286\n",
            "Epoch index and hidden dimension and ratio: 5000 20 1.4327471615038603\n",
            "Epoch index and hidden dimension and ratio: 5000 20 3.5446733921029607\n",
            "MI(X;T): [10.623751701423057, 7.79745425686883, 5.439231651566555, 2.997914734369734], MI(Y;T): [2.624159975032632, 3.183617795462172, 3.1953092841534128, 2.967303838614956]\n",
            "Epoch index and hidden dimension and ratio: 5000 1024 1.0073691508750784\n",
            "Epoch index and hidden dimension and ratio: 5000 20 0.9512218458854111\n",
            "Epoch index and hidden dimension and ratio: 5000 20 1.4327505894092958\n",
            "Epoch index and hidden dimension and ratio: 5000 20 3.5447030053222357\n",
            "MI(X;T): [10.62377866479839, 7.799188578614157, 5.437423750624653, 2.995877477297424], MI(Y;T): [2.624159975032632, 3.1838755658918227, 3.1952072421424162, 2.9671748760383907]\n",
            "Epoch index and hidden dimension and ratio: 5000 1024 1.007371426981301\n",
            "Epoch index and hidden dimension and ratio: 5000 20 0.951234321338601\n",
            "Epoch index and hidden dimension and ratio: 5000 20 1.4327622696796687\n",
            "Epoch index and hidden dimension and ratio: 5000 20 3.5447332766130506\n",
            "MI(X;T): [10.623724076713302, 7.799289498446189, 5.437436421471945, 2.996500759713985], MI(Y;T): [2.624148813208546, 3.183845189953815, 3.19555027828389, 2.9670309867217615]\n",
            "Epoch index and hidden dimension and ratio: 5000 1024 1.0073737030875234\n",
            "Epoch index and hidden dimension and ratio: 5000 20 0.9512480985782106\n",
            "Epoch index and hidden dimension and ratio: 5000 20 1.4327917242745218\n",
            "Epoch index and hidden dimension and ratio: 5000 20 3.544810600018936\n",
            "MI(X;T): [10.623791325611698, 7.7996084139527255, 5.43974535765113, 2.996577810148763], MI(Y;T): [2.6240154987677062, 3.1836011607277523, 3.1955761431410865, 2.9669091286468885]\n",
            "Epoch 5002/10000\n",
            "Step 0: Train Loss: 3.810047530805605e-07, Train Acc: 1.0\n",
            "Epoch 5003/10000\n",
            "Step 0: Train Loss: 4.0588653860140766e-07, Train Acc: 1.0\n",
            "Epoch 5004/10000\n",
            "Step 0: Train Loss: 3.98223335196235e-07, Train Acc: 1.0\n",
            "Epoch 5005/10000\n",
            "Step 0: Train Loss: 4.4101625462644733e-07, Train Acc: 1.0\n",
            "Epoch 5006/10000\n",
            "Step 0: Train Loss: 4.239342104028765e-07, Train Acc: 1.0\n",
            "Epoch 5007/10000\n",
            "Step 0: Train Loss: 4.2075259898410877e-07, Train Acc: 1.0\n",
            "Epoch 5008/10000\n",
            "Step 0: Train Loss: 4.186457829291612e-07, Train Acc: 1.0\n",
            "Epoch 5009/10000\n",
            "Step 0: Train Loss: 4.343276032159338e-07, Train Acc: 1.0\n",
            "Epoch 5010/10000\n",
            "Step 0: Train Loss: 4.1002357420438784e-07, Train Acc: 1.0\n",
            "Epoch 5011/10000\n",
            "Step 0: Train Loss: 4.23129080218132e-07, Train Acc: 1.0\n",
            "Epoch 5012/10000\n",
            "Step 0: Train Loss: 4.524327437138709e-07, Train Acc: 1.0\n",
            "Epoch 5013/10000\n",
            "Step 0: Train Loss: 4.1433787600908545e-07, Train Acc: 1.0\n",
            "Epoch 5014/10000\n",
            "Step 0: Train Loss: 4.3024979845540656e-07, Train Acc: 1.0\n",
            "Epoch 5015/10000\n",
            "Step 0: Train Loss: 4.282476879780006e-07, Train Acc: 1.0\n",
            "Epoch 5016/10000\n",
            "Step 0: Train Loss: 4.892293645752943e-07, Train Acc: 1.0\n",
            "Epoch 5017/10000\n",
            "Step 0: Train Loss: 4.6049407842474466e-07, Train Acc: 1.0\n",
            "Epoch 5018/10000\n",
            "Step 0: Train Loss: 3.8423848991442355e-07, Train Acc: 1.0\n",
            "Epoch 5019/10000\n",
            "Step 0: Train Loss: 3.035257236660982e-07, Train Acc: 1.0\n",
            "Epoch 5020/10000\n",
            "Step 0: Train Loss: 4.5348107846621133e-07, Train Acc: 1.0\n",
            "Epoch 5021/10000\n",
            "Step 0: Train Loss: 4.923210212837148e-07, Train Acc: 1.0\n",
            "Epoch 5022/10000\n",
            "Step 0: Train Loss: 4.034293397126021e-07, Train Acc: 1.0\n",
            "Epoch 5023/10000\n",
            "Step 0: Train Loss: 3.5625220107249334e-07, Train Acc: 1.0\n",
            "Epoch 5024/10000\n",
            "Step 0: Train Loss: 4.588897013491078e-07, Train Acc: 1.0\n",
            "Epoch 5025/10000\n",
            "Step 0: Train Loss: 3.7790996998410264e-07, Train Acc: 1.0\n",
            "Epoch 5026/10000\n",
            "Step 0: Train Loss: 3.9733328094371245e-07, Train Acc: 1.0\n",
            "Epoch 5027/10000\n",
            "Step 0: Train Loss: 3.8112739275675267e-07, Train Acc: 1.0\n",
            "Epoch 5028/10000\n",
            "Step 0: Train Loss: 3.6550906656884763e-07, Train Acc: 1.0\n",
            "Epoch 5029/10000\n",
            "Step 0: Train Loss: 4.0900167164181767e-07, Train Acc: 1.0\n",
            "Epoch 5030/10000\n",
            "Step 0: Train Loss: 4.15777606121992e-07, Train Acc: 1.0\n",
            "Epoch 5031/10000\n",
            "Step 0: Train Loss: 4.0046242588687164e-07, Train Acc: 1.0\n",
            "Epoch 5032/10000\n",
            "Step 0: Train Loss: 4.2699639379861765e-07, Train Acc: 1.0\n",
            "Epoch 5033/10000\n",
            "Step 0: Train Loss: 4.106292976757686e-07, Train Acc: 1.0\n",
            "Epoch 5034/10000\n",
            "Step 0: Train Loss: 3.140766580145282e-07, Train Acc: 1.0\n",
            "Epoch 5035/10000\n",
            "Step 0: Train Loss: 3.764672271699965e-07, Train Acc: 1.0\n",
            "Epoch 5036/10000\n",
            "Step 0: Train Loss: 4.033334448649839e-07, Train Acc: 1.0\n",
            "Epoch 5037/10000\n",
            "Step 0: Train Loss: 4.829799991057371e-07, Train Acc: 1.0\n",
            "Epoch 5038/10000\n",
            "Step 0: Train Loss: 3.351627810843638e-07, Train Acc: 1.0\n",
            "Epoch 5039/10000\n",
            "Step 0: Train Loss: 3.536656834057794e-07, Train Acc: 1.0\n",
            "Epoch 5040/10000\n",
            "Step 0: Train Loss: 3.5689274113792635e-07, Train Acc: 1.0\n",
            "Epoch 5041/10000\n",
            "Step 0: Train Loss: 3.7968601418469916e-07, Train Acc: 1.0\n",
            "Epoch 5042/10000\n",
            "Step 0: Train Loss: 3.2768869573374104e-07, Train Acc: 1.0\n",
            "Epoch 5043/10000\n",
            "Step 0: Train Loss: 4.4162069912090374e-07, Train Acc: 1.0\n",
            "Epoch 5044/10000\n",
            "Step 0: Train Loss: 3.6048172091796005e-07, Train Acc: 1.0\n",
            "Epoch 5045/10000\n",
            "Step 0: Train Loss: 3.8511933553309063e-07, Train Acc: 1.0\n",
            "Epoch 5046/10000\n",
            "Step 0: Train Loss: 3.596331339394965e-07, Train Acc: 1.0\n",
            "Epoch 5047/10000\n",
            "Step 0: Train Loss: 3.83164262984792e-07, Train Acc: 1.0\n",
            "Epoch 5048/10000\n",
            "Step 0: Train Loss: 3.3189633086294634e-07, Train Acc: 1.0\n",
            "Epoch 5049/10000\n",
            "Step 0: Train Loss: 3.549241114342294e-07, Train Acc: 1.0\n",
            "Epoch 5050/10000\n",
            "Step 0: Train Loss: 3.5453348345981794e-07, Train Acc: 1.0\n",
            "Epoch 5051/10000\n",
            "Step 0: Train Loss: 3.698165187415725e-07, Train Acc: 1.0\n",
            "Epoch 5052/10000\n",
            "Step 0: Train Loss: 4.4924729536433006e-07, Train Acc: 1.0\n",
            "Epoch 5053/10000\n",
            "Step 0: Train Loss: 3.570731337276811e-07, Train Acc: 1.0\n",
            "Epoch 5054/10000\n",
            "Step 0: Train Loss: 3.308956877390301e-07, Train Acc: 1.0\n",
            "Epoch 5055/10000\n",
            "Step 0: Train Loss: 4.3461099608066434e-07, Train Acc: 1.0\n",
            "Epoch 5056/10000\n",
            "Step 0: Train Loss: 3.6775335843231005e-07, Train Acc: 1.0\n",
            "Epoch 5057/10000\n",
            "Step 0: Train Loss: 3.024779857696558e-07, Train Acc: 1.0\n",
            "Epoch 5058/10000\n",
            "Step 0: Train Loss: 3.1796284361007565e-07, Train Acc: 1.0\n",
            "Epoch 5059/10000\n",
            "Step 0: Train Loss: 3.8067381069595285e-07, Train Acc: 1.0\n",
            "Epoch 5060/10000\n",
            "Step 0: Train Loss: 3.700777710946568e-07, Train Acc: 1.0\n",
            "Epoch 5061/10000\n",
            "Step 0: Train Loss: 3.499052354527521e-07, Train Acc: 1.0\n",
            "Epoch 5062/10000\n",
            "Step 0: Train Loss: 3.3403065913262253e-07, Train Acc: 1.0\n",
            "Epoch 5063/10000\n",
            "Step 0: Train Loss: 3.756306909963314e-07, Train Acc: 1.0\n",
            "Epoch 5064/10000\n",
            "Step 0: Train Loss: 2.9005670398873917e-07, Train Acc: 1.0\n",
            "Epoch 5065/10000\n",
            "Step 0: Train Loss: 3.2718483566895884e-07, Train Acc: 1.0\n",
            "Epoch 5066/10000\n",
            "Step 0: Train Loss: 3.5041071555497183e-07, Train Acc: 1.0\n",
            "Epoch 5067/10000\n",
            "Step 0: Train Loss: 3.380231419214397e-07, Train Acc: 1.0\n",
            "Epoch 5068/10000\n",
            "Step 0: Train Loss: 3.867527027523465e-07, Train Acc: 1.0\n",
            "Epoch 5069/10000\n",
            "Step 0: Train Loss: 2.969453021250956e-07, Train Acc: 1.0\n",
            "Epoch 5070/10000\n",
            "Step 0: Train Loss: 3.3771340213206713e-07, Train Acc: 1.0\n",
            "Epoch 5071/10000\n",
            "Step 0: Train Loss: 3.1654298027206096e-07, Train Acc: 1.0\n",
            "Epoch 5072/10000\n",
            "Step 0: Train Loss: 3.218328004095383e-07, Train Acc: 1.0\n",
            "Epoch 5073/10000\n",
            "Step 0: Train Loss: 3.2715482234380033e-07, Train Acc: 1.0\n",
            "Epoch 5074/10000\n",
            "Step 0: Train Loss: 3.317854009310395e-07, Train Acc: 1.0\n",
            "Epoch 5075/10000\n",
            "Step 0: Train Loss: 3.5999934766550723e-07, Train Acc: 1.0\n",
            "Epoch 5076/10000\n",
            "Step 0: Train Loss: 3.433881943237793e-07, Train Acc: 1.0\n",
            "Epoch 5077/10000\n",
            "Step 0: Train Loss: 3.4295419482077705e-07, Train Acc: 1.0\n",
            "Epoch 5078/10000\n",
            "Step 0: Train Loss: 3.361595588557975e-07, Train Acc: 1.0\n",
            "Epoch 5079/10000\n",
            "Step 0: Train Loss: 3.5477111737236555e-07, Train Acc: 1.0\n",
            "Epoch 5080/10000\n",
            "Step 0: Train Loss: 3.574538425255014e-07, Train Acc: 1.0\n",
            "Epoch 5081/10000\n",
            "Step 0: Train Loss: 2.9831750225639553e-07, Train Acc: 1.0\n",
            "Epoch 5082/10000\n",
            "Step 0: Train Loss: 3.0687471053170157e-07, Train Acc: 1.0\n",
            "Epoch 5083/10000\n",
            "Step 0: Train Loss: 3.7844608868908836e-07, Train Acc: 1.0\n",
            "Epoch 5084/10000\n",
            "Step 0: Train Loss: 2.959091318643914e-07, Train Acc: 1.0\n",
            "Epoch 5085/10000\n",
            "Step 0: Train Loss: 3.1588731985721097e-07, Train Acc: 1.0\n",
            "Epoch 5086/10000\n",
            "Step 0: Train Loss: 3.0092627412159345e-07, Train Acc: 1.0\n",
            "Epoch 5087/10000\n",
            "Step 0: Train Loss: 3.789668028275628e-07, Train Acc: 1.0\n",
            "Epoch 5088/10000\n",
            "Step 0: Train Loss: 2.869559239115915e-07, Train Acc: 1.0\n",
            "Epoch 5089/10000\n",
            "Step 0: Train Loss: 3.07424016909863e-07, Train Acc: 1.0\n",
            "Epoch 5090/10000\n",
            "Step 0: Train Loss: 3.078894792452047e-07, Train Acc: 1.0\n",
            "Epoch 5091/10000\n",
            "Step 0: Train Loss: 3.257584637594846e-07, Train Acc: 1.0\n",
            "Epoch 5092/10000\n",
            "Step 0: Train Loss: 2.7906443733627384e-07, Train Acc: 1.0\n",
            "Epoch 5093/10000\n",
            "Step 0: Train Loss: 3.1966501978786255e-07, Train Acc: 1.0\n",
            "Epoch 5094/10000\n",
            "Step 0: Train Loss: 3.1053536986291874e-07, Train Acc: 1.0\n",
            "Epoch 5095/10000\n",
            "Step 0: Train Loss: 2.7546434466785286e-07, Train Acc: 1.0\n",
            "Epoch 5096/10000\n",
            "Step 0: Train Loss: 2.7055483542426373e-07, Train Acc: 1.0\n",
            "Epoch 5097/10000\n",
            "Step 0: Train Loss: 3.538036423833546e-07, Train Acc: 1.0\n",
            "Epoch 5098/10000\n",
            "Step 0: Train Loss: 2.5853734086922486e-07, Train Acc: 1.0\n",
            "Epoch 5099/10000\n",
            "Step 0: Train Loss: 3.262225334310642e-07, Train Acc: 1.0\n",
            "Epoch 5100/10000\n",
            "Step 0: Train Loss: 3.193678708157677e-07, Train Acc: 1.0\n",
            "Epoch 5101/10000\n",
            "Step 0: Train Loss: 3.218703739094053e-07, Train Acc: 1.0\n",
            "Epoch 5102/10000\n",
            "Step 0: Train Loss: 2.85547230305383e-07, Train Acc: 1.0\n",
            "Epoch 5103/10000\n",
            "Step 0: Train Loss: 3.2107294600791647e-07, Train Acc: 1.0\n",
            "Epoch 5104/10000\n",
            "Step 0: Train Loss: 3.66930294148915e-07, Train Acc: 1.0\n",
            "Epoch 5105/10000\n",
            "Step 0: Train Loss: 2.915454899721226e-07, Train Acc: 1.0\n",
            "Epoch 5106/10000\n",
            "Step 0: Train Loss: 3.3556653988853213e-07, Train Acc: 1.0\n",
            "Epoch 5107/10000\n",
            "Step 0: Train Loss: 3.1441928172171174e-07, Train Acc: 1.0\n",
            "Epoch 5108/10000\n",
            "Step 0: Train Loss: 2.600157813503756e-07, Train Acc: 1.0\n",
            "Epoch 5109/10000\n",
            "Step 0: Train Loss: 2.8609781566046877e-07, Train Acc: 1.0\n",
            "Epoch 5110/10000\n",
            "Step 0: Train Loss: 2.9319124905669014e-07, Train Acc: 1.0\n",
            "Epoch 5111/10000\n",
            "Step 0: Train Loss: 3.0771022352382715e-07, Train Acc: 1.0\n",
            "Epoch 5112/10000\n",
            "Step 0: Train Loss: 3.0313185561681166e-07, Train Acc: 1.0\n",
            "Epoch 5113/10000\n",
            "Step 0: Train Loss: 2.659766096257954e-07, Train Acc: 1.0\n",
            "Epoch 5114/10000\n",
            "Step 0: Train Loss: 3.031307471701439e-07, Train Acc: 1.0\n",
            "Epoch 5115/10000\n",
            "Step 0: Train Loss: 3.481193857624021e-07, Train Acc: 1.0\n",
            "Epoch 5116/10000\n",
            "Step 0: Train Loss: 3.4649727354008064e-07, Train Acc: 1.0\n",
            "Epoch 5117/10000\n",
            "Step 0: Train Loss: 2.942879291367717e-07, Train Acc: 1.0\n",
            "Epoch 5118/10000\n",
            "Step 0: Train Loss: 2.984345996992488e-07, Train Acc: 1.0\n",
            "Epoch 5119/10000\n",
            "Step 0: Train Loss: 2.5260095526391524e-07, Train Acc: 1.0\n",
            "Epoch 5120/10000\n",
            "Step 0: Train Loss: 2.5456779439991806e-07, Train Acc: 1.0\n",
            "Epoch 5121/10000\n",
            "Step 0: Train Loss: 2.590742553820746e-07, Train Acc: 1.0\n",
            "Epoch 5122/10000\n",
            "Step 0: Train Loss: 3.1126029398365063e-07, Train Acc: 1.0\n",
            "Epoch 5123/10000\n",
            "Step 0: Train Loss: 3.5586521107688895e-07, Train Acc: 1.0\n",
            "Epoch 5124/10000\n",
            "Step 0: Train Loss: 2.70089174136956e-07, Train Acc: 1.0\n",
            "Epoch 5125/10000\n",
            "Step 0: Train Loss: 2.967196053305088e-07, Train Acc: 1.0\n",
            "Epoch 5126/10000\n",
            "Step 0: Train Loss: 2.819600410930434e-07, Train Acc: 1.0\n",
            "Epoch 5127/10000\n",
            "Step 0: Train Loss: 3.516446156481834e-07, Train Acc: 1.0\n",
            "Epoch 5128/10000\n",
            "Step 0: Train Loss: 3.002930668571935e-07, Train Acc: 1.0\n",
            "Epoch 5129/10000\n",
            "Step 0: Train Loss: 3.2141795713869215e-07, Train Acc: 1.0\n",
            "Epoch 5130/10000\n",
            "Step 0: Train Loss: 2.6682030807023693e-07, Train Acc: 1.0\n",
            "Epoch 5131/10000\n",
            "Step 0: Train Loss: 2.52909785558586e-07, Train Acc: 1.0\n",
            "Epoch 5132/10000\n",
            "Step 0: Train Loss: 2.804471819217724e-07, Train Acc: 1.0\n",
            "Epoch 5133/10000\n",
            "Step 0: Train Loss: 2.2511247266265855e-07, Train Acc: 1.0\n",
            "Epoch 5134/10000\n",
            "Step 0: Train Loss: 2.7734785135180573e-07, Train Acc: 1.0\n",
            "Epoch 5135/10000\n",
            "Step 0: Train Loss: 2.851919305157935e-07, Train Acc: 1.0\n",
            "Epoch 5136/10000\n",
            "Step 0: Train Loss: 2.7270033342574607e-07, Train Acc: 1.0\n",
            "Epoch 5137/10000\n",
            "Step 0: Train Loss: 2.9282108471306856e-07, Train Acc: 1.0\n",
            "Epoch 5138/10000\n",
            "Step 0: Train Loss: 2.7955411496805027e-07, Train Acc: 1.0\n",
            "Epoch 5139/10000\n",
            "Step 0: Train Loss: 2.769303648619825e-07, Train Acc: 1.0\n",
            "Epoch 5140/10000\n",
            "Step 0: Train Loss: 3.003645758781204e-07, Train Acc: 1.0\n",
            "Epoch 5141/10000\n",
            "Step 0: Train Loss: 2.542923311921186e-07, Train Acc: 1.0\n",
            "Epoch 5142/10000\n",
            "Step 0: Train Loss: 2.942151695606299e-07, Train Acc: 1.0\n",
            "Epoch 5143/10000\n",
            "Step 0: Train Loss: 2.6039833755930886e-07, Train Acc: 1.0\n",
            "Epoch 5144/10000\n",
            "Step 0: Train Loss: 2.805057874866179e-07, Train Acc: 1.0\n",
            "Epoch 5145/10000\n",
            "Step 0: Train Loss: 2.415393112187303e-07, Train Acc: 1.0\n",
            "Epoch 5146/10000\n",
            "Step 0: Train Loss: 2.358886348474698e-07, Train Acc: 1.0\n",
            "Epoch 5147/10000\n",
            "Step 0: Train Loss: 2.362939568456568e-07, Train Acc: 1.0\n",
            "Epoch 5148/10000\n",
            "Step 0: Train Loss: 2.6230253524772706e-07, Train Acc: 1.0\n",
            "Epoch 5149/10000\n",
            "Step 0: Train Loss: 2.497518210020644e-07, Train Acc: 1.0\n",
            "Epoch 5150/10000\n",
            "Step 0: Train Loss: 2.2681655309497728e-07, Train Acc: 1.0\n",
            "Epoch 5151/10000\n",
            "Step 0: Train Loss: 2.3375423552352004e-07, Train Acc: 1.0\n",
            "Epoch 5152/10000\n",
            "Step 0: Train Loss: 2.4250491037491884e-07, Train Acc: 1.0\n",
            "Epoch 5153/10000\n",
            "Step 0: Train Loss: 2.067431097430017e-07, Train Acc: 1.0\n",
            "Epoch 5154/10000\n",
            "Step 0: Train Loss: 2.8009009156448883e-07, Train Acc: 1.0\n",
            "Epoch 5155/10000\n",
            "Step 0: Train Loss: 2.5612899889893015e-07, Train Acc: 1.0\n",
            "Epoch 5156/10000\n",
            "Step 0: Train Loss: 2.6534473818173865e-07, Train Acc: 1.0\n",
            "Epoch 5157/10000\n",
            "Step 0: Train Loss: 2.585967422419344e-07, Train Acc: 1.0\n",
            "Epoch 5158/10000\n",
            "Step 0: Train Loss: 2.6413920295453863e-07, Train Acc: 1.0\n",
            "Epoch 5159/10000\n",
            "Step 0: Train Loss: 2.8864644718851196e-07, Train Acc: 1.0\n",
            "Epoch 5160/10000\n",
            "Step 0: Train Loss: 2.43958680812284e-07, Train Acc: 1.0\n",
            "Epoch 5161/10000\n",
            "Step 0: Train Loss: 2.6979017775374814e-07, Train Acc: 1.0\n",
            "Epoch 5162/10000\n",
            "Step 0: Train Loss: 2.760480981578439e-07, Train Acc: 1.0\n",
            "Epoch 5163/10000\n",
            "Step 0: Train Loss: 2.786085815387196e-07, Train Acc: 1.0\n",
            "Epoch 5164/10000\n",
            "Step 0: Train Loss: 2.6443771616868617e-07, Train Acc: 1.0\n",
            "Epoch 5165/10000\n",
            "Step 0: Train Loss: 2.3808237870071025e-07, Train Acc: 1.0\n",
            "Epoch 5166/10000\n",
            "Step 0: Train Loss: 2.357101749339563e-07, Train Acc: 1.0\n",
            "Epoch 5167/10000\n",
            "Step 0: Train Loss: 2.3546971306132036e-07, Train Acc: 1.0\n",
            "Epoch 5168/10000\n",
            "Step 0: Train Loss: 2.3600722443006816e-07, Train Acc: 1.0\n",
            "Epoch 5169/10000\n",
            "Step 0: Train Loss: 2.3804635418400721e-07, Train Acc: 1.0\n",
            "Epoch 5170/10000\n",
            "Step 0: Train Loss: 2.056097656577549e-07, Train Acc: 1.0\n",
            "Epoch 5171/10000\n",
            "Step 0: Train Loss: 2.6187430535173917e-07, Train Acc: 1.0\n",
            "Epoch 5172/10000\n",
            "Step 0: Train Loss: 2.6966856125909544e-07, Train Acc: 1.0\n",
            "Epoch 5173/10000\n",
            "Step 0: Train Loss: 2.5028640493474086e-07, Train Acc: 1.0\n",
            "Epoch 5174/10000\n",
            "Step 0: Train Loss: 2.416337281374581e-07, Train Acc: 1.0\n",
            "Epoch 5175/10000\n",
            "Step 0: Train Loss: 2.227045570180053e-07, Train Acc: 1.0\n",
            "Epoch 5176/10000\n",
            "Step 0: Train Loss: 1.9391653438560752e-07, Train Acc: 1.0\n",
            "Epoch 5177/10000\n",
            "Step 0: Train Loss: 2.3527977077719697e-07, Train Acc: 1.0\n",
            "Epoch 5178/10000\n",
            "Step 0: Train Loss: 2.7788399847850087e-07, Train Acc: 1.0\n",
            "Epoch 5179/10000\n",
            "Step 0: Train Loss: 2.310130753357953e-07, Train Acc: 1.0\n",
            "Epoch 5180/10000\n",
            "Step 0: Train Loss: 2.4555538402637467e-07, Train Acc: 1.0\n",
            "Epoch 5181/10000\n",
            "Step 0: Train Loss: 2.2788978526477877e-07, Train Acc: 1.0\n",
            "Epoch 5182/10000\n",
            "Step 0: Train Loss: 2.304876289827007e-07, Train Acc: 1.0\n",
            "Epoch 5183/10000\n",
            "Step 0: Train Loss: 2.182341631851159e-07, Train Acc: 1.0\n",
            "Epoch 5184/10000\n",
            "Step 0: Train Loss: 2.4957353161880746e-07, Train Acc: 1.0\n",
            "Epoch 5185/10000\n",
            "Step 0: Train Loss: 2.6081096393681946e-07, Train Acc: 1.0\n",
            "Epoch 5186/10000\n",
            "Step 0: Train Loss: 2.5280345994360687e-07, Train Acc: 1.0\n",
            "Epoch 5187/10000\n",
            "Step 0: Train Loss: 2.187232439609943e-07, Train Acc: 1.0\n",
            "Epoch 5188/10000\n",
            "Step 0: Train Loss: 1.8292504933015152e-07, Train Acc: 1.0\n",
            "Epoch 5189/10000\n",
            "Step 0: Train Loss: 2.1747057132870395e-07, Train Acc: 1.0\n",
            "Epoch 5190/10000\n",
            "Step 0: Train Loss: 2.2464618609774334e-07, Train Acc: 1.0\n",
            "Epoch 5191/10000\n",
            "Step 0: Train Loss: 2.2935560650694242e-07, Train Acc: 1.0\n",
            "Epoch 5192/10000\n",
            "Step 0: Train Loss: 2.2835379809293954e-07, Train Acc: 1.0\n",
            "Epoch 5193/10000\n",
            "Step 0: Train Loss: 2.0220097951550997e-07, Train Acc: 1.0\n",
            "Epoch 5194/10000\n",
            "Step 0: Train Loss: 2.0395303579334723e-07, Train Acc: 1.0\n",
            "Epoch 5195/10000\n",
            "Step 0: Train Loss: 1.9578784815621475e-07, Train Acc: 1.0\n",
            "Epoch 5196/10000\n",
            "Step 0: Train Loss: 2.1543158368331206e-07, Train Acc: 1.0\n",
            "Epoch 5197/10000\n",
            "Step 0: Train Loss: 2.5745126208676083e-07, Train Acc: 1.0\n",
            "Epoch 5198/10000\n",
            "Step 0: Train Loss: 1.8831346437764296e-07, Train Acc: 1.0\n",
            "Epoch 5199/10000\n",
            "Step 0: Train Loss: 2.556034246481431e-07, Train Acc: 1.0\n",
            "Epoch 5200/10000\n",
            "Step 0: Train Loss: 2.2438447899730818e-07, Train Acc: 1.0\n",
            "Epoch 5201/10000\n",
            "Step 0: Train Loss: 1.9774215331835876e-07, Train Acc: 1.0\n",
            "Epoch 5202/10000\n",
            "Step 0: Train Loss: 1.8007611402026669e-07, Train Acc: 1.0\n",
            "Epoch 5203/10000\n",
            "Step 0: Train Loss: 2.0046026349973545e-07, Train Acc: 1.0\n",
            "Epoch 5204/10000\n",
            "Step 0: Train Loss: 2.112833499268163e-07, Train Acc: 1.0\n",
            "Epoch 5205/10000\n",
            "Step 0: Train Loss: 1.919842702591268e-07, Train Acc: 1.0\n",
            "Epoch 5206/10000\n",
            "Step 0: Train Loss: 2.487126096184511e-07, Train Acc: 1.0\n",
            "Epoch 5207/10000\n",
            "Step 0: Train Loss: 1.9993481714664085e-07, Train Acc: 1.0\n",
            "Epoch 5208/10000\n",
            "Step 0: Train Loss: 2.2470696592336026e-07, Train Acc: 1.0\n",
            "Epoch 5209/10000\n",
            "Step 0: Train Loss: 2.2736469418305205e-07, Train Acc: 1.0\n",
            "Epoch 5210/10000\n",
            "Step 0: Train Loss: 1.8579790150852205e-07, Train Acc: 1.0\n",
            "Epoch 5211/10000\n",
            "Step 0: Train Loss: 2.2622010931172554e-07, Train Acc: 1.0\n",
            "Epoch 5212/10000\n",
            "Step 0: Train Loss: 1.7313787736839004e-07, Train Acc: 1.0\n",
            "Epoch 5213/10000\n",
            "Step 0: Train Loss: 1.9767072956256015e-07, Train Acc: 1.0\n",
            "Epoch 5214/10000\n",
            "Step 0: Train Loss: 1.7869270152459649e-07, Train Acc: 1.0\n",
            "Epoch 5215/10000\n",
            "Step 0: Train Loss: 2.0116250709634187e-07, Train Acc: 1.0\n",
            "Epoch 5216/10000\n",
            "Step 0: Train Loss: 2.1786442516713578e-07, Train Acc: 1.0\n",
            "Epoch 5217/10000\n",
            "Step 0: Train Loss: 1.6479383191381203e-07, Train Acc: 1.0\n",
            "Epoch 5218/10000\n",
            "Step 0: Train Loss: 1.953227268813862e-07, Train Acc: 1.0\n",
            "Epoch 5219/10000\n",
            "Step 0: Train Loss: 1.8755009989490645e-07, Train Acc: 1.0\n",
            "Epoch 5220/10000\n",
            "Step 0: Train Loss: 2.1255901572203584e-07, Train Acc: 1.0\n",
            "Epoch 5221/10000\n",
            "Step 0: Train Loss: 1.9504690840221883e-07, Train Acc: 1.0\n",
            "Epoch 5222/10000\n",
            "Step 0: Train Loss: 1.915196037316491e-07, Train Acc: 1.0\n",
            "Epoch 5223/10000\n",
            "Step 0: Train Loss: 2.054064367484898e-07, Train Acc: 1.0\n",
            "Epoch 5224/10000\n",
            "Step 0: Train Loss: 1.9119779892662336e-07, Train Acc: 1.0\n",
            "Epoch 5225/10000\n",
            "Step 0: Train Loss: 1.8605972229579493e-07, Train Acc: 1.0\n",
            "Epoch 5226/10000\n",
            "Step 0: Train Loss: 1.7974200261505757e-07, Train Acc: 1.0\n",
            "Epoch 5227/10000\n",
            "Step 0: Train Loss: 1.8311597216325026e-07, Train Acc: 1.0\n",
            "Epoch 5228/10000\n",
            "Step 0: Train Loss: 2.0787520327303355e-07, Train Acc: 1.0\n",
            "Epoch 5229/10000\n",
            "Step 0: Train Loss: 2.06944932301667e-07, Train Acc: 1.0\n",
            "Epoch 5230/10000\n",
            "Step 0: Train Loss: 1.8392574929748662e-07, Train Acc: 1.0\n",
            "Epoch 5231/10000\n",
            "Step 0: Train Loss: 1.7337684710128087e-07, Train Acc: 1.0\n",
            "Epoch 5232/10000\n",
            "Step 0: Train Loss: 1.987658890811872e-07, Train Acc: 1.0\n",
            "Epoch 5233/10000\n",
            "Step 0: Train Loss: 1.8645413035756064e-07, Train Acc: 1.0\n",
            "Epoch 5234/10000\n",
            "Step 0: Train Loss: 2.1683892725832266e-07, Train Acc: 1.0\n",
            "Epoch 5235/10000\n",
            "Step 0: Train Loss: 1.8592837136566231e-07, Train Acc: 1.0\n",
            "Epoch 5236/10000\n",
            "Step 0: Train Loss: 1.860959741861734e-07, Train Acc: 1.0\n",
            "Epoch 5237/10000\n",
            "Step 0: Train Loss: 1.7877624713946716e-07, Train Acc: 1.0\n",
            "Epoch 5238/10000\n",
            "Step 0: Train Loss: 1.9770681092268205e-07, Train Acc: 1.0\n",
            "Epoch 5239/10000\n",
            "Step 0: Train Loss: 2.013179027926526e-07, Train Acc: 1.0\n",
            "Epoch 5240/10000\n",
            "Step 0: Train Loss: 1.6872786545718554e-07, Train Acc: 1.0\n",
            "Epoch 5241/10000\n",
            "Step 0: Train Loss: 1.7913396277435822e-07, Train Acc: 1.0\n",
            "Epoch 5242/10000\n",
            "Step 0: Train Loss: 1.7536694940645248e-07, Train Acc: 1.0\n",
            "Epoch 5243/10000\n",
            "Step 0: Train Loss: 1.768331969742576e-07, Train Acc: 1.0\n",
            "Epoch 5244/10000\n",
            "Step 0: Train Loss: 1.800865021550635e-07, Train Acc: 1.0\n",
            "Epoch 5245/10000\n",
            "Step 0: Train Loss: 1.8758584019451519e-07, Train Acc: 1.0\n",
            "Epoch 5246/10000\n",
            "Step 0: Train Loss: 1.594050615949527e-07, Train Acc: 1.0\n",
            "Epoch 5247/10000\n",
            "Step 0: Train Loss: 1.766779291756393e-07, Train Acc: 1.0\n",
            "Epoch 5248/10000\n",
            "Step 0: Train Loss: 1.702772465250746e-07, Train Acc: 1.0\n",
            "Epoch 5249/10000\n",
            "Step 0: Train Loss: 1.8738293761089153e-07, Train Acc: 1.0\n",
            "Epoch 5250/10000\n",
            "Step 0: Train Loss: 1.6752169074152334e-07, Train Acc: 1.0\n",
            "Epoch 5251/10000\n",
            "Step 0: Train Loss: 2.278636372921028e-07, Train Acc: 1.0\n",
            "Epoch 5252/10000\n",
            "Step 0: Train Loss: 1.6001308722479735e-07, Train Acc: 1.0\n",
            "Epoch 5253/10000\n",
            "Step 0: Train Loss: 1.5611524872838345e-07, Train Acc: 1.0\n",
            "Epoch 5254/10000\n",
            "Step 0: Train Loss: 1.6439993544281606e-07, Train Acc: 1.0\n",
            "Epoch 5255/10000\n",
            "Step 0: Train Loss: 1.8144666569241963e-07, Train Acc: 1.0\n",
            "Epoch 5256/10000\n",
            "Step 0: Train Loss: 1.9901713699255197e-07, Train Acc: 1.0\n",
            "Epoch 5257/10000\n",
            "Step 0: Train Loss: 1.544941596876015e-07, Train Acc: 1.0\n",
            "Epoch 5258/10000\n",
            "Step 0: Train Loss: 1.8163726167585992e-07, Train Acc: 1.0\n",
            "Epoch 5259/10000\n",
            "Step 0: Train Loss: 1.985874433785284e-07, Train Acc: 1.0\n",
            "Epoch 5260/10000\n",
            "Step 0: Train Loss: 1.4793795344303362e-07, Train Acc: 1.0\n",
            "Epoch 5261/10000\n",
            "Step 0: Train Loss: 1.7356677517454955e-07, Train Acc: 1.0\n",
            "Epoch 5262/10000\n",
            "Step 0: Train Loss: 1.8232877607715636e-07, Train Acc: 1.0\n",
            "Epoch 5263/10000\n",
            "Step 0: Train Loss: 1.9602437362209457e-07, Train Acc: 1.0\n",
            "Epoch 5264/10000\n",
            "Step 0: Train Loss: 1.9569051801227033e-07, Train Acc: 1.0\n",
            "Epoch 5265/10000\n",
            "Step 0: Train Loss: 2.035822745938276e-07, Train Acc: 1.0\n",
            "Epoch 5266/10000\n",
            "Step 0: Train Loss: 1.8301875570614357e-07, Train Acc: 1.0\n",
            "Epoch 5267/10000\n",
            "Step 0: Train Loss: 1.931646664843356e-07, Train Acc: 1.0\n",
            "Epoch 5268/10000\n",
            "Step 0: Train Loss: 1.527419613012171e-07, Train Acc: 1.0\n",
            "Epoch 5269/10000\n",
            "Step 0: Train Loss: 1.967875817854292e-07, Train Acc: 1.0\n",
            "Epoch 5270/10000\n",
            "Step 0: Train Loss: 1.6039464867390052e-07, Train Acc: 1.0\n",
            "Epoch 5271/10000\n",
            "Step 0: Train Loss: 1.7330413015770318e-07, Train Acc: 1.0\n",
            "Epoch 5272/10000\n",
            "Step 0: Train Loss: 1.7144542141522834e-07, Train Acc: 1.0\n",
            "Epoch 5273/10000\n",
            "Step 0: Train Loss: 1.6743983621836378e-07, Train Acc: 1.0\n",
            "Epoch 5274/10000\n",
            "Step 0: Train Loss: 1.7235089444511686e-07, Train Acc: 1.0\n",
            "Epoch 5275/10000\n",
            "Step 0: Train Loss: 1.7586795308943692e-07, Train Acc: 1.0\n",
            "Epoch 5276/10000\n",
            "Step 0: Train Loss: 1.5509013451264764e-07, Train Acc: 1.0\n",
            "Epoch 5277/10000\n",
            "Step 0: Train Loss: 1.7626150849991973e-07, Train Acc: 1.0\n",
            "Epoch 5278/10000\n",
            "Step 0: Train Loss: 1.4735374520569167e-07, Train Acc: 1.0\n",
            "Epoch 5279/10000\n",
            "Step 0: Train Loss: 1.7252963857572468e-07, Train Acc: 1.0\n",
            "Epoch 5280/10000\n",
            "Step 0: Train Loss: 1.9995849243059638e-07, Train Acc: 1.0\n",
            "Epoch 5281/10000\n",
            "Step 0: Train Loss: 1.5989421342510468e-07, Train Acc: 1.0\n",
            "Epoch 5282/10000\n",
            "Step 0: Train Loss: 1.664741944296111e-07, Train Acc: 1.0\n",
            "Epoch 5283/10000\n",
            "Step 0: Train Loss: 1.543150460747711e-07, Train Acc: 1.0\n",
            "Epoch 5284/10000\n",
            "Step 0: Train Loss: 1.4191776642746845e-07, Train Acc: 1.0\n",
            "Epoch 5285/10000\n",
            "Step 0: Train Loss: 1.799685378500726e-07, Train Acc: 1.0\n",
            "Epoch 5286/10000\n",
            "Step 0: Train Loss: 1.4760387045953394e-07, Train Acc: 1.0\n",
            "Epoch 5287/10000\n",
            "Step 0: Train Loss: 1.630278632092086e-07, Train Acc: 1.0\n",
            "Epoch 5288/10000\n",
            "Step 0: Train Loss: 1.426081581712424e-07, Train Acc: 1.0\n",
            "Epoch 5289/10000\n",
            "Step 0: Train Loss: 1.6161057203589735e-07, Train Acc: 1.0\n",
            "Epoch 5290/10000\n",
            "Step 0: Train Loss: 1.3476531535161484e-07, Train Acc: 1.0\n",
            "Epoch 5291/10000\n",
            "Step 0: Train Loss: 1.5275367104550241e-07, Train Acc: 1.0\n",
            "Epoch 5292/10000\n",
            "Step 0: Train Loss: 1.3219062111602398e-07, Train Acc: 1.0\n",
            "Epoch 5293/10000\n",
            "Step 0: Train Loss: 1.36553353513591e-07, Train Acc: 1.0\n",
            "Epoch 5294/10000\n",
            "Step 0: Train Loss: 1.7790587492072518e-07, Train Acc: 1.0\n",
            "Epoch 5295/10000\n",
            "Step 0: Train Loss: 1.487005363287608e-07, Train Acc: 1.0\n",
            "Epoch 5296/10000\n",
            "Step 0: Train Loss: 1.8787117994634173e-07, Train Acc: 1.0\n",
            "Epoch 5297/10000\n",
            "Step 0: Train Loss: 1.3734042170199245e-07, Train Acc: 1.0\n",
            "Epoch 5298/10000\n",
            "Step 0: Train Loss: 1.4158386818508006e-07, Train Acc: 1.0\n",
            "Epoch 5299/10000\n",
            "Step 0: Train Loss: 1.3685064459423302e-07, Train Acc: 1.0\n",
            "Epoch 5300/10000\n",
            "Step 0: Train Loss: 1.6795215174170153e-07, Train Acc: 1.0\n",
            "Epoch 5301/10000\n",
            "Step 0: Train Loss: 1.3559974831878208e-07, Train Acc: 1.0\n",
            "Epoch 5302/10000\n",
            "Step 0: Train Loss: 1.6027556171138713e-07, Train Acc: 1.0\n",
            "Epoch 5303/10000\n",
            "Step 0: Train Loss: 1.2609888244696776e-07, Train Acc: 1.0\n",
            "Epoch 5304/10000\n",
            "Step 0: Train Loss: 1.3230969386768265e-07, Train Acc: 1.0\n",
            "Epoch 5305/10000\n",
            "Step 0: Train Loss: 1.97776870436428e-07, Train Acc: 1.0\n",
            "Epoch 5306/10000\n",
            "Step 0: Train Loss: 1.5767636796226725e-07, Train Acc: 1.0\n",
            "Epoch 5307/10000\n",
            "Step 0: Train Loss: 1.442184611732955e-07, Train Acc: 1.0\n",
            "Epoch 5308/10000\n",
            "Step 0: Train Loss: 1.3413347232926753e-07, Train Acc: 1.0\n",
            "Epoch 5309/10000\n",
            "Step 0: Train Loss: 1.0889710466699398e-07, Train Acc: 1.0\n",
            "Epoch 5310/10000\n",
            "Step 0: Train Loss: 1.3661295383826655e-07, Train Acc: 1.0\n",
            "Epoch 5311/10000\n",
            "Step 0: Train Loss: 1.5717570533979597e-07, Train Acc: 1.0\n",
            "Epoch 5312/10000\n",
            "Step 0: Train Loss: 1.3847227364749415e-07, Train Acc: 1.0\n",
            "Epoch 5313/10000\n",
            "Step 0: Train Loss: 1.623967023078876e-07, Train Acc: 1.0\n",
            "Epoch 5314/10000\n",
            "Step 0: Train Loss: 1.331433878704047e-07, Train Acc: 1.0\n",
            "Epoch 5315/10000\n",
            "Step 0: Train Loss: 1.2603896948348847e-07, Train Acc: 1.0\n",
            "Epoch 5316/10000\n",
            "Step 0: Train Loss: 1.6878685471510835e-07, Train Acc: 1.0\n",
            "Epoch 5317/10000\n",
            "Step 0: Train Loss: 1.275887200336001e-07, Train Acc: 1.0\n",
            "Epoch 5318/10000\n",
            "Step 0: Train Loss: 1.5739003345061064e-07, Train Acc: 1.0\n",
            "Epoch 5319/10000\n",
            "Step 0: Train Loss: 1.3203522541971324e-07, Train Acc: 1.0\n",
            "Epoch 5320/10000\n",
            "Step 0: Train Loss: 1.2750545863582374e-07, Train Acc: 1.0\n",
            "Epoch 5321/10000\n",
            "Step 0: Train Loss: 1.465781735987548e-07, Train Acc: 1.0\n",
            "Epoch 5322/10000\n",
            "Step 0: Train Loss: 1.3900883288897603e-07, Train Acc: 1.0\n",
            "Epoch 5323/10000\n",
            "Step 0: Train Loss: 1.2601550736235367e-07, Train Acc: 1.0\n",
            "Epoch 5324/10000\n",
            "Step 0: Train Loss: 1.2733859477975784e-07, Train Acc: 1.0\n",
            "Epoch 5325/10000\n",
            "Step 0: Train Loss: 1.3171310797588376e-07, Train Acc: 1.0\n",
            "Epoch 5326/10000\n",
            "Step 0: Train Loss: 1.5532768316006695e-07, Train Acc: 1.0\n",
            "Epoch 5327/10000\n",
            "Step 0: Train Loss: 1.3062845027889125e-07, Train Acc: 1.0\n",
            "Epoch 5328/10000\n",
            "Step 0: Train Loss: 1.3763796857801935e-07, Train Acc: 1.0\n",
            "Epoch 5329/10000\n",
            "Step 0: Train Loss: 1.3421684741388162e-07, Train Acc: 1.0\n",
            "Epoch 5330/10000\n",
            "Step 0: Train Loss: 1.5030978772756498e-07, Train Acc: 1.0\n",
            "Epoch 5331/10000\n",
            "Step 0: Train Loss: 1.3079538518923073e-07, Train Acc: 1.0\n",
            "Epoch 5332/10000\n",
            "Step 0: Train Loss: 1.2748164124332106e-07, Train Acc: 1.0\n",
            "Epoch 5333/10000\n",
            "Step 0: Train Loss: 1.3350177141546737e-07, Train Acc: 1.0\n",
            "Epoch 5334/10000\n",
            "Step 0: Train Loss: 1.568540710650268e-07, Train Acc: 1.0\n",
            "Epoch 5335/10000\n",
            "Step 0: Train Loss: 1.2881646682672e-07, Train Acc: 1.0\n",
            "Epoch 5336/10000\n",
            "Step 0: Train Loss: 1.3664848097505455e-07, Train Acc: 1.0\n",
            "Epoch 5337/10000\n",
            "Step 0: Train Loss: 1.3592156733466254e-07, Train Acc: 1.0\n",
            "Epoch 5338/10000\n",
            "Step 0: Train Loss: 1.2938916427174263e-07, Train Acc: 1.0\n",
            "Epoch 5339/10000\n",
            "Step 0: Train Loss: 1.303422720866365e-07, Train Acc: 1.0\n",
            "Epoch 5340/10000\n",
            "Step 0: Train Loss: 1.022332440925311e-07, Train Acc: 1.0\n",
            "Epoch 5341/10000\n",
            "Step 0: Train Loss: 1.2752941813687357e-07, Train Acc: 1.0\n",
            "Epoch 5342/10000\n",
            "Step 0: Train Loss: 1.2322564657551993e-07, Train Acc: 1.0\n",
            "Epoch 5343/10000\n",
            "Step 0: Train Loss: 1.2869736565335188e-07, Train Acc: 1.0\n",
            "Epoch 5344/10000\n",
            "Step 0: Train Loss: 1.491173691192671e-07, Train Acc: 1.0\n",
            "Epoch 5345/10000\n",
            "Step 0: Train Loss: 1.144879817616129e-07, Train Acc: 1.0\n",
            "Epoch 5346/10000\n",
            "Step 0: Train Loss: 1.491406464992906e-07, Train Acc: 1.0\n",
            "Epoch 5347/10000\n",
            "Step 0: Train Loss: 1.3645741603340866e-07, Train Acc: 1.0\n",
            "Epoch 5348/10000\n",
            "Step 0: Train Loss: 1.368982083249648e-07, Train Acc: 1.0\n",
            "Epoch 5349/10000\n",
            "Step 0: Train Loss: 1.2833942264478537e-07, Train Acc: 1.0\n",
            "Epoch 5350/10000\n",
            "Step 0: Train Loss: 1.2903107915462897e-07, Train Acc: 1.0\n",
            "Epoch 5351/10000\n",
            "Step 0: Train Loss: 1.1476208783278707e-07, Train Acc: 1.0\n",
            "Epoch 5352/10000\n",
            "Step 0: Train Loss: 1.1910135100379193e-07, Train Acc: 1.0\n",
            "Epoch 5353/10000\n",
            "Step 0: Train Loss: 1.3550419453167706e-07, Train Acc: 1.0\n",
            "Epoch 5354/10000\n",
            "Step 0: Train Loss: 1.228326169666616e-07, Train Acc: 1.0\n",
            "Epoch 5355/10000\n",
            "Step 0: Train Loss: 1.278508250379673e-07, Train Acc: 1.0\n",
            "Epoch 5356/10000\n",
            "Step 0: Train Loss: 1.1230630292402566e-07, Train Acc: 1.0\n",
            "Epoch 5357/10000\n",
            "Step 0: Train Loss: 1.1002978794749652e-07, Train Acc: 1.0\n",
            "Epoch 5358/10000\n",
            "Step 0: Train Loss: 1.3980718449602136e-07, Train Acc: 1.0\n",
            "Epoch 5359/10000\n",
            "Step 0: Train Loss: 1.5536339503796626e-07, Train Acc: 1.0\n",
            "Epoch 5360/10000\n",
            "Step 0: Train Loss: 1.4323968855478597e-07, Train Acc: 1.0\n",
            "Epoch 5361/10000\n",
            "Step 0: Train Loss: 1.3228556383637624e-07, Train Acc: 1.0\n",
            "Epoch 5362/10000\n",
            "Step 0: Train Loss: 1.1981664727045427e-07, Train Acc: 1.0\n",
            "Epoch 5363/10000\n",
            "Step 0: Train Loss: 1.154054416474537e-07, Train Acc: 1.0\n",
            "Epoch 5364/10000\n",
            "Step 0: Train Loss: 1.228559938226681e-07, Train Acc: 1.0\n",
            "Epoch 5365/10000\n",
            "Step 0: Train Loss: 1.1159130508531234e-07, Train Acc: 1.0\n",
            "Epoch 5366/10000\n",
            "Step 0: Train Loss: 8.941858453681562e-08, Train Acc: 1.0\n",
            "Epoch 5367/10000\n",
            "Step 0: Train Loss: 1.1184155823684705e-07, Train Acc: 1.0\n",
            "Epoch 5368/10000\n",
            "Step 0: Train Loss: 9.714318593978533e-08, Train Acc: 1.0\n",
            "Epoch 5369/10000\n",
            "Step 0: Train Loss: 1.290185878133343e-07, Train Acc: 1.0\n",
            "Epoch 5370/10000\n",
            "Step 0: Train Loss: 1.4289484795426688e-07, Train Acc: 1.0\n",
            "Epoch 5371/10000\n",
            "Step 0: Train Loss: 1.206507960205272e-07, Train Acc: 1.0\n",
            "Epoch 5372/10000\n",
            "Step 0: Train Loss: 1.2794639303592703e-07, Train Acc: 1.0\n",
            "Epoch 5373/10000\n",
            "Step 0: Train Loss: 1.2570512808451895e-07, Train Acc: 1.0\n",
            "Epoch 5374/10000\n",
            "Step 0: Train Loss: 9.582009852238116e-08, Train Acc: 1.0\n",
            "Epoch 5375/10000\n",
            "Step 0: Train Loss: 1.0505872438670849e-07, Train Acc: 1.0\n",
            "Epoch 5376/10000\n",
            "Step 0: Train Loss: 1.1293843016346727e-07, Train Acc: 1.0\n",
            "Epoch 5377/10000\n",
            "Step 0: Train Loss: 1.0343737955054166e-07, Train Acc: 1.0\n",
            "Epoch 5378/10000\n",
            "Step 0: Train Loss: 9.12064663793899e-08, Train Acc: 1.0\n",
            "Epoch 5379/10000\n",
            "Step 0: Train Loss: 1.1433265001414838e-07, Train Acc: 1.0\n",
            "Epoch 5380/10000\n",
            "Step 0: Train Loss: 1.2184284514660249e-07, Train Acc: 1.0\n",
            "Epoch 5381/10000\n",
            "Step 0: Train Loss: 1.2524044734618656e-07, Train Acc: 1.0\n",
            "Epoch 5382/10000\n",
            "Step 0: Train Loss: 1.0572584585588629e-07, Train Acc: 1.0\n",
            "Epoch 5383/10000\n",
            "Step 0: Train Loss: 9.825171787269937e-08, Train Acc: 1.0\n",
            "Epoch 5384/10000\n",
            "Step 0: Train Loss: 1.0683483253615123e-07, Train Acc: 1.0\n",
            "Epoch 5385/10000\n",
            "Step 0: Train Loss: 1.2798182069673203e-07, Train Acc: 1.0\n",
            "Epoch 5386/10000\n",
            "Step 0: Train Loss: 9.946769807811506e-08, Train Acc: 1.0\n",
            "Epoch 5387/10000\n",
            "Step 0: Train Loss: 9.739338935332853e-08, Train Acc: 1.0\n",
            "Epoch 5388/10000\n",
            "Step 0: Train Loss: 1.0589313603759365e-07, Train Acc: 1.0\n",
            "Epoch 5389/10000\n",
            "Step 0: Train Loss: 1.1498846674840024e-07, Train Acc: 1.0\n",
            "Epoch 5390/10000\n",
            "Step 0: Train Loss: 9.161204417296176e-08, Train Acc: 1.0\n",
            "Epoch 5391/10000\n",
            "Step 0: Train Loss: 1.206864084224435e-07, Train Acc: 1.0\n",
            "Epoch 5392/10000\n",
            "Step 0: Train Loss: 1.1514312348026579e-07, Train Acc: 1.0\n",
            "Epoch 5393/10000\n",
            "Step 0: Train Loss: 1.0621455714954209e-07, Train Acc: 1.0\n",
            "Epoch 5394/10000\n",
            "Step 0: Train Loss: 1.2112766967220523e-07, Train Acc: 1.0\n",
            "Epoch 5395/10000\n",
            "Step 0: Train Loss: 8.995483824492112e-08, Train Acc: 1.0\n",
            "Epoch 5396/10000\n",
            "Step 0: Train Loss: 1.0042136011634284e-07, Train Acc: 1.0\n",
            "Epoch 5397/10000\n",
            "Step 0: Train Loss: 1.2726671627660835e-07, Train Acc: 1.0\n",
            "Epoch 5398/10000\n",
            "Step 0: Train Loss: 1.079909708323612e-07, Train Acc: 1.0\n",
            "Epoch 5399/10000\n",
            "Step 0: Train Loss: 1.0111264714396384e-07, Train Acc: 1.0\n",
            "Epoch 5400/10000\n",
            "Step 0: Train Loss: 8.770192039264657e-08, Train Acc: 1.0\n",
            "Epoch 5401/10000\n",
            "Step 0: Train Loss: 1.1777784436617367e-07, Train Acc: 1.0\n",
            "Epoch 5402/10000\n",
            "Step 0: Train Loss: 1.0089813429203787e-07, Train Acc: 1.0\n",
            "Epoch 5403/10000\n",
            "Step 0: Train Loss: 9.481867380145559e-08, Train Acc: 1.0\n",
            "Epoch 5404/10000\n",
            "Step 0: Train Loss: 1.1455922077630021e-07, Train Acc: 1.0\n",
            "Epoch 5405/10000\n",
            "Step 0: Train Loss: 1.0858675381086869e-07, Train Acc: 1.0\n",
            "Epoch 5406/10000\n",
            "Step 0: Train Loss: 9.335246886621462e-08, Train Acc: 1.0\n",
            "Epoch 5407/10000\n",
            "Step 0: Train Loss: 1.1196068072649723e-07, Train Acc: 1.0\n",
            "Epoch 5408/10000\n",
            "Step 0: Train Loss: 9.984898952097865e-08, Train Acc: 1.0\n",
            "Epoch 5409/10000\n",
            "Step 0: Train Loss: 1.0575009667945778e-07, Train Acc: 1.0\n",
            "Epoch 5410/10000\n",
            "Step 0: Train Loss: 9.401964717881128e-08, Train Acc: 1.0\n",
            "Epoch 5411/10000\n",
            "Step 0: Train Loss: 8.320785127580166e-08, Train Acc: 1.0\n",
            "Epoch 5412/10000\n",
            "Step 0: Train Loss: 1.0828884455804655e-07, Train Acc: 1.0\n",
            "Epoch 5413/10000\n",
            "Step 0: Train Loss: 1.2413192962412722e-07, Train Acc: 1.0\n",
            "Epoch 5414/10000\n",
            "Step 0: Train Loss: 9.469960104979691e-08, Train Acc: 1.0\n",
            "Epoch 5415/10000\n",
            "Step 0: Train Loss: 1.1558449131143789e-07, Train Acc: 1.0\n",
            "Epoch 5416/10000\n",
            "Step 0: Train Loss: 8.422095731930312e-08, Train Acc: 1.0\n",
            "Epoch 5417/10000\n",
            "Step 0: Train Loss: 1.0737094413570958e-07, Train Acc: 1.0\n",
            "Epoch 5418/10000\n",
            "Step 0: Train Loss: 1.0200678701721699e-07, Train Acc: 1.0\n",
            "Epoch 5419/10000\n",
            "Step 0: Train Loss: 9.46278433389125e-08, Train Acc: 1.0\n",
            "Epoch 5420/10000\n",
            "Step 0: Train Loss: 9.32091381855571e-08, Train Acc: 1.0\n",
            "Epoch 5421/10000\n",
            "Step 0: Train Loss: 9.772723075229806e-08, Train Acc: 1.0\n",
            "Epoch 5422/10000\n",
            "Step 0: Train Loss: 9.677376766603629e-08, Train Acc: 1.0\n",
            "Epoch 5423/10000\n",
            "Step 0: Train Loss: 1.1042261149896149e-07, Train Acc: 1.0\n",
            "Epoch 5424/10000\n",
            "Step 0: Train Loss: 1.1029136715023924e-07, Train Acc: 1.0\n",
            "Epoch 5425/10000\n",
            "Step 0: Train Loss: 1.1825439116819325e-07, Train Acc: 1.0\n",
            "Epoch 5426/10000\n",
            "Step 0: Train Loss: 1.0816967943583222e-07, Train Acc: 1.0\n",
            "Epoch 5427/10000\n",
            "Step 0: Train Loss: 9.688086066717005e-08, Train Acc: 1.0\n",
            "Epoch 5428/10000\n",
            "Step 0: Train Loss: 8.689111297144336e-08, Train Acc: 1.0\n",
            "Epoch 5429/10000\n",
            "Step 0: Train Loss: 9.696428548977565e-08, Train Acc: 1.0\n",
            "Epoch 5430/10000\n",
            "Step 0: Train Loss: 9.409120593772968e-08, Train Acc: 1.0\n",
            "Epoch 5431/10000\n",
            "Step 0: Train Loss: 9.351915508659658e-08, Train Acc: 1.0\n",
            "Epoch 5432/10000\n",
            "Step 0: Train Loss: 9.684509194585189e-08, Train Acc: 1.0\n",
            "Epoch 5433/10000\n",
            "Step 0: Train Loss: 8.052557376458935e-08, Train Acc: 1.0\n",
            "Epoch 5434/10000\n",
            "Step 0: Train Loss: 9.694062441667484e-08, Train Acc: 1.0\n",
            "Epoch 5435/10000\n",
            "Step 0: Train Loss: 8.720120092675643e-08, Train Acc: 1.0\n",
            "Epoch 5436/10000\n",
            "Step 0: Train Loss: 8.81786448303501e-08, Train Acc: 1.0\n",
            "Epoch 5437/10000\n",
            "Step 0: Train Loss: 8.32793034533097e-08, Train Acc: 1.0\n",
            "Epoch 5438/10000\n",
            "Step 0: Train Loss: 8.87984725750357e-08, Train Acc: 1.0\n",
            "Epoch 5439/10000\n",
            "Step 0: Train Loss: 7.952418457080057e-08, Train Acc: 1.0\n",
            "Epoch 5440/10000\n",
            "Step 0: Train Loss: 9.877636131250256e-08, Train Acc: 1.0\n",
            "Epoch 5441/10000\n",
            "Step 0: Train Loss: 8.963304765075009e-08, Train Acc: 1.0\n",
            "Epoch 5442/10000\n",
            "Step 0: Train Loss: 9.491372310321822e-08, Train Acc: 1.0\n",
            "Epoch 5443/10000\n",
            "Step 0: Train Loss: 6.916508965559842e-08, Train Acc: 1.0\n",
            "Epoch 5444/10000\n",
            "Step 0: Train Loss: 1.01696720378186e-07, Train Acc: 1.0\n",
            "Epoch 5445/10000\n",
            "Step 0: Train Loss: 8.3958674679252e-08, Train Acc: 1.0\n",
            "Epoch 5446/10000\n",
            "Step 0: Train Loss: 9.955122237670366e-08, Train Acc: 1.0\n",
            "Epoch 5447/10000\n",
            "Step 0: Train Loss: 8.807130313925882e-08, Train Acc: 1.0\n",
            "Epoch 5448/10000\n",
            "Step 0: Train Loss: 1.0262623106882529e-07, Train Acc: 1.0\n",
            "Epoch 5449/10000\n",
            "Step 0: Train Loss: 9.443684234611283e-08, Train Acc: 1.0\n",
            "Epoch 5450/10000\n",
            "Step 0: Train Loss: 7.172798177634832e-08, Train Acc: 1.0\n",
            "Epoch 5451/10000\n",
            "Step 0: Train Loss: 8.691483799339039e-08, Train Acc: 1.0\n",
            "Epoch 5452/10000\n",
            "Step 0: Train Loss: 7.957194014807101e-08, Train Acc: 1.0\n",
            "Epoch 5453/10000\n",
            "Step 0: Train Loss: 8.63427516151205e-08, Train Acc: 1.0\n",
            "Epoch 5454/10000\n",
            "Step 0: Train Loss: 1.0961208118942523e-07, Train Acc: 1.0\n",
            "Epoch 5455/10000\n",
            "Step 0: Train Loss: 8.103821613758555e-08, Train Acc: 1.0\n",
            "Epoch 5456/10000\n",
            "Step 0: Train Loss: 9.860901428737634e-08, Train Acc: 1.0\n",
            "Epoch 5457/10000\n",
            "Step 0: Train Loss: 8.196798262360971e-08, Train Acc: 1.0\n",
            "Epoch 5458/10000\n",
            "Step 0: Train Loss: 8.475727497625485e-08, Train Acc: 1.0\n",
            "Epoch 5459/10000\n",
            "Step 0: Train Loss: 7.748574404331521e-08, Train Acc: 1.0\n",
            "Epoch 5460/10000\n",
            "Step 0: Train Loss: 8.736787293628367e-08, Train Acc: 1.0\n",
            "Epoch 5461/10000\n",
            "Step 0: Train Loss: 6.444425793006303e-08, Train Acc: 1.0\n",
            "Epoch 5462/10000\n",
            "Step 0: Train Loss: 9.037208315021417e-08, Train Acc: 1.0\n",
            "Epoch 5463/10000\n",
            "Step 0: Train Loss: 8.362481906942776e-08, Train Acc: 1.0\n",
            "Epoch 5464/10000\n",
            "Step 0: Train Loss: 8.571112886102128e-08, Train Acc: 1.0\n",
            "Epoch 5465/10000\n",
            "Step 0: Train Loss: 9.676148948756236e-08, Train Acc: 1.0\n",
            "Epoch 5466/10000\n",
            "Step 0: Train Loss: 1.0689395679719382e-07, Train Acc: 1.0\n",
            "Epoch 5467/10000\n",
            "Step 0: Train Loss: 8.435200271605936e-08, Train Acc: 1.0\n",
            "Epoch 5468/10000\n",
            "Step 0: Train Loss: 8.943021612140001e-08, Train Acc: 1.0\n",
            "Epoch 5469/10000\n",
            "Step 0: Train Loss: 8.095464210100545e-08, Train Acc: 1.0\n",
            "Epoch 5470/10000\n",
            "Step 0: Train Loss: 8.085937253099473e-08, Train Acc: 1.0\n",
            "Epoch 5471/10000\n",
            "Step 0: Train Loss: 7.827254222547708e-08, Train Acc: 1.0\n",
            "Epoch 5472/10000\n",
            "Step 0: Train Loss: 7.159685111446379e-08, Train Acc: 1.0\n",
            "Epoch 5473/10000\n",
            "Step 0: Train Loss: 7.662745815650851e-08, Train Acc: 1.0\n",
            "Epoch 5474/10000\n",
            "Step 0: Train Loss: 6.912922856372461e-08, Train Acc: 1.0\n",
            "Epoch 5475/10000\n",
            "Step 0: Train Loss: 8.945418983330455e-08, Train Acc: 1.0\n",
            "Epoch 5476/10000\n",
            "Step 0: Train Loss: 7.110794086884198e-08, Train Acc: 1.0\n",
            "Epoch 5477/10000\n",
            "Step 0: Train Loss: 8.109756777230359e-08, Train Acc: 1.0\n",
            "Epoch 5478/10000\n",
            "Step 0: Train Loss: 8.480492397211492e-08, Train Acc: 1.0\n",
            "Epoch 5479/10000\n",
            "Step 0: Train Loss: 8.531776529707713e-08, Train Acc: 1.0\n",
            "Epoch 5480/10000\n",
            "Step 0: Train Loss: 9.55694687831965e-08, Train Acc: 1.0\n",
            "Epoch 5481/10000\n",
            "Step 0: Train Loss: 7.818890424005076e-08, Train Acc: 1.0\n",
            "Epoch 5482/10000\n",
            "Step 0: Train Loss: 7.593587270093849e-08, Train Acc: 1.0\n",
            "Epoch 5483/10000\n",
            "Step 0: Train Loss: 9.970580805429563e-08, Train Acc: 1.0\n",
            "Epoch 5484/10000\n",
            "Step 0: Train Loss: 8.310036747616323e-08, Train Acc: 1.0\n",
            "Epoch 5485/10000\n",
            "Step 0: Train Loss: 6.910542538207665e-08, Train Acc: 1.0\n",
            "Epoch 5486/10000\n",
            "Step 0: Train Loss: 9.053893990085271e-08, Train Acc: 1.0\n",
            "Epoch 5487/10000\n",
            "Step 0: Train Loss: 8.479312185727395e-08, Train Acc: 1.0\n",
            "Epoch 5488/10000\n",
            "Step 0: Train Loss: 7.722354666839237e-08, Train Acc: 1.0\n",
            "Epoch 5489/10000\n",
            "Step 0: Train Loss: 8.201551082720471e-08, Train Acc: 1.0\n",
            "Epoch 5490/10000\n",
            "Step 0: Train Loss: 8.174141896688525e-08, Train Acc: 1.0\n",
            "Epoch 5491/10000\n",
            "Step 0: Train Loss: 6.706683564061677e-08, Train Acc: 1.0\n",
            "Epoch 5492/10000\n",
            "Step 0: Train Loss: 7.570952931246211e-08, Train Acc: 1.0\n",
            "Epoch 5493/10000\n",
            "Step 0: Train Loss: 7.455303574488426e-08, Train Acc: 1.0\n",
            "Epoch 5494/10000\n",
            "Step 0: Train Loss: 9.219566976526039e-08, Train Acc: 1.0\n",
            "Epoch 5495/10000\n",
            "Step 0: Train Loss: 7.659167522433563e-08, Train Acc: 1.0\n",
            "Epoch 5496/10000\n",
            "Step 0: Train Loss: 7.817719449576543e-08, Train Acc: 1.0\n",
            "Epoch 5497/10000\n",
            "Step 0: Train Loss: 8.735601397802384e-08, Train Acc: 1.0\n",
            "Epoch 5498/10000\n",
            "Step 0: Train Loss: 8.095458525758659e-08, Train Acc: 1.0\n",
            "Epoch 5499/10000\n",
            "Step 0: Train Loss: 7.107237109948983e-08, Train Acc: 1.0\n",
            "Epoch 5500/10000\n",
            "Step 0: Train Loss: 6.846146760608463e-08, Train Acc: 1.0\n",
            "Epoch 5501/10000\n",
            "Step 0: Train Loss: 6.717421285884484e-08, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 5500 1024 1.016708088128777\n",
            "Epoch index and hidden dimension and ratio: 5500 20 0.9824169877920639\n",
            "Epoch index and hidden dimension and ratio: 5500 20 1.5232766205422947\n",
            "Epoch index and hidden dimension and ratio: 5500 20 3.894140966985127\n",
            "MI(X;T): [10.634937097474069, 7.867459931358757, 5.461289770629303, 3.156448760433737], MI(Y;T): [2.6247203497104667, 3.1844652840641814, 3.191577712786785, 2.96195852200281]\n",
            "Epoch index and hidden dimension and ratio: 5500 1024 1.0167099971210927\n",
            "Epoch index and hidden dimension and ratio: 5500 20 0.9824266427080108\n",
            "Epoch index and hidden dimension and ratio: 5500 20 1.5233040437857788\n",
            "Epoch index and hidden dimension and ratio: 5500 20 3.894230793750262\n",
            "MI(X;T): [10.634937097474069, 7.867581314701391, 5.461569728822045, 3.1556831966439107], MI(Y;T): [2.6247203497104663, 3.184465284064181, 3.191280831217636, 2.9619326538380797]\n",
            "Epoch index and hidden dimension and ratio: 5500 1024 1.0167121263817525\n",
            "Epoch index and hidden dimension and ratio: 5500 20 0.9824381418213858\n",
            "Epoch index and hidden dimension and ratio: 5500 20 1.5233305783130389\n",
            "Epoch index and hidden dimension and ratio: 5500 20 3.8942788329726414\n",
            "MI(X;T): [10.634813828465607, 7.867099908666829, 5.4614102933652084, 3.155991156835122], MI(Y;T): [2.6247203497104663, 3.184370751170033, 3.191166488650929, 2.9618617737046913]\n",
            "Epoch index and hidden dimension and ratio: 5500 1024 1.0167140353740682\n",
            "Epoch index and hidden dimension and ratio: 5500 20 0.9824525699542053\n",
            "Epoch index and hidden dimension and ratio: 5500 20 1.5233567319619172\n",
            "Epoch index and hidden dimension and ratio: 5500 20 3.894324897980403\n",
            "MI(X;T): [10.634187516065467, 7.86718963436414, 5.461083179335341, 3.1563681216789723], MI(Y;T): [2.6247203497104663, 3.1843958515655135, 3.1911369039694275, 2.961855022070851]\n",
            "Epoch index and hidden dimension and ratio: 5500 1024 1.0167165317486349\n",
            "Epoch index and hidden dimension and ratio: 5500 20 0.9824608146015308\n",
            "Epoch index and hidden dimension and ratio: 5500 20 1.523362445137643\n",
            "Epoch index and hidden dimension and ratio: 5500 20 3.8943255560519425\n",
            "MI(X;T): [10.634286181989332, 7.866960347868529, 5.461101157638591, 3.1569615806353326], MI(Y;T): [2.6247203497104663, 3.1843958515655135, 3.1910268245129036, 2.9618195056034224]\n",
            "Epoch index and hidden dimension and ratio: 5500 1024 1.0167186610092944\n",
            "Epoch index and hidden dimension and ratio: 5500 20 0.9824666726404199\n",
            "Epoch index and hidden dimension and ratio: 5500 20 1.5233552084483903\n",
            "Epoch index and hidden dimension and ratio: 5500 20 3.8942850846522665\n",
            "MI(X;T): [10.634286181989332, 7.867226681229003, 5.460919551957113, 3.155342650277759], MI(Y;T): [2.624631782256988, 3.184381436287379, 3.191166698934298, 2.9617803104843143]\n",
            "Epoch 5502/10000\n",
            "Step 0: Train Loss: 7.955974723472536e-08, Train Acc: 1.0\n",
            "Epoch 5503/10000\n",
            "Step 0: Train Loss: 6.381260675425438e-08, Train Acc: 1.0\n",
            "Epoch 5504/10000\n",
            "Step 0: Train Loss: 8.552019892249518e-08, Train Acc: 1.0\n",
            "Epoch 5505/10000\n",
            "Step 0: Train Loss: 8.948978091893878e-08, Train Acc: 1.0\n",
            "Epoch 5506/10000\n",
            "Step 0: Train Loss: 6.055819312678068e-08, Train Acc: 1.0\n",
            "Epoch 5507/10000\n",
            "Step 0: Train Loss: 8.97997409765594e-08, Train Acc: 1.0\n",
            "Epoch 5508/10000\n",
            "Step 0: Train Loss: 6.867630020224169e-08, Train Acc: 1.0\n",
            "Epoch 5509/10000\n",
            "Step 0: Train Loss: 5.969982908027305e-08, Train Acc: 1.0\n",
            "Epoch 5510/10000\n",
            "Step 0: Train Loss: 7.481544628262782e-08, Train Acc: 1.0\n",
            "Epoch 5511/10000\n",
            "Step 0: Train Loss: 7.604333518429485e-08, Train Acc: 1.0\n",
            "Epoch 5512/10000\n",
            "Step 0: Train Loss: 7.510139710120711e-08, Train Acc: 1.0\n",
            "Epoch 5513/10000\n",
            "Step 0: Train Loss: 7.731883755468516e-08, Train Acc: 1.0\n",
            "Epoch 5514/10000\n",
            "Step 0: Train Loss: 5.8746127962194805e-08, Train Acc: 1.0\n",
            "Epoch 5515/10000\n",
            "Step 0: Train Loss: 6.915305306165465e-08, Train Acc: 1.0\n",
            "Epoch 5516/10000\n",
            "Step 0: Train Loss: 8.01796531391119e-08, Train Acc: 1.0\n",
            "Epoch 5517/10000\n",
            "Step 0: Train Loss: 8.156246167345671e-08, Train Acc: 1.0\n",
            "Epoch 5518/10000\n",
            "Step 0: Train Loss: 8.299294762537102e-08, Train Acc: 1.0\n",
            "Epoch 5519/10000\n",
            "Step 0: Train Loss: 6.705499799863901e-08, Train Acc: 1.0\n",
            "Epoch 5520/10000\n",
            "Step 0: Train Loss: 7.666304924214273e-08, Train Acc: 1.0\n",
            "Epoch 5521/10000\n",
            "Step 0: Train Loss: 7.959564385373596e-08, Train Acc: 1.0\n",
            "Epoch 5522/10000\n",
            "Step 0: Train Loss: 7.131064450049962e-08, Train Acc: 1.0\n",
            "Epoch 5523/10000\n",
            "Step 0: Train Loss: 6.819942655056366e-08, Train Acc: 1.0\n",
            "Epoch 5524/10000\n",
            "Step 0: Train Loss: 6.148802356165106e-08, Train Acc: 1.0\n",
            "Epoch 5525/10000\n",
            "Step 0: Train Loss: 7.826057668580688e-08, Train Acc: 1.0\n",
            "Epoch 5526/10000\n",
            "Step 0: Train Loss: 6.170256483528647e-08, Train Acc: 1.0\n",
            "Epoch 5527/10000\n",
            "Step 0: Train Loss: 6.041508271437124e-08, Train Acc: 1.0\n",
            "Epoch 5528/10000\n",
            "Step 0: Train Loss: 7.456496575741767e-08, Train Acc: 1.0\n",
            "Epoch 5529/10000\n",
            "Step 0: Train Loss: 6.270391850193846e-08, Train Acc: 1.0\n",
            "Epoch 5530/10000\n",
            "Step 0: Train Loss: 6.638740046582825e-08, Train Acc: 1.0\n",
            "Epoch 5531/10000\n",
            "Step 0: Train Loss: 7.665113344046404e-08, Train Acc: 1.0\n",
            "Epoch 5532/10000\n",
            "Step 0: Train Loss: 6.962981302649496e-08, Train Acc: 1.0\n",
            "Epoch 5533/10000\n",
            "Step 0: Train Loss: 6.470641977784908e-08, Train Acc: 1.0\n",
            "Epoch 5534/10000\n",
            "Step 0: Train Loss: 7.251466627167247e-08, Train Acc: 1.0\n",
            "Epoch 5535/10000\n",
            "Step 0: Train Loss: 6.734109803119281e-08, Train Acc: 1.0\n",
            "Epoch 5536/10000\n",
            "Step 0: Train Loss: 6.170257904614118e-08, Train Acc: 1.0\n",
            "Epoch 5537/10000\n",
            "Step 0: Train Loss: 6.922450523916268e-08, Train Acc: 1.0\n",
            "Epoch 5538/10000\n",
            "Step 0: Train Loss: 6.686413200895913e-08, Train Acc: 1.0\n",
            "Epoch 5539/10000\n",
            "Step 0: Train Loss: 6.020047038646226e-08, Train Acc: 1.0\n",
            "Epoch 5540/10000\n",
            "Step 0: Train Loss: 8.380360583259971e-08, Train Acc: 1.0\n",
            "Epoch 5541/10000\n",
            "Step 0: Train Loss: 5.800704272473922e-08, Train Acc: 1.0\n",
            "Epoch 5542/10000\n",
            "Step 0: Train Loss: 6.470654767554151e-08, Train Acc: 1.0\n",
            "Epoch 5543/10000\n",
            "Step 0: Train Loss: 6.43966018287756e-08, Train Acc: 1.0\n",
            "Epoch 5544/10000\n",
            "Step 0: Train Loss: 6.951058395543441e-08, Train Acc: 1.0\n",
            "Epoch 5545/10000\n",
            "Step 0: Train Loss: 6.419395504053682e-08, Train Acc: 1.0\n",
            "Epoch 5546/10000\n",
            "Step 0: Train Loss: 6.774624239369587e-08, Train Acc: 1.0\n",
            "Epoch 5547/10000\n",
            "Step 0: Train Loss: 7.079803054921285e-08, Train Acc: 1.0\n",
            "Epoch 5548/10000\n",
            "Step 0: Train Loss: 5.531293112426283e-08, Train Acc: 1.0\n",
            "Epoch 5549/10000\n",
            "Step 0: Train Loss: 6.813967701191359e-08, Train Acc: 1.0\n",
            "Epoch 5550/10000\n",
            "Step 0: Train Loss: 6.715021072523086e-08, Train Acc: 1.0\n",
            "Epoch 5551/10000\n",
            "Step 0: Train Loss: 6.729333534849502e-08, Train Acc: 1.0\n",
            "Epoch 5552/10000\n",
            "Step 0: Train Loss: 6.774637739681566e-08, Train Acc: 1.0\n",
            "Epoch 5553/10000\n",
            "Step 0: Train Loss: 7.255043499299063e-08, Train Acc: 1.0\n",
            "Epoch 5554/10000\n",
            "Step 0: Train Loss: 7.243116328936594e-08, Train Acc: 1.0\n",
            "Epoch 5555/10000\n",
            "Step 0: Train Loss: 6.892643966693868e-08, Train Acc: 1.0\n",
            "Epoch 5556/10000\n",
            "Step 0: Train Loss: 7.23834077120955e-08, Train Acc: 1.0\n",
            "Epoch 5557/10000\n",
            "Step 0: Train Loss: 5.675537551041998e-08, Train Acc: 1.0\n",
            "Epoch 5558/10000\n",
            "Step 0: Train Loss: 6.148783882053976e-08, Train Acc: 1.0\n",
            "Epoch 5559/10000\n",
            "Step 0: Train Loss: 7.187083639337288e-08, Train Acc: 1.0\n",
            "Epoch 5560/10000\n",
            "Step 0: Train Loss: 6.293024767956013e-08, Train Acc: 1.0\n",
            "Epoch 5561/10000\n",
            "Step 0: Train Loss: 5.428776361782184e-08, Train Acc: 1.0\n",
            "Epoch 5562/10000\n",
            "Step 0: Train Loss: 6.155944731744967e-08, Train Acc: 1.0\n",
            "Epoch 5563/10000\n",
            "Step 0: Train Loss: 5.040157802227441e-08, Train Acc: 1.0\n",
            "Epoch 5564/10000\n",
            "Step 0: Train Loss: 5.662425550667649e-08, Train Acc: 1.0\n",
            "Epoch 5565/10000\n",
            "Step 0: Train Loss: 5.539643055385568e-08, Train Acc: 1.0\n",
            "Epoch 5566/10000\n",
            "Step 0: Train Loss: 4.39881766567396e-08, Train Acc: 1.0\n",
            "Epoch 5567/10000\n",
            "Step 0: Train Loss: 5.682693426933838e-08, Train Acc: 1.0\n",
            "Epoch 5568/10000\n",
            "Step 0: Train Loss: 6.188129475503956e-08, Train Acc: 1.0\n",
            "Epoch 5569/10000\n",
            "Step 0: Train Loss: 6.699531951426252e-08, Train Acc: 1.0\n",
            "Epoch 5570/10000\n",
            "Step 0: Train Loss: 6.904567584342658e-08, Train Acc: 1.0\n",
            "Epoch 5571/10000\n",
            "Step 0: Train Loss: 6.097525329096243e-08, Train Acc: 1.0\n",
            "Epoch 5572/10000\n",
            "Step 0: Train Loss: 4.850617685292491e-08, Train Acc: 1.0\n",
            "Epoch 5573/10000\n",
            "Step 0: Train Loss: 5.704146488483275e-08, Train Acc: 1.0\n",
            "Epoch 5574/10000\n",
            "Step 0: Train Loss: 5.968789196231228e-08, Train Acc: 1.0\n",
            "Epoch 5575/10000\n",
            "Step 0: Train Loss: 5.5491788941708364e-08, Train Acc: 1.0\n",
            "Epoch 5576/10000\n",
            "Step 0: Train Loss: 6.209594261008533e-08, Train Acc: 1.0\n",
            "Epoch 5577/10000\n",
            "Step 0: Train Loss: 5.7279848419966584e-08, Train Acc: 1.0\n",
            "Epoch 5578/10000\n",
            "Step 0: Train Loss: 5.5861182346461646e-08, Train Acc: 1.0\n",
            "Epoch 5579/10000\n",
            "Step 0: Train Loss: 6.009307185195212e-08, Train Acc: 1.0\n",
            "Epoch 5580/10000\n",
            "Step 0: Train Loss: 5.173669848090867e-08, Train Acc: 1.0\n",
            "Epoch 5581/10000\n",
            "Step 0: Train Loss: 6.434886046235988e-08, Train Acc: 1.0\n",
            "Epoch 5582/10000\n",
            "Step 0: Train Loss: 5.3500961882946285e-08, Train Acc: 1.0\n",
            "Epoch 5583/10000\n",
            "Step 0: Train Loss: 5.3179125103497427e-08, Train Acc: 1.0\n",
            "Epoch 5584/10000\n",
            "Step 0: Train Loss: 6.279918807194917e-08, Train Acc: 1.0\n",
            "Epoch 5585/10000\n",
            "Step 0: Train Loss: 5.034186045804745e-08, Train Acc: 1.0\n",
            "Epoch 5586/10000\n",
            "Step 0: Train Loss: 5.7112977458473324e-08, Train Acc: 1.0\n",
            "Epoch 5587/10000\n",
            "Step 0: Train Loss: 5.489566845540139e-08, Train Acc: 1.0\n",
            "Epoch 5588/10000\n",
            "Step 0: Train Loss: 5.304802286332233e-08, Train Acc: 1.0\n",
            "Epoch 5589/10000\n",
            "Step 0: Train Loss: 5.76613352620825e-08, Train Acc: 1.0\n",
            "Epoch 5590/10000\n",
            "Step 0: Train Loss: 6.328791357645969e-08, Train Acc: 1.0\n",
            "Epoch 5591/10000\n",
            "Step 0: Train Loss: 5.261873425865815e-08, Train Acc: 1.0\n",
            "Epoch 5592/10000\n",
            "Step 0: Train Loss: 5.022276283739302e-08, Train Acc: 1.0\n",
            "Epoch 5593/10000\n",
            "Step 0: Train Loss: 5.331021313281781e-08, Train Acc: 1.0\n",
            "Epoch 5594/10000\n",
            "Step 0: Train Loss: 5.176056916411653e-08, Train Acc: 1.0\n",
            "Epoch 5595/10000\n",
            "Step 0: Train Loss: 5.722023033172263e-08, Train Acc: 1.0\n",
            "Epoch 5596/10000\n",
            "Step 0: Train Loss: 5.3763169916010156e-08, Train Acc: 1.0\n",
            "Epoch 5597/10000\n",
            "Step 0: Train Loss: 4.6694168531757896e-08, Train Acc: 1.0\n",
            "Epoch 5598/10000\n",
            "Step 0: Train Loss: 5.4228149082291566e-08, Train Acc: 1.0\n",
            "Epoch 5599/10000\n",
            "Step 0: Train Loss: 5.9926243523023e-08, Train Acc: 1.0\n",
            "Epoch 5600/10000\n",
            "Step 0: Train Loss: 5.333405539431624e-08, Train Acc: 1.0\n",
            "Epoch 5601/10000\n",
            "Step 0: Train Loss: 5.82096397749865e-08, Train Acc: 1.0\n",
            "Epoch 5602/10000\n",
            "Step 0: Train Loss: 4.7385551482648225e-08, Train Acc: 1.0\n",
            "Epoch 5603/10000\n",
            "Step 0: Train Loss: 5.35605693130492e-08, Train Acc: 1.0\n",
            "Epoch 5604/10000\n",
            "Step 0: Train Loss: 6.251313777738687e-08, Train Acc: 1.0\n",
            "Epoch 5605/10000\n",
            "Step 0: Train Loss: 5.6469165343742134e-08, Train Acc: 1.0\n",
            "Epoch 5606/10000\n",
            "Step 0: Train Loss: 5.116444512509588e-08, Train Acc: 1.0\n",
            "Epoch 5607/10000\n",
            "Step 0: Train Loss: 6.073683067597813e-08, Train Acc: 1.0\n",
            "Epoch 5608/10000\n",
            "Step 0: Train Loss: 4.938829079037532e-08, Train Acc: 1.0\n",
            "Epoch 5609/10000\n",
            "Step 0: Train Loss: 4.7170992445444426e-08, Train Acc: 1.0\n",
            "Epoch 5610/10000\n",
            "Step 0: Train Loss: 5.2678398532179926e-08, Train Acc: 1.0\n",
            "Epoch 5611/10000\n",
            "Step 0: Train Loss: 5.202275232818465e-08, Train Acc: 1.0\n",
            "Epoch 5612/10000\n",
            "Step 0: Train Loss: 4.918567242384597e-08, Train Acc: 1.0\n",
            "Epoch 5613/10000\n",
            "Step 0: Train Loss: 6.227471516240257e-08, Train Acc: 1.0\n",
            "Epoch 5614/10000\n",
            "Step 0: Train Loss: 5.869845765005266e-08, Train Acc: 1.0\n",
            "Epoch 5615/10000\n",
            "Step 0: Train Loss: 5.227309785027501e-08, Train Acc: 1.0\n",
            "Epoch 5616/10000\n",
            "Step 0: Train Loss: 5.1283620905451244e-08, Train Acc: 1.0\n",
            "Epoch 5617/10000\n",
            "Step 0: Train Loss: 5.0210807955863856e-08, Train Acc: 1.0\n",
            "Epoch 5618/10000\n",
            "Step 0: Train Loss: 5.996200513891381e-08, Train Acc: 1.0\n",
            "Epoch 5619/10000\n",
            "Step 0: Train Loss: 5.234453581692833e-08, Train Acc: 1.0\n",
            "Epoch 5620/10000\n",
            "Step 0: Train Loss: 4.765967531739079e-08, Train Acc: 1.0\n",
            "Epoch 5621/10000\n",
            "Step 0: Train Loss: 5.062802799216115e-08, Train Acc: 1.0\n",
            "Epoch 5622/10000\n",
            "Step 0: Train Loss: 5.623075338689887e-08, Train Acc: 1.0\n",
            "Epoch 5623/10000\n",
            "Step 0: Train Loss: 5.853147655443536e-08, Train Acc: 1.0\n",
            "Epoch 5624/10000\n",
            "Step 0: Train Loss: 5.682687032049216e-08, Train Acc: 1.0\n",
            "Epoch 5625/10000\n",
            "Step 0: Train Loss: 3.8230322019217056e-08, Train Acc: 1.0\n",
            "Epoch 5626/10000\n",
            "Step 0: Train Loss: 4.70159982057794e-08, Train Acc: 1.0\n",
            "Epoch 5627/10000\n",
            "Step 0: Train Loss: 5.8412329195789425e-08, Train Acc: 1.0\n",
            "Epoch 5628/10000\n",
            "Step 0: Train Loss: 5.141483327975038e-08, Train Acc: 1.0\n",
            "Epoch 5629/10000\n",
            "Step 0: Train Loss: 4.374971140919115e-08, Train Acc: 1.0\n",
            "Epoch 5630/10000\n",
            "Step 0: Train Loss: 5.6838743489606713e-08, Train Acc: 1.0\n",
            "Epoch 5631/10000\n",
            "Step 0: Train Loss: 4.677758980164981e-08, Train Acc: 1.0\n",
            "Epoch 5632/10000\n",
            "Step 0: Train Loss: 5.555127202683252e-08, Train Acc: 1.0\n",
            "Epoch 5633/10000\n",
            "Step 0: Train Loss: 4.615771587168638e-08, Train Acc: 1.0\n",
            "Epoch 5634/10000\n",
            "Step 0: Train Loss: 4.9662403966976854e-08, Train Acc: 1.0\n",
            "Epoch 5635/10000\n",
            "Step 0: Train Loss: 4.894718230730177e-08, Train Acc: 1.0\n",
            "Epoch 5636/10000\n",
            "Step 0: Train Loss: 4.618154747504377e-08, Train Acc: 1.0\n",
            "Epoch 5637/10000\n",
            "Step 0: Train Loss: 5.376319123229223e-08, Train Acc: 1.0\n",
            "Epoch 5638/10000\n",
            "Step 0: Train Loss: 5.328638508217409e-08, Train Acc: 1.0\n",
            "Epoch 5639/10000\n",
            "Step 0: Train Loss: 5.3834700253219125e-08, Train Acc: 1.0\n",
            "Epoch 5640/10000\n",
            "Step 0: Train Loss: 3.901715217580204e-08, Train Acc: 1.0\n",
            "Epoch 5641/10000\n",
            "Step 0: Train Loss: 4.42861249894122e-08, Train Acc: 1.0\n",
            "Epoch 5642/10000\n",
            "Step 0: Train Loss: 4.9996142337249694e-08, Train Acc: 1.0\n",
            "Epoch 5643/10000\n",
            "Step 0: Train Loss: 4.661063357502826e-08, Train Acc: 1.0\n",
            "Epoch 5644/10000\n",
            "Step 0: Train Loss: 5.925866375378064e-08, Train Acc: 1.0\n",
            "Epoch 5645/10000\n",
            "Step 0: Train Loss: 4.227154803970734e-08, Train Acc: 1.0\n",
            "Epoch 5646/10000\n",
            "Step 0: Train Loss: 4.290330224421268e-08, Train Acc: 1.0\n",
            "Epoch 5647/10000\n",
            "Step 0: Train Loss: 3.973239515175919e-08, Train Acc: 1.0\n",
            "Epoch 5648/10000\n",
            "Step 0: Train Loss: 5.069955832937012e-08, Train Acc: 1.0\n",
            "Epoch 5649/10000\n",
            "Step 0: Train Loss: 4.4596067283464436e-08, Train Acc: 1.0\n",
            "Epoch 5650/10000\n",
            "Step 0: Train Loss: 4.2939038991107736e-08, Train Acc: 1.0\n",
            "Epoch 5651/10000\n",
            "Step 0: Train Loss: 4.518021157196017e-08, Train Acc: 1.0\n",
            "Epoch 5652/10000\n",
            "Step 0: Train Loss: 5.240423561758689e-08, Train Acc: 1.0\n",
            "Epoch 5653/10000\n",
            "Step 0: Train Loss: 4.597890068680499e-08, Train Acc: 1.0\n",
            "Epoch 5654/10000\n",
            "Step 0: Train Loss: 4.012576937384438e-08, Train Acc: 1.0\n",
            "Epoch 5655/10000\n",
            "Step 0: Train Loss: 4.214037474525867e-08, Train Acc: 1.0\n",
            "Epoch 5656/10000\n",
            "Step 0: Train Loss: 4.2521836718378836e-08, Train Acc: 1.0\n",
            "Epoch 5657/10000\n",
            "Step 0: Train Loss: 5.516984558084914e-08, Train Acc: 1.0\n",
            "Epoch 5658/10000\n",
            "Step 0: Train Loss: 3.9458154077465224e-08, Train Acc: 1.0\n",
            "Epoch 5659/10000\n",
            "Step 0: Train Loss: 3.582234597843126e-08, Train Acc: 1.0\n",
            "Epoch 5660/10000\n",
            "Step 0: Train Loss: 4.6872912662365707e-08, Train Acc: 1.0\n",
            "Epoch 5661/10000\n",
            "Step 0: Train Loss: 4.175889145585643e-08, Train Acc: 1.0\n",
            "Epoch 5662/10000\n",
            "Step 0: Train Loss: 4.5549690241841745e-08, Train Acc: 1.0\n",
            "Epoch 5663/10000\n",
            "Step 0: Train Loss: 3.993501707100222e-08, Train Acc: 1.0\n",
            "Epoch 5664/10000\n",
            "Step 0: Train Loss: 3.820649041585966e-08, Train Acc: 1.0\n",
            "Epoch 5665/10000\n",
            "Step 0: Train Loss: 5.0008139851343e-08, Train Acc: 1.0\n",
            "Epoch 5666/10000\n",
            "Step 0: Train Loss: 4.898291194876947e-08, Train Acc: 1.0\n",
            "Epoch 5667/10000\n",
            "Step 0: Train Loss: 3.563156880659335e-08, Train Acc: 1.0\n",
            "Epoch 5668/10000\n",
            "Step 0: Train Loss: 4.3880842071075676e-08, Train Acc: 1.0\n",
            "Epoch 5669/10000\n",
            "Step 0: Train Loss: 4.376160944730145e-08, Train Acc: 1.0\n",
            "Epoch 5670/10000\n",
            "Step 0: Train Loss: 4.2617209317086235e-08, Train Acc: 1.0\n",
            "Epoch 5671/10000\n",
            "Step 0: Train Loss: 4.404769882171422e-08, Train Acc: 1.0\n",
            "Epoch 5672/10000\n",
            "Step 0: Train Loss: 4.3630450363707496e-08, Train Acc: 1.0\n",
            "Epoch 5673/10000\n",
            "Step 0: Train Loss: 4.974585721129188e-08, Train Acc: 1.0\n",
            "Epoch 5674/10000\n",
            "Step 0: Train Loss: 4.304639134034005e-08, Train Acc: 1.0\n",
            "Epoch 5675/10000\n",
            "Step 0: Train Loss: 4.150855659190711e-08, Train Acc: 1.0\n",
            "Epoch 5676/10000\n",
            "Step 0: Train Loss: 4.0495269360008024e-08, Train Acc: 1.0\n",
            "Epoch 5677/10000\n",
            "Step 0: Train Loss: 4.2009265399656215e-08, Train Acc: 1.0\n",
            "Epoch 5678/10000\n",
            "Step 0: Train Loss: 4.289134025725616e-08, Train Acc: 1.0\n",
            "Epoch 5679/10000\n",
            "Step 0: Train Loss: 3.8874048868819955e-08, Train Acc: 1.0\n",
            "Epoch 5680/10000\n",
            "Step 0: Train Loss: 4.6098076467160354e-08, Train Acc: 1.0\n",
            "Epoch 5681/10000\n",
            "Step 0: Train Loss: 3.844488460913453e-08, Train Acc: 1.0\n",
            "Epoch 5682/10000\n",
            "Step 0: Train Loss: 3.5667330422484156e-08, Train Acc: 1.0\n",
            "Epoch 5683/10000\n",
            "Step 0: Train Loss: 4.1782740822782216e-08, Train Acc: 1.0\n",
            "Epoch 5684/10000\n",
            "Step 0: Train Loss: 4.153241661697393e-08, Train Acc: 1.0\n",
            "Epoch 5685/10000\n",
            "Step 0: Train Loss: 2.7883027442499042e-08, Train Acc: 1.0\n",
            "Epoch 5686/10000\n",
            "Step 0: Train Loss: 3.747930676922806e-08, Train Acc: 1.0\n",
            "Epoch 5687/10000\n",
            "Step 0: Train Loss: 4.3439737140715806e-08, Train Acc: 1.0\n",
            "Epoch 5688/10000\n",
            "Step 0: Train Loss: 4.124629171542438e-08, Train Acc: 1.0\n",
            "Epoch 5689/10000\n",
            "Step 0: Train Loss: 3.5667358844193586e-08, Train Acc: 1.0\n",
            "Epoch 5690/10000\n",
            "Step 0: Train Loss: 4.196153469138153e-08, Train Acc: 1.0\n",
            "Epoch 5691/10000\n",
            "Step 0: Train Loss: 3.828993300203365e-08, Train Acc: 1.0\n",
            "Epoch 5692/10000\n",
            "Step 0: Train Loss: 3.8659489831616156e-08, Train Acc: 1.0\n",
            "Epoch 5693/10000\n",
            "Step 0: Train Loss: 4.043568679890086e-08, Train Acc: 1.0\n",
            "Epoch 5694/10000\n",
            "Step 0: Train Loss: 3.776538193278611e-08, Train Acc: 1.0\n",
            "Epoch 5695/10000\n",
            "Step 0: Train Loss: 3.56911868948373e-08, Train Acc: 1.0\n",
            "Epoch 5696/10000\n",
            "Step 0: Train Loss: 3.38077015271665e-08, Train Acc: 1.0\n",
            "Epoch 5697/10000\n",
            "Step 0: Train Loss: 4.136551723377124e-08, Train Acc: 1.0\n",
            "Epoch 5698/10000\n",
            "Step 0: Train Loss: 3.4546786764622084e-08, Train Acc: 1.0\n",
            "Epoch 5699/10000\n",
            "Step 0: Train Loss: 3.491634359420459e-08, Train Acc: 1.0\n",
            "Epoch 5700/10000\n",
            "Step 0: Train Loss: 3.8313793027100473e-08, Train Acc: 1.0\n",
            "Epoch 5701/10000\n",
            "Step 0: Train Loss: 3.480901611396803e-08, Train Acc: 1.0\n",
            "Epoch 5702/10000\n",
            "Step 0: Train Loss: 3.652561986200453e-08, Train Acc: 1.0\n",
            "Epoch 5703/10000\n",
            "Step 0: Train Loss: 3.614415078345701e-08, Train Acc: 1.0\n",
            "Epoch 5704/10000\n",
            "Step 0: Train Loss: 4.466755498810926e-08, Train Acc: 1.0\n",
            "Epoch 5705/10000\n",
            "Step 0: Train Loss: 3.7312389622456976e-08, Train Acc: 1.0\n",
            "Epoch 5706/10000\n",
            "Step 0: Train Loss: 3.0112236260038117e-08, Train Acc: 1.0\n",
            "Epoch 5707/10000\n",
            "Step 0: Train Loss: 4.1901927261278615e-08, Train Acc: 1.0\n",
            "Epoch 5708/10000\n",
            "Step 0: Train Loss: 3.596536046757137e-08, Train Acc: 1.0\n",
            "Epoch 5709/10000\n",
            "Step 0: Train Loss: 4.1580094034543436e-08, Train Acc: 1.0\n",
            "Epoch 5710/10000\n",
            "Step 0: Train Loss: 3.752699839765228e-08, Train Acc: 1.0\n",
            "Epoch 5711/10000\n",
            "Step 0: Train Loss: 3.9434336684962545e-08, Train Acc: 1.0\n",
            "Epoch 5712/10000\n",
            "Step 0: Train Loss: 4.106747297782931e-08, Train Acc: 1.0\n",
            "Epoch 5713/10000\n",
            "Step 0: Train Loss: 3.312820240353176e-08, Train Acc: 1.0\n",
            "Epoch 5714/10000\n",
            "Step 0: Train Loss: 3.8170703930973104e-08, Train Acc: 1.0\n",
            "Epoch 5715/10000\n",
            "Step 0: Train Loss: 3.557194006020836e-08, Train Acc: 1.0\n",
            "Epoch 5716/10000\n",
            "Step 0: Train Loss: 3.5560027811243344e-08, Train Acc: 1.0\n",
            "Epoch 5717/10000\n",
            "Step 0: Train Loss: 4.3105941927024105e-08, Train Acc: 1.0\n",
            "Epoch 5718/10000\n",
            "Step 0: Train Loss: 3.818263039079284e-08, Train Acc: 1.0\n",
            "Epoch 5719/10000\n",
            "Step 0: Train Loss: 3.044599949930671e-08, Train Acc: 1.0\n",
            "Epoch 5720/10000\n",
            "Step 0: Train Loss: 3.5393160402463764e-08, Train Acc: 1.0\n",
            "Epoch 5721/10000\n",
            "Step 0: Train Loss: 4.246220797199385e-08, Train Acc: 1.0\n",
            "Epoch 5722/10000\n",
            "Step 0: Train Loss: 2.6702840827397267e-08, Train Acc: 1.0\n",
            "Epoch 5723/10000\n",
            "Step 0: Train Loss: 3.807536685940249e-08, Train Acc: 1.0\n",
            "Epoch 5724/10000\n",
            "Step 0: Train Loss: 3.769389422814129e-08, Train Acc: 1.0\n",
            "Epoch 5725/10000\n",
            "Step 0: Train Loss: 3.288979399940217e-08, Train Acc: 1.0\n",
            "Epoch 5726/10000\n",
            "Step 0: Train Loss: 2.4378282503789706e-08, Train Acc: 1.0\n",
            "Epoch 5727/10000\n",
            "Step 0: Train Loss: 3.40818537836185e-08, Train Acc: 1.0\n",
            "Epoch 5728/10000\n",
            "Step 0: Train Loss: 3.218645971969636e-08, Train Acc: 1.0\n",
            "Epoch 5729/10000\n",
            "Step 0: Train Loss: 3.125659375768919e-08, Train Acc: 1.0\n",
            "Epoch 5730/10000\n",
            "Step 0: Train Loss: 3.6322994390047825e-08, Train Acc: 1.0\n",
            "Epoch 5731/10000\n",
            "Step 0: Train Loss: 3.2949376560509336e-08, Train Acc: 1.0\n",
            "Epoch 5732/10000\n",
            "Step 0: Train Loss: 3.341428822523085e-08, Train Acc: 1.0\n",
            "Epoch 5733/10000\n",
            "Step 0: Train Loss: 3.896942502024103e-08, Train Acc: 1.0\n",
            "Epoch 5734/10000\n",
            "Step 0: Train Loss: 3.6299134364981e-08, Train Acc: 1.0\n",
            "Epoch 5735/10000\n",
            "Step 0: Train Loss: 3.1137389555624395e-08, Train Acc: 1.0\n",
            "Epoch 5736/10000\n",
            "Step 0: Train Loss: 3.2460604870721e-08, Train Acc: 1.0\n",
            "Epoch 5737/10000\n",
            "Step 0: Train Loss: 3.310436014203333e-08, Train Acc: 1.0\n",
            "Epoch 5738/10000\n",
            "Step 0: Train Loss: 3.345003207755326e-08, Train Acc: 1.0\n",
            "Epoch 5739/10000\n",
            "Step 0: Train Loss: 3.329510178673445e-08, Train Acc: 1.0\n",
            "Epoch 5740/10000\n",
            "Step 0: Train Loss: 3.2901695590226154e-08, Train Acc: 1.0\n",
            "Epoch 5741/10000\n",
            "Step 0: Train Loss: 3.619186017544962e-08, Train Acc: 1.0\n",
            "Epoch 5742/10000\n",
            "Step 0: Train Loss: 3.402223214266087e-08, Train Acc: 1.0\n",
            "Epoch 5743/10000\n",
            "Step 0: Train Loss: 2.8240657812261816e-08, Train Acc: 1.0\n",
            "Epoch 5744/10000\n",
            "Step 0: Train Loss: 3.8063422636014366e-08, Train Acc: 1.0\n",
            "Epoch 5745/10000\n",
            "Step 0: Train Loss: 3.3295080470452376e-08, Train Acc: 1.0\n",
            "Epoch 5746/10000\n",
            "Step 0: Train Loss: 3.7073963454758996e-08, Train Acc: 1.0\n",
            "Epoch 5747/10000\n",
            "Step 0: Train Loss: 3.124470282500624e-08, Train Acc: 1.0\n",
            "Epoch 5748/10000\n",
            "Step 0: Train Loss: 3.396265668698106e-08, Train Acc: 1.0\n",
            "Epoch 5749/10000\n",
            "Step 0: Train Loss: 3.160233319476902e-08, Train Acc: 1.0\n",
            "Epoch 5750/10000\n",
            "Step 0: Train Loss: 2.8848610611476033e-08, Train Acc: 1.0\n",
            "Epoch 5751/10000\n",
            "Step 0: Train Loss: 3.00287652521547e-08, Train Acc: 1.0\n",
            "Epoch 5752/10000\n",
            "Step 0: Train Loss: 3.210298871181294e-08, Train Acc: 1.0\n",
            "Epoch 5753/10000\n",
            "Step 0: Train Loss: 2.9289676461985437e-08, Train Acc: 1.0\n",
            "Epoch 5754/10000\n",
            "Step 0: Train Loss: 3.056520014865782e-08, Train Acc: 1.0\n",
            "Epoch 5755/10000\n",
            "Step 0: Train Loss: 3.839721784970607e-08, Train Acc: 1.0\n",
            "Epoch 5756/10000\n",
            "Step 0: Train Loss: 2.5069686770962107e-08, Train Acc: 1.0\n",
            "Epoch 5757/10000\n",
            "Step 0: Train Loss: 3.132812054218448e-08, Train Acc: 1.0\n",
            "Epoch 5758/10000\n",
            "Step 0: Train Loss: 3.329509468130709e-08, Train Acc: 1.0\n",
            "Epoch 5759/10000\n",
            "Step 0: Train Loss: 3.2699048801987374e-08, Train Acc: 1.0\n",
            "Epoch 5760/10000\n",
            "Step 0: Train Loss: 3.063674114400783e-08, Train Acc: 1.0\n",
            "Epoch 5761/10000\n",
            "Step 0: Train Loss: 2.889627381819082e-08, Train Acc: 1.0\n",
            "Epoch 5762/10000\n",
            "Step 0: Train Loss: 3.722896479985138e-08, Train Acc: 1.0\n",
            "Epoch 5763/10000\n",
            "Step 0: Train Loss: 3.1578494485984265e-08, Train Acc: 1.0\n",
            "Epoch 5764/10000\n",
            "Step 0: Train Loss: 3.0386388516490115e-08, Train Acc: 1.0\n",
            "Epoch 5765/10000\n",
            "Step 0: Train Loss: 3.2353341339330655e-08, Train Acc: 1.0\n",
            "Epoch 5766/10000\n",
            "Step 0: Train Loss: 3.011219718018765e-08, Train Acc: 1.0\n",
            "Epoch 5767/10000\n",
            "Step 0: Train Loss: 3.005260751365313e-08, Train Acc: 1.0\n",
            "Epoch 5768/10000\n",
            "Step 0: Train Loss: 3.2818260109479525e-08, Train Acc: 1.0\n",
            "Epoch 5769/10000\n",
            "Step 0: Train Loss: 3.0612881118941004e-08, Train Acc: 1.0\n",
            "Epoch 5770/10000\n",
            "Step 0: Train Loss: 2.892012140875977e-08, Train Acc: 1.0\n",
            "Epoch 5771/10000\n",
            "Step 0: Train Loss: 2.7000863767057126e-08, Train Acc: 1.0\n",
            "Epoch 5772/10000\n",
            "Step 0: Train Loss: 2.734655346614545e-08, Train Acc: 1.0\n",
            "Epoch 5773/10000\n",
            "Step 0: Train Loss: 3.252023006439231e-08, Train Acc: 1.0\n",
            "Epoch 5774/10000\n",
            "Step 0: Train Loss: 2.9253900635239916e-08, Train Acc: 1.0\n",
            "Epoch 5775/10000\n",
            "Step 0: Train Loss: 2.696510392752316e-08, Train Acc: 1.0\n",
            "Epoch 5776/10000\n",
            "Step 0: Train Loss: 3.1471188322029775e-08, Train Acc: 1.0\n",
            "Epoch 5777/10000\n",
            "Step 0: Train Loss: 2.706047297351688e-08, Train Acc: 1.0\n",
            "Epoch 5778/10000\n",
            "Step 0: Train Loss: 2.7549226899736823e-08, Train Acc: 1.0\n",
            "Epoch 5779/10000\n",
            "Step 0: Train Loss: 3.00764426697242e-08, Train Acc: 1.0\n",
            "Epoch 5780/10000\n",
            "Step 0: Train Loss: 3.113741442462015e-08, Train Acc: 1.0\n",
            "Epoch 5781/10000\n",
            "Step 0: Train Loss: 3.0088337155120826e-08, Train Acc: 1.0\n",
            "Epoch 5782/10000\n",
            "Step 0: Train Loss: 2.5129288871994504e-08, Train Acc: 1.0\n",
            "Epoch 5783/10000\n",
            "Step 0: Train Loss: 2.7835328708647467e-08, Train Acc: 1.0\n",
            "Epoch 5784/10000\n",
            "Step 0: Train Loss: 2.6345198023136618e-08, Train Acc: 1.0\n",
            "Epoch 5785/10000\n",
            "Step 0: Train Loss: 2.9563851811076347e-08, Train Acc: 1.0\n",
            "Epoch 5786/10000\n",
            "Step 0: Train Loss: 2.70366058430227e-08, Train Acc: 1.0\n",
            "Epoch 5787/10000\n",
            "Step 0: Train Loss: 2.89916499696119e-08, Train Acc: 1.0\n",
            "Epoch 5788/10000\n",
            "Step 0: Train Loss: 3.268710102588557e-08, Train Acc: 1.0\n",
            "Epoch 5789/10000\n",
            "Step 0: Train Loss: 2.7942618885390402e-08, Train Acc: 1.0\n",
            "Epoch 5790/10000\n",
            "Step 0: Train Loss: 2.7930672885645436e-08, Train Acc: 1.0\n",
            "Epoch 5791/10000\n",
            "Step 0: Train Loss: 3.0040684606547075e-08, Train Acc: 1.0\n",
            "Epoch 5792/10000\n",
            "Step 0: Train Loss: 2.6106800277148068e-08, Train Acc: 1.0\n",
            "Epoch 5793/10000\n",
            "Step 0: Train Loss: 2.486703110093913e-08, Train Acc: 1.0\n",
            "Epoch 5794/10000\n",
            "Step 0: Train Loss: 2.4723965097450673e-08, Train Acc: 1.0\n",
            "Epoch 5795/10000\n",
            "Step 0: Train Loss: 2.566571311035659e-08, Train Acc: 1.0\n",
            "Epoch 5796/10000\n",
            "Step 0: Train Loss: 2.499814932832578e-08, Train Acc: 1.0\n",
            "Epoch 5797/10000\n",
            "Step 0: Train Loss: 2.8121425188487592e-08, Train Acc: 1.0\n",
            "Epoch 5798/10000\n",
            "Step 0: Train Loss: 2.937312615358678e-08, Train Acc: 1.0\n",
            "Epoch 5799/10000\n",
            "Step 0: Train Loss: 2.5176966289564007e-08, Train Acc: 1.0\n",
            "Epoch 5800/10000\n",
            "Step 0: Train Loss: 3.0302956588457164e-08, Train Acc: 1.0\n",
            "Epoch 5801/10000\n",
            "Step 0: Train Loss: 2.5868381214877445e-08, Train Acc: 1.0\n",
            "Epoch 5802/10000\n",
            "Step 0: Train Loss: 2.6917410522742102e-08, Train Acc: 1.0\n",
            "Epoch 5803/10000\n",
            "Step 0: Train Loss: 2.284046018985464e-08, Train Acc: 1.0\n",
            "Epoch 5804/10000\n",
            "Step 0: Train Loss: 2.8383666972331412e-08, Train Acc: 1.0\n",
            "Epoch 5805/10000\n",
            "Step 0: Train Loss: 3.008837623497129e-08, Train Acc: 1.0\n",
            "Epoch 5806/10000\n",
            "Step 0: Train Loss: 2.701278489780634e-08, Train Acc: 1.0\n",
            "Epoch 5807/10000\n",
            "Step 0: Train Loss: 2.6214088677534164e-08, Train Acc: 1.0\n",
            "Epoch 5808/10000\n",
            "Step 0: Train Loss: 2.8872445767547106e-08, Train Acc: 1.0\n",
            "Epoch 5809/10000\n",
            "Step 0: Train Loss: 2.739423976549915e-08, Train Acc: 1.0\n",
            "Epoch 5810/10000\n",
            "Step 0: Train Loss: 2.6833964383854436e-08, Train Acc: 1.0\n",
            "Epoch 5811/10000\n",
            "Step 0: Train Loss: 2.5963752037228005e-08, Train Acc: 1.0\n",
            "Epoch 5812/10000\n",
            "Step 0: Train Loss: 2.638096674445478e-08, Train Acc: 1.0\n",
            "Epoch 5813/10000\n",
            "Step 0: Train Loss: 2.4676278798096973e-08, Train Acc: 1.0\n",
            "Epoch 5814/10000\n",
            "Step 0: Train Loss: 2.6428638832953766e-08, Train Acc: 1.0\n",
            "Epoch 5815/10000\n",
            "Step 0: Train Loss: 2.692932277170712e-08, Train Acc: 1.0\n",
            "Epoch 5816/10000\n",
            "Step 0: Train Loss: 2.7441922512139172e-08, Train Acc: 1.0\n",
            "Epoch 5817/10000\n",
            "Step 0: Train Loss: 2.3353070588427727e-08, Train Acc: 1.0\n",
            "Epoch 5818/10000\n",
            "Step 0: Train Loss: 2.530810228051905e-08, Train Acc: 1.0\n",
            "Epoch 5819/10000\n",
            "Step 0: Train Loss: 2.5296165162558282e-08, Train Acc: 1.0\n",
            "Epoch 5820/10000\n",
            "Step 0: Train Loss: 2.534385501462566e-08, Train Acc: 1.0\n",
            "Epoch 5821/10000\n",
            "Step 0: Train Loss: 3.199571096956788e-08, Train Acc: 1.0\n",
            "Epoch 5822/10000\n",
            "Step 0: Train Loss: 2.1851036535736057e-08, Train Acc: 1.0\n",
            "Epoch 5823/10000\n",
            "Step 0: Train Loss: 2.2196747551106455e-08, Train Acc: 1.0\n",
            "Epoch 5824/10000\n",
            "Step 0: Train Loss: 2.20417657459393e-08, Train Acc: 1.0\n",
            "Epoch 5825/10000\n",
            "Step 0: Train Loss: 2.4568999279495074e-08, Train Acc: 1.0\n",
            "Epoch 5826/10000\n",
            "Step 0: Train Loss: 2.654786790401431e-08, Train Acc: 1.0\n",
            "Epoch 5827/10000\n",
            "Step 0: Train Loss: 2.1457656984580353e-08, Train Acc: 1.0\n",
            "Epoch 5828/10000\n",
            "Step 0: Train Loss: 2.174375346442048e-08, Train Acc: 1.0\n",
            "Epoch 5829/10000\n",
            "Step 0: Train Loss: 2.6321368196136063e-08, Train Acc: 1.0\n",
            "Epoch 5830/10000\n",
            "Step 0: Train Loss: 2.040861524221782e-08, Train Acc: 1.0\n",
            "Epoch 5831/10000\n",
            "Step 0: Train Loss: 2.4700121059595403e-08, Train Acc: 1.0\n",
            "Epoch 5832/10000\n",
            "Step 0: Train Loss: 2.0337077799581493e-08, Train Acc: 1.0\n",
            "Epoch 5833/10000\n",
            "Step 0: Train Loss: 2.043245750371625e-08, Train Acc: 1.0\n",
            "Epoch 5834/10000\n",
            "Step 0: Train Loss: 2.2208658023714634e-08, Train Acc: 1.0\n",
            "Epoch 5835/10000\n",
            "Step 0: Train Loss: 2.6380954309956905e-08, Train Acc: 1.0\n",
            "Epoch 5836/10000\n",
            "Step 0: Train Loss: 2.4664373654559313e-08, Train Acc: 1.0\n",
            "Epoch 5837/10000\n",
            "Step 0: Train Loss: 2.3901433721107423e-08, Train Acc: 1.0\n",
            "Epoch 5838/10000\n",
            "Step 0: Train Loss: 2.1302664521272163e-08, Train Acc: 1.0\n",
            "Epoch 5839/10000\n",
            "Step 0: Train Loss: 2.157684875214727e-08, Train Acc: 1.0\n",
            "Epoch 5840/10000\n",
            "Step 0: Train Loss: 2.4223298922265712e-08, Train Acc: 1.0\n",
            "Epoch 5841/10000\n",
            "Step 0: Train Loss: 2.5367686617983054e-08, Train Acc: 1.0\n",
            "Epoch 5842/10000\n",
            "Step 0: Train Loss: 2.6619387583082244e-08, Train Acc: 1.0\n",
            "Epoch 5843/10000\n",
            "Step 0: Train Loss: 2.1243073078380803e-08, Train Acc: 1.0\n",
            "Epoch 5844/10000\n",
            "Step 0: Train Loss: 2.435442425507972e-08, Train Acc: 1.0\n",
            "Epoch 5845/10000\n",
            "Step 0: Train Loss: 2.335305637757301e-08, Train Acc: 1.0\n",
            "Epoch 5846/10000\n",
            "Step 0: Train Loss: 2.276893340535935e-08, Train Acc: 1.0\n",
            "Epoch 5847/10000\n",
            "Step 0: Train Loss: 2.4199456660767282e-08, Train Acc: 1.0\n",
            "Epoch 5848/10000\n",
            "Step 0: Train Loss: 2.1946409134443456e-08, Train Acc: 1.0\n",
            "Epoch 5849/10000\n",
            "Step 0: Train Loss: 2.0110581644416925e-08, Train Acc: 1.0\n",
            "Epoch 5850/10000\n",
            "Step 0: Train Loss: 2.353186978609756e-08, Train Acc: 1.0\n",
            "Epoch 5851/10000\n",
            "Step 0: Train Loss: 2.2995433113237596e-08, Train Acc: 1.0\n",
            "Epoch 5852/10000\n",
            "Step 0: Train Loss: 2.1636459734963864e-08, Train Acc: 1.0\n",
            "Epoch 5853/10000\n",
            "Step 0: Train Loss: 1.975295660372467e-08, Train Acc: 1.0\n",
            "Epoch 5854/10000\n",
            "Step 0: Train Loss: 2.1576839870363074e-08, Train Acc: 1.0\n",
            "Epoch 5855/10000\n",
            "Step 0: Train Loss: 2.397294274203432e-08, Train Acc: 1.0\n",
            "Epoch 5856/10000\n",
            "Step 0: Train Loss: 2.219673689296542e-08, Train Acc: 1.0\n",
            "Epoch 5857/10000\n",
            "Step 0: Train Loss: 2.1624547485998846e-08, Train Acc: 1.0\n",
            "Epoch 5858/10000\n",
            "Step 0: Train Loss: 2.1469563904474853e-08, Train Acc: 1.0\n",
            "Epoch 5859/10000\n",
            "Step 0: Train Loss: 2.051589298446288e-08, Train Acc: 1.0\n",
            "Epoch 5860/10000\n",
            "Step 0: Train Loss: 2.1123858218174973e-08, Train Acc: 1.0\n",
            "Epoch 5861/10000\n",
            "Step 0: Train Loss: 1.8584707106583664e-08, Train Acc: 1.0\n",
            "Epoch 5862/10000\n",
            "Step 0: Train Loss: 1.8382058541988044e-08, Train Acc: 1.0\n",
            "Epoch 5863/10000\n",
            "Step 0: Train Loss: 1.8465497575448353e-08, Train Acc: 1.0\n",
            "Epoch 5864/10000\n",
            "Step 0: Train Loss: 2.315040958933423e-08, Train Acc: 1.0\n",
            "Epoch 5865/10000\n",
            "Step 0: Train Loss: 1.6748895603768688e-08, Train Acc: 1.0\n",
            "Epoch 5866/10000\n",
            "Step 0: Train Loss: 2.2423241929914184e-08, Train Acc: 1.0\n",
            "Epoch 5867/10000\n",
            "Step 0: Train Loss: 1.9955615826461326e-08, Train Acc: 1.0\n",
            "Epoch 5868/10000\n",
            "Step 0: Train Loss: 2.1290752272307145e-08, Train Acc: 1.0\n",
            "Epoch 5869/10000\n",
            "Step 0: Train Loss: 1.7952892505945783e-08, Train Acc: 1.0\n",
            "Epoch 5870/10000\n",
            "Step 0: Train Loss: 2.325769443700665e-08, Train Acc: 1.0\n",
            "Epoch 5871/10000\n",
            "Step 0: Train Loss: 1.696347240454088e-08, Train Acc: 1.0\n",
            "Epoch 5872/10000\n",
            "Step 0: Train Loss: 2.174374635899312e-08, Train Acc: 1.0\n",
            "Epoch 5873/10000\n",
            "Step 0: Train Loss: 2.259012177319164e-08, Train Acc: 1.0\n",
            "Epoch 5874/10000\n",
            "Step 0: Train Loss: 2.6547857245873274e-08, Train Acc: 1.0\n",
            "Epoch 5875/10000\n",
            "Step 0: Train Loss: 2.055164571856949e-08, Train Acc: 1.0\n",
            "Epoch 5876/10000\n",
            "Step 0: Train Loss: 2.0360920061079923e-08, Train Acc: 1.0\n",
            "Epoch 5877/10000\n",
            "Step 0: Train Loss: 1.924034798150842e-08, Train Acc: 1.0\n",
            "Epoch 5878/10000\n",
            "Step 0: Train Loss: 2.0968887071148856e-08, Train Acc: 1.0\n",
            "Epoch 5879/10000\n",
            "Step 0: Train Loss: 1.9967531628140023e-08, Train Acc: 1.0\n",
            "Epoch 5880/10000\n",
            "Step 0: Train Loss: 2.064701121184953e-08, Train Acc: 1.0\n",
            "Epoch 5881/10000\n",
            "Step 0: Train Loss: 2.309079505380396e-08, Train Acc: 1.0\n",
            "Epoch 5882/10000\n",
            "Step 0: Train Loss: 1.9717193211477024e-08, Train Acc: 1.0\n",
            "Epoch 5883/10000\n",
            "Step 0: Train Loss: 2.3233853951865058e-08, Train Acc: 1.0\n",
            "Epoch 5884/10000\n",
            "Step 0: Train Loss: 1.869199550696976e-08, Train Acc: 1.0\n",
            "Epoch 5885/10000\n",
            "Step 0: Train Loss: 1.8775445198571106e-08, Train Acc: 1.0\n",
            "Epoch 5886/10000\n",
            "Step 0: Train Loss: 1.9001932471951477e-08, Train Acc: 1.0\n",
            "Epoch 5887/10000\n",
            "Step 0: Train Loss: 2.160069634271622e-08, Train Acc: 1.0\n",
            "Epoch 5888/10000\n",
            "Step 0: Train Loss: 2.1696052954212064e-08, Train Acc: 1.0\n",
            "Epoch 5889/10000\n",
            "Step 0: Train Loss: 2.0730469785235073e-08, Train Acc: 1.0\n",
            "Epoch 5890/10000\n",
            "Step 0: Train Loss: 1.926419557207737e-08, Train Acc: 1.0\n",
            "Epoch 5891/10000\n",
            "Step 0: Train Loss: 1.9252274441328154e-08, Train Acc: 1.0\n",
            "Epoch 5892/10000\n",
            "Step 0: Train Loss: 1.857278775219129e-08, Train Acc: 1.0\n",
            "Epoch 5893/10000\n",
            "Step 0: Train Loss: 2.1231139513133712e-08, Train Acc: 1.0\n",
            "Epoch 5894/10000\n",
            "Step 0: Train Loss: 1.4913073442812674e-08, Train Acc: 1.0\n",
            "Epoch 5895/10000\n",
            "Step 0: Train Loss: 1.7178042099885715e-08, Train Acc: 1.0\n",
            "Epoch 5896/10000\n",
            "Step 0: Train Loss: 2.2578197089728747e-08, Train Acc: 1.0\n",
            "Epoch 5897/10000\n",
            "Step 0: Train Loss: 1.9323795896752927e-08, Train Acc: 1.0\n",
            "Epoch 5898/10000\n",
            "Step 0: Train Loss: 2.1934472016482687e-08, Train Acc: 1.0\n",
            "Epoch 5899/10000\n",
            "Step 0: Train Loss: 2.005097954338453e-08, Train Acc: 1.0\n",
            "Epoch 5900/10000\n",
            "Step 0: Train Loss: 2.0909274311975423e-08, Train Acc: 1.0\n",
            "Epoch 5901/10000\n",
            "Step 0: Train Loss: 1.709459240828437e-08, Train Acc: 1.0\n",
            "Epoch 5902/10000\n",
            "Step 0: Train Loss: 1.645086733503831e-08, Train Acc: 1.0\n",
            "Epoch 5903/10000\n",
            "Step 0: Train Loss: 1.8489338060589944e-08, Train Acc: 1.0\n",
            "Epoch 5904/10000\n",
            "Step 0: Train Loss: 1.7714480549102518e-08, Train Acc: 1.0\n",
            "Epoch 5905/10000\n",
            "Step 0: Train Loss: 1.807211447157897e-08, Train Acc: 1.0\n",
            "Epoch 5906/10000\n",
            "Step 0: Train Loss: 2.057548798006792e-08, Train Acc: 1.0\n",
            "Epoch 5907/10000\n",
            "Step 0: Train Loss: 1.7499907301044004e-08, Train Acc: 1.0\n",
            "Epoch 5908/10000\n",
            "Step 0: Train Loss: 1.9812553375686548e-08, Train Acc: 1.0\n",
            "Epoch 5909/10000\n",
            "Step 0: Train Loss: 1.935956639442793e-08, Train Acc: 1.0\n",
            "Epoch 5910/10000\n",
            "Step 0: Train Loss: 2.0766218966628003e-08, Train Acc: 1.0\n",
            "Epoch 5911/10000\n",
            "Step 0: Train Loss: 1.87396835826803e-08, Train Acc: 1.0\n",
            "Epoch 5912/10000\n",
            "Step 0: Train Loss: 2.0420515056684962e-08, Train Acc: 1.0\n",
            "Epoch 5913/10000\n",
            "Step 0: Train Loss: 1.9264179584865815e-08, Train Acc: 1.0\n",
            "Epoch 5914/10000\n",
            "Step 0: Train Loss: 2.129073806145243e-08, Train Acc: 1.0\n",
            "Epoch 5915/10000\n",
            "Step 0: Train Loss: 1.9836397413541818e-08, Train Acc: 1.0\n",
            "Epoch 5916/10000\n",
            "Step 0: Train Loss: 1.6546223946534155e-08, Train Acc: 1.0\n",
            "Epoch 5917/10000\n",
            "Step 0: Train Loss: 1.564024998401692e-08, Train Acc: 1.0\n",
            "Epoch 5918/10000\n",
            "Step 0: Train Loss: 2.1672212469070473e-08, Train Acc: 1.0\n",
            "Epoch 5919/10000\n",
            "Step 0: Train Loss: 1.7476061486831895e-08, Train Acc: 1.0\n",
            "Epoch 5920/10000\n",
            "Step 0: Train Loss: 1.9991365007854256e-08, Train Acc: 1.0\n",
            "Epoch 5921/10000\n",
            "Step 0: Train Loss: 1.7428380516548714e-08, Train Acc: 1.0\n",
            "Epoch 5922/10000\n",
            "Step 0: Train Loss: 2.1052324328252325e-08, Train Acc: 1.0\n",
            "Epoch 5923/10000\n",
            "Step 0: Train Loss: 2.0039054859921634e-08, Train Acc: 1.0\n",
            "Epoch 5924/10000\n",
            "Step 0: Train Loss: 2.005097066160033e-08, Train Acc: 1.0\n",
            "Epoch 5925/10000\n",
            "Step 0: Train Loss: 1.702306562378908e-08, Train Acc: 1.0\n",
            "Epoch 5926/10000\n",
            "Step 0: Train Loss: 1.7201880808670467e-08, Train Acc: 1.0\n",
            "Epoch 5927/10000\n",
            "Step 0: Train Loss: 1.7750243941350163e-08, Train Acc: 1.0\n",
            "Epoch 5928/10000\n",
            "Step 0: Train Loss: 1.704691143800119e-08, Train Acc: 1.0\n",
            "Epoch 5929/10000\n",
            "Step 0: Train Loss: 1.7642953764607228e-08, Train Acc: 1.0\n",
            "Epoch 5930/10000\n",
            "Step 0: Train Loss: 1.7237642424561272e-08, Train Acc: 1.0\n",
            "Epoch 5931/10000\n",
            "Step 0: Train Loss: 1.6450858453254114e-08, Train Acc: 1.0\n",
            "Epoch 5932/10000\n",
            "Step 0: Train Loss: 1.9180747656832864e-08, Train Acc: 1.0\n",
            "Epoch 5933/10000\n",
            "Step 0: Train Loss: 1.6164765526127667e-08, Train Acc: 1.0\n",
            "Epoch 5934/10000\n",
            "Step 0: Train Loss: 1.6641600097955234e-08, Train Acc: 1.0\n",
            "Epoch 5935/10000\n",
            "Step 0: Train Loss: 1.552103157109741e-08, Train Acc: 1.0\n",
            "Epoch 5936/10000\n",
            "Step 0: Train Loss: 1.5330297031823648e-08, Train Acc: 1.0\n",
            "Epoch 5937/10000\n",
            "Step 0: Train Loss: 1.5020358290485092e-08, Train Acc: 1.0\n",
            "Epoch 5938/10000\n",
            "Step 0: Train Loss: 1.584289854861254e-08, Train Acc: 1.0\n",
            "Epoch 5939/10000\n",
            "Step 0: Train Loss: 1.667735638477552e-08, Train Acc: 1.0\n",
            "Epoch 5940/10000\n",
            "Step 0: Train Loss: 1.3899794204519367e-08, Train Acc: 1.0\n",
            "Epoch 5941/10000\n",
            "Step 0: Train Loss: 1.6808490599373727e-08, Train Acc: 1.0\n",
            "Epoch 5942/10000\n",
            "Step 0: Train Loss: 1.4662727920722318e-08, Train Acc: 1.0\n",
            "Epoch 5943/10000\n",
            "Step 0: Train Loss: 1.9371476867036108e-08, Train Acc: 1.0\n",
            "Epoch 5944/10000\n",
            "Step 0: Train Loss: 1.5163408306761994e-08, Train Acc: 1.0\n",
            "Epoch 5945/10000\n",
            "Step 0: Train Loss: 1.5854817903004914e-08, Train Acc: 1.0\n",
            "Epoch 5946/10000\n",
            "Step 0: Train Loss: 1.7380694217195014e-08, Train Acc: 1.0\n",
            "Epoch 5947/10000\n",
            "Step 0: Train Loss: 1.7535661811507453e-08, Train Acc: 1.0\n",
            "Epoch 5948/10000\n",
            "Step 0: Train Loss: 1.5950183396284956e-08, Train Acc: 1.0\n",
            "Epoch 5949/10000\n",
            "Step 0: Train Loss: 1.533029347910997e-08, Train Acc: 1.0\n",
            "Epoch 5950/10000\n",
            "Step 0: Train Loss: 1.7845602329202848e-08, Train Acc: 1.0\n",
            "Epoch 5951/10000\n",
            "Step 0: Train Loss: 1.542566252510369e-08, Train Acc: 1.0\n",
            "Epoch 5952/10000\n",
            "Step 0: Train Loss: 1.7046897227146474e-08, Train Acc: 1.0\n",
            "Epoch 5953/10000\n",
            "Step 0: Train Loss: 1.362560997364426e-08, Train Acc: 1.0\n",
            "Epoch 5954/10000\n",
            "Step 0: Train Loss: 1.5926348240213883e-08, Train Acc: 1.0\n",
            "Epoch 5955/10000\n",
            "Step 0: Train Loss: 1.4030925754582313e-08, Train Acc: 1.0\n",
            "Epoch 5956/10000\n",
            "Step 0: Train Loss: 1.6176679551449524e-08, Train Acc: 1.0\n",
            "Epoch 5957/10000\n",
            "Step 0: Train Loss: 1.920459702375865e-08, Train Acc: 1.0\n",
            "Epoch 5958/10000\n",
            "Step 0: Train Loss: 1.6784644785161618e-08, Train Acc: 1.0\n",
            "Epoch 5959/10000\n",
            "Step 0: Train Loss: 1.6868094476762963e-08, Train Acc: 1.0\n",
            "Epoch 5960/10000\n",
            "Step 0: Train Loss: 1.6391256352221717e-08, Train Acc: 1.0\n",
            "Epoch 5961/10000\n",
            "Step 0: Train Loss: 1.4841544881960544e-08, Train Acc: 1.0\n",
            "Epoch 5962/10000\n",
            "Step 0: Train Loss: 1.6617743625602088e-08, Train Acc: 1.0\n",
            "Epoch 5963/10000\n",
            "Step 0: Train Loss: 1.3303749213378069e-08, Train Acc: 1.0\n",
            "Epoch 5964/10000\n",
            "Step 0: Train Loss: 1.2695783091487556e-08, Train Acc: 1.0\n",
            "Epoch 5965/10000\n",
            "Step 0: Train Loss: 1.616475842070031e-08, Train Acc: 1.0\n",
            "Epoch 5966/10000\n",
            "Step 0: Train Loss: 1.4805776160642381e-08, Train Acc: 1.0\n",
            "Epoch 5967/10000\n",
            "Step 0: Train Loss: 1.4364707645597719e-08, Train Acc: 1.0\n",
            "Epoch 5968/10000\n",
            "Step 0: Train Loss: 1.5139566045263564e-08, Train Acc: 1.0\n",
            "Epoch 5969/10000\n",
            "Step 0: Train Loss: 1.3089167971713778e-08, Train Acc: 1.0\n",
            "Epoch 5970/10000\n",
            "Step 0: Train Loss: 1.4817699955926855e-08, Train Acc: 1.0\n",
            "Epoch 5971/10000\n",
            "Step 0: Train Loss: 1.4603123155154663e-08, Train Acc: 1.0\n",
            "Epoch 5972/10000\n",
            "Step 0: Train Loss: 1.2266626825407911e-08, Train Acc: 1.0\n",
            "Epoch 5973/10000\n",
            "Step 0: Train Loss: 1.2671934612740188e-08, Train Acc: 1.0\n",
            "Epoch 5974/10000\n",
            "Step 0: Train Loss: 1.670120042263079e-08, Train Acc: 1.0\n",
            "Epoch 5975/10000\n",
            "Step 0: Train Loss: 1.4019001071119419e-08, Train Acc: 1.0\n",
            "Epoch 5976/10000\n",
            "Step 0: Train Loss: 1.4567360651085437e-08, Train Acc: 1.0\n",
            "Epoch 5977/10000\n",
            "Step 0: Train Loss: 1.5854814350291235e-08, Train Acc: 1.0\n",
            "Epoch 5978/10000\n",
            "Step 0: Train Loss: 1.450775855005304e-08, Train Acc: 1.0\n",
            "Epoch 5979/10000\n",
            "Step 0: Train Loss: 1.3494481088116572e-08, Train Acc: 1.0\n",
            "Epoch 5980/10000\n",
            "Step 0: Train Loss: 1.355408141279213e-08, Train Acc: 1.0\n",
            "Epoch 5981/10000\n",
            "Step 0: Train Loss: 1.5544872056239e-08, Train Acc: 1.0\n",
            "Epoch 5982/10000\n",
            "Step 0: Train Loss: 1.5926344687500205e-08, Train Acc: 1.0\n",
            "Epoch 5983/10000\n",
            "Step 0: Train Loss: 1.069306865275621e-08, Train Acc: 1.0\n",
            "Epoch 5984/10000\n",
            "Step 0: Train Loss: 1.7178033218101518e-08, Train Acc: 1.0\n",
            "Epoch 5985/10000\n",
            "Step 0: Train Loss: 1.147984818317127e-08, Train Acc: 1.0\n",
            "Epoch 5986/10000\n",
            "Step 0: Train Loss: 1.485346423635292e-08, Train Acc: 1.0\n",
            "Epoch 5987/10000\n",
            "Step 0: Train Loss: 1.2588489362030941e-08, Train Acc: 1.0\n",
            "Epoch 5988/10000\n",
            "Step 0: Train Loss: 1.623627987612508e-08, Train Acc: 1.0\n",
            "Epoch 5989/10000\n",
            "Step 0: Train Loss: 1.4615042509547038e-08, Train Acc: 1.0\n",
            "Epoch 5990/10000\n",
            "Step 0: Train Loss: 1.3065324822036928e-08, Train Acc: 1.0\n",
            "Epoch 5991/10000\n",
            "Step 0: Train Loss: 1.2910352786832391e-08, Train Acc: 1.0\n",
            "Epoch 5992/10000\n",
            "Step 0: Train Loss: 1.3458715919512088e-08, Train Acc: 1.0\n",
            "Epoch 5993/10000\n",
            "Step 0: Train Loss: 1.3339508164733616e-08, Train Acc: 1.0\n",
            "Epoch 5994/10000\n",
            "Step 0: Train Loss: 1.3601771264859508e-08, Train Acc: 1.0\n",
            "Epoch 5995/10000\n",
            "Step 0: Train Loss: 1.3816345401096441e-08, Train Acc: 1.0\n",
            "Epoch 5996/10000\n",
            "Step 0: Train Loss: 1.7154199838387285e-08, Train Acc: 1.0\n",
            "Epoch 5997/10000\n",
            "Step 0: Train Loss: 1.5604474157271397e-08, Train Acc: 1.0\n",
            "Epoch 5998/10000\n",
            "Step 0: Train Loss: 1.33275870339844e-08, Train Acc: 1.0\n",
            "Epoch 5999/10000\n",
            "Step 0: Train Loss: 1.2063974708098613e-08, Train Acc: 1.0\n",
            "Epoch 6000/10000\n",
            "Step 0: Train Loss: 1.3530240927650539e-08, Train Acc: 1.0\n",
            "Epoch 6001/10000\n",
            "Step 0: Train Loss: 1.3566004319898184e-08, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 6000 1024 1.0249698397569824\n",
            "Epoch index and hidden dimension and ratio: 6000 20 1.0113700195550017\n",
            "Epoch index and hidden dimension and ratio: 6000 20 1.6099316577223726\n",
            "Epoch index and hidden dimension and ratio: 6000 20 4.215061727521695\n",
            "MI(X;T): [10.637495986579982, 7.913646989674183, 5.468857676366896, 3.4538072263310844], MI(Y;T): [2.6239380143523103, 3.188592839489294, 3.191855529310242, 2.956470482447086]\n",
            "Epoch index and hidden dimension and ratio: 6000 1024 1.0249719690176422\n",
            "Epoch index and hidden dimension and ratio: 6000 20 1.0113713213414215\n",
            "Epoch index and hidden dimension and ratio: 6000 20 1.6099112172492203\n",
            "Epoch index and hidden dimension and ratio: 6000 20 4.214941958501515\n",
            "MI(X;T): [10.637495986579982, 7.9136359219389805, 5.468154063375984, 3.4538748946773428], MI(Y;T): [2.6239380143523103, 3.188592839489294, 3.1920685054445785, 2.9563079417333626]\n",
            "Epoch index and hidden dimension and ratio: 6000 1024 1.024973731164395\n",
            "Epoch index and hidden dimension and ratio: 6000 20 1.0113719722346315\n",
            "Epoch index and hidden dimension and ratio: 6000 20 1.6098881106273957\n",
            "Epoch index and hidden dimension and ratio: 6000 20 4.2148034344424605\n",
            "MI(X;T): [10.637495986579982, 7.91326820774723, 5.468335447050828, 3.4536094845287892], MI(Y;T): [2.6239380143523103, 3.188492839489294, 3.191888680059754, 2.956308287070833]\n",
            "Epoch index and hidden dimension and ratio: 6000 1024 1.024975493311148\n",
            "Epoch index and hidden dimension and ratio: 6000 20 1.0113760945582941\n",
            "Epoch index and hidden dimension and ratio: 6000 20 1.6098963629923329\n",
            "Epoch index and hidden dimension and ratio: 6000 20 4.214784350367816\n",
            "MI(X;T): [10.637586638322801, 7.913306958329507, 5.468173191425937, 3.4536094845287892], MI(Y;T): [2.6239380143523103, 3.188785786451634, 3.191861762807748, 2.9561632809020573]\n",
            "Epoch index and hidden dimension and ratio: 6000 1024 1.0249776225718077\n",
            "Epoch index and hidden dimension and ratio: 6000 20 1.0113881360826773\n",
            "Epoch index and hidden dimension and ratio: 6000 20 1.6099517173171434\n",
            "Epoch index and hidden dimension and ratio: 6000 20 4.215005462405072\n",
            "MI(X;T): [10.637620012346124, 7.913106048578727, 5.468514076404304, 3.4536590761608243], MI(Y;T): [2.6238982956191004, 3.1886857864516336, 3.191896105012969, 2.9562324796208563]\n",
            "Epoch index and hidden dimension and ratio: 6000 1024 1.0249797518324675\n",
            "Epoch index and hidden dimension and ratio: 6000 20 1.0114020218044886\n",
            "Epoch index and hidden dimension and ratio: 6000 20 1.6100124039392982\n",
            "Epoch index and hidden dimension and ratio: 6000 20 4.2152624393412275\n",
            "MI(X;T): [10.637620012346124, 7.913482284177467, 5.468016279275814, 3.454148583679384], MI(Y;T): [2.6238982956191004, 3.1889144701944847, 3.1919394018496456, 2.9562320335761623]\n",
            "Epoch 6002/10000\n",
            "Step 0: Train Loss: 1.175402619679744e-08, Train Acc: 1.0\n",
            "Epoch 6003/10000\n",
            "Step 0: Train Loss: 1.286266915201395e-08, Train Acc: 1.0\n",
            "Epoch 6004/10000\n",
            "Step 0: Train Loss: 1.5246849116579142e-08, Train Acc: 1.0\n",
            "Epoch 6005/10000\n",
            "Step 0: Train Loss: 1.323221976434752e-08, Train Acc: 1.0\n",
            "Epoch 6006/10000\n",
            "Step 0: Train Loss: 1.3756738859171946e-08, Train Acc: 1.0\n",
            "Epoch 6007/10000\n",
            "Step 0: Train Loss: 1.1563293433880517e-08, Train Acc: 1.0\n",
            "Epoch 6008/10000\n",
            "Step 0: Train Loss: 1.3267979603881486e-08, Train Acc: 1.0\n",
            "Epoch 6009/10000\n",
            "Step 0: Train Loss: 1.3017643851753746e-08, Train Acc: 1.0\n",
            "Epoch 6010/10000\n",
            "Step 0: Train Loss: 1.5437585432209744e-08, Train Acc: 1.0\n",
            "Epoch 6011/10000\n",
            "Step 0: Train Loss: 1.2409675065327974e-08, Train Acc: 1.0\n",
            "Epoch 6012/10000\n",
            "Step 0: Train Loss: 1.313684894199696e-08, Train Acc: 1.0\n",
            "Epoch 6013/10000\n",
            "Step 0: Train Loss: 1.1956684531355677e-08, Train Acc: 1.0\n",
            "Epoch 6014/10000\n",
            "Step 0: Train Loss: 1.1491766649385227e-08, Train Acc: 1.0\n",
            "Epoch 6015/10000\n",
            "Step 0: Train Loss: 1.3852107905165667e-08, Train Acc: 1.0\n",
            "Epoch 6016/10000\n",
            "Step 0: Train Loss: 1.2743463173592318e-08, Train Acc: 1.0\n",
            "Epoch 6017/10000\n",
            "Step 0: Train Loss: 1.5628316418769828e-08, Train Acc: 1.0\n",
            "Epoch 6018/10000\n",
            "Step 0: Train Loss: 1.471040711464866e-08, Train Acc: 1.0\n",
            "Epoch 6019/10000\n",
            "Step 0: Train Loss: 1.1837473223863526e-08, Train Acc: 1.0\n",
            "Epoch 6020/10000\n",
            "Step 0: Train Loss: 1.1420243417603615e-08, Train Acc: 1.0\n",
            "Epoch 6021/10000\n",
            "Step 0: Train Loss: 1.2338148280832684e-08, Train Acc: 1.0\n",
            "Epoch 6022/10000\n",
            "Step 0: Train Loss: 1.2695773321524939e-08, Train Acc: 1.0\n",
            "Epoch 6023/10000\n",
            "Step 0: Train Loss: 1.399515436872889e-08, Train Acc: 1.0\n",
            "Epoch 6024/10000\n",
            "Step 0: Train Loss: 1.022815165896418e-08, Train Acc: 1.0\n",
            "Epoch 6025/10000\n",
            "Step 0: Train Loss: 9.500976894116775e-09, Train Acc: 1.0\n",
            "Epoch 6026/10000\n",
            "Step 0: Train Loss: 1.0335440947528696e-08, Train Acc: 1.0\n",
            "Epoch 6027/10000\n",
            "Step 0: Train Loss: 1.4638886547402308e-08, Train Acc: 1.0\n",
            "Epoch 6028/10000\n",
            "Step 0: Train Loss: 1.4030920425511795e-08, Train Acc: 1.0\n",
            "Epoch 6029/10000\n",
            "Step 0: Train Loss: 1.2314310460226352e-08, Train Acc: 1.0\n",
            "Epoch 6030/10000\n",
            "Step 0: Train Loss: 1.1408316069605462e-08, Train Acc: 1.0\n",
            "Epoch 6031/10000\n",
            "Step 0: Train Loss: 1.2898433432440015e-08, Train Acc: 1.0\n",
            "Epoch 6032/10000\n",
            "Step 0: Train Loss: 1.1503686891956022e-08, Train Acc: 1.0\n",
            "Epoch 6033/10000\n",
            "Step 0: Train Loss: 1.4841536000176347e-08, Train Acc: 1.0\n",
            "Epoch 6034/10000\n",
            "Step 0: Train Loss: 1.2719622688450727e-08, Train Acc: 1.0\n",
            "Epoch 6035/10000\n",
            "Step 0: Train Loss: 1.2338148280832684e-08, Train Acc: 1.0\n",
            "Epoch 6036/10000\n",
            "Step 0: Train Loss: 1.1968600333034374e-08, Train Acc: 1.0\n",
            "Epoch 6037/10000\n",
            "Step 0: Train Loss: 1.214741640609418e-08, Train Acc: 1.0\n",
            "Epoch 6038/10000\n",
            "Step 0: Train Loss: 1.212357503277417e-08, Train Acc: 1.0\n",
            "Epoch 6039/10000\n",
            "Step 0: Train Loss: 1.0144706408254933e-08, Train Acc: 1.0\n",
            "Epoch 6040/10000\n",
            "Step 0: Train Loss: 1.1587133919022108e-08, Train Acc: 1.0\n",
            "Epoch 6041/10000\n",
            "Step 0: Train Loss: 1.0120860594042824e-08, Train Acc: 1.0\n",
            "Epoch 6042/10000\n",
            "Step 0: Train Loss: 1.0216228751858125e-08, Train Acc: 1.0\n",
            "Epoch 6043/10000\n",
            "Step 0: Train Loss: 1.173018482347743e-08, Train Acc: 1.0\n",
            "Epoch 6044/10000\n",
            "Step 0: Train Loss: 9.632103115109203e-09, Train Acc: 1.0\n",
            "Epoch 6045/10000\n",
            "Step 0: Train Loss: 9.429449221443065e-09, Train Acc: 1.0\n",
            "Epoch 6046/10000\n",
            "Step 0: Train Loss: 9.667870060070527e-09, Train Acc: 1.0\n",
            "Epoch 6047/10000\n",
            "Step 0: Train Loss: 1.1420241641246776e-08, Train Acc: 1.0\n",
            "Epoch 6048/10000\n",
            "Step 0: Train Loss: 1.0132782612970459e-08, Train Acc: 1.0\n",
            "Epoch 6049/10000\n",
            "Step 0: Train Loss: 1.2993800702076896e-08, Train Acc: 1.0\n",
            "Epoch 6050/10000\n",
            "Step 0: Train Loss: 8.094307446526727e-09, Train Acc: 1.0\n",
            "Epoch 6051/10000\n",
            "Step 0: Train Loss: 1.1026851431950035e-08, Train Acc: 1.0\n",
            "Epoch 6052/10000\n",
            "Step 0: Train Loss: 1.2278547067978707e-08, Train Acc: 1.0\n",
            "Epoch 6053/10000\n",
            "Step 0: Train Loss: 1.2421598860612448e-08, Train Acc: 1.0\n",
            "Epoch 6054/10000\n",
            "Step 0: Train Loss: 1.0454646037771909e-08, Train Acc: 1.0\n",
            "Epoch 6055/10000\n",
            "Step 0: Train Loss: 1.0466568056699543e-08, Train Acc: 1.0\n",
            "Epoch 6056/10000\n",
            "Step 0: Train Loss: 1.1324871707074635e-08, Train Acc: 1.0\n",
            "Epoch 6057/10000\n",
            "Step 0: Train Loss: 1.0192387378538115e-08, Train Acc: 1.0\n",
            "Epoch 6058/10000\n",
            "Step 0: Train Loss: 1.1301032110111464e-08, Train Acc: 1.0\n",
            "Epoch 6059/10000\n",
            "Step 0: Train Loss: 8.761879222163316e-09, Train Acc: 1.0\n",
            "Epoch 6060/10000\n",
            "Step 0: Train Loss: 1.2171256891235771e-08, Train Acc: 1.0\n",
            "Epoch 6061/10000\n",
            "Step 0: Train Loss: 1.069306776457779e-08, Train Acc: 1.0\n",
            "Epoch 6062/10000\n",
            "Step 0: Train Loss: 1.2350072076117158e-08, Train Acc: 1.0\n",
            "Epoch 6063/10000\n",
            "Step 0: Train Loss: 1.0263913274854986e-08, Train Acc: 1.0\n",
            "Epoch 6064/10000\n",
            "Step 0: Train Loss: 9.179111692958486e-09, Train Acc: 1.0\n",
            "Epoch 6065/10000\n",
            "Step 0: Train Loss: 1.2040127117529664e-08, Train Acc: 1.0\n",
            "Epoch 6066/10000\n",
            "Step 0: Train Loss: 1.2052042919208361e-08, Train Acc: 1.0\n",
            "Epoch 6067/10000\n",
            "Step 0: Train Loss: 9.274476298060108e-09, Train Acc: 1.0\n",
            "Epoch 6068/10000\n",
            "Step 0: Train Loss: 8.571144682889553e-09, Train Acc: 1.0\n",
            "Epoch 6069/10000\n",
            "Step 0: Train Loss: 1.0108940351472029e-08, Train Acc: 1.0\n",
            "Epoch 6070/10000\n",
            "Step 0: Train Loss: 1.3029560541610863e-08, Train Acc: 1.0\n",
            "Epoch 6071/10000\n",
            "Step 0: Train Loss: 1.0693065100042531e-08, Train Acc: 1.0\n",
            "Epoch 6072/10000\n",
            "Step 0: Train Loss: 9.298320335915378e-09, Train Acc: 1.0\n",
            "Epoch 6073/10000\n",
            "Step 0: Train Loss: 9.46521172551229e-09, Train Acc: 1.0\n",
            "Epoch 6074/10000\n",
            "Step 0: Train Loss: 7.49826245538543e-09, Train Acc: 1.0\n",
            "Epoch 6075/10000\n",
            "Step 0: Train Loss: 1.1813626521472997e-08, Train Acc: 1.0\n",
            "Epoch 6076/10000\n",
            "Step 0: Train Loss: 1.128911009118383e-08, Train Acc: 1.0\n",
            "Epoch 6077/10000\n",
            "Step 0: Train Loss: 1.0097020997079653e-08, Train Acc: 1.0\n",
            "Epoch 6078/10000\n",
            "Step 0: Train Loss: 9.727472161102924e-09, Train Acc: 1.0\n",
            "Epoch 6079/10000\n",
            "Step 0: Train Loss: 9.310237913950914e-09, Train Acc: 1.0\n",
            "Epoch 6080/10000\n",
            "Step 0: Train Loss: 1.0418883533702683e-08, Train Acc: 1.0\n",
            "Epoch 6081/10000\n",
            "Step 0: Train Loss: 1.1360633322965441e-08, Train Acc: 1.0\n",
            "Epoch 6082/10000\n",
            "Step 0: Train Loss: 9.024139657753949e-09, Train Acc: 1.0\n",
            "Epoch 6083/10000\n",
            "Step 0: Train Loss: 8.821484875909391e-09, Train Acc: 1.0\n",
            "Epoch 6084/10000\n",
            "Step 0: Train Loss: 1.064538146522409e-08, Train Acc: 1.0\n",
            "Epoch 6085/10000\n",
            "Step 0: Train Loss: 9.500974229581516e-09, Train Acc: 1.0\n",
            "Epoch 6086/10000\n",
            "Step 0: Train Loss: 9.48905487518914e-09, Train Acc: 1.0\n",
            "Epoch 6087/10000\n",
            "Step 0: Train Loss: 9.012216750647895e-09, Train Acc: 1.0\n",
            "Epoch 6088/10000\n",
            "Step 0: Train Loss: 1.0561934438158005e-08, Train Acc: 1.0\n",
            "Epoch 6089/10000\n",
            "Step 0: Train Loss: 9.155267655103216e-09, Train Acc: 1.0\n",
            "Epoch 6090/10000\n",
            "Step 0: Train Loss: 9.596339722861558e-09, Train Acc: 1.0\n",
            "Epoch 6091/10000\n",
            "Step 0: Train Loss: 8.451932487218983e-09, Train Acc: 1.0\n",
            "Epoch 6092/10000\n",
            "Step 0: Train Loss: 9.81091741181217e-09, Train Acc: 1.0\n",
            "Epoch 6093/10000\n",
            "Step 0: Train Loss: 8.761879222163316e-09, Train Acc: 1.0\n",
            "Epoch 6094/10000\n",
            "Step 0: Train Loss: 9.286395652452484e-09, Train Acc: 1.0\n",
            "Epoch 6095/10000\n",
            "Step 0: Train Loss: 9.787077814848999e-09, Train Acc: 1.0\n",
            "Epoch 6096/10000\n",
            "Step 0: Train Loss: 1.0192385602181275e-08, Train Acc: 1.0\n",
            "Epoch 6097/10000\n",
            "Step 0: Train Loss: 8.165831566486759e-09, Train Acc: 1.0\n",
            "Epoch 6098/10000\n",
            "Step 0: Train Loss: 9.632103115109203e-09, Train Acc: 1.0\n",
            "Epoch 6099/10000\n",
            "Step 0: Train Loss: 8.67842953056197e-09, Train Acc: 1.0\n",
            "Epoch 6100/10000\n",
            "Step 0: Train Loss: 9.41752809069385e-09, Train Acc: 1.0\n",
            "Epoch 6101/10000\n",
            "Step 0: Train Loss: 1.1408318734140721e-08, Train Acc: 1.0\n",
            "Epoch 6102/10000\n",
            "Step 0: Train Loss: 7.975097027212996e-09, Train Acc: 1.0\n",
            "Epoch 6103/10000\n",
            "Step 0: Train Loss: 1.015662132175521e-08, Train Acc: 1.0\n",
            "Epoch 6104/10000\n",
            "Step 0: Train Loss: 1.0526172822267199e-08, Train Acc: 1.0\n",
            "Epoch 6105/10000\n",
            "Step 0: Train Loss: 7.987019046140631e-09, Train Acc: 1.0\n",
            "Epoch 6106/10000\n",
            "Step 0: Train Loss: 9.191029270994022e-09, Train Acc: 1.0\n",
            "Epoch 6107/10000\n",
            "Step 0: Train Loss: 8.440012244648187e-09, Train Acc: 1.0\n",
            "Epoch 6108/10000\n",
            "Step 0: Train Loss: 9.655945376607633e-09, Train Acc: 1.0\n",
            "Epoch 6109/10000\n",
            "Step 0: Train Loss: 1.099108803970239e-08, Train Acc: 1.0\n",
            "Epoch 6110/10000\n",
            "Step 0: Train Loss: 9.214871532492452e-09, Train Acc: 1.0\n",
            "Epoch 6111/10000\n",
            "Step 0: Train Loss: 1.0335437394815017e-08, Train Acc: 1.0\n",
            "Epoch 6112/10000\n",
            "Step 0: Train Loss: 9.918204924019847e-09, Train Acc: 1.0\n",
            "Epoch 6113/10000\n",
            "Step 0: Train Loss: 8.547300645034284e-09, Train Acc: 1.0\n",
            "Epoch 6114/10000\n",
            "Step 0: Train Loss: 8.344645863189726e-09, Train Acc: 1.0\n",
            "Epoch 6115/10000\n",
            "Step 0: Train Loss: 8.523458383535853e-09, Train Acc: 1.0\n",
            "Epoch 6116/10000\n",
            "Step 0: Train Loss: 9.73939062731688e-09, Train Acc: 1.0\n",
            "Epoch 6117/10000\n",
            "Step 0: Train Loss: 1.0001651062907513e-08, Train Acc: 1.0\n",
            "Epoch 6118/10000\n",
            "Step 0: Train Loss: 7.808204749437664e-09, Train Acc: 1.0\n",
            "Epoch 6119/10000\n",
            "Step 0: Train Loss: 8.201594958734404e-09, Train Acc: 1.0\n",
            "Epoch 6120/10000\n",
            "Step 0: Train Loss: 9.83475789695376e-09, Train Acc: 1.0\n",
            "Epoch 6121/10000\n",
            "Step 0: Train Loss: 8.249279481731264e-09, Train Acc: 1.0\n",
            "Epoch 6122/10000\n",
            "Step 0: Train Loss: 8.797638173518862e-09, Train Acc: 1.0\n",
            "Epoch 6123/10000\n",
            "Step 0: Train Loss: 9.822838542561385e-09, Train Acc: 1.0\n",
            "Epoch 6124/10000\n",
            "Step 0: Train Loss: 8.571142906532714e-09, Train Acc: 1.0\n",
            "Epoch 6125/10000\n",
            "Step 0: Train Loss: 8.428091113898972e-09, Train Acc: 1.0\n",
            "Epoch 6126/10000\n",
            "Step 0: Train Loss: 1.018046358325364e-08, Train Acc: 1.0\n",
            "Epoch 6127/10000\n",
            "Step 0: Train Loss: 9.78707515031374e-09, Train Acc: 1.0\n",
            "Epoch 6128/10000\n",
            "Step 0: Train Loss: 8.893006331334163e-09, Train Acc: 1.0\n",
            "Epoch 6129/10000\n",
            "Step 0: Train Loss: 8.404248852400542e-09, Train Acc: 1.0\n",
            "Epoch 6130/10000\n",
            "Step 0: Train Loss: 9.107581355749517e-09, Train Acc: 1.0\n",
            "Epoch 6131/10000\n",
            "Step 0: Train Loss: 9.632104003287623e-09, Train Acc: 1.0\n",
            "Epoch 6132/10000\n",
            "Step 0: Train Loss: 8.618827429529574e-09, Train Acc: 1.0\n",
            "Epoch 6133/10000\n",
            "Step 0: Train Loss: 9.119503374677151e-09, Train Acc: 1.0\n",
            "Epoch 6134/10000\n",
            "Step 0: Train Loss: 7.307526583844037e-09, Train Acc: 1.0\n",
            "Epoch 6135/10000\n",
            "Step 0: Train Loss: 8.165831566486759e-09, Train Acc: 1.0\n",
            "Epoch 6136/10000\n",
            "Step 0: Train Loss: 5.412100989587998e-09, Train Acc: 1.0\n",
            "Epoch 6137/10000\n",
            "Step 0: Train Loss: 8.1658289019515e-09, Train Acc: 1.0\n",
            "Epoch 6138/10000\n",
            "Step 0: Train Loss: 8.18967205162835e-09, Train Acc: 1.0\n",
            "Epoch 6139/10000\n",
            "Step 0: Train Loss: 8.881086976941788e-09, Train Acc: 1.0\n",
            "Epoch 6140/10000\n",
            "Step 0: Train Loss: 7.486340436457795e-09, Train Acc: 1.0\n",
            "Epoch 6141/10000\n",
            "Step 0: Train Loss: 8.225436332054414e-09, Train Acc: 1.0\n",
            "Epoch 6142/10000\n",
            "Step 0: Train Loss: 8.022780662031437e-09, Train Acc: 1.0\n",
            "Epoch 6143/10000\n",
            "Step 0: Train Loss: 8.702271792060401e-09, Train Acc: 1.0\n",
            "Epoch 6144/10000\n",
            "Step 0: Train Loss: 6.5445879826597775e-09, Train Acc: 1.0\n",
            "Epoch 6145/10000\n",
            "Step 0: Train Loss: 8.082385427599093e-09, Train Acc: 1.0\n",
            "Epoch 6146/10000\n",
            "Step 0: Train Loss: 8.046621147173028e-09, Train Acc: 1.0\n",
            "Epoch 6147/10000\n",
            "Step 0: Train Loss: 8.535379514285069e-09, Train Acc: 1.0\n",
            "Epoch 6148/10000\n",
            "Step 0: Train Loss: 7.927412504216136e-09, Train Acc: 1.0\n",
            "Epoch 6149/10000\n",
            "Step 0: Train Loss: 7.748599983870008e-09, Train Acc: 1.0\n",
            "Epoch 6150/10000\n",
            "Step 0: Train Loss: 7.975097915391416e-09, Train Acc: 1.0\n",
            "Epoch 6151/10000\n",
            "Step 0: Train Loss: 8.594984279852724e-09, Train Acc: 1.0\n",
            "Epoch 6152/10000\n",
            "Step 0: Train Loss: 8.785719707304906e-09, Train Acc: 1.0\n",
            "Epoch 6153/10000\n",
            "Step 0: Train Loss: 7.033345905682609e-09, Train Acc: 1.0\n",
            "Epoch 6154/10000\n",
            "Step 0: Train Loss: 7.784362487939234e-09, Train Acc: 1.0\n",
            "Epoch 6155/10000\n",
            "Step 0: Train Loss: 6.997582513434963e-09, Train Acc: 1.0\n",
            "Epoch 6156/10000\n",
            "Step 0: Train Loss: 7.89165088832533e-09, Train Acc: 1.0\n",
            "Epoch 6157/10000\n",
            "Step 0: Train Loss: 8.749957203235681e-09, Train Acc: 1.0\n",
            "Epoch 6158/10000\n",
            "Step 0: Train Loss: 7.84396725350689e-09, Train Acc: 1.0\n",
            "Epoch 6159/10000\n",
            "Step 0: Train Loss: 7.987018157962211e-09, Train Acc: 1.0\n",
            "Epoch 6160/10000\n",
            "Step 0: Train Loss: 7.283683878256397e-09, Train Acc: 1.0\n",
            "Epoch 6161/10000\n",
            "Step 0: Train Loss: 7.903571130896125e-09, Train Acc: 1.0\n",
            "Epoch 6162/10000\n",
            "Step 0: Train Loss: 7.557863668239406e-09, Train Acc: 1.0\n",
            "Epoch 6163/10000\n",
            "Step 0: Train Loss: 8.058543166100662e-09, Train Acc: 1.0\n",
            "Epoch 6164/10000\n",
            "Step 0: Train Loss: 5.60283552886176e-09, Train Acc: 1.0\n",
            "Epoch 6165/10000\n",
            "Step 0: Train Loss: 6.258486173749134e-09, Train Acc: 1.0\n",
            "Epoch 6166/10000\n",
            "Step 0: Train Loss: 8.296962228371285e-09, Train Acc: 1.0\n",
            "Epoch 6167/10000\n",
            "Step 0: Train Loss: 7.832046122757674e-09, Train Acc: 1.0\n",
            "Epoch 6168/10000\n",
            "Step 0: Train Loss: 7.987017269783792e-09, Train Acc: 1.0\n",
            "Epoch 6169/10000\n",
            "Step 0: Train Loss: 6.6041918600490135e-09, Train Acc: 1.0\n",
            "Epoch 6170/10000\n",
            "Step 0: Train Loss: 6.949897990438103e-09, Train Acc: 1.0\n",
            "Epoch 6171/10000\n",
            "Step 0: Train Loss: 8.034703569137491e-09, Train Acc: 1.0\n",
            "Epoch 6172/10000\n",
            "Step 0: Train Loss: 7.557864556417826e-09, Train Acc: 1.0\n",
            "Epoch 6173/10000\n",
            "Step 0: Train Loss: 6.66379484925983e-09, Train Acc: 1.0\n",
            "Epoch 6174/10000\n",
            "Step 0: Train Loss: 7.331368401253258e-09, Train Acc: 1.0\n",
            "Epoch 6175/10000\n",
            "Step 0: Train Loss: 7.116790268213435e-09, Train Acc: 1.0\n",
            "Epoch 6176/10000\n",
            "Step 0: Train Loss: 6.055830503726156e-09, Train Acc: 1.0\n",
            "Epoch 6177/10000\n",
            "Step 0: Train Loss: 7.10487091382106e-09, Train Acc: 1.0\n",
            "Epoch 6178/10000\n",
            "Step 0: Train Loss: 7.462494622245686e-09, Train Acc: 1.0\n",
            "Epoch 6179/10000\n",
            "Step 0: Train Loss: 6.794926399322776e-09, Train Acc: 1.0\n",
            "Epoch 6180/10000\n",
            "Step 0: Train Loss: 6.020067555567721e-09, Train Acc: 1.0\n",
            "Epoch 6181/10000\n",
            "Step 0: Train Loss: 7.033343685236559e-09, Train Acc: 1.0\n",
            "Epoch 6182/10000\n",
            "Step 0: Train Loss: 1.0275830852890522e-08, Train Acc: 1.0\n",
            "Epoch 6183/10000\n",
            "Step 0: Train Loss: 6.914135042279668e-09, Train Acc: 1.0\n",
            "Epoch 6184/10000\n",
            "Step 0: Train Loss: 5.769727362547883e-09, Train Acc: 1.0\n",
            "Epoch 6185/10000\n",
            "Step 0: Train Loss: 6.914135042279668e-09, Train Acc: 1.0\n",
            "Epoch 6186/10000\n",
            "Step 0: Train Loss: 6.473061642253697e-09, Train Acc: 1.0\n",
            "Epoch 6187/10000\n",
            "Step 0: Train Loss: 6.71148026043511e-09, Train Acc: 1.0\n",
            "Epoch 6188/10000\n",
            "Step 0: Train Loss: 7.414812763784084e-09, Train Acc: 1.0\n",
            "Epoch 6189/10000\n",
            "Step 0: Train Loss: 6.806846641893571e-09, Train Acc: 1.0\n",
            "Epoch 6190/10000\n",
            "Step 0: Train Loss: 6.747241432236706e-09, Train Acc: 1.0\n",
            "Epoch 6191/10000\n",
            "Step 0: Train Loss: 7.355207998216429e-09, Train Acc: 1.0\n",
            "Epoch 6192/10000\n",
            "Step 0: Train Loss: 6.437297361827632e-09, Train Acc: 1.0\n",
            "Epoch 6193/10000\n",
            "Step 0: Train Loss: 6.246563710732289e-09, Train Acc: 1.0\n",
            "Epoch 6194/10000\n",
            "Step 0: Train Loss: 8.26119883612364e-09, Train Acc: 1.0\n",
            "Epoch 6195/10000\n",
            "Step 0: Train Loss: 6.5088237022337125e-09, Train Acc: 1.0\n",
            "Epoch 6196/10000\n",
            "Step 0: Train Loss: 5.7458855451386626e-09, Train Acc: 1.0\n",
            "Epoch 6197/10000\n",
            "Step 0: Train Loss: 6.771084137824346e-09, Train Acc: 1.0\n",
            "Epoch 6198/10000\n",
            "Step 0: Train Loss: 4.839895595409871e-09, Train Acc: 1.0\n",
            "Epoch 6199/10000\n",
            "Step 0: Train Loss: 5.793569624046313e-09, Train Acc: 1.0\n",
            "Epoch 6200/10000\n",
            "Step 0: Train Loss: 6.3538521111183854e-09, Train Acc: 1.0\n",
            "Epoch 6201/10000\n",
            "Step 0: Train Loss: 6.544586206302938e-09, Train Acc: 1.0\n",
            "Epoch 6202/10000\n",
            "Step 0: Train Loss: 7.86780685047006e-09, Train Acc: 1.0\n",
            "Epoch 6203/10000\n",
            "Step 0: Train Loss: 6.532665963732143e-09, Train Acc: 1.0\n",
            "Epoch 6204/10000\n",
            "Step 0: Train Loss: 6.699558241507475e-09, Train Acc: 1.0\n",
            "Epoch 6205/10000\n",
            "Step 0: Train Loss: 6.031986465870887e-09, Train Acc: 1.0\n",
            "Epoch 6206/10000\n",
            "Step 0: Train Loss: 5.328653074343492e-09, Train Acc: 1.0\n",
            "Epoch 6207/10000\n",
            "Step 0: Train Loss: 6.818766884464367e-09, Train Acc: 1.0\n",
            "Epoch 6208/10000\n",
            "Step 0: Train Loss: 7.331366180807208e-09, Train Acc: 1.0\n",
            "Epoch 6209/10000\n",
            "Step 0: Train Loss: 5.960461013643226e-09, Train Acc: 1.0\n",
            "Epoch 6210/10000\n",
            "Step 0: Train Loss: 6.353850778850756e-09, Train Acc: 1.0\n",
            "Epoch 6211/10000\n",
            "Step 0: Train Loss: 5.769727806637093e-09, Train Acc: 1.0\n",
            "Epoch 6212/10000\n",
            "Step 0: Train Loss: 6.330009849619955e-09, Train Acc: 1.0\n",
            "Epoch 6213/10000\n",
            "Step 0: Train Loss: 7.307524363397988e-09, Train Acc: 1.0\n",
            "Epoch 6214/10000\n",
            "Step 0: Train Loss: 6.6041918600490135e-09, Train Acc: 1.0\n",
            "Epoch 6215/10000\n",
            "Step 0: Train Loss: 5.972381700303231e-09, Train Acc: 1.0\n",
            "Epoch 6216/10000\n",
            "Step 0: Train Loss: 5.877016207023189e-09, Train Acc: 1.0\n",
            "Epoch 6217/10000\n",
            "Step 0: Train Loss: 4.923342178386747e-09, Train Acc: 1.0\n",
            "Epoch 6218/10000\n",
            "Step 0: Train Loss: 6.473061642253697e-09, Train Acc: 1.0\n",
            "Epoch 6219/10000\n",
            "Step 0: Train Loss: 5.543230319204895e-09, Train Acc: 1.0\n",
            "Epoch 6220/10000\n",
            "Step 0: Train Loss: 5.817410553277114e-09, Train Acc: 1.0\n",
            "Epoch 6221/10000\n",
            "Step 0: Train Loss: 5.781647161029468e-09, Train Acc: 1.0\n",
            "Epoch 6222/10000\n",
            "Step 0: Train Loss: 5.6624389621617865e-09, Train Acc: 1.0\n",
            "Epoch 6223/10000\n",
            "Step 0: Train Loss: 4.4822678901823565e-09, Train Acc: 1.0\n",
            "Epoch 6224/10000\n",
            "Step 0: Train Loss: 6.031986465870887e-09, Train Acc: 1.0\n",
            "Epoch 6225/10000\n",
            "Step 0: Train Loss: 5.662437629894157e-09, Train Acc: 1.0\n",
            "Epoch 6226/10000\n",
            "Step 0: Train Loss: 5.841252370686334e-09, Train Acc: 1.0\n",
            "Epoch 6227/10000\n",
            "Step 0: Train Loss: 5.6624389621617865e-09, Train Acc: 1.0\n",
            "Epoch 6228/10000\n",
            "Step 0: Train Loss: 6.198879631824639e-09, Train Acc: 1.0\n",
            "Epoch 6229/10000\n",
            "Step 0: Train Loss: 5.781648049207888e-09, Train Acc: 1.0\n",
            "Epoch 6230/10000\n",
            "Step 0: Train Loss: 7.3432873115564234e-09, Train Acc: 1.0\n",
            "Epoch 6231/10000\n",
            "Step 0: Train Loss: 6.9618182330088985e-09, Train Acc: 1.0\n",
            "Epoch 6232/10000\n",
            "Step 0: Train Loss: 5.6624389621617865e-09, Train Acc: 1.0\n",
            "Epoch 6233/10000\n",
            "Step 0: Train Loss: 5.936620528501635e-09, Train Acc: 1.0\n",
            "Epoch 6234/10000\n",
            "Step 0: Train Loss: 6.759162562985921e-09, Train Acc: 1.0\n",
            "Epoch 6235/10000\n",
            "Step 0: Train Loss: 6.306167588121525e-09, Train Acc: 1.0\n",
            "Epoch 6236/10000\n",
            "Step 0: Train Loss: 6.568428023712158e-09, Train Acc: 1.0\n",
            "Epoch 6237/10000\n",
            "Step 0: Train Loss: 5.650518275501781e-09, Train Acc: 1.0\n",
            "Epoch 6238/10000\n",
            "Step 0: Train Loss: 6.1869585010754236e-09, Train Acc: 1.0\n",
            "Epoch 6239/10000\n",
            "Step 0: Train Loss: 5.5789923791849105e-09, Train Acc: 1.0\n",
            "Epoch 6240/10000\n",
            "Step 0: Train Loss: 6.091591675527752e-09, Train Acc: 1.0\n",
            "Epoch 6241/10000\n",
            "Step 0: Train Loss: 6.401535301847616e-09, Train Acc: 1.0\n",
            "Epoch 6242/10000\n",
            "Step 0: Train Loss: 4.982945611686773e-09, Train Acc: 1.0\n",
            "Epoch 6243/10000\n",
            "Step 0: Train Loss: 6.139275310346193e-09, Train Acc: 1.0\n",
            "Epoch 6244/10000\n",
            "Step 0: Train Loss: 6.222721893323069e-09, Train Acc: 1.0\n",
            "Epoch 6245/10000\n",
            "Step 0: Train Loss: 5.662438518072577e-09, Train Acc: 1.0\n",
            "Epoch 6246/10000\n",
            "Step 0: Train Loss: 4.756449012432995e-09, Train Acc: 1.0\n",
            "Epoch 6247/10000\n",
            "Step 0: Train Loss: 4.982946943954403e-09, Train Acc: 1.0\n",
            "Epoch 6248/10000\n",
            "Step 0: Train Loss: 5.16175990838974e-09, Train Acc: 1.0\n",
            "Epoch 6249/10000\n",
            "Step 0: Train Loss: 5.0187090039344184e-09, Train Acc: 1.0\n",
            "Epoch 6250/10000\n",
            "Step 0: Train Loss: 5.674360092911002e-09, Train Acc: 1.0\n",
            "Epoch 6251/10000\n",
            "Step 0: Train Loss: 5.352494003574293e-09, Train Acc: 1.0\n",
            "Epoch 6252/10000\n",
            "Step 0: Train Loss: 5.4955457962080345e-09, Train Acc: 1.0\n",
            "Epoch 6253/10000\n",
            "Step 0: Train Loss: 5.55515056177569e-09, Train Acc: 1.0\n",
            "Epoch 6254/10000\n",
            "Step 0: Train Loss: 5.316731499505067e-09, Train Acc: 1.0\n",
            "Epoch 6255/10000\n",
            "Step 0: Train Loss: 4.100797923456412e-09, Train Acc: 1.0\n",
            "Epoch 6256/10000\n",
            "Step 0: Train Loss: 5.543229875115685e-09, Train Acc: 1.0\n",
            "Epoch 6257/10000\n",
            "Step 0: Train Loss: 4.541872655750012e-09, Train Acc: 1.0\n",
            "Epoch 6258/10000\n",
            "Step 0: Train Loss: 4.589555402390033e-09, Train Acc: 1.0\n",
            "Epoch 6259/10000\n",
            "Step 0: Train Loss: 5.042550821343639e-09, Train Acc: 1.0\n",
            "Epoch 6260/10000\n",
            "Step 0: Train Loss: 5.364416022501928e-09, Train Acc: 1.0\n",
            "Epoch 6261/10000\n",
            "Step 0: Train Loss: 5.280969439525052e-09, Train Acc: 1.0\n",
            "Epoch 6262/10000\n",
            "Step 0: Train Loss: 5.9008575803431995e-09, Train Acc: 1.0\n",
            "Epoch 6263/10000\n",
            "Step 0: Train Loss: 5.125996516142095e-09, Train Acc: 1.0\n",
            "Epoch 6264/10000\n",
            "Step 0: Train Loss: 5.233285804706611e-09, Train Acc: 1.0\n",
            "Epoch 6265/10000\n",
            "Step 0: Train Loss: 5.388257839911148e-09, Train Acc: 1.0\n",
            "Epoch 6266/10000\n",
            "Step 0: Train Loss: 5.316731943594277e-09, Train Acc: 1.0\n",
            "Epoch 6267/10000\n",
            "Step 0: Train Loss: 4.112719498294837e-09, Train Acc: 1.0\n",
            "Epoch 6268/10000\n",
            "Step 0: Train Loss: 4.851816726159086e-09, Train Acc: 1.0\n",
            "Epoch 6269/10000\n",
            "Step 0: Train Loss: 5.578991935095701e-09, Train Acc: 1.0\n",
            "Epoch 6270/10000\n",
            "Step 0: Train Loss: 5.185601281709751e-09, Train Acc: 1.0\n",
            "Epoch 6271/10000\n",
            "Step 0: Train Loss: 5.1856021698881705e-09, Train Acc: 1.0\n",
            "Epoch 6272/10000\n",
            "Step 0: Train Loss: 3.98158972458873e-09, Train Acc: 1.0\n",
            "Epoch 6273/10000\n",
            "Step 0: Train Loss: 4.565714029070023e-09, Train Acc: 1.0\n",
            "Epoch 6274/10000\n",
            "Step 0: Train Loss: 5.7220428395510226e-09, Train Acc: 1.0\n",
            "Epoch 6275/10000\n",
            "Step 0: Train Loss: 6.437297805916842e-09, Train Acc: 1.0\n",
            "Epoch 6276/10000\n",
            "Step 0: Train Loss: 4.959104238366763e-09, Train Acc: 1.0\n",
            "Epoch 6277/10000\n",
            "Step 0: Train Loss: 4.851815837980666e-09, Train Acc: 1.0\n",
            "Epoch 6278/10000\n",
            "Step 0: Train Loss: 4.839894707231451e-09, Train Acc: 1.0\n",
            "Epoch 6279/10000\n",
            "Step 0: Train Loss: 4.518030394251582e-09, Train Acc: 1.0\n",
            "Epoch 6280/10000\n",
            "Step 0: Train Loss: 3.826617245294983e-09, Train Acc: 1.0\n",
            "Epoch 6281/10000\n",
            "Step 0: Train Loss: 4.279611776070169e-09, Train Acc: 1.0\n",
            "Epoch 6282/10000\n",
            "Step 0: Train Loss: 4.160403133113277e-09, Train Acc: 1.0\n",
            "Epoch 6283/10000\n",
            "Step 0: Train Loss: 3.862379749364209e-09, Train Acc: 1.0\n",
            "Epoch 6284/10000\n",
            "Step 0: Train Loss: 4.827974908749866e-09, Train Acc: 1.0\n",
            "Epoch 6285/10000\n",
            "Step 0: Train Loss: 4.100797923456412e-09, Train Acc: 1.0\n",
            "Epoch 6286/10000\n",
            "Step 0: Train Loss: 5.125997404320515e-09, Train Acc: 1.0\n",
            "Epoch 6287/10000\n",
            "Step 0: Train Loss: 4.339216541637825e-09, Train Acc: 1.0\n",
            "Epoch 6288/10000\n",
            "Step 0: Train Loss: 4.494188576842362e-09, Train Acc: 1.0\n",
            "Epoch 6289/10000\n",
            "Step 0: Train Loss: 4.4107424379546956e-09, Train Acc: 1.0\n",
            "Epoch 6290/10000\n",
            "Step 0: Train Loss: 4.839894707231451e-09, Train Acc: 1.0\n",
            "Epoch 6291/10000\n",
            "Step 0: Train Loss: 5.90085713625399e-09, Train Acc: 1.0\n",
            "Epoch 6292/10000\n",
            "Step 0: Train Loss: 5.4836242213696096e-09, Train Acc: 1.0\n",
            "Epoch 6293/10000\n",
            "Step 0: Train Loss: 4.959104238366763e-09, Train Acc: 1.0\n",
            "Epoch 6294/10000\n",
            "Step 0: Train Loss: 4.208086323842508e-09, Train Acc: 1.0\n",
            "Epoch 6295/10000\n",
            "Step 0: Train Loss: 4.255770402750159e-09, Train Acc: 1.0\n",
            "Epoch 6296/10000\n",
            "Step 0: Train Loss: 4.76836925500379e-09, Train Acc: 1.0\n",
            "Epoch 6297/10000\n",
            "Step 0: Train Loss: 4.255770402750159e-09, Train Acc: 1.0\n",
            "Epoch 6298/10000\n",
            "Step 0: Train Loss: 5.3405742050927074e-09, Train Acc: 1.0\n",
            "Epoch 6299/10000\n",
            "Step 0: Train Loss: 3.6239615752720056e-09, Train Acc: 1.0\n",
            "Epoch 6300/10000\n",
            "Step 0: Train Loss: 5.066392638752859e-09, Train Acc: 1.0\n",
            "Epoch 6301/10000\n",
            "Step 0: Train Loss: 4.74452743759457e-09, Train Acc: 1.0\n",
            "Epoch 6302/10000\n",
            "Step 0: Train Loss: 4.911421047637532e-09, Train Acc: 1.0\n",
            "Epoch 6303/10000\n",
            "Step 0: Train Loss: 4.923341290208327e-09, Train Acc: 1.0\n",
            "Epoch 6304/10000\n",
            "Step 0: Train Loss: 4.231928585340938e-09, Train Acc: 1.0\n",
            "Epoch 6305/10000\n",
            "Step 0: Train Loss: 4.053114288637971e-09, Train Acc: 1.0\n",
            "Epoch 6306/10000\n",
            "Step 0: Train Loss: 5.53130830027726e-09, Train Acc: 1.0\n",
            "Epoch 6307/10000\n",
            "Step 0: Train Loss: 3.6716449880458413e-09, Train Acc: 1.0\n",
            "Epoch 6308/10000\n",
            "Step 0: Train Loss: 4.887579230228312e-09, Train Acc: 1.0\n",
            "Epoch 6309/10000\n",
            "Step 0: Train Loss: 4.267691533499374e-09, Train Acc: 1.0\n",
            "Epoch 6310/10000\n",
            "Step 0: Train Loss: 3.182887287067615e-09, Train Acc: 1.0\n",
            "Epoch 6311/10000\n",
            "Step 0: Train Loss: 4.065035863476396e-09, Train Acc: 1.0\n",
            "Epoch 6312/10000\n",
            "Step 0: Train Loss: 4.863737412819091e-09, Train Acc: 1.0\n",
            "Epoch 6313/10000\n",
            "Step 0: Train Loss: 4.112719054205627e-09, Train Acc: 1.0\n",
            "Epoch 6314/10000\n",
            "Step 0: Train Loss: 4.243848383822524e-09, Train Acc: 1.0\n",
            "Epoch 6315/10000\n",
            "Step 0: Train Loss: 3.659724079341231e-09, Train Acc: 1.0\n",
            "Epoch 6316/10000\n",
            "Step 0: Train Loss: 4.1604026890240675e-09, Train Acc: 1.0\n",
            "Epoch 6317/10000\n",
            "Step 0: Train Loss: 4.0888772367964066e-09, Train Acc: 1.0\n",
            "Epoch 6318/10000\n",
            "Step 0: Train Loss: 4.589555402390033e-09, Train Acc: 1.0\n",
            "Epoch 6319/10000\n",
            "Step 0: Train Loss: 4.124640184954842e-09, Train Acc: 1.0\n",
            "Epoch 6320/10000\n",
            "Step 0: Train Loss: 4.482267446093147e-09, Train Acc: 1.0\n",
            "Epoch 6321/10000\n",
            "Step 0: Train Loss: 4.017351784568746e-09, Train Acc: 1.0\n",
            "Epoch 6322/10000\n",
            "Step 0: Train Loss: 3.1352036522491744e-09, Train Acc: 1.0\n",
            "Epoch 6323/10000\n",
            "Step 0: Train Loss: 3.9219840708426545e-09, Train Acc: 1.0\n",
            "Epoch 6324/10000\n",
            "Step 0: Train Loss: 3.4809104487720788e-09, Train Acc: 1.0\n",
            "Epoch 6325/10000\n",
            "Step 0: Train Loss: 4.673002429456119e-09, Train Acc: 1.0\n",
            "Epoch 6326/10000\n",
            "Step 0: Train Loss: 3.874300436024214e-09, Train Acc: 1.0\n",
            "Epoch 6327/10000\n",
            "Step 0: Train Loss: 4.601476977228458e-09, Train Acc: 1.0\n",
            "Epoch 6328/10000\n",
            "Step 0: Train Loss: 4.661080854617694e-09, Train Acc: 1.0\n",
            "Epoch 6329/10000\n",
            "Step 0: Train Loss: 5.412098769141949e-09, Train Acc: 1.0\n",
            "Epoch 6330/10000\n",
            "Step 0: Train Loss: 4.017351784568746e-09, Train Acc: 1.0\n",
            "Epoch 6331/10000\n",
            "Step 0: Train Loss: 4.4703463153439316e-09, Train Acc: 1.0\n",
            "Epoch 6332/10000\n",
            "Step 0: Train Loss: 4.422663124614701e-09, Train Acc: 1.0\n",
            "Epoch 6333/10000\n",
            "Step 0: Train Loss: 4.9233408461191175e-09, Train Acc: 1.0\n",
            "Epoch 6334/10000\n",
            "Step 0: Train Loss: 3.3378586561383372e-09, Train Acc: 1.0\n",
            "Epoch 6335/10000\n",
            "Step 0: Train Loss: 4.017351784568746e-09, Train Acc: 1.0\n",
            "Epoch 6336/10000\n",
            "Step 0: Train Loss: 3.635881817842801e-09, Train Acc: 1.0\n",
            "Epoch 6337/10000\n",
            "Step 0: Train Loss: 3.767012923816537e-09, Train Acc: 1.0\n",
            "Epoch 6338/10000\n",
            "Step 0: Train Loss: 3.56435636561514e-09, Train Acc: 1.0\n",
            "Epoch 6339/10000\n",
            "Step 0: Train Loss: 5.13791764689131e-09, Train Acc: 1.0\n",
            "Epoch 6340/10000\n",
            "Step 0: Train Loss: 3.898142253433434e-09, Train Acc: 1.0\n",
            "Epoch 6341/10000\n",
            "Step 0: Train Loss: 4.792211072413011e-09, Train Acc: 1.0\n",
            "Epoch 6342/10000\n",
            "Step 0: Train Loss: 4.458426072773136e-09, Train Acc: 1.0\n",
            "Epoch 6343/10000\n",
            "Step 0: Train Loss: 3.2424920526352707e-09, Train Acc: 1.0\n",
            "Epoch 6344/10000\n",
            "Step 0: Train Loss: 4.398820419027061e-09, Train Acc: 1.0\n",
            "Epoch 6345/10000\n",
            "Step 0: Train Loss: 4.410741993865486e-09, Train Acc: 1.0\n",
            "Epoch 6346/10000\n",
            "Step 0: Train Loss: 3.98158883641031e-09, Train Acc: 1.0\n",
            "Epoch 6347/10000\n",
            "Step 0: Train Loss: 4.7802903857530055e-09, Train Acc: 1.0\n",
            "Epoch 6348/10000\n",
            "Step 0: Train Loss: 3.767012035638118e-09, Train Acc: 1.0\n",
            "Epoch 6349/10000\n",
            "Step 0: Train Loss: 4.3868997323670555e-09, Train Acc: 1.0\n",
            "Epoch 6350/10000\n",
            "Step 0: Train Loss: 3.564357031748955e-09, Train Acc: 1.0\n",
            "Epoch 6351/10000\n",
            "Step 0: Train Loss: 3.814696114545768e-09, Train Acc: 1.0\n",
            "Epoch 6352/10000\n",
            "Step 0: Train Loss: 3.5524356789551348e-09, Train Acc: 1.0\n",
            "Epoch 6353/10000\n",
            "Step 0: Train Loss: 3.778933166387333e-09, Train Acc: 1.0\n",
            "Epoch 6354/10000\n",
            "Step 0: Train Loss: 3.6835663408396613e-09, Train Acc: 1.0\n",
            "Epoch 6355/10000\n",
            "Step 0: Train Loss: 3.790854741225758e-09, Train Acc: 1.0\n",
            "Epoch 6356/10000\n",
            "Step 0: Train Loss: 4.422662680525491e-09, Train Acc: 1.0\n",
            "Epoch 6357/10000\n",
            "Step 0: Train Loss: 3.2305709218860557e-09, Train Acc: 1.0\n",
            "Epoch 6358/10000\n",
            "Step 0: Train Loss: 4.017351784568746e-09, Train Acc: 1.0\n",
            "Epoch 6359/10000\n",
            "Step 0: Train Loss: 3.170965934273795e-09, Train Acc: 1.0\n",
            "Epoch 6360/10000\n",
            "Step 0: Train Loss: 3.0040734344538578e-09, Train Acc: 1.0\n",
            "Epoch 6361/10000\n",
            "Step 0: Train Loss: 4.053113844548761e-09, Train Acc: 1.0\n",
            "Epoch 6362/10000\n",
            "Step 0: Train Loss: 4.32729541088861e-09, Train Acc: 1.0\n",
            "Epoch 6363/10000\n",
            "Step 0: Train Loss: 3.492831357476689e-09, Train Acc: 1.0\n",
            "Epoch 6364/10000\n",
            "Step 0: Train Loss: 2.861022307953931e-09, Train Acc: 1.0\n",
            "Epoch 6365/10000\n",
            "Step 0: Train Loss: 2.8252593597954956e-09, Train Acc: 1.0\n",
            "Epoch 6366/10000\n",
            "Step 0: Train Loss: 4.279611776070169e-09, Train Acc: 1.0\n",
            "Epoch 6367/10000\n",
            "Step 0: Train Loss: 2.8133384510908854e-09, Train Acc: 1.0\n",
            "Epoch 6368/10000\n",
            "Step 0: Train Loss: 4.112719054205627e-09, Train Acc: 1.0\n",
            "Epoch 6369/10000\n",
            "Step 0: Train Loss: 3.492830691342874e-09, Train Acc: 1.0\n",
            "Epoch 6370/10000\n",
            "Step 0: Train Loss: 4.029272027139541e-09, Train Acc: 1.0\n",
            "Epoch 6371/10000\n",
            "Step 0: Train Loss: 3.1590454696583947e-09, Train Acc: 1.0\n",
            "Epoch 6372/10000\n",
            "Step 0: Train Loss: 3.337859100227547e-09, Train Acc: 1.0\n",
            "Epoch 6373/10000\n",
            "Step 0: Train Loss: 3.635882261932011e-09, Train Acc: 1.0\n",
            "Epoch 6374/10000\n",
            "Step 0: Train Loss: 2.9563895775908122e-09, Train Acc: 1.0\n",
            "Epoch 6375/10000\n",
            "Step 0: Train Loss: 3.480910226727474e-09, Train Acc: 1.0\n",
            "Epoch 6376/10000\n",
            "Step 0: Train Loss: 2.932547760181592e-09, Train Acc: 1.0\n",
            "Epoch 6377/10000\n",
            "Step 0: Train Loss: 3.2305709218860557e-09, Train Acc: 1.0\n",
            "Epoch 6378/10000\n",
            "Step 0: Train Loss: 3.1232827435445643e-09, Train Acc: 1.0\n",
            "Epoch 6379/10000\n",
            "Step 0: Train Loss: 4.208086323842508e-09, Train Acc: 1.0\n",
            "Epoch 6380/10000\n",
            "Step 0: Train Loss: 3.325938413567542e-09, Train Acc: 1.0\n",
            "Epoch 6381/10000\n",
            "Step 0: Train Loss: 4.172323819773283e-09, Train Acc: 1.0\n",
            "Epoch 6382/10000\n",
            "Step 0: Train Loss: 3.087519795386129e-09, Train Acc: 1.0\n",
            "Epoch 6383/10000\n",
            "Step 0: Train Loss: 3.492830691342874e-09, Train Acc: 1.0\n",
            "Epoch 6384/10000\n",
            "Step 0: Train Loss: 3.421305461159818e-09, Train Acc: 1.0\n",
            "Epoch 6385/10000\n",
            "Step 0: Train Loss: 3.1232822994553544e-09, Train Acc: 1.0\n",
            "Epoch 6386/10000\n",
            "Step 0: Train Loss: 3.898141809344224e-09, Train Acc: 1.0\n",
            "Epoch 6387/10000\n",
            "Step 0: Train Loss: 3.6120400004335806e-09, Train Acc: 1.0\n",
            "Epoch 6388/10000\n",
            "Step 0: Train Loss: 3.159045025569185e-09, Train Acc: 1.0\n",
            "Epoch 6389/10000\n",
            "Step 0: Train Loss: 3.492830691342874e-09, Train Acc: 1.0\n",
            "Epoch 6390/10000\n",
            "Step 0: Train Loss: 3.4451477226582483e-09, Train Acc: 1.0\n",
            "Epoch 6391/10000\n",
            "Step 0: Train Loss: 3.314017060773722e-09, Train Acc: 1.0\n",
            "Epoch 6392/10000\n",
            "Step 0: Train Loss: 3.409384552455208e-09, Train Acc: 1.0\n",
            "Epoch 6393/10000\n",
            "Step 0: Train Loss: 3.409384552455208e-09, Train Acc: 1.0\n",
            "Epoch 6394/10000\n",
            "Step 0: Train Loss: 2.9563895775908122e-09, Train Acc: 1.0\n",
            "Epoch 6395/10000\n",
            "Step 0: Train Loss: 2.8848639033185464e-09, Train Acc: 1.0\n",
            "Epoch 6396/10000\n",
            "Step 0: Train Loss: 3.492830691342874e-09, Train Acc: 1.0\n",
            "Epoch 6397/10000\n",
            "Step 0: Train Loss: 2.8133384510908854e-09, Train Acc: 1.0\n",
            "Epoch 6398/10000\n",
            "Step 0: Train Loss: 3.015994343158468e-09, Train Acc: 1.0\n",
            "Epoch 6399/10000\n",
            "Step 0: Train Loss: 3.719328400819677e-09, Train Acc: 1.0\n",
            "Epoch 6400/10000\n",
            "Step 0: Train Loss: 2.8967852561123664e-09, Train Acc: 1.0\n",
            "Epoch 6401/10000\n",
            "Step 0: Train Loss: 3.5285943056351243e-09, Train Acc: 1.0\n",
            "Epoch 6402/10000\n",
            "Step 0: Train Loss: 3.3020961520691117e-09, Train Acc: 1.0\n",
            "Epoch 6403/10000\n",
            "Step 0: Train Loss: 3.3020959300245067e-09, Train Acc: 1.0\n",
            "Epoch 6404/10000\n",
            "Step 0: Train Loss: 3.480909782638264e-09, Train Acc: 1.0\n",
            "Epoch 6405/10000\n",
            "Step 0: Train Loss: 3.325937747433727e-09, Train Acc: 1.0\n",
            "Epoch 6406/10000\n",
            "Step 0: Train Loss: 3.3020961520691117e-09, Train Acc: 1.0\n",
            "Epoch 6407/10000\n",
            "Step 0: Train Loss: 3.2305706998414507e-09, Train Acc: 1.0\n",
            "Epoch 6408/10000\n",
            "Step 0: Train Loss: 2.8252593597954956e-09, Train Acc: 1.0\n",
            "Epoch 6409/10000\n",
            "Step 0: Train Loss: 3.4332263698644283e-09, Train Acc: 1.0\n",
            "Epoch 6410/10000\n",
            "Step 0: Train Loss: 3.1352027640707547e-09, Train Acc: 1.0\n",
            "Epoch 6411/10000\n",
            "Step 0: Train Loss: 2.5749198329094725e-09, Train Acc: 1.0\n",
            "Epoch 6412/10000\n",
            "Step 0: Train Loss: 3.015994121113863e-09, Train Acc: 1.0\n",
            "Epoch 6413/10000\n",
            "Step 0: Train Loss: 2.539157106795642e-09, Train Acc: 1.0\n",
            "Epoch 6414/10000\n",
            "Step 0: Train Loss: 2.9087057207277667e-09, Train Acc: 1.0\n",
            "Epoch 6415/10000\n",
            "Step 0: Train Loss: 3.2782543346598914e-09, Train Acc: 1.0\n",
            "Epoch 6416/10000\n",
            "Step 0: Train Loss: 3.4570686313628585e-09, Train Acc: 1.0\n",
            "Epoch 6417/10000\n",
            "Step 0: Train Loss: 3.54051410411671e-09, Train Acc: 1.0\n",
            "Epoch 6418/10000\n",
            "Step 0: Train Loss: 3.015994343158468e-09, Train Acc: 1.0\n",
            "Epoch 6419/10000\n",
            "Step 0: Train Loss: 3.55243545691053e-09, Train Acc: 1.0\n",
            "Epoch 6420/10000\n",
            "Step 0: Train Loss: 3.6597236352520213e-09, Train Acc: 1.0\n",
            "Epoch 6421/10000\n",
            "Step 0: Train Loss: 3.314017060773722e-09, Train Acc: 1.0\n",
            "Epoch 6422/10000\n",
            "Step 0: Train Loss: 2.8967848120231565e-09, Train Acc: 1.0\n",
            "Epoch 6423/10000\n",
            "Step 0: Train Loss: 3.3497800089321572e-09, Train Acc: 1.0\n",
            "Epoch 6424/10000\n",
            "Step 0: Train Loss: 2.872943216658541e-09, Train Acc: 1.0\n",
            "Epoch 6425/10000\n",
            "Step 0: Train Loss: 3.421305461159818e-09, Train Acc: 1.0\n",
            "Epoch 6426/10000\n",
            "Step 0: Train Loss: 3.0398357164784784e-09, Train Acc: 1.0\n",
            "Epoch 6427/10000\n",
            "Step 0: Train Loss: 2.7537339075678346e-09, Train Acc: 1.0\n",
            "Epoch 6428/10000\n",
            "Step 0: Train Loss: 2.717971181454004e-09, Train Acc: 1.0\n",
            "Epoch 6429/10000\n",
            "Step 0: Train Loss: 2.3365014367726644e-09, Train Acc: 1.0\n",
            "Epoch 6430/10000\n",
            "Step 0: Train Loss: 2.4318687064095457e-09, Train Acc: 1.0\n",
            "Epoch 6431/10000\n",
            "Step 0: Train Loss: 3.2901750213198966e-09, Train Acc: 1.0\n",
            "Epoch 6432/10000\n",
            "Step 0: Train Loss: 3.170965934273795e-09, Train Acc: 1.0\n",
            "Epoch 6433/10000\n",
            "Step 0: Train Loss: 3.409384552455208e-09, Train Acc: 1.0\n",
            "Epoch 6434/10000\n",
            "Step 0: Train Loss: 2.2649762065896084e-09, Train Acc: 1.0\n",
            "Epoch 6435/10000\n",
            "Step 0: Train Loss: 2.9563895775908122e-09, Train Acc: 1.0\n",
            "Epoch 6436/10000\n",
            "Step 0: Train Loss: 2.944468668886202e-09, Train Acc: 1.0\n",
            "Epoch 6437/10000\n",
            "Step 0: Train Loss: 3.015993899069258e-09, Train Acc: 1.0\n",
            "Epoch 6438/10000\n",
            "Step 0: Train Loss: 3.087519573341524e-09, Train Acc: 1.0\n",
            "Epoch 6439/10000\n",
            "Step 0: Train Loss: 2.133845544705082e-09, Train Acc: 1.0\n",
            "Epoch 6440/10000\n",
            "Step 0: Train Loss: 2.9087057207277667e-09, Train Acc: 1.0\n",
            "Epoch 6441/10000\n",
            "Step 0: Train Loss: 2.849101399249321e-09, Train Acc: 1.0\n",
            "Epoch 6442/10000\n",
            "Step 0: Train Loss: 3.099440482046134e-09, Train Acc: 1.0\n",
            "Epoch 6443/10000\n",
            "Step 0: Train Loss: 2.4080268890003254e-09, Train Acc: 1.0\n",
            "Epoch 6444/10000\n",
            "Step 0: Train Loss: 2.1815294015681275e-09, Train Acc: 1.0\n",
            "Epoch 6445/10000\n",
            "Step 0: Train Loss: 2.5033943806818115e-09, Train Acc: 1.0\n",
            "Epoch 6446/10000\n",
            "Step 0: Train Loss: 3.4451472785690385e-09, Train Acc: 1.0\n",
            "Epoch 6447/10000\n",
            "Step 0: Train Loss: 2.753733463478625e-09, Train Acc: 1.0\n",
            "Epoch 6448/10000\n",
            "Step 0: Train Loss: 2.527236198091032e-09, Train Acc: 1.0\n",
            "Epoch 6449/10000\n",
            "Step 0: Train Loss: 3.170965934273795e-09, Train Acc: 1.0\n",
            "Epoch 6450/10000\n",
            "Step 0: Train Loss: 2.610682781067908e-09, Train Acc: 1.0\n",
            "Epoch 6451/10000\n",
            "Step 0: Train Loss: 2.6464455071817383e-09, Train Acc: 1.0\n",
            "Epoch 6452/10000\n",
            "Step 0: Train Loss: 2.8729429946139362e-09, Train Acc: 1.0\n",
            "Epoch 6453/10000\n",
            "Step 0: Train Loss: 2.062320314522026e-09, Train Acc: 1.0\n",
            "Epoch 6454/10000\n",
            "Step 0: Train Loss: 2.8848641253631513e-09, Train Acc: 1.0\n",
            "Epoch 6455/10000\n",
            "Step 0: Train Loss: 2.8133382290462805e-09, Train Acc: 1.0\n",
            "Epoch 6456/10000\n",
            "Step 0: Train Loss: 2.837180046455501e-09, Train Acc: 1.0\n",
            "Epoch 6457/10000\n",
            "Step 0: Train Loss: 2.944468668886202e-09, Train Acc: 1.0\n",
            "Epoch 6458/10000\n",
            "Step 0: Train Loss: 2.002715770998975e-09, Train Acc: 1.0\n",
            "Epoch 6459/10000\n",
            "Step 0: Train Loss: 2.8133384510908854e-09, Train Acc: 1.0\n",
            "Epoch 6460/10000\n",
            "Step 0: Train Loss: 2.217292349726563e-09, Train Acc: 1.0\n",
            "Epoch 6461/10000\n",
            "Step 0: Train Loss: 2.9802311729554276e-09, Train Acc: 1.0\n",
            "Epoch 6462/10000\n",
            "Step 0: Train Loss: 2.241134389180388e-09, Train Acc: 1.0\n",
            "Epoch 6463/10000\n",
            "Step 0: Train Loss: 2.3484223454772746e-09, Train Acc: 1.0\n",
            "Epoch 6464/10000\n",
            "Step 0: Train Loss: 2.074241223226636e-09, Train Acc: 1.0\n",
            "Epoch 6465/10000\n",
            "Step 0: Train Loss: 2.5749198329094725e-09, Train Acc: 1.0\n",
            "Epoch 6466/10000\n",
            "Step 0: Train Loss: 2.2768971152942186e-09, Train Acc: 1.0\n",
            "Epoch 6467/10000\n",
            "Step 0: Train Loss: 2.5868407416140826e-09, Train Acc: 1.0\n",
            "Epoch 6468/10000\n",
            "Step 0: Train Loss: 2.6822082332955688e-09, Train Acc: 1.0\n",
            "Epoch 6469/10000\n",
            "Step 0: Train Loss: 2.396105980295715e-09, Train Acc: 1.0\n",
            "Epoch 6470/10000\n",
            "Step 0: Train Loss: 2.372264162886495e-09, Train Acc: 1.0\n",
            "Epoch 6471/10000\n",
            "Step 0: Train Loss: 2.4437900592033657e-09, Train Acc: 1.0\n",
            "Epoch 6472/10000\n",
            "Step 0: Train Loss: 2.300738710658834e-09, Train Acc: 1.0\n",
            "Epoch 6473/10000\n",
            "Step 0: Train Loss: 2.8371802685001057e-09, Train Acc: 1.0\n",
            "Epoch 6474/10000\n",
            "Step 0: Train Loss: 2.312659397318839e-09, Train Acc: 1.0\n",
            "Epoch 6475/10000\n",
            "Step 0: Train Loss: 2.539157106795642e-09, Train Acc: 1.0\n",
            "Epoch 6476/10000\n",
            "Step 0: Train Loss: 2.777575724977055e-09, Train Acc: 1.0\n",
            "Epoch 6477/10000\n",
            "Step 0: Train Loss: 2.455710745863371e-09, Train Acc: 1.0\n",
            "Epoch 6478/10000\n",
            "Step 0: Train Loss: 2.2411339450911782e-09, Train Acc: 1.0\n",
            "Epoch 6479/10000\n",
            "Step 0: Train Loss: 2.598761428274088e-09, Train Acc: 1.0\n",
            "Epoch 6480/10000\n",
            "Step 0: Train Loss: 2.694129142000179e-09, Train Acc: 1.0\n",
            "Epoch 6481/10000\n",
            "Step 0: Train Loss: 2.467631654567981e-09, Train Acc: 1.0\n",
            "Epoch 6482/10000\n",
            "Step 0: Train Loss: 2.372263718797285e-09, Train Acc: 1.0\n",
            "Epoch 6483/10000\n",
            "Step 0: Train Loss: 2.384185071591105e-09, Train Acc: 1.0\n",
            "Epoch 6484/10000\n",
            "Step 0: Train Loss: 2.682208011250964e-09, Train Acc: 1.0\n",
            "Epoch 6485/10000\n",
            "Step 0: Train Loss: 2.1815296236127324e-09, Train Acc: 1.0\n",
            "Epoch 6486/10000\n",
            "Step 0: Train Loss: 1.7642971528175622e-09, Train Acc: 1.0\n",
            "Epoch 6487/10000\n",
            "Step 0: Train Loss: 2.932547538136987e-09, Train Acc: 1.0\n",
            "Epoch 6488/10000\n",
            "Step 0: Train Loss: 2.5749196108648675e-09, Train Acc: 1.0\n",
            "Epoch 6489/10000\n",
            "Step 0: Train Loss: 2.8252593597954956e-09, Train Acc: 1.0\n",
            "Epoch 6490/10000\n",
            "Step 0: Train Loss: 1.9311900967267093e-09, Train Acc: 1.0\n",
            "Epoch 6491/10000\n",
            "Step 0: Train Loss: 2.7179709594093993e-09, Train Acc: 1.0\n",
            "Epoch 6492/10000\n",
            "Step 0: Train Loss: 1.8596645334767459e-09, Train Acc: 1.0\n",
            "Epoch 6493/10000\n",
            "Step 0: Train Loss: 2.062320092477421e-09, Train Acc: 1.0\n",
            "Epoch 6494/10000\n",
            "Step 0: Train Loss: 1.8835064619082686e-09, Train Acc: 1.0\n",
            "Epoch 6495/10000\n",
            "Step 0: Train Loss: 2.408027333089535e-09, Train Acc: 1.0\n",
            "Epoch 6496/10000\n",
            "Step 0: Train Loss: 2.3245805280680543e-09, Train Acc: 1.0\n",
            "Epoch 6497/10000\n",
            "Step 0: Train Loss: 2.4080268890003254e-09, Train Acc: 1.0\n",
            "Epoch 6498/10000\n",
            "Step 0: Train Loss: 2.3603434762264897e-09, Train Acc: 1.0\n",
            "Epoch 6499/10000\n",
            "Step 0: Train Loss: 2.2888180239988287e-09, Train Acc: 1.0\n",
            "Epoch 6500/10000\n",
            "Step 0: Train Loss: 2.4914734719772014e-09, Train Acc: 1.0\n",
            "Epoch 6501/10000\n",
            "Step 0: Train Loss: 1.8596645334767459e-09, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 6500 1024 1.0342288196017682\n",
            "Epoch index and hidden dimension and ratio: 6500 20 1.043805763919731\n",
            "Epoch index and hidden dimension and ratio: 6500 20 1.7176397474522411\n",
            "Epoch index and hidden dimension and ratio: 6500 20 4.520018988654271\n",
            "MI(X;T): [10.656404913386504, 7.970887351472429, 5.493894295162896, 3.56196536363948], MI(Y;T): [2.6268972012568805, 3.192817412850645, 3.1838233808667225, 2.9398282358859213]\n",
            "Epoch index and hidden dimension and ratio: 6500 1024 1.034233004700306\n",
            "Epoch index and hidden dimension and ratio: 6500 20 1.0438210599101638\n",
            "Epoch index and hidden dimension and ratio: 6500 20 1.7176554904253523\n",
            "Epoch index and hidden dimension and ratio: 6500 20 4.520062421375875\n",
            "MI(X;T): [10.656488552792137, 7.9704999537410375, 5.494055311844492, 3.5620506130705962], MI(Y;T): [2.6268696875592377, 3.192831937553418, 3.1837891781891434, 2.9397683149997844]\n",
            "Epoch index and hidden dimension and ratio: 6500 1024 1.0342397595961923\n",
            "Epoch index and hidden dimension and ratio: 6500 20 1.0438401527776542\n",
            "Epoch index and hidden dimension and ratio: 6500 20 1.7177189701556397\n",
            "Epoch index and hidden dimension and ratio: 6500 20 4.520325320955885\n",
            "MI(X;T): [10.656703886370416, 7.970772809743913, 5.493770014301004, 3.561890622762392], MI(Y;T): [2.6268954594738965, 3.192831937553418, 3.1837764516860076, 2.9400447112301613]\n",
            "Epoch index and hidden dimension and ratio: 6500 1024 1.034244825768107\n",
            "Epoch index and hidden dimension and ratio: 6500 20 1.0438558826968936\n",
            "Epoch index and hidden dimension and ratio: 6500 20 1.7177881630616527\n",
            "Epoch index and hidden dimension and ratio: 6500 20 4.520592168965132\n",
            "MI(X;T): [10.656703886370416, 7.970973281784606, 5.492704802709198, 3.5623869151955443], MI(Y;T): [2.6267814181230698, 3.1929058250605338, 3.1839116930266056, 2.9399418184569237]\n",
            "Epoch index and hidden dimension and ratio: 6500 1024 1.034251360395649\n",
            "Epoch index and hidden dimension and ratio: 6500 20 1.0438755179753922\n",
            "Epoch index and hidden dimension and ratio: 6500 20 1.717865989210985\n",
            "Epoch index and hidden dimension and ratio: 6500 20 4.520866913832853\n",
            "MI(X;T): [10.656769335613102, 7.970926964257865, 5.491754190901915, 3.5620366446689626], MI(Y;T): [2.6267814181230698, 3.1929141810148653, 3.1839036921233674, 2.9400096459800222]\n",
            "Epoch index and hidden dimension and ratio: 6500 1024 1.0342540770385598\n",
            "Epoch index and hidden dimension and ratio: 6500 20 1.0438948278072862\n",
            "Epoch index and hidden dimension and ratio: 6500 20 1.7179502902928065\n",
            "Epoch index and hidden dimension and ratio: 6500 20 4.521086380691259\n",
            "MI(X;T): [10.656939831089813, 7.969740977329886, 5.491827253744569, 3.5625024917011694], MI(Y;T): [2.626724155771684, 3.192932250125441, 3.183753884364787, 2.9394778645662645]\n",
            "Epoch 6502/10000\n",
            "Step 0: Train Loss: 2.8967845899785516e-09, Train Acc: 1.0\n",
            "Epoch 6503/10000\n",
            "Step 0: Train Loss: 2.300738488614229e-09, Train Acc: 1.0\n",
            "Epoch 6504/10000\n",
            "Step 0: Train Loss: 2.7418125547740146e-09, Train Acc: 1.0\n",
            "Epoch 6505/10000\n",
            "Step 0: Train Loss: 1.7285343156814292e-09, Train Acc: 1.0\n",
            "Epoch 6506/10000\n",
            "Step 0: Train Loss: 2.0861621319312462e-09, Train Acc: 1.0\n",
            "Epoch 6507/10000\n",
            "Step 0: Train Loss: 2.2649757625003986e-09, Train Acc: 1.0\n",
            "Epoch 6508/10000\n",
            "Step 0: Train Loss: 2.5272366421802417e-09, Train Acc: 1.0\n",
            "Epoch 6509/10000\n",
            "Step 0: Train Loss: 2.3484219013880647e-09, Train Acc: 1.0\n",
            "Epoch 6510/10000\n",
            "Step 0: Train Loss: 1.6570089744760708e-09, Train Acc: 1.0\n",
            "Epoch 6511/10000\n",
            "Step 0: Train Loss: 2.3484223454772746e-09, Train Acc: 1.0\n",
            "Epoch 6512/10000\n",
            "Step 0: Train Loss: 2.229213036386568e-09, Train Acc: 1.0\n",
            "Epoch 6513/10000\n",
            "Step 0: Train Loss: 2.4080266669557204e-09, Train Acc: 1.0\n",
            "Epoch 6514/10000\n",
            "Step 0: Train Loss: 2.6464450630925285e-09, Train Acc: 1.0\n",
            "Epoch 6515/10000\n",
            "Step 0: Train Loss: 2.0146366797035853e-09, Train Acc: 1.0\n",
            "Epoch 6516/10000\n",
            "Step 0: Train Loss: 2.2053712189773478e-09, Train Acc: 1.0\n",
            "Epoch 6517/10000\n",
            "Step 0: Train Loss: 1.99079464024976e-09, Train Acc: 1.0\n",
            "Epoch 6518/10000\n",
            "Step 0: Train Loss: 2.4676312104787712e-09, Train Acc: 1.0\n",
            "Epoch 6519/10000\n",
            "Step 0: Train Loss: 1.8596644224544434e-09, Train Acc: 1.0\n",
            "Epoch 6520/10000\n",
            "Step 0: Train Loss: 1.8835064619082686e-09, Train Acc: 1.0\n",
            "Epoch 6521/10000\n",
            "Step 0: Train Loss: 2.133845544705082e-09, Train Acc: 1.0\n",
            "Epoch 6522/10000\n",
            "Step 0: Train Loss: 2.300738932703439e-09, Train Acc: 1.0\n",
            "Epoch 6523/10000\n",
            "Step 0: Train Loss: 2.062320092477421e-09, Train Acc: 1.0\n",
            "Epoch 6524/10000\n",
            "Step 0: Train Loss: 2.2768971152942186e-09, Train Acc: 1.0\n",
            "Epoch 6525/10000\n",
            "Step 0: Train Loss: 1.7881389702267825e-09, Train Acc: 1.0\n",
            "Epoch 6526/10000\n",
            "Step 0: Train Loss: 2.2530550758403933e-09, Train Acc: 1.0\n",
            "Epoch 6527/10000\n",
            "Step 0: Train Loss: 1.7404552243860394e-09, Train Acc: 1.0\n",
            "Epoch 6528/10000\n",
            "Step 0: Train Loss: 2.0861621319312462e-09, Train Acc: 1.0\n",
            "Epoch 6529/10000\n",
            "Step 0: Train Loss: 2.2530548537957884e-09, Train Acc: 1.0\n",
            "Epoch 6530/10000\n",
            "Step 0: Train Loss: 1.5020368282492313e-09, Train Acc: 1.0\n",
            "Epoch 6531/10000\n",
            "Step 0: Train Loss: 2.1219248580450767e-09, Train Acc: 1.0\n",
            "Epoch 6532/10000\n",
            "Step 0: Train Loss: 1.907348279317489e-09, Train Acc: 1.0\n",
            "Epoch 6533/10000\n",
            "Step 0: Train Loss: 1.919269188022099e-09, Train Acc: 1.0\n",
            "Epoch 6534/10000\n",
            "Step 0: Train Loss: 1.513957847976144e-09, Train Acc: 1.0\n",
            "Epoch 6535/10000\n",
            "Step 0: Train Loss: 1.6331667129776406e-09, Train Acc: 1.0\n",
            "Epoch 6536/10000\n",
            "Step 0: Train Loss: 1.8715855532036585e-09, Train Acc: 1.0\n",
            "Epoch 6537/10000\n",
            "Step 0: Train Loss: 2.1100041713850715e-09, Train Acc: 1.0\n",
            "Epoch 6538/10000\n",
            "Step 0: Train Loss: 1.8119807876360028e-09, Train Acc: 1.0\n",
            "Epoch 6539/10000\n",
            "Step 0: Train Loss: 1.668929661136076e-09, Train Acc: 1.0\n",
            "Epoch 6540/10000\n",
            "Step 0: Train Loss: 1.919269188022099e-09, Train Acc: 1.0\n",
            "Epoch 6541/10000\n",
            "Step 0: Train Loss: 1.99079464024976e-09, Train Acc: 1.0\n",
            "Epoch 6542/10000\n",
            "Step 0: Train Loss: 1.6212461373399378e-09, Train Acc: 1.0\n",
            "Epoch 6543/10000\n",
            "Step 0: Train Loss: 2.4080266669557204e-09, Train Acc: 1.0\n",
            "Epoch 6544/10000\n",
            "Step 0: Train Loss: 1.7404552243860394e-09, Train Acc: 1.0\n",
            "Epoch 6545/10000\n",
            "Step 0: Train Loss: 2.0980828185912515e-09, Train Acc: 1.0\n",
            "Epoch 6546/10000\n",
            "Step 0: Train Loss: 1.704692498272209e-09, Train Acc: 1.0\n",
            "Epoch 6547/10000\n",
            "Step 0: Train Loss: 1.978873509500545e-09, Train Acc: 1.0\n",
            "Epoch 6548/10000\n",
            "Step 0: Train Loss: 2.467631654567981e-09, Train Acc: 1.0\n",
            "Epoch 6549/10000\n",
            "Step 0: Train Loss: 2.622603245683308e-09, Train Acc: 1.0\n",
            "Epoch 6550/10000\n",
            "Step 0: Train Loss: 2.300738488614229e-09, Train Acc: 1.0\n",
            "Epoch 6551/10000\n",
            "Step 0: Train Loss: 1.478195010840011e-09, Train Acc: 1.0\n",
            "Epoch 6552/10000\n",
            "Step 0: Train Loss: 1.9311900967267093e-09, Train Acc: 1.0\n",
            "Epoch 6553/10000\n",
            "Step 0: Train Loss: 2.0384784971128056e-09, Train Acc: 1.0\n",
            "Epoch 6554/10000\n",
            "Step 0: Train Loss: 1.5735622804768923e-09, Train Acc: 1.0\n",
            "Epoch 6555/10000\n",
            "Step 0: Train Loss: 2.0027153269097653e-09, Train Acc: 1.0\n",
            "Epoch 6556/10000\n",
            "Step 0: Train Loss: 1.513957847976144e-09, Train Acc: 1.0\n",
            "Epoch 6557/10000\n",
            "Step 0: Train Loss: 1.7762180615221723e-09, Train Acc: 1.0\n",
            "Epoch 6558/10000\n",
            "Step 0: Train Loss: 1.8358227160675256e-09, Train Acc: 1.0\n",
            "Epoch 6559/10000\n",
            "Step 0: Train Loss: 1.8715855532036585e-09, Train Acc: 1.0\n",
            "Epoch 6560/10000\n",
            "Step 0: Train Loss: 1.6570087524314658e-09, Train Acc: 1.0\n",
            "Epoch 6561/10000\n",
            "Step 0: Train Loss: 1.8477432917052283e-09, Train Acc: 1.0\n",
            "Epoch 6562/10000\n",
            "Step 0: Train Loss: 1.9311900967267093e-09, Train Acc: 1.0\n",
            "Epoch 6563/10000\n",
            "Step 0: Train Loss: 2.157687584158907e-09, Train Acc: 1.0\n",
            "Epoch 6564/10000\n",
            "Step 0: Train Loss: 1.2755391187724285e-09, Train Acc: 1.0\n",
            "Epoch 6565/10000\n",
            "Step 0: Train Loss: 1.8596645334767459e-09, Train Acc: 1.0\n",
            "Epoch 6566/10000\n",
            "Step 0: Train Loss: 1.6808505698406861e-09, Train Acc: 1.0\n",
            "Epoch 6567/10000\n",
            "Step 0: Train Loss: 2.455710745863371e-09, Train Acc: 1.0\n",
            "Epoch 6568/10000\n",
            "Step 0: Train Loss: 1.5974043199307175e-09, Train Acc: 1.0\n",
            "Epoch 6569/10000\n",
            "Step 0: Train Loss: 1.7762180615221723e-09, Train Acc: 1.0\n",
            "Epoch 6570/10000\n",
            "Step 0: Train Loss: 1.7285342046591268e-09, Train Acc: 1.0\n",
            "Epoch 6571/10000\n",
            "Step 0: Train Loss: 2.1219246360004718e-09, Train Acc: 1.0\n",
            "Epoch 6572/10000\n",
            "Step 0: Train Loss: 1.6927715895675988e-09, Train Acc: 1.0\n",
            "Epoch 6573/10000\n",
            "Step 0: Train Loss: 1.8000598789313926e-09, Train Acc: 1.0\n",
            "Epoch 6574/10000\n",
            "Step 0: Train Loss: 1.5377994433407594e-09, Train Acc: 1.0\n",
            "Epoch 6575/10000\n",
            "Step 0: Train Loss: 1.2993810472039513e-09, Train Acc: 1.0\n",
            "Epoch 6576/10000\n",
            "Step 0: Train Loss: 1.7642971528175622e-09, Train Acc: 1.0\n",
            "Epoch 6577/10000\n",
            "Step 0: Train Loss: 2.1815291795235225e-09, Train Acc: 1.0\n",
            "Epoch 6578/10000\n",
            "Step 0: Train Loss: 1.4543531934307907e-09, Train Acc: 1.0\n",
            "Epoch 6579/10000\n",
            "Step 0: Train Loss: 1.5139577369538415e-09, Train Acc: 1.0\n",
            "Epoch 6580/10000\n",
            "Step 0: Train Loss: 1.9431110054313194e-09, Train Acc: 1.0\n",
            "Epoch 6581/10000\n",
            "Step 0: Train Loss: 1.2874601384993412e-09, Train Acc: 1.0\n",
            "Epoch 6582/10000\n",
            "Step 0: Train Loss: 1.3828276301808273e-09, Train Acc: 1.0\n",
            "Epoch 6583/10000\n",
            "Step 0: Train Loss: 1.5377994433407594e-09, Train Acc: 1.0\n",
            "Epoch 6584/10000\n",
            "Step 0: Train Loss: 1.8477432917052283e-09, Train Acc: 1.0\n",
            "Epoch 6585/10000\n",
            "Step 0: Train Loss: 1.8477436247721357e-09, Train Acc: 1.0\n",
            "Epoch 6586/10000\n",
            "Step 0: Train Loss: 1.895427370612879e-09, Train Acc: 1.0\n",
            "Epoch 6587/10000\n",
            "Step 0: Train Loss: 1.6450878437268557e-09, Train Acc: 1.0\n",
            "Epoch 6588/10000\n",
            "Step 0: Train Loss: 2.0265571443189856e-09, Train Acc: 1.0\n",
            "Epoch 6589/10000\n",
            "Step 0: Train Loss: 1.7881389702267825e-09, Train Acc: 1.0\n",
            "Epoch 6590/10000\n",
            "Step 0: Train Loss: 1.6570087524314658e-09, Train Acc: 1.0\n",
            "Epoch 6591/10000\n",
            "Step 0: Train Loss: 1.6570087524314658e-09, Train Acc: 1.0\n",
            "Epoch 6592/10000\n",
            "Step 0: Train Loss: 1.8119806766137003e-09, Train Acc: 1.0\n",
            "Epoch 6593/10000\n",
            "Step 0: Train Loss: 1.8715855532036585e-09, Train Acc: 1.0\n",
            "Epoch 6594/10000\n",
            "Step 0: Train Loss: 1.3351436622954793e-09, Train Acc: 1.0\n",
            "Epoch 6595/10000\n",
            "Step 0: Train Loss: 1.6331664909330357e-09, Train Acc: 1.0\n",
            "Epoch 6596/10000\n",
            "Step 0: Train Loss: 1.8239018073629154e-09, Train Acc: 1.0\n",
            "Epoch 6597/10000\n",
            "Step 0: Train Loss: 1.978873509500545e-09, Train Acc: 1.0\n",
            "Epoch 6598/10000\n",
            "Step 0: Train Loss: 1.2874601384993412e-09, Train Acc: 1.0\n",
            "Epoch 6599/10000\n",
            "Step 0: Train Loss: 1.3709066104539147e-09, Train Acc: 1.0\n",
            "Epoch 6600/10000\n",
            "Step 0: Train Loss: 1.883506017819059e-09, Train Acc: 1.0\n",
            "Epoch 6601/10000\n",
            "Step 0: Train Loss: 1.990794862294365e-09, Train Acc: 1.0\n",
            "Epoch 6602/10000\n",
            "Step 0: Train Loss: 1.8596642004098385e-09, Train Acc: 1.0\n",
            "Epoch 6603/10000\n",
            "Step 0: Train Loss: 1.7285343156814292e-09, Train Acc: 1.0\n",
            "Epoch 6604/10000\n",
            "Step 0: Train Loss: 1.3589857017493046e-09, Train Acc: 1.0\n",
            "Epoch 6605/10000\n",
            "Step 0: Train Loss: 1.5974040978861126e-09, Train Acc: 1.0\n",
            "Epoch 6606/10000\n",
            "Step 0: Train Loss: 1.8239018073629154e-09, Train Acc: 1.0\n",
            "Epoch 6607/10000\n",
            "Step 0: Train Loss: 1.788138637159875e-09, Train Acc: 1.0\n",
            "Epoch 6608/10000\n",
            "Step 0: Train Loss: 1.3828275191585249e-09, Train Acc: 1.0\n",
            "Epoch 6609/10000\n",
            "Step 0: Train Loss: 1.6331669350222455e-09, Train Acc: 1.0\n",
            "Epoch 6610/10000\n",
            "Step 0: Train Loss: 1.8477436247721357e-09, Train Acc: 1.0\n",
            "Epoch 6611/10000\n",
            "Step 0: Train Loss: 1.4901159195446212e-09, Train Acc: 1.0\n",
            "Epoch 6612/10000\n",
            "Step 0: Train Loss: 1.7285343156814292e-09, Train Acc: 1.0\n",
            "Epoch 6613/10000\n",
            "Step 0: Train Loss: 1.5854829671368975e-09, Train Acc: 1.0\n",
            "Epoch 6614/10000\n",
            "Step 0: Train Loss: 1.5258787566807541e-09, Train Acc: 1.0\n",
            "Epoch 6615/10000\n",
            "Step 0: Train Loss: 1.8596644224544434e-09, Train Acc: 1.0\n",
            "Epoch 6616/10000\n",
            "Step 0: Train Loss: 1.8000598789313926e-09, Train Acc: 1.0\n",
            "Epoch 6617/10000\n",
            "Step 0: Train Loss: 1.97887373154515e-09, Train Acc: 1.0\n",
            "Epoch 6618/10000\n",
            "Step 0: Train Loss: 1.2159345752493778e-09, Train Acc: 1.0\n",
            "Epoch 6619/10000\n",
            "Step 0: Train Loss: 1.4543530824084883e-09, Train Acc: 1.0\n",
            "Epoch 6620/10000\n",
            "Step 0: Train Loss: 1.5139575149092366e-09, Train Acc: 1.0\n",
            "Epoch 6621/10000\n",
            "Step 0: Train Loss: 1.3470647930446944e-09, Train Acc: 1.0\n",
            "Epoch 6622/10000\n",
            "Step 0: Train Loss: 1.5497203520453695e-09, Train Acc: 1.0\n",
            "Epoch 6623/10000\n",
            "Step 0: Train Loss: 1.4185901342500529e-09, Train Acc: 1.0\n",
            "Epoch 6624/10000\n",
            "Step 0: Train Loss: 1.323222975635474e-09, Train Acc: 1.0\n",
            "Epoch 6625/10000\n",
            "Step 0: Train Loss: 1.4901159195446212e-09, Train Acc: 1.0\n",
            "Epoch 6626/10000\n",
            "Step 0: Train Loss: 1.6450878437268557e-09, Train Acc: 1.0\n",
            "Epoch 6627/10000\n",
            "Step 0: Train Loss: 1.3470647930446944e-09, Train Acc: 1.0\n",
            "Epoch 6628/10000\n",
            "Step 0: Train Loss: 1.5258787566807541e-09, Train Acc: 1.0\n",
            "Epoch 6629/10000\n",
            "Step 0: Train Loss: 1.5854834112261074e-09, Train Acc: 1.0\n",
            "Epoch 6630/10000\n",
            "Step 0: Train Loss: 1.3828276301808273e-09, Train Acc: 1.0\n",
            "Epoch 6631/10000\n",
            "Step 0: Train Loss: 1.716613406976819e-09, Train Acc: 1.0\n",
            "Epoch 6632/10000\n",
            "Step 0: Train Loss: 1.3828276301808273e-09, Train Acc: 1.0\n",
            "Epoch 6633/10000\n",
            "Step 0: Train Loss: 1.5258785346361492e-09, Train Acc: 1.0\n",
            "Epoch 6634/10000\n",
            "Step 0: Train Loss: 1.2636183210901208e-09, Train Acc: 1.0\n",
            "Epoch 6635/10000\n",
            "Step 0: Train Loss: 1.466273880090796e-09, Train Acc: 1.0\n",
            "Epoch 6636/10000\n",
            "Step 0: Train Loss: 1.788138637159875e-09, Train Acc: 1.0\n",
            "Epoch 6637/10000\n",
            "Step 0: Train Loss: 1.9192687439328893e-09, Train Acc: 1.0\n",
            "Epoch 6638/10000\n",
            "Step 0: Train Loss: 1.4781947887954061e-09, Train Acc: 1.0\n",
            "Epoch 6639/10000\n",
            "Step 0: Train Loss: 2.074241001182031e-09, Train Acc: 1.0\n",
            "Epoch 6640/10000\n",
            "Step 0: Train Loss: 1.311301844886259e-09, Train Acc: 1.0\n",
            "Epoch 6641/10000\n",
            "Step 0: Train Loss: 1.2278553729316855e-09, Train Acc: 1.0\n",
            "Epoch 6642/10000\n",
            "Step 0: Train Loss: 1.502036495182324e-09, Train Acc: 1.0\n",
            "Epoch 6643/10000\n",
            "Step 0: Train Loss: 1.430511264999268e-09, Train Acc: 1.0\n",
            "Epoch 6644/10000\n",
            "Step 0: Train Loss: 1.8358223830006182e-09, Train Acc: 1.0\n",
            "Epoch 6645/10000\n",
            "Step 0: Train Loss: 1.4185901342500529e-09, Train Acc: 1.0\n",
            "Epoch 6646/10000\n",
            "Step 0: Train Loss: 9.775159570679648e-10, Train Acc: 1.0\n",
            "Epoch 6647/10000\n",
            "Step 0: Train Loss: 1.5258785346361492e-09, Train Acc: 1.0\n",
            "Epoch 6648/10000\n",
            "Step 0: Train Loss: 1.5974040978861126e-09, Train Acc: 1.0\n",
            "Epoch 6649/10000\n",
            "Step 0: Train Loss: 1.4543531934307907e-09, Train Acc: 1.0\n",
            "Epoch 6650/10000\n",
            "Step 0: Train Loss: 1.2040136665447676e-09, Train Acc: 1.0\n",
            "Epoch 6651/10000\n",
            "Step 0: Train Loss: 1.275539229794731e-09, Train Acc: 1.0\n",
            "Epoch 6652/10000\n",
            "Step 0: Train Loss: 1.4662739911130984e-09, Train Acc: 1.0\n",
            "Epoch 6653/10000\n",
            "Step 0: Train Loss: 1.6331667129776406e-09, Train Acc: 1.0\n",
            "Epoch 6654/10000\n",
            "Step 0: Train Loss: 1.7642971528175622e-09, Train Acc: 1.0\n",
            "Epoch 6655/10000\n",
            "Step 0: Train Loss: 1.6570087524314658e-09, Train Acc: 1.0\n",
            "Epoch 6656/10000\n",
            "Step 0: Train Loss: 1.1563299207040245e-09, Train Acc: 1.0\n",
            "Epoch 6657/10000\n",
            "Step 0: Train Loss: 1.3709066104539147e-09, Train Acc: 1.0\n",
            "Epoch 6658/10000\n",
            "Step 0: Train Loss: 1.740454891319132e-09, Train Acc: 1.0\n",
            "Epoch 6659/10000\n",
            "Step 0: Train Loss: 1.2874601384993412e-09, Train Acc: 1.0\n",
            "Epoch 6660/10000\n",
            "Step 0: Train Loss: 1.6093250065907228e-09, Train Acc: 1.0\n",
            "Epoch 6661/10000\n",
            "Step 0: Train Loss: 1.4424321737038781e-09, Train Acc: 1.0\n",
            "Epoch 6662/10000\n",
            "Step 0: Train Loss: 1.6927713675229938e-09, Train Acc: 1.0\n",
            "Epoch 6663/10000\n",
            "Step 0: Train Loss: 1.5020367172269289e-09, Train Acc: 1.0\n",
            "Epoch 6664/10000\n",
            "Step 0: Train Loss: 1.6927715895675988e-09, Train Acc: 1.0\n",
            "Epoch 6665/10000\n",
            "Step 0: Train Loss: 1.5735622804768923e-09, Train Acc: 1.0\n",
            "Epoch 6666/10000\n",
            "Step 0: Train Loss: 1.0132789052264002e-09, Train Acc: 1.0\n",
            "Epoch 6667/10000\n",
            "Step 0: Train Loss: 1.4901158085223187e-09, Train Acc: 1.0\n",
            "Epoch 6668/10000\n",
            "Step 0: Train Loss: 1.5974040978861126e-09, Train Acc: 1.0\n",
            "Epoch 6669/10000\n",
            "Step 0: Train Loss: 1.156329809681722e-09, Train Acc: 1.0\n",
            "Epoch 6670/10000\n",
            "Step 0: Train Loss: 1.1682507183863322e-09, Train Acc: 1.0\n",
            "Epoch 6671/10000\n",
            "Step 0: Train Loss: 1.3828276301808273e-09, Train Acc: 1.0\n",
            "Epoch 6672/10000\n",
            "Step 0: Train Loss: 1.4901155864777138e-09, Train Acc: 1.0\n",
            "Epoch 6673/10000\n",
            "Step 0: Train Loss: 1.2874600274770387e-09, Train Acc: 1.0\n",
            "Epoch 6674/10000\n",
            "Step 0: Train Loss: 1.227855483953988e-09, Train Acc: 1.0\n",
            "Epoch 6675/10000\n",
            "Step 0: Train Loss: 1.788138637159875e-09, Train Acc: 1.0\n",
            "Epoch 6676/10000\n",
            "Step 0: Train Loss: 1.4185903562946578e-09, Train Acc: 1.0\n",
            "Epoch 6677/10000\n",
            "Step 0: Train Loss: 1.0490415203179282e-09, Train Acc: 1.0\n",
            "Epoch 6678/10000\n",
            "Step 0: Train Loss: 1.5497203520453695e-09, Train Acc: 1.0\n",
            "Epoch 6679/10000\n",
            "Step 0: Train Loss: 1.6927715895675988e-09, Train Acc: 1.0\n",
            "Epoch 6680/10000\n",
            "Step 0: Train Loss: 1.6093250065907228e-09, Train Acc: 1.0\n",
            "Epoch 6681/10000\n",
            "Step 0: Train Loss: 1.3232225315462642e-09, Train Acc: 1.0\n",
            "Epoch 6682/10000\n",
            "Step 0: Train Loss: 1.1324881032948042e-09, Train Acc: 1.0\n",
            "Epoch 6683/10000\n",
            "Step 0: Train Loss: 1.358985590727002e-09, Train Acc: 1.0\n",
            "Epoch 6684/10000\n",
            "Step 0: Train Loss: 1.1563299207040245e-09, Train Acc: 1.0\n",
            "Epoch 6685/10000\n",
            "Step 0: Train Loss: 1.3709064994316122e-09, Train Acc: 1.0\n",
            "Epoch 6686/10000\n",
            "Step 0: Train Loss: 1.5377994433407594e-09, Train Acc: 1.0\n",
            "Epoch 6687/10000\n",
            "Step 0: Train Loss: 1.1801717381132448e-09, Train Acc: 1.0\n",
            "Epoch 6688/10000\n",
            "Step 0: Train Loss: 1.5735622804768923e-09, Train Acc: 1.0\n",
            "Epoch 6689/10000\n",
            "Step 0: Train Loss: 1.6927715895675988e-09, Train Acc: 1.0\n",
            "Epoch 6690/10000\n",
            "Step 0: Train Loss: 9.894369767948774e-10, Train Acc: 1.0\n",
            "Epoch 6691/10000\n",
            "Step 0: Train Loss: 1.1682508294086347e-09, Train Acc: 1.0\n",
            "Epoch 6692/10000\n",
            "Step 0: Train Loss: 1.8239018073629154e-09, Train Acc: 1.0\n",
            "Epoch 6693/10000\n",
            "Step 0: Train Loss: 1.192092646817855e-09, Train Acc: 1.0\n",
            "Epoch 6694/10000\n",
            "Step 0: Train Loss: 1.1801717381132448e-09, Train Acc: 1.0\n",
            "Epoch 6695/10000\n",
            "Step 0: Train Loss: 1.5139574038869341e-09, Train Acc: 1.0\n",
            "Epoch 6696/10000\n",
            "Step 0: Train Loss: 1.3947485388854375e-09, Train Acc: 1.0\n",
            "Epoch 6697/10000\n",
            "Step 0: Train Loss: 1.3351436622954793e-09, Train Acc: 1.0\n",
            "Epoch 6698/10000\n",
            "Step 0: Train Loss: 1.0132787942040977e-09, Train Acc: 1.0\n",
            "Epoch 6699/10000\n",
            "Step 0: Train Loss: 1.1682508294086347e-09, Train Acc: 1.0\n",
            "Epoch 6700/10000\n",
            "Step 0: Train Loss: 1.2636183210901208e-09, Train Acc: 1.0\n",
            "Epoch 6701/10000\n",
            "Step 0: Train Loss: 1.2516973013632082e-09, Train Acc: 1.0\n",
            "Epoch 6702/10000\n",
            "Step 0: Train Loss: 1.6212460263176354e-09, Train Acc: 1.0\n",
            "Epoch 6703/10000\n",
            "Step 0: Train Loss: 1.1682508294086347e-09, Train Acc: 1.0\n",
            "Epoch 6704/10000\n",
            "Step 0: Train Loss: 1.0251997029087079e-09, Train Acc: 1.0\n",
            "Epoch 6705/10000\n",
            "Step 0: Train Loss: 1.2516971903409058e-09, Train Acc: 1.0\n",
            "Epoch 6706/10000\n",
            "Step 0: Train Loss: 1.716613406976819e-09, Train Acc: 1.0\n",
            "Epoch 6707/10000\n",
            "Step 0: Train Loss: 1.0848043574540611e-09, Train Acc: 1.0\n",
            "Epoch 6708/10000\n",
            "Step 0: Train Loss: 1.1563299207040245e-09, Train Acc: 1.0\n",
            "Epoch 6709/10000\n",
            "Step 0: Train Loss: 1.3947485388854375e-09, Train Acc: 1.0\n",
            "Epoch 6710/10000\n",
            "Step 0: Train Loss: 1.1563299207040245e-09, Train Acc: 1.0\n",
            "Epoch 6711/10000\n",
            "Step 0: Train Loss: 1.275539229794731e-09, Train Acc: 1.0\n",
            "Epoch 6712/10000\n",
            "Step 0: Train Loss: 1.0967252661586713e-09, Train Acc: 1.0\n",
            "Epoch 6713/10000\n",
            "Step 0: Train Loss: 1.2397762816362956e-09, Train Acc: 1.0\n",
            "Epoch 6714/10000\n",
            "Step 0: Train Loss: 1.5258785346361492e-09, Train Acc: 1.0\n",
            "Epoch 6715/10000\n",
            "Step 0: Train Loss: 1.2040136665447676e-09, Train Acc: 1.0\n",
            "Epoch 6716/10000\n",
            "Step 0: Train Loss: 1.1801717381132448e-09, Train Acc: 1.0\n",
            "Epoch 6717/10000\n",
            "Step 0: Train Loss: 9.775159570679648e-10, Train Acc: 1.0\n",
            "Epoch 6718/10000\n",
            "Step 0: Train Loss: 1.5139575149092366e-09, Train Acc: 1.0\n",
            "Epoch 6719/10000\n",
            "Step 0: Train Loss: 1.3351437733177818e-09, Train Acc: 1.0\n",
            "Epoch 6720/10000\n",
            "Step 0: Train Loss: 1.1086461748632814e-09, Train Acc: 1.0\n",
            "Epoch 6721/10000\n",
            "Step 0: Train Loss: 1.3828274081362224e-09, Train Acc: 1.0\n",
            "Epoch 6722/10000\n",
            "Step 0: Train Loss: 1.4781948998177086e-09, Train Acc: 1.0\n",
            "Epoch 6723/10000\n",
            "Step 0: Train Loss: 1.4066692255454427e-09, Train Acc: 1.0\n",
            "Epoch 6724/10000\n",
            "Step 0: Train Loss: 9.059903938180014e-10, Train Acc: 1.0\n",
            "Epoch 6725/10000\n",
            "Step 0: Train Loss: 1.3709066104539147e-09, Train Acc: 1.0\n",
            "Epoch 6726/10000\n",
            "Step 0: Train Loss: 1.4066694475900476e-09, Train Acc: 1.0\n",
            "Epoch 6727/10000\n",
            "Step 0: Train Loss: 9.894369767948774e-10, Train Acc: 1.0\n",
            "Epoch 6728/10000\n",
            "Step 0: Train Loss: 1.5258785346361492e-09, Train Acc: 1.0\n",
            "Epoch 6729/10000\n",
            "Step 0: Train Loss: 1.1801717381132448e-09, Train Acc: 1.0\n",
            "Epoch 6730/10000\n",
            "Step 0: Train Loss: 1.3709066104539147e-09, Train Acc: 1.0\n",
            "Epoch 6731/10000\n",
            "Step 0: Train Loss: 1.1324881032948042e-09, Train Acc: 1.0\n",
            "Epoch 6732/10000\n",
            "Step 0: Train Loss: 1.1801717381132448e-09, Train Acc: 1.0\n",
            "Epoch 6733/10000\n",
            "Step 0: Train Loss: 1.2874600274770387e-09, Train Acc: 1.0\n",
            "Epoch 6734/10000\n",
            "Step 0: Train Loss: 1.5258786456584517e-09, Train Acc: 1.0\n",
            "Epoch 6735/10000\n",
            "Step 0: Train Loss: 1.0609625400448408e-09, Train Acc: 1.0\n",
            "Epoch 6736/10000\n",
            "Step 0: Train Loss: 1.3351438843400842e-09, Train Acc: 1.0\n",
            "Epoch 6737/10000\n",
            "Step 0: Train Loss: 1.275539229794731e-09, Train Acc: 1.0\n",
            "Epoch 6738/10000\n",
            "Step 0: Train Loss: 1.2278553729316855e-09, Train Acc: 1.0\n",
            "Epoch 6739/10000\n",
            "Step 0: Train Loss: 1.239776392658598e-09, Train Acc: 1.0\n",
            "Epoch 6740/10000\n",
            "Step 0: Train Loss: 1.239776392658598e-09, Train Acc: 1.0\n",
            "Epoch 6741/10000\n",
            "Step 0: Train Loss: 1.120567194590194e-09, Train Acc: 1.0\n",
            "Epoch 6742/10000\n",
            "Step 0: Train Loss: 1.156329809681722e-09, Train Acc: 1.0\n",
            "Epoch 6743/10000\n",
            "Step 0: Train Loss: 1.5258785346361492e-09, Train Acc: 1.0\n",
            "Epoch 6744/10000\n",
            "Step 0: Train Loss: 1.2040135555224651e-09, Train Acc: 1.0\n",
            "Epoch 6745/10000\n",
            "Step 0: Train Loss: 1.358985590727002e-09, Train Acc: 1.0\n",
            "Epoch 6746/10000\n",
            "Step 0: Train Loss: 9.894369767948774e-10, Train Acc: 1.0\n",
            "Epoch 6747/10000\n",
            "Step 0: Train Loss: 1.2636182100678184e-09, Train Acc: 1.0\n",
            "Epoch 6748/10000\n",
            "Step 0: Train Loss: 1.227855483953988e-09, Train Acc: 1.0\n",
            "Epoch 6749/10000\n",
            "Step 0: Train Loss: 9.655950483633546e-10, Train Acc: 1.0\n",
            "Epoch 6750/10000\n",
            "Step 0: Train Loss: 1.4305111539769655e-09, Train Acc: 1.0\n",
            "Epoch 6751/10000\n",
            "Step 0: Train Loss: 1.2516974123855107e-09, Train Acc: 1.0\n",
            "Epoch 6752/10000\n",
            "Step 0: Train Loss: 1.4901159195446212e-09, Train Acc: 1.0\n",
            "Epoch 6753/10000\n",
            "Step 0: Train Loss: 1.1086461748632814e-09, Train Acc: 1.0\n",
            "Epoch 6754/10000\n",
            "Step 0: Train Loss: 1.6808505698406861e-09, Train Acc: 1.0\n",
            "Epoch 6755/10000\n",
            "Step 0: Train Loss: 9.179113580337628e-10, Train Acc: 1.0\n",
            "Epoch 6756/10000\n",
            "Step 0: Train Loss: 1.3828276301808273e-09, Train Acc: 1.0\n",
            "Epoch 6757/10000\n",
            "Step 0: Train Loss: 1.1444090119994144e-09, Train Acc: 1.0\n",
            "Epoch 6758/10000\n",
            "Step 0: Train Loss: 1.2874599164547362e-09, Train Acc: 1.0\n",
            "Epoch 6759/10000\n",
            "Step 0: Train Loss: 1.3947480947962276e-09, Train Acc: 1.0\n",
            "Epoch 6760/10000\n",
            "Step 0: Train Loss: 1.323222975635474e-09, Train Acc: 1.0\n",
            "Epoch 6761/10000\n",
            "Step 0: Train Loss: 1.1324881032948042e-09, Train Acc: 1.0\n",
            "Epoch 6762/10000\n",
            "Step 0: Train Loss: 1.3232227535908692e-09, Train Acc: 1.0\n",
            "Epoch 6763/10000\n",
            "Step 0: Train Loss: 1.0013578854994876e-09, Train Acc: 1.0\n",
            "Epoch 6764/10000\n",
            "Step 0: Train Loss: 7.15255632499634e-10, Train Acc: 1.0\n",
            "Epoch 6765/10000\n",
            "Step 0: Train Loss: 1.358985590727002e-09, Train Acc: 1.0\n",
            "Epoch 6766/10000\n",
            "Step 0: Train Loss: 1.1444090119994144e-09, Train Acc: 1.0\n",
            "Epoch 6767/10000\n",
            "Step 0: Train Loss: 1.2040136665447676e-09, Train Acc: 1.0\n",
            "Epoch 6768/10000\n",
            "Step 0: Train Loss: 1.0132787942040977e-09, Train Acc: 1.0\n",
            "Epoch 6769/10000\n",
            "Step 0: Train Loss: 1.1086461748632814e-09, Train Acc: 1.0\n",
            "Epoch 6770/10000\n",
            "Step 0: Train Loss: 1.2874600274770387e-09, Train Acc: 1.0\n",
            "Epoch 6771/10000\n",
            "Step 0: Train Loss: 1.1324881032948042e-09, Train Acc: 1.0\n",
            "Epoch 6772/10000\n",
            "Step 0: Train Loss: 1.0967252661586713e-09, Train Acc: 1.0\n",
            "Epoch 6773/10000\n",
            "Step 0: Train Loss: 9.894369767948774e-10, Train Acc: 1.0\n",
            "Epoch 6774/10000\n",
            "Step 0: Train Loss: 1.1444090119994144e-09, Train Acc: 1.0\n",
            "Epoch 6775/10000\n",
            "Step 0: Train Loss: 1.4424321737038781e-09, Train Acc: 1.0\n",
            "Epoch 6776/10000\n",
            "Step 0: Train Loss: 9.536741396587445e-10, Train Acc: 1.0\n",
            "Epoch 6777/10000\n",
            "Step 0: Train Loss: 1.239776392658598e-09, Train Acc: 1.0\n",
            "Epoch 6778/10000\n",
            "Step 0: Train Loss: 1.2755391187724285e-09, Train Acc: 1.0\n",
            "Epoch 6779/10000\n",
            "Step 0: Train Loss: 1.0967251551363688e-09, Train Acc: 1.0\n",
            "Epoch 6780/10000\n",
            "Step 0: Train Loss: 1.0848042464317587e-09, Train Acc: 1.0\n",
            "Epoch 6781/10000\n",
            "Step 0: Train Loss: 9.894369767948774e-10, Train Acc: 1.0\n",
            "Epoch 6782/10000\n",
            "Step 0: Train Loss: 1.1920925357955525e-09, Train Acc: 1.0\n",
            "Epoch 6783/10000\n",
            "Step 0: Train Loss: 1.311301844886259e-09, Train Acc: 1.0\n",
            "Epoch 6784/10000\n",
            "Step 0: Train Loss: 1.1920927578401574e-09, Train Acc: 1.0\n",
            "Epoch 6785/10000\n",
            "Step 0: Train Loss: 9.536739176141396e-10, Train Acc: 1.0\n",
            "Epoch 6786/10000\n",
            "Step 0: Train Loss: 9.775159570679648e-10, Train Acc: 1.0\n",
            "Epoch 6787/10000\n",
            "Step 0: Train Loss: 1.1205670835678916e-09, Train Acc: 1.0\n",
            "Epoch 6788/10000\n",
            "Step 0: Train Loss: 9.655950483633546e-10, Train Acc: 1.0\n",
            "Epoch 6789/10000\n",
            "Step 0: Train Loss: 9.179113580337628e-10, Train Acc: 1.0\n",
            "Epoch 6790/10000\n",
            "Step 0: Train Loss: 1.1324881032948042e-09, Train Acc: 1.0\n",
            "Epoch 6791/10000\n",
            "Step 0: Train Loss: 1.3113019559085615e-09, Train Acc: 1.0\n",
            "Epoch 6792/10000\n",
            "Step 0: Train Loss: 7.629393783403771e-10, Train Acc: 1.0\n",
            "Epoch 6793/10000\n",
            "Step 0: Train Loss: 1.3351438843400842e-09, Train Acc: 1.0\n",
            "Epoch 6794/10000\n",
            "Step 0: Train Loss: 1.4901158085223187e-09, Train Acc: 1.0\n",
            "Epoch 6795/10000\n",
            "Step 0: Train Loss: 1.0609625400448408e-09, Train Acc: 1.0\n",
            "Epoch 6796/10000\n",
            "Step 0: Train Loss: 1.3351438843400842e-09, Train Acc: 1.0\n",
            "Epoch 6797/10000\n",
            "Step 0: Train Loss: 1.3470645710000895e-09, Train Acc: 1.0\n",
            "Epoch 6798/10000\n",
            "Step 0: Train Loss: 1.1682508294086347e-09, Train Acc: 1.0\n",
            "Epoch 6799/10000\n",
            "Step 0: Train Loss: 1.4066692255454427e-09, Train Acc: 1.0\n",
            "Epoch 6800/10000\n",
            "Step 0: Train Loss: 1.3470645710000895e-09, Train Acc: 1.0\n",
            "Epoch 6801/10000\n",
            "Step 0: Train Loss: 1.2159345752493778e-09, Train Acc: 1.0\n",
            "Epoch 6802/10000\n",
            "Step 0: Train Loss: 1.1086461748632814e-09, Train Acc: 1.0\n",
            "Epoch 6803/10000\n",
            "Step 0: Train Loss: 1.3232228646131716e-09, Train Acc: 1.0\n",
            "Epoch 6804/10000\n",
            "Step 0: Train Loss: 1.037120611613318e-09, Train Acc: 1.0\n",
            "Epoch 6805/10000\n",
            "Step 0: Train Loss: 1.0848043574540611e-09, Train Acc: 1.0\n",
            "Epoch 6806/10000\n",
            "Step 0: Train Loss: 1.1920927578401574e-09, Train Acc: 1.0\n",
            "Epoch 6807/10000\n",
            "Step 0: Train Loss: 1.3351436622954793e-09, Train Acc: 1.0\n",
            "Epoch 6808/10000\n",
            "Step 0: Train Loss: 1.120567194590194e-09, Train Acc: 1.0\n",
            "Epoch 6809/10000\n",
            "Step 0: Train Loss: 1.6093250065907228e-09, Train Acc: 1.0\n",
            "Epoch 6810/10000\n",
            "Step 0: Train Loss: 1.2993810472039513e-09, Train Acc: 1.0\n",
            "Epoch 6811/10000\n",
            "Step 0: Train Loss: 1.3351435512731769e-09, Train Acc: 1.0\n",
            "Epoch 6812/10000\n",
            "Step 0: Train Loss: 1.2040136665447676e-09, Train Acc: 1.0\n",
            "Epoch 6813/10000\n",
            "Step 0: Train Loss: 1.4066694475900476e-09, Train Acc: 1.0\n",
            "Epoch 6814/10000\n",
            "Step 0: Train Loss: 1.0013578854994876e-09, Train Acc: 1.0\n",
            "Epoch 6815/10000\n",
            "Step 0: Train Loss: 1.0848041354094562e-09, Train Acc: 1.0\n",
            "Epoch 6816/10000\n",
            "Step 0: Train Loss: 1.2993808251593464e-09, Train Acc: 1.0\n",
            "Epoch 6817/10000\n",
            "Step 0: Train Loss: 1.5854831891815024e-09, Train Acc: 1.0\n",
            "Epoch 6818/10000\n",
            "Step 0: Train Loss: 9.655950483633546e-10, Train Acc: 1.0\n",
            "Epoch 6819/10000\n",
            "Step 0: Train Loss: 1.0609625400448408e-09, Train Acc: 1.0\n",
            "Epoch 6820/10000\n",
            "Step 0: Train Loss: 1.1801717381132448e-09, Train Acc: 1.0\n",
            "Epoch 6821/10000\n",
            "Step 0: Train Loss: 1.0609624290225383e-09, Train Acc: 1.0\n",
            "Epoch 6822/10000\n",
            "Step 0: Train Loss: 1.0609625400448408e-09, Train Acc: 1.0\n",
            "Epoch 6823/10000\n",
            "Step 0: Train Loss: 1.037120611613318e-09, Train Acc: 1.0\n",
            "Epoch 6824/10000\n",
            "Step 0: Train Loss: 1.1801717381132448e-09, Train Acc: 1.0\n",
            "Epoch 6825/10000\n",
            "Step 0: Train Loss: 9.775159570679648e-10, Train Acc: 1.0\n",
            "Epoch 6826/10000\n",
            "Step 0: Train Loss: 1.120567194590194e-09, Train Acc: 1.0\n",
            "Epoch 6827/10000\n",
            "Step 0: Train Loss: 9.536741396587445e-10, Train Acc: 1.0\n",
            "Epoch 6828/10000\n",
            "Step 0: Train Loss: 1.227855483953988e-09, Train Acc: 1.0\n",
            "Epoch 6829/10000\n",
            "Step 0: Train Loss: 1.0848042464317587e-09, Train Acc: 1.0\n",
            "Epoch 6830/10000\n",
            "Step 0: Train Loss: 1.0609625400448408e-09, Train Acc: 1.0\n",
            "Epoch 6831/10000\n",
            "Step 0: Train Loss: 1.4185901342500529e-09, Train Acc: 1.0\n",
            "Epoch 6832/10000\n",
            "Step 0: Train Loss: 1.227855483953988e-09, Train Acc: 1.0\n",
            "Epoch 6833/10000\n",
            "Step 0: Train Loss: 1.1920925357955525e-09, Train Acc: 1.0\n",
            "Epoch 6834/10000\n",
            "Step 0: Train Loss: 1.5616411497276772e-09, Train Acc: 1.0\n",
            "Epoch 6835/10000\n",
            "Step 0: Train Loss: 1.2159344642270753e-09, Train Acc: 1.0\n",
            "Epoch 6836/10000\n",
            "Step 0: Train Loss: 1.2397762816362956e-09, Train Acc: 1.0\n",
            "Epoch 6837/10000\n",
            "Step 0: Train Loss: 1.6570087524314658e-09, Train Acc: 1.0\n",
            "Epoch 6838/10000\n",
            "Step 0: Train Loss: 9.775159570679648e-10, Train Acc: 1.0\n",
            "Epoch 6839/10000\n",
            "Step 0: Train Loss: 1.0848043574540611e-09, Train Acc: 1.0\n",
            "Epoch 6840/10000\n",
            "Step 0: Train Loss: 1.2755388967278236e-09, Train Acc: 1.0\n",
            "Epoch 6841/10000\n",
            "Step 0: Train Loss: 1.7166131849322142e-09, Train Acc: 1.0\n",
            "Epoch 6842/10000\n",
            "Step 0: Train Loss: 1.2040136665447676e-09, Train Acc: 1.0\n",
            "Epoch 6843/10000\n",
            "Step 0: Train Loss: 1.5616413717722821e-09, Train Acc: 1.0\n",
            "Epoch 6844/10000\n",
            "Step 0: Train Loss: 1.0967252661586713e-09, Train Acc: 1.0\n",
            "Epoch 6845/10000\n",
            "Step 0: Train Loss: 1.1324881032948042e-09, Train Acc: 1.0\n",
            "Epoch 6846/10000\n",
            "Step 0: Train Loss: 1.1801717381132448e-09, Train Acc: 1.0\n",
            "Epoch 6847/10000\n",
            "Step 0: Train Loss: 1.227855483953988e-09, Train Acc: 1.0\n",
            "Epoch 6848/10000\n",
            "Step 0: Train Loss: 1.1324881032948042e-09, Train Acc: 1.0\n",
            "Epoch 6849/10000\n",
            "Step 0: Train Loss: 9.894369767948774e-10, Train Acc: 1.0\n",
            "Epoch 6850/10000\n",
            "Step 0: Train Loss: 1.3470645710000895e-09, Train Acc: 1.0\n",
            "Epoch 6851/10000\n",
            "Step 0: Train Loss: 1.1920927578401574e-09, Train Acc: 1.0\n",
            "Epoch 6852/10000\n",
            "Step 0: Train Loss: 1.1444090119994144e-09, Train Acc: 1.0\n",
            "Epoch 6853/10000\n",
            "Step 0: Train Loss: 1.2516974123855107e-09, Train Acc: 1.0\n",
            "Epoch 6854/10000\n",
            "Step 0: Train Loss: 1.311301844886259e-09, Train Acc: 1.0\n",
            "Epoch 6855/10000\n",
            "Step 0: Train Loss: 1.0967251551363688e-09, Train Acc: 1.0\n",
            "Epoch 6856/10000\n",
            "Step 0: Train Loss: 1.2040136665447676e-09, Train Acc: 1.0\n",
            "Epoch 6857/10000\n",
            "Step 0: Train Loss: 1.3947483168408326e-09, Train Acc: 1.0\n",
            "Epoch 6858/10000\n",
            "Step 0: Train Loss: 1.2874601384993412e-09, Train Acc: 1.0\n",
            "Epoch 6859/10000\n",
            "Step 0: Train Loss: 1.239776392658598e-09, Train Acc: 1.0\n",
            "Epoch 6860/10000\n",
            "Step 0: Train Loss: 1.0132787942040977e-09, Train Acc: 1.0\n",
            "Epoch 6861/10000\n",
            "Step 0: Train Loss: 1.227855483953988e-09, Train Acc: 1.0\n",
            "Epoch 6862/10000\n",
            "Step 0: Train Loss: 8.225439218634278e-10, Train Acc: 1.0\n",
            "Epoch 6863/10000\n",
            "Step 0: Train Loss: 1.0132787942040977e-09, Train Acc: 1.0\n",
            "Epoch 6864/10000\n",
            "Step 0: Train Loss: 1.275539229794731e-09, Train Acc: 1.0\n",
            "Epoch 6865/10000\n",
            "Step 0: Train Loss: 1.0728833377271485e-09, Train Acc: 1.0\n",
            "Epoch 6866/10000\n",
            "Step 0: Train Loss: 1.1444090119994144e-09, Train Acc: 1.0\n",
            "Epoch 6867/10000\n",
            "Step 0: Train Loss: 9.059903938180014e-10, Train Acc: 1.0\n",
            "Epoch 6868/10000\n",
            "Step 0: Train Loss: 1.1205669725455891e-09, Train Acc: 1.0\n",
            "Epoch 6869/10000\n",
            "Step 0: Train Loss: 1.4066692255454427e-09, Train Acc: 1.0\n",
            "Epoch 6870/10000\n",
            "Step 0: Train Loss: 1.1682508294086347e-09, Train Acc: 1.0\n",
            "Epoch 6871/10000\n",
            "Step 0: Train Loss: 1.0251997029087079e-09, Train Acc: 1.0\n",
            "Epoch 6872/10000\n",
            "Step 0: Train Loss: 1.072883448749451e-09, Train Acc: 1.0\n",
            "Epoch 6873/10000\n",
            "Step 0: Train Loss: 1.0132787942040977e-09, Train Acc: 1.0\n",
            "Epoch 6874/10000\n",
            "Step 0: Train Loss: 9.775159570679648e-10, Train Acc: 1.0\n",
            "Epoch 6875/10000\n",
            "Step 0: Train Loss: 1.6689295501137735e-09, Train Acc: 1.0\n",
            "Epoch 6876/10000\n",
            "Step 0: Train Loss: 1.3470647930446944e-09, Train Acc: 1.0\n",
            "Epoch 6877/10000\n",
            "Step 0: Train Loss: 1.239776392658598e-09, Train Acc: 1.0\n",
            "Epoch 6878/10000\n",
            "Step 0: Train Loss: 1.4781946777731036e-09, Train Acc: 1.0\n",
            "Epoch 6879/10000\n",
            "Step 0: Train Loss: 1.0371205005910156e-09, Train Acc: 1.0\n",
            "Epoch 6880/10000\n",
            "Step 0: Train Loss: 1.037120611613318e-09, Train Acc: 1.0\n",
            "Epoch 6881/10000\n",
            "Step 0: Train Loss: 1.1324881032948042e-09, Train Acc: 1.0\n",
            "Epoch 6882/10000\n",
            "Step 0: Train Loss: 1.5258783125915443e-09, Train Acc: 1.0\n",
            "Epoch 6883/10000\n",
            "Step 0: Train Loss: 1.263618099045516e-09, Train Acc: 1.0\n",
            "Epoch 6884/10000\n",
            "Step 0: Train Loss: 1.1801717381132448e-09, Train Acc: 1.0\n",
            "Epoch 6885/10000\n",
            "Step 0: Train Loss: 1.0609625400448408e-09, Train Acc: 1.0\n",
            "Epoch 6886/10000\n",
            "Step 0: Train Loss: 1.0848043574540611e-09, Train Acc: 1.0\n",
            "Epoch 6887/10000\n",
            "Step 0: Train Loss: 1.7523758000237422e-09, Train Acc: 1.0\n",
            "Epoch 6888/10000\n",
            "Step 0: Train Loss: 1.2397762816362956e-09, Train Acc: 1.0\n",
            "Epoch 6889/10000\n",
            "Step 0: Train Loss: 1.2874601384993412e-09, Train Acc: 1.0\n",
            "Epoch 6890/10000\n",
            "Step 0: Train Loss: 1.0490415203179282e-09, Train Acc: 1.0\n",
            "Epoch 6891/10000\n",
            "Step 0: Train Loss: 1.3828276301808273e-09, Train Acc: 1.0\n",
            "Epoch 6892/10000\n",
            "Step 0: Train Loss: 9.417532309541343e-10, Train Acc: 1.0\n",
            "Epoch 6893/10000\n",
            "Step 0: Train Loss: 1.0967252661586713e-09, Train Acc: 1.0\n",
            "Epoch 6894/10000\n",
            "Step 0: Train Loss: 1.5020368282492313e-09, Train Acc: 1.0\n",
            "Epoch 6895/10000\n",
            "Step 0: Train Loss: 1.1324881032948042e-09, Train Acc: 1.0\n",
            "Epoch 6896/10000\n",
            "Step 0: Train Loss: 1.549720241023067e-09, Train Acc: 1.0\n",
            "Epoch 6897/10000\n",
            "Step 0: Train Loss: 1.2636183210901208e-09, Train Acc: 1.0\n",
            "Epoch 6898/10000\n",
            "Step 0: Train Loss: 1.0013578854994876e-09, Train Acc: 1.0\n",
            "Epoch 6899/10000\n",
            "Step 0: Train Loss: 9.894369767948774e-10, Train Acc: 1.0\n",
            "Epoch 6900/10000\n",
            "Step 0: Train Loss: 1.0848043574540611e-09, Train Acc: 1.0\n",
            "Epoch 6901/10000\n",
            "Step 0: Train Loss: 1.4901155864777138e-09, Train Acc: 1.0\n",
            "Epoch 6902/10000\n",
            "Step 0: Train Loss: 1.5377994433407594e-09, Train Acc: 1.0\n",
            "Epoch 6903/10000\n",
            "Step 0: Train Loss: 1.0251997029087079e-09, Train Acc: 1.0\n",
            "Epoch 6904/10000\n",
            "Step 0: Train Loss: 1.2040134445001627e-09, Train Acc: 1.0\n",
            "Epoch 6905/10000\n",
            "Step 0: Train Loss: 1.5377996653853643e-09, Train Acc: 1.0\n",
            "Epoch 6906/10000\n",
            "Step 0: Train Loss: 1.0609625400448408e-09, Train Acc: 1.0\n",
            "Epoch 6907/10000\n",
            "Step 0: Train Loss: 1.2516971903409058e-09, Train Acc: 1.0\n",
            "Epoch 6908/10000\n",
            "Step 0: Train Loss: 1.3709066104539147e-09, Train Acc: 1.0\n",
            "Epoch 6909/10000\n",
            "Step 0: Train Loss: 1.0132787942040977e-09, Train Acc: 1.0\n",
            "Epoch 6910/10000\n",
            "Step 0: Train Loss: 7.867811957495974e-10, Train Acc: 1.0\n",
            "Epoch 6911/10000\n",
            "Step 0: Train Loss: 8.463858502949506e-10, Train Acc: 1.0\n",
            "Epoch 6912/10000\n",
            "Step 0: Train Loss: 1.227855483953988e-09, Train Acc: 1.0\n",
            "Epoch 6913/10000\n",
            "Step 0: Train Loss: 1.3351438843400842e-09, Train Acc: 1.0\n",
            "Epoch 6914/10000\n",
            "Step 0: Train Loss: 9.417533419764368e-10, Train Acc: 1.0\n",
            "Epoch 6915/10000\n",
            "Step 0: Train Loss: 1.3947485388854375e-09, Train Acc: 1.0\n",
            "Epoch 6916/10000\n",
            "Step 0: Train Loss: 1.1324881032948042e-09, Train Acc: 1.0\n",
            "Epoch 6917/10000\n",
            "Step 0: Train Loss: 1.3232226425685667e-09, Train Acc: 1.0\n",
            "Epoch 6918/10000\n",
            "Step 0: Train Loss: 1.275539229794731e-09, Train Acc: 1.0\n",
            "Epoch 6919/10000\n",
            "Step 0: Train Loss: 1.2874598054324338e-09, Train Acc: 1.0\n",
            "Epoch 6920/10000\n",
            "Step 0: Train Loss: 9.179113580337628e-10, Train Acc: 1.0\n",
            "Epoch 6921/10000\n",
            "Step 0: Train Loss: 1.1920927578401574e-09, Train Acc: 1.0\n",
            "Epoch 6922/10000\n",
            "Step 0: Train Loss: 1.466273658046191e-09, Train Acc: 1.0\n",
            "Epoch 6923/10000\n",
            "Step 0: Train Loss: 1.2516971903409058e-09, Train Acc: 1.0\n",
            "Epoch 6924/10000\n",
            "Step 0: Train Loss: 1.0728833377271485e-09, Train Acc: 1.0\n",
            "Epoch 6925/10000\n",
            "Step 0: Train Loss: 1.1324881032948042e-09, Train Acc: 1.0\n",
            "Epoch 6926/10000\n",
            "Step 0: Train Loss: 1.0132787942040977e-09, Train Acc: 1.0\n",
            "Epoch 6927/10000\n",
            "Step 0: Train Loss: 1.4424318406369707e-09, Train Acc: 1.0\n",
            "Epoch 6928/10000\n",
            "Step 0: Train Loss: 1.0967252661586713e-09, Train Acc: 1.0\n",
            "Epoch 6929/10000\n",
            "Step 0: Train Loss: 1.0609625400448408e-09, Train Acc: 1.0\n",
            "Epoch 6930/10000\n",
            "Step 0: Train Loss: 1.1563299207040245e-09, Train Acc: 1.0\n",
            "Epoch 6931/10000\n",
            "Step 0: Train Loss: 9.536741396587445e-10, Train Acc: 1.0\n",
            "Epoch 6932/10000\n",
            "Step 0: Train Loss: 1.2040135555224651e-09, Train Acc: 1.0\n",
            "Epoch 6933/10000\n",
            "Step 0: Train Loss: 1.263618099045516e-09, Train Acc: 1.0\n",
            "Epoch 6934/10000\n",
            "Step 0: Train Loss: 9.775159570679648e-10, Train Acc: 1.0\n",
            "Epoch 6935/10000\n",
            "Step 0: Train Loss: 1.2040136665447676e-09, Train Acc: 1.0\n",
            "Epoch 6936/10000\n",
            "Step 0: Train Loss: 1.3947483168408326e-09, Train Acc: 1.0\n",
            "Epoch 6937/10000\n",
            "Step 0: Train Loss: 1.0848043574540611e-09, Train Acc: 1.0\n",
            "Epoch 6938/10000\n",
            "Step 0: Train Loss: 1.358985590727002e-09, Train Acc: 1.0\n",
            "Epoch 6939/10000\n",
            "Step 0: Train Loss: 1.3232228646131716e-09, Train Acc: 1.0\n",
            "Epoch 6940/10000\n",
            "Step 0: Train Loss: 1.3351438843400842e-09, Train Acc: 1.0\n",
            "Epoch 6941/10000\n",
            "Step 0: Train Loss: 1.1920927578401574e-09, Train Acc: 1.0\n",
            "Epoch 6942/10000\n",
            "Step 0: Train Loss: 1.1324881032948042e-09, Train Acc: 1.0\n",
            "Epoch 6943/10000\n",
            "Step 0: Train Loss: 1.2159345752493778e-09, Train Acc: 1.0\n",
            "Epoch 6944/10000\n",
            "Step 0: Train Loss: 1.3351435512731769e-09, Train Acc: 1.0\n",
            "Epoch 6945/10000\n",
            "Step 0: Train Loss: 1.311301622841654e-09, Train Acc: 1.0\n",
            "Epoch 6946/10000\n",
            "Step 0: Train Loss: 1.263618099045516e-09, Train Acc: 1.0\n",
            "Epoch 6947/10000\n",
            "Step 0: Train Loss: 1.275539229794731e-09, Train Acc: 1.0\n",
            "Epoch 6948/10000\n",
            "Step 0: Train Loss: 1.2993809361816488e-09, Train Acc: 1.0\n",
            "Epoch 6949/10000\n",
            "Step 0: Train Loss: 1.1920927578401574e-09, Train Acc: 1.0\n",
            "Epoch 6950/10000\n",
            "Step 0: Train Loss: 1.227855483953988e-09, Train Acc: 1.0\n",
            "Epoch 6951/10000\n",
            "Step 0: Train Loss: 1.4185901342500529e-09, Train Acc: 1.0\n",
            "Epoch 6952/10000\n",
            "Step 0: Train Loss: 7.867811957495974e-10, Train Acc: 1.0\n",
            "Epoch 6953/10000\n",
            "Step 0: Train Loss: 1.0848043574540611e-09, Train Acc: 1.0\n",
            "Epoch 6954/10000\n",
            "Step 0: Train Loss: 1.2159343532047728e-09, Train Acc: 1.0\n",
            "Epoch 6955/10000\n",
            "Step 0: Train Loss: 1.3709062773870073e-09, Train Acc: 1.0\n",
            "Epoch 6956/10000\n",
            "Step 0: Train Loss: 1.1086461748632814e-09, Train Acc: 1.0\n",
            "Epoch 6957/10000\n",
            "Step 0: Train Loss: 1.0848043574540611e-09, Train Acc: 1.0\n",
            "Epoch 6958/10000\n",
            "Step 0: Train Loss: 1.2516971903409058e-09, Train Acc: 1.0\n",
            "Epoch 6959/10000\n",
            "Step 0: Train Loss: 1.2278553729316855e-09, Train Acc: 1.0\n",
            "Epoch 6960/10000\n",
            "Step 0: Train Loss: 1.0490415203179282e-09, Train Acc: 1.0\n",
            "Epoch 6961/10000\n",
            "Step 0: Train Loss: 1.3947485388854375e-09, Train Acc: 1.0\n",
            "Epoch 6962/10000\n",
            "Step 0: Train Loss: 1.3351435512731769e-09, Train Acc: 1.0\n",
            "Epoch 6963/10000\n",
            "Step 0: Train Loss: 1.1086461748632814e-09, Train Acc: 1.0\n",
            "Epoch 6964/10000\n",
            "Step 0: Train Loss: 1.7046919431606966e-09, Train Acc: 1.0\n",
            "Epoch 6965/10000\n",
            "Step 0: Train Loss: 1.5616414827945846e-09, Train Acc: 1.0\n",
            "Epoch 6966/10000\n",
            "Step 0: Train Loss: 1.3351438843400842e-09, Train Acc: 1.0\n",
            "Epoch 6967/10000\n",
            "Step 0: Train Loss: 1.2874601384993412e-09, Train Acc: 1.0\n",
            "Epoch 6968/10000\n",
            "Step 0: Train Loss: 1.1920927578401574e-09, Train Acc: 1.0\n",
            "Epoch 6969/10000\n",
            "Step 0: Train Loss: 1.263618099045516e-09, Train Acc: 1.0\n",
            "Epoch 6970/10000\n",
            "Step 0: Train Loss: 1.5139572928646317e-09, Train Acc: 1.0\n",
            "Epoch 6971/10000\n",
            "Step 0: Train Loss: 1.0490415203179282e-09, Train Acc: 1.0\n",
            "Epoch 6972/10000\n",
            "Step 0: Train Loss: 9.298322667383729e-10, Train Acc: 1.0\n",
            "Epoch 6973/10000\n",
            "Step 0: Train Loss: 1.1086461748632814e-09, Train Acc: 1.0\n",
            "Epoch 6974/10000\n",
            "Step 0: Train Loss: 1.8715846650252388e-09, Train Acc: 1.0\n",
            "Epoch 6975/10000\n",
            "Step 0: Train Loss: 1.2874601384993412e-09, Train Acc: 1.0\n",
            "Epoch 6976/10000\n",
            "Step 0: Train Loss: 9.536741396587445e-10, Train Acc: 1.0\n",
            "Epoch 6977/10000\n",
            "Step 0: Train Loss: 9.775159570679648e-10, Train Acc: 1.0\n",
            "Epoch 6978/10000\n",
            "Step 0: Train Loss: 1.692771145478389e-09, Train Acc: 1.0\n",
            "Epoch 6979/10000\n",
            "Step 0: Train Loss: 1.3709062773870073e-09, Train Acc: 1.0\n",
            "Epoch 6980/10000\n",
            "Step 0: Train Loss: 1.1324881032948042e-09, Train Acc: 1.0\n",
            "Epoch 6981/10000\n",
            "Step 0: Train Loss: 9.775159570679648e-10, Train Acc: 1.0\n",
            "Epoch 6982/10000\n",
            "Step 0: Train Loss: 1.6093248955684203e-09, Train Acc: 1.0\n",
            "Epoch 6983/10000\n",
            "Step 0: Train Loss: 1.2874601384993412e-09, Train Acc: 1.0\n",
            "Epoch 6984/10000\n",
            "Step 0: Train Loss: 2.34842167934346e-09, Train Acc: 1.0\n",
            "Epoch 6985/10000\n",
            "Step 0: Train Loss: 1.3351438843400842e-09, Train Acc: 1.0\n",
            "Epoch 6986/10000\n",
            "Step 0: Train Loss: 1.2397762816362956e-09, Train Acc: 1.0\n",
            "Epoch 6987/10000\n",
            "Step 0: Train Loss: 1.4305111539769655e-09, Train Acc: 1.0\n",
            "Epoch 6988/10000\n",
            "Step 0: Train Loss: 1.0132787942040977e-09, Train Acc: 1.0\n",
            "Epoch 6989/10000\n",
            "Step 0: Train Loss: 1.2040136665447676e-09, Train Acc: 1.0\n",
            "Epoch 6990/10000\n",
            "Step 0: Train Loss: 1.0132787942040977e-09, Train Acc: 1.0\n",
            "Epoch 6991/10000\n",
            "Step 0: Train Loss: 1.1801717381132448e-09, Train Acc: 1.0\n",
            "Epoch 6992/10000\n",
            "Step 0: Train Loss: 1.323222975635474e-09, Train Acc: 1.0\n",
            "Epoch 6993/10000\n",
            "Step 0: Train Loss: 1.1444090119994144e-09, Train Acc: 1.0\n",
            "Epoch 6994/10000\n",
            "Step 0: Train Loss: 1.2755391187724285e-09, Train Acc: 1.0\n",
            "Epoch 6995/10000\n",
            "Step 0: Train Loss: 1.263618099045516e-09, Train Acc: 1.0\n",
            "Epoch 6996/10000\n",
            "Step 0: Train Loss: 1.5497205740899744e-09, Train Acc: 1.0\n",
            "Epoch 6997/10000\n",
            "Step 0: Train Loss: 1.0967252661586713e-09, Train Acc: 1.0\n",
            "Epoch 6998/10000\n",
            "Step 0: Train Loss: 1.4185901342500529e-09, Train Acc: 1.0\n",
            "Epoch 6999/10000\n",
            "Step 0: Train Loss: 1.668929661136076e-09, Train Acc: 1.0\n",
            "Epoch 7000/10000\n",
            "Step 0: Train Loss: 1.466273880090796e-09, Train Acc: 1.0\n",
            "Epoch 7001/10000\n",
            "Step 0: Train Loss: 1.3828274081362224e-09, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 7000 1024 1.0754403659802592\n",
            "Epoch index and hidden dimension and ratio: 7000 20 1.192891768826164\n",
            "Epoch index and hidden dimension and ratio: 7000 20 2.249362853947106\n",
            "Epoch index and hidden dimension and ratio: 7000 20 6.197863850427578\n",
            "MI(X;T): [10.698025777277362, 8.17572667283537, 5.623823310442939, 3.4227224214066414], MI(Y;T): [2.610718423485368, 3.1808515244644555, 3.131021188603981, 2.759165872601999]\n",
            "Epoch index and hidden dimension and ratio: 7000 1024 1.075508796012497\n",
            "Epoch index and hidden dimension and ratio: 7000 20 1.1932506279492243\n",
            "Epoch index and hidden dimension and ratio: 7000 20 2.2508010507164955\n",
            "Epoch index and hidden dimension and ratio: 7000 20 6.202875723272034\n",
            "MI(X;T): [10.698489878369632, 8.176177263234901, 5.6246397778112645, 3.4242550868511152], MI(Y;T): [2.6110507876232276, 3.1801913670762194, 3.1306082274439344, 2.7595420382053355]\n",
            "Epoch index and hidden dimension and ratio: 7000 1024 1.0755833201355884\n",
            "Epoch index and hidden dimension and ratio: 7000 20 1.1935839937548967\n",
            "Epoch index and hidden dimension and ratio: 7000 20 2.252257529648208\n",
            "Epoch index and hidden dimension and ratio: 7000 20 6.208652275245331\n",
            "MI(X;T): [10.69834173094754, 8.179298978356247, 5.620653658751882, 3.425689248274362], MI(Y;T): [2.611205169767727, 3.179764270005867, 3.1304296379117087, 2.759284437565468]\n",
            "Epoch index and hidden dimension and ratio: 7000 1024 1.075640663327839\n",
            "Epoch index and hidden dimension and ratio: 7000 20 1.193793147439679\n",
            "Epoch index and hidden dimension and ratio: 7000 20 2.2532739670895685\n",
            "Epoch index and hidden dimension and ratio: 7000 20 6.213448958696387\n",
            "MI(X;T): [10.69935474011304, 8.181188986664408, 5.626572379747791, 3.4243738996909663], MI(Y;T): [2.6109809512195206, 3.179747390420454, 3.1307431958567395, 2.7599061824013074]\n",
            "Epoch index and hidden dimension and ratio: 7000 1024 1.0756834688093784\n",
            "Epoch index and hidden dimension and ratio: 7000 20 1.193892191689786\n",
            "Epoch index and hidden dimension and ratio: 7000 20 2.2537394004720355\n",
            "Epoch index and hidden dimension and ratio: 7000 20 6.215645601495073\n",
            "MI(X;T): [10.699712139146808, 8.177758761678735, 5.627445872590645, 3.4208346590435474], MI(Y;T): [2.6111440074659624, 3.179547316030285, 3.130418655378981, 2.759625142433557]\n",
            "Epoch index and hidden dimension and ratio: 7000 1024 1.0757114428890808\n",
            "Epoch index and hidden dimension and ratio: 7000 20 1.1939658511047069\n",
            "Epoch index and hidden dimension and ratio: 7000 20 2.2541205327726805\n",
            "Epoch index and hidden dimension and ratio: 7000 20 6.217983071603201\n",
            "MI(X;T): [10.699541076195832, 8.17645451943329, 5.627151469420102, 3.4205361193903583], MI(Y;T): [2.6113360404210697, 3.179598992475389, 3.1300029400443488, 2.7587931064815745]\n",
            "Epoch 7002/10000\n",
            "Step 0: Train Loss: 1.2874601384993412e-09, Train Acc: 1.0\n",
            "Epoch 7003/10000\n",
            "Step 0: Train Loss: 1.657008530386861e-09, Train Acc: 1.0\n",
            "Epoch 7004/10000\n",
            "Step 0: Train Loss: 1.4781947887954061e-09, Train Acc: 1.0\n",
            "Epoch 7005/10000\n",
            "Step 0: Train Loss: 1.347064682022392e-09, Train Acc: 1.0\n",
            "Epoch 7006/10000\n",
            "Step 0: Train Loss: 1.358985590727002e-09, Train Acc: 1.0\n",
            "Epoch 7007/10000\n",
            "Step 0: Train Loss: 1.0967252661586713e-09, Train Acc: 1.0\n",
            "Epoch 7008/10000\n",
            "Step 0: Train Loss: 1.513957625931539e-09, Train Acc: 1.0\n",
            "Epoch 7009/10000\n",
            "Step 0: Train Loss: 1.2993809361816488e-09, Train Acc: 1.0\n",
            "Epoch 7010/10000\n",
            "Step 0: Train Loss: 1.3589857017493046e-09, Train Acc: 1.0\n",
            "Epoch 7011/10000\n",
            "Step 0: Train Loss: 1.2278553729316855e-09, Train Acc: 1.0\n",
            "Epoch 7012/10000\n",
            "Step 0: Train Loss: 1.2040136665447676e-09, Train Acc: 1.0\n",
            "Epoch 7013/10000\n",
            "Step 0: Train Loss: 1.0013578854994876e-09, Train Acc: 1.0\n",
            "Epoch 7014/10000\n",
            "Step 0: Train Loss: 1.0967252661586713e-09, Train Acc: 1.0\n",
            "Epoch 7015/10000\n",
            "Step 0: Train Loss: 1.0490415203179282e-09, Train Acc: 1.0\n",
            "Epoch 7016/10000\n",
            "Step 0: Train Loss: 1.1086461748632814e-09, Train Acc: 1.0\n",
            "Epoch 7017/10000\n",
            "Step 0: Train Loss: 1.4066692255454427e-09, Train Acc: 1.0\n",
            "Epoch 7018/10000\n",
            "Step 0: Train Loss: 1.7762180615221723e-09, Train Acc: 1.0\n",
            "Epoch 7019/10000\n",
            "Step 0: Train Loss: 1.3828276301808273e-09, Train Acc: 1.0\n",
            "Epoch 7020/10000\n",
            "Step 0: Train Loss: 1.4185901342500529e-09, Train Acc: 1.0\n",
            "Epoch 7021/10000\n",
            "Step 0: Train Loss: 1.4543529713861858e-09, Train Acc: 1.0\n",
            "Epoch 7022/10000\n",
            "Step 0: Train Loss: 1.3828274081362224e-09, Train Acc: 1.0\n",
            "Epoch 7023/10000\n",
            "Step 0: Train Loss: 1.2874599164547362e-09, Train Acc: 1.0\n",
            "Epoch 7024/10000\n",
            "Step 0: Train Loss: 1.668929661136076e-09, Train Acc: 1.0\n",
            "Epoch 7025/10000\n",
            "Step 0: Train Loss: 1.6808505698406861e-09, Train Acc: 1.0\n",
            "Epoch 7026/10000\n",
            "Step 0: Train Loss: 1.2993809361816488e-09, Train Acc: 1.0\n",
            "Epoch 7027/10000\n",
            "Step 0: Train Loss: 1.2993810472039513e-09, Train Acc: 1.0\n",
            "Epoch 7028/10000\n",
            "Step 0: Train Loss: 1.8239018073629154e-09, Train Acc: 1.0\n",
            "Epoch 7029/10000\n",
            "Step 0: Train Loss: 1.5377996653853643e-09, Train Acc: 1.0\n",
            "Epoch 7030/10000\n",
            "Step 0: Train Loss: 1.275539229794731e-09, Train Acc: 1.0\n",
            "Epoch 7031/10000\n",
            "Step 0: Train Loss: 1.5377994433407594e-09, Train Acc: 1.0\n",
            "Epoch 7032/10000\n",
            "Step 0: Train Loss: 1.1324881032948042e-09, Train Acc: 1.0\n",
            "Epoch 7033/10000\n",
            "Step 0: Train Loss: 1.740455113363737e-09, Train Acc: 1.0\n",
            "Epoch 7034/10000\n",
            "Step 0: Train Loss: 1.5020368282492313e-09, Train Acc: 1.0\n",
            "Epoch 7035/10000\n",
            "Step 0: Train Loss: 2.3603432541818847e-09, Train Acc: 1.0\n",
            "Epoch 7036/10000\n",
            "Step 0: Train Loss: 1.3828276301808273e-09, Train Acc: 1.0\n",
            "Epoch 7037/10000\n",
            "Step 0: Train Loss: 1.3470645710000895e-09, Train Acc: 1.0\n",
            "Epoch 7038/10000\n",
            "Step 0: Train Loss: 1.6331669350222455e-09, Train Acc: 1.0\n",
            "Epoch 7039/10000\n",
            "Step 0: Train Loss: 1.5735622804768923e-09, Train Acc: 1.0\n",
            "Epoch 7040/10000\n",
            "Step 0: Train Loss: 1.3589857017493046e-09, Train Acc: 1.0\n",
            "Epoch 7041/10000\n",
            "Step 0: Train Loss: 1.430511264999268e-09, Train Acc: 1.0\n",
            "Epoch 7042/10000\n",
            "Step 0: Train Loss: 1.5377994433407594e-09, Train Acc: 1.0\n",
            "Epoch 7043/10000\n",
            "Step 0: Train Loss: 1.704692276227604e-09, Train Acc: 1.0\n",
            "Epoch 7044/10000\n",
            "Step 0: Train Loss: 1.037120611613318e-09, Train Acc: 1.0\n",
            "Epoch 7045/10000\n",
            "Step 0: Train Loss: 1.5377996653853643e-09, Train Acc: 1.0\n",
            "Epoch 7046/10000\n",
            "Step 0: Train Loss: 1.752376022068347e-09, Train Acc: 1.0\n",
            "Epoch 7047/10000\n",
            "Step 0: Train Loss: 1.4901155864777138e-09, Train Acc: 1.0\n",
            "Epoch 7048/10000\n",
            "Step 0: Train Loss: 1.4185903562946578e-09, Train Acc: 1.0\n",
            "Epoch 7049/10000\n",
            "Step 0: Train Loss: 1.5497203520453695e-09, Train Acc: 1.0\n",
            "Epoch 7050/10000\n",
            "Step 0: Train Loss: 1.4066692255454427e-09, Train Acc: 1.0\n",
            "Epoch 7051/10000\n",
            "Step 0: Train Loss: 2.0980828185912515e-09, Train Acc: 1.0\n",
            "Epoch 7052/10000\n",
            "Step 0: Train Loss: 1.2516974123855107e-09, Train Acc: 1.0\n",
            "Epoch 7053/10000\n",
            "Step 0: Train Loss: 1.347064682022392e-09, Train Acc: 1.0\n",
            "Epoch 7054/10000\n",
            "Step 0: Train Loss: 1.752376022068347e-09, Train Acc: 1.0\n",
            "Epoch 7055/10000\n",
            "Step 0: Train Loss: 1.4901159195446212e-09, Train Acc: 1.0\n",
            "Epoch 7056/10000\n",
            "Step 0: Train Loss: 1.2516971903409058e-09, Train Acc: 1.0\n",
            "Epoch 7057/10000\n",
            "Step 0: Train Loss: 1.3947485388854375e-09, Train Acc: 1.0\n",
            "Epoch 7058/10000\n",
            "Step 0: Train Loss: 1.7642971528175622e-09, Train Acc: 1.0\n",
            "Epoch 7059/10000\n",
            "Step 0: Train Loss: 1.3113019559085615e-09, Train Acc: 1.0\n",
            "Epoch 7060/10000\n",
            "Step 0: Train Loss: 1.3470645710000895e-09, Train Acc: 1.0\n",
            "Epoch 7061/10000\n",
            "Step 0: Train Loss: 1.6212460263176354e-09, Train Acc: 1.0\n",
            "Epoch 7062/10000\n",
            "Step 0: Train Loss: 1.263618099045516e-09, Train Acc: 1.0\n",
            "Epoch 7063/10000\n",
            "Step 0: Train Loss: 1.4066692255454427e-09, Train Acc: 1.0\n",
            "Epoch 7064/10000\n",
            "Step 0: Train Loss: 1.1086461748632814e-09, Train Acc: 1.0\n",
            "Epoch 7065/10000\n",
            "Step 0: Train Loss: 2.0384782750682007e-09, Train Acc: 1.0\n",
            "Epoch 7066/10000\n",
            "Step 0: Train Loss: 1.3113019559085615e-09, Train Acc: 1.0\n",
            "Epoch 7067/10000\n",
            "Step 0: Train Loss: 1.5974040978861126e-09, Train Acc: 1.0\n",
            "Epoch 7068/10000\n",
            "Step 0: Train Loss: 1.7881387481821776e-09, Train Acc: 1.0\n",
            "Epoch 7069/10000\n",
            "Step 0: Train Loss: 2.074241001182031e-09, Train Acc: 1.0\n",
            "Epoch 7070/10000\n",
            "Step 0: Train Loss: 1.3947479837739252e-09, Train Acc: 1.0\n",
            "Epoch 7071/10000\n",
            "Step 0: Train Loss: 1.4901158085223187e-09, Train Acc: 1.0\n",
            "Epoch 7072/10000\n",
            "Step 0: Train Loss: 1.4781948998177086e-09, Train Acc: 1.0\n",
            "Epoch 7073/10000\n",
            "Step 0: Train Loss: 1.2874599164547362e-09, Train Acc: 1.0\n",
            "Epoch 7074/10000\n",
            "Step 0: Train Loss: 1.4662741021354009e-09, Train Acc: 1.0\n",
            "Epoch 7075/10000\n",
            "Step 0: Train Loss: 1.513957847976144e-09, Train Acc: 1.0\n",
            "Epoch 7076/10000\n",
            "Step 0: Train Loss: 2.0146362356143754e-09, Train Acc: 1.0\n",
            "Epoch 7077/10000\n",
            "Step 0: Train Loss: 1.704692276227604e-09, Train Acc: 1.0\n",
            "Epoch 7078/10000\n",
            "Step 0: Train Loss: 1.6212460263176354e-09, Train Acc: 1.0\n",
            "Epoch 7079/10000\n",
            "Step 0: Train Loss: 1.4424321737038781e-09, Train Acc: 1.0\n",
            "Epoch 7080/10000\n",
            "Step 0: Train Loss: 1.668929661136076e-09, Train Acc: 1.0\n",
            "Epoch 7081/10000\n",
            "Step 0: Train Loss: 1.9431107833867145e-09, Train Acc: 1.0\n",
            "Epoch 7082/10000\n",
            "Step 0: Train Loss: 2.002715770998975e-09, Train Acc: 1.0\n",
            "Epoch 7083/10000\n",
            "Step 0: Train Loss: 1.8715853311590536e-09, Train Acc: 1.0\n",
            "Epoch 7084/10000\n",
            "Step 0: Train Loss: 1.8715855532036585e-09, Train Acc: 1.0\n",
            "Epoch 7085/10000\n",
            "Step 0: Train Loss: 2.2768966712050087e-09, Train Acc: 1.0\n",
            "Epoch 7086/10000\n",
            "Step 0: Train Loss: 2.1219248580450767e-09, Train Acc: 1.0\n",
            "Epoch 7087/10000\n",
            "Step 0: Train Loss: 2.0503994058174158e-09, Train Acc: 1.0\n",
            "Epoch 7088/10000\n",
            "Step 0: Train Loss: 25.273103713989258, Train Acc: 0.6008999943733215\n",
            "Epoch 7089/10000\n",
            "Step 0: Train Loss: 4.898646354675293, Train Acc: 0.8801000118255615\n",
            "Epoch 7090/10000\n",
            "Step 0: Train Loss: 2.0709307193756104, Train Acc: 0.8855999708175659\n",
            "Epoch 7091/10000\n",
            "Step 0: Train Loss: 0.6711297631263733, Train Acc: 0.9294000267982483\n",
            "Epoch 7092/10000\n",
            "Step 0: Train Loss: 0.5131618976593018, Train Acc: 0.9348000288009644\n",
            "Epoch 7093/10000\n",
            "Step 0: Train Loss: 0.3069964349269867, Train Acc: 0.9509000182151794\n",
            "Epoch 7094/10000\n",
            "Step 0: Train Loss: 0.22829672694206238, Train Acc: 0.9569000005722046\n",
            "Epoch 7095/10000\n",
            "Step 0: Train Loss: 0.19832950830459595, Train Acc: 0.9585000276565552\n",
            "Epoch 7096/10000\n",
            "Step 0: Train Loss: 0.18337780237197876, Train Acc: 0.9639000296592712\n",
            "Epoch 7097/10000\n",
            "Step 0: Train Loss: 0.1495300531387329, Train Acc: 0.967199981212616\n",
            "Epoch 7098/10000\n",
            "Step 0: Train Loss: 0.1340278685092926, Train Acc: 0.9708999991416931\n",
            "Epoch 7099/10000\n",
            "Step 0: Train Loss: 0.11411452293395996, Train Acc: 0.9732000231742859\n",
            "Epoch 7100/10000\n",
            "Step 0: Train Loss: 0.112632617354393, Train Acc: 0.9725000262260437\n",
            "Epoch 7101/10000\n",
            "Step 0: Train Loss: 0.09712866693735123, Train Acc: 0.9778000116348267\n",
            "Epoch 7102/10000\n",
            "Step 0: Train Loss: 0.08845051378011703, Train Acc: 0.9793000221252441\n",
            "Epoch 7103/10000\n",
            "Step 0: Train Loss: 0.07620079070329666, Train Acc: 0.9797000288963318\n",
            "Epoch 7104/10000\n",
            "Step 0: Train Loss: 0.07169656455516815, Train Acc: 0.9796000123023987\n",
            "Epoch 7105/10000\n",
            "Step 0: Train Loss: 0.07702606171369553, Train Acc: 0.9803000092506409\n",
            "Epoch 7106/10000\n",
            "Step 0: Train Loss: 0.07419203966856003, Train Acc: 0.9794999957084656\n",
            "Epoch 7107/10000\n",
            "Step 0: Train Loss: 0.075772725045681, Train Acc: 0.9811999797821045\n",
            "Epoch 7108/10000\n",
            "Step 0: Train Loss: 0.06799713522195816, Train Acc: 0.9825999736785889\n",
            "Epoch 7109/10000\n",
            "Step 0: Train Loss: 0.059756793081760406, Train Acc: 0.9846000075340271\n",
            "Epoch 7110/10000\n",
            "Step 0: Train Loss: 0.059348344802856445, Train Acc: 0.9850999712944031\n",
            "Epoch 7111/10000\n",
            "Step 0: Train Loss: 0.05162249878048897, Train Acc: 0.9864000082015991\n",
            "Epoch 7112/10000\n",
            "Step 0: Train Loss: 0.056326813995838165, Train Acc: 0.9868999719619751\n",
            "Epoch 7113/10000\n",
            "Step 0: Train Loss: 0.05828363075852394, Train Acc: 0.9861000180244446\n",
            "Epoch 7114/10000\n",
            "Step 0: Train Loss: 0.04924410954117775, Train Acc: 0.9872000217437744\n",
            "Epoch 7115/10000\n",
            "Step 0: Train Loss: 0.044548362493515015, Train Acc: 0.989300012588501\n",
            "Epoch 7116/10000\n",
            "Step 0: Train Loss: 0.04326370358467102, Train Acc: 0.9883999824523926\n",
            "Epoch 7117/10000\n",
            "Step 0: Train Loss: 0.04334943741559982, Train Acc: 0.9894000291824341\n",
            "Epoch 7118/10000\n",
            "Step 0: Train Loss: 0.04966180399060249, Train Acc: 0.9879999756813049\n",
            "Epoch 7119/10000\n",
            "Step 0: Train Loss: 0.0353156141936779, Train Acc: 0.9901999831199646\n",
            "Epoch 7120/10000\n",
            "Step 0: Train Loss: 0.043181512504816055, Train Acc: 0.9889000058174133\n",
            "Epoch 7121/10000\n",
            "Step 0: Train Loss: 0.026814395561814308, Train Acc: 0.9922000169754028\n",
            "Epoch 7122/10000\n",
            "Step 0: Train Loss: 0.04569550231099129, Train Acc: 0.988099992275238\n",
            "Epoch 7123/10000\n",
            "Step 0: Train Loss: 0.032236114144325256, Train Acc: 0.9914000034332275\n",
            "Epoch 7124/10000\n",
            "Step 0: Train Loss: 0.03461594507098198, Train Acc: 0.9909999966621399\n",
            "Epoch 7125/10000\n",
            "Step 0: Train Loss: 0.035493403673172, Train Acc: 0.9915000200271606\n",
            "Epoch 7126/10000\n",
            "Step 0: Train Loss: 0.03241557627916336, Train Acc: 0.9907000064849854\n",
            "Epoch 7127/10000\n",
            "Step 0: Train Loss: 0.026955893263220787, Train Acc: 0.9918000102043152\n",
            "Epoch 7128/10000\n",
            "Step 0: Train Loss: 0.029316702857613564, Train Acc: 0.9922999739646912\n",
            "Epoch 7129/10000\n",
            "Step 0: Train Loss: 0.02469678968191147, Train Acc: 0.9922000169754028\n",
            "Epoch 7130/10000\n",
            "Step 0: Train Loss: 0.029032593593001366, Train Acc: 0.9916999936103821\n",
            "Epoch 7131/10000\n",
            "Step 0: Train Loss: 0.025685949251055717, Train Acc: 0.9923999905586243\n",
            "Epoch 7132/10000\n",
            "Step 0: Train Loss: 0.029236333444714546, Train Acc: 0.9927999973297119\n",
            "Epoch 7133/10000\n",
            "Step 0: Train Loss: 0.027422113344073296, Train Acc: 0.9926999807357788\n",
            "Epoch 7134/10000\n",
            "Step 0: Train Loss: 0.025220200419425964, Train Acc: 0.9927999973297119\n",
            "Epoch 7135/10000\n",
            "Step 0: Train Loss: 0.02922903746366501, Train Acc: 0.9921000003814697\n",
            "Epoch 7136/10000\n",
            "Step 0: Train Loss: 0.027641933411359787, Train Acc: 0.9925000071525574\n",
            "Epoch 7137/10000\n",
            "Step 0: Train Loss: 0.023339148610830307, Train Acc: 0.9933000206947327\n",
            "Epoch 7138/10000\n",
            "Step 0: Train Loss: 0.022269975394010544, Train Acc: 0.9943000078201294\n",
            "Epoch 7139/10000\n",
            "Step 0: Train Loss: 0.022175917401909828, Train Acc: 0.9930999875068665\n",
            "Epoch 7140/10000\n",
            "Step 0: Train Loss: 0.023222263902425766, Train Acc: 0.9939000010490417\n",
            "Epoch 7141/10000\n",
            "Step 0: Train Loss: 0.023906197398900986, Train Acc: 0.9936000108718872\n",
            "Epoch 7142/10000\n",
            "Step 0: Train Loss: 0.02086493745446205, Train Acc: 0.9944000244140625\n",
            "Epoch 7143/10000\n",
            "Step 0: Train Loss: 0.02084093913435936, Train Acc: 0.9950000047683716\n",
            "Epoch 7144/10000\n",
            "Step 0: Train Loss: 0.020701508969068527, Train Acc: 0.9940999746322632\n",
            "Epoch 7145/10000\n",
            "Step 0: Train Loss: 0.018835535272955894, Train Acc: 0.9951000213623047\n",
            "Epoch 7146/10000\n",
            "Step 0: Train Loss: 0.024583693593740463, Train Acc: 0.993399977684021\n",
            "Epoch 7147/10000\n",
            "Step 0: Train Loss: 0.020625265315175056, Train Acc: 0.9934999942779541\n",
            "Epoch 7148/10000\n",
            "Step 0: Train Loss: 0.016996413469314575, Train Acc: 0.9951000213623047\n",
            "Epoch 7149/10000\n",
            "Step 0: Train Loss: 0.018431346863508224, Train Acc: 0.9944000244140625\n",
            "Epoch 7150/10000\n",
            "Step 0: Train Loss: 0.018141556531190872, Train Acc: 0.9944000244140625\n",
            "Epoch 7151/10000\n",
            "Step 0: Train Loss: 0.015736129134893417, Train Acc: 0.9948999881744385\n",
            "Epoch 7152/10000\n",
            "Step 0: Train Loss: 0.015550948679447174, Train Acc: 0.995199978351593\n",
            "Epoch 7153/10000\n",
            "Step 0: Train Loss: 0.01214530598372221, Train Acc: 0.9968000054359436\n",
            "Epoch 7154/10000\n",
            "Step 0: Train Loss: 0.01739000901579857, Train Acc: 0.9958000183105469\n",
            "Epoch 7155/10000\n",
            "Step 0: Train Loss: 0.01625746674835682, Train Acc: 0.995199978351593\n",
            "Epoch 7156/10000\n",
            "Step 0: Train Loss: 0.014335067942738533, Train Acc: 0.9965000152587891\n",
            "Epoch 7157/10000\n",
            "Step 0: Train Loss: 0.018352245911955833, Train Acc: 0.995199978351593\n",
            "Epoch 7158/10000\n",
            "Step 0: Train Loss: 0.01661526784300804, Train Acc: 0.9954000115394592\n",
            "Epoch 7159/10000\n",
            "Step 0: Train Loss: 0.018169449642300606, Train Acc: 0.994700014591217\n",
            "Epoch 7160/10000\n",
            "Step 0: Train Loss: 0.018322348594665527, Train Acc: 0.9950000047683716\n",
            "Epoch 7161/10000\n",
            "Step 0: Train Loss: 0.014609080739319324, Train Acc: 0.9951000213623047\n",
            "Epoch 7162/10000\n",
            "Step 0: Train Loss: 0.01714305952191353, Train Acc: 0.995199978351593\n",
            "Epoch 7163/10000\n",
            "Step 0: Train Loss: 0.012083930894732475, Train Acc: 0.9959999918937683\n",
            "Epoch 7164/10000\n",
            "Step 0: Train Loss: 0.011561783961951733, Train Acc: 0.9965000152587891\n",
            "Epoch 7165/10000\n",
            "Step 0: Train Loss: 0.011885036714375019, Train Acc: 0.9972000122070312\n",
            "Epoch 7166/10000\n",
            "Step 0: Train Loss: 0.015223385766148567, Train Acc: 0.9958999752998352\n",
            "Epoch 7167/10000\n",
            "Step 0: Train Loss: 0.013711902312934399, Train Acc: 0.9959999918937683\n",
            "Epoch 7168/10000\n",
            "Step 0: Train Loss: 0.012307687662541866, Train Acc: 0.996399998664856\n",
            "Epoch 7169/10000\n",
            "Step 0: Train Loss: 0.013363553211092949, Train Acc: 0.9961000084877014\n",
            "Epoch 7170/10000\n",
            "Step 0: Train Loss: 0.012076000683009624, Train Acc: 0.9962999820709229\n",
            "Epoch 7171/10000\n",
            "Step 0: Train Loss: 0.012529291212558746, Train Acc: 0.9962999820709229\n",
            "Epoch 7172/10000\n",
            "Step 0: Train Loss: 0.011389221996068954, Train Acc: 0.9970999956130981\n",
            "Epoch 7173/10000\n",
            "Step 0: Train Loss: 0.014617778360843658, Train Acc: 0.9954000115394592\n",
            "Epoch 7174/10000\n",
            "Step 0: Train Loss: 0.013694720342755318, Train Acc: 0.9958000183105469\n",
            "Epoch 7175/10000\n",
            "Step 0: Train Loss: 0.013140258379280567, Train Acc: 0.9962999820709229\n",
            "Epoch 7176/10000\n",
            "Step 0: Train Loss: 0.01126549206674099, Train Acc: 0.9968000054359436\n",
            "Epoch 7177/10000\n",
            "Step 0: Train Loss: 0.011904063634574413, Train Acc: 0.9959999918937683\n",
            "Epoch 7178/10000\n",
            "Step 0: Train Loss: 0.008352834731340408, Train Acc: 0.9980999827384949\n",
            "Epoch 7179/10000\n",
            "Step 0: Train Loss: 0.00938112847507, Train Acc: 0.9973000288009644\n",
            "Epoch 7180/10000\n",
            "Step 0: Train Loss: 0.013093965128064156, Train Acc: 0.9962999820709229\n",
            "Epoch 7181/10000\n",
            "Step 0: Train Loss: 0.012199850752949715, Train Acc: 0.996999979019165\n",
            "Epoch 7182/10000\n",
            "Step 0: Train Loss: 0.013540475629270077, Train Acc: 0.996399998664856\n",
            "Epoch 7183/10000\n",
            "Step 0: Train Loss: 0.009127177298069, Train Acc: 0.9976999759674072\n",
            "Epoch 7184/10000\n",
            "Step 0: Train Loss: 0.01226693857461214, Train Acc: 0.9972000122070312\n",
            "Epoch 7185/10000\n",
            "Step 0: Train Loss: 0.008959304541349411, Train Acc: 0.9976000189781189\n",
            "Epoch 7186/10000\n",
            "Step 0: Train Loss: 0.009745794348418713, Train Acc: 0.9968000054359436\n",
            "Epoch 7187/10000\n",
            "Step 0: Train Loss: 0.012951891869306564, Train Acc: 0.996999979019165\n",
            "Epoch 7188/10000\n",
            "Step 0: Train Loss: 0.008629024028778076, Train Acc: 0.9975000023841858\n",
            "Epoch 7189/10000\n",
            "Step 0: Train Loss: 0.010196258313953876, Train Acc: 0.9969000220298767\n",
            "Epoch 7190/10000\n",
            "Step 0: Train Loss: 0.007822827436029911, Train Acc: 0.9977999925613403\n",
            "Epoch 7191/10000\n",
            "Step 0: Train Loss: 0.011009639129042625, Train Acc: 0.9975000023841858\n",
            "Epoch 7192/10000\n",
            "Step 0: Train Loss: 0.00817220937460661, Train Acc: 0.9973999857902527\n",
            "Epoch 7193/10000\n",
            "Step 0: Train Loss: 0.008389301598072052, Train Acc: 0.9976999759674072\n",
            "Epoch 7194/10000\n",
            "Step 0: Train Loss: 0.009008259512484074, Train Acc: 0.9977999925613403\n",
            "Epoch 7195/10000\n",
            "Step 0: Train Loss: 0.008490695618093014, Train Acc: 0.9980000257492065\n",
            "Epoch 7196/10000\n",
            "Step 0: Train Loss: 0.00808519683778286, Train Acc: 0.998199999332428\n",
            "Epoch 7197/10000\n",
            "Step 0: Train Loss: 0.00852909218519926, Train Acc: 0.9975000023841858\n",
            "Epoch 7198/10000\n",
            "Step 0: Train Loss: 0.010506294667720795, Train Acc: 0.9972000122070312\n",
            "Epoch 7199/10000\n",
            "Step 0: Train Loss: 0.008896422572433949, Train Acc: 0.9976000189781189\n",
            "Epoch 7200/10000\n",
            "Step 0: Train Loss: 0.010285903699696064, Train Acc: 0.9968000054359436\n",
            "Epoch 7201/10000\n",
            "Step 0: Train Loss: 0.006562724243849516, Train Acc: 0.9983999729156494\n",
            "Epoch 7202/10000\n",
            "Step 0: Train Loss: 0.008296985179185867, Train Acc: 0.9976000189781189\n",
            "Epoch 7203/10000\n",
            "Step 0: Train Loss: 0.007282997947186232, Train Acc: 0.9979000091552734\n",
            "Epoch 7204/10000\n",
            "Step 0: Train Loss: 0.008345291018486023, Train Acc: 0.9980999827384949\n",
            "Epoch 7205/10000\n",
            "Step 0: Train Loss: 0.0067457351833581924, Train Acc: 0.9979000091552734\n",
            "Epoch 7206/10000\n",
            "Step 0: Train Loss: 0.006223093252629042, Train Acc: 0.9984999895095825\n",
            "Epoch 7207/10000\n",
            "Step 0: Train Loss: 0.008112329058349133, Train Acc: 0.9980000257492065\n",
            "Epoch 7208/10000\n",
            "Step 0: Train Loss: 0.007535924669355154, Train Acc: 0.9979000091552734\n",
            "Epoch 7209/10000\n",
            "Step 0: Train Loss: 0.006461812648922205, Train Acc: 0.9984999895095825\n",
            "Epoch 7210/10000\n",
            "Step 0: Train Loss: 0.007122039794921875, Train Acc: 0.9980000257492065\n",
            "Epoch 7211/10000\n",
            "Step 0: Train Loss: 0.006881460547447205, Train Acc: 0.9980999827384949\n",
            "Epoch 7212/10000\n",
            "Step 0: Train Loss: 0.006598890759050846, Train Acc: 0.9979000091552734\n",
            "Epoch 7213/10000\n",
            "Step 0: Train Loss: 0.005266114603728056, Train Acc: 0.9984999895095825\n",
            "Epoch 7214/10000\n",
            "Step 0: Train Loss: 0.008732607588171959, Train Acc: 0.9977999925613403\n",
            "Epoch 7215/10000\n",
            "Step 0: Train Loss: 0.007539297454059124, Train Acc: 0.9980000257492065\n",
            "Epoch 7216/10000\n",
            "Step 0: Train Loss: 0.00733549427241087, Train Acc: 0.9983000159263611\n",
            "Epoch 7217/10000\n",
            "Step 0: Train Loss: 0.006541948765516281, Train Acc: 0.9983000159263611\n",
            "Epoch 7218/10000\n",
            "Step 0: Train Loss: 0.006248367950320244, Train Acc: 0.998199999332428\n",
            "Epoch 7219/10000\n",
            "Step 0: Train Loss: 0.007518829312175512, Train Acc: 0.9980999827384949\n",
            "Epoch 7220/10000\n",
            "Step 0: Train Loss: 0.0068125044927001, Train Acc: 0.9977999925613403\n",
            "Epoch 7221/10000\n",
            "Step 0: Train Loss: 0.0068322219885885715, Train Acc: 0.998199999332428\n",
            "Epoch 7222/10000\n",
            "Step 0: Train Loss: 0.0054710921831429005, Train Acc: 0.9984999895095825\n",
            "Epoch 7223/10000\n",
            "Step 0: Train Loss: 0.005811611190438271, Train Acc: 0.9983999729156494\n",
            "Epoch 7224/10000\n",
            "Step 0: Train Loss: 0.00563404755666852, Train Acc: 0.9988999962806702\n",
            "Epoch 7225/10000\n",
            "Step 0: Train Loss: 0.005630922503769398, Train Acc: 0.9987000226974487\n",
            "Epoch 7226/10000\n",
            "Step 0: Train Loss: 0.006293505895882845, Train Acc: 0.9983999729156494\n",
            "Epoch 7227/10000\n",
            "Step 0: Train Loss: 0.005081578157842159, Train Acc: 0.9991000294685364\n",
            "Epoch 7228/10000\n",
            "Step 0: Train Loss: 0.0067795319482684135, Train Acc: 0.9980000257492065\n",
            "Epoch 7229/10000\n",
            "Step 0: Train Loss: 0.006015308201313019, Train Acc: 0.9986000061035156\n",
            "Epoch 7230/10000\n",
            "Step 0: Train Loss: 0.00451301597058773, Train Acc: 0.9988999962806702\n",
            "Epoch 7231/10000\n",
            "Step 0: Train Loss: 0.004873990081250668, Train Acc: 0.9987999796867371\n",
            "Epoch 7232/10000\n",
            "Step 0: Train Loss: 0.006536509841680527, Train Acc: 0.9983999729156494\n",
            "Epoch 7233/10000\n",
            "Step 0: Train Loss: 0.004053350072354078, Train Acc: 0.9990000128746033\n",
            "Epoch 7234/10000\n",
            "Step 0: Train Loss: 0.006146454252302647, Train Acc: 0.9987000226974487\n",
            "Epoch 7235/10000\n",
            "Step 0: Train Loss: 0.004402469843626022, Train Acc: 0.9990000128746033\n",
            "Epoch 7236/10000\n",
            "Step 0: Train Loss: 0.004510227590799332, Train Acc: 0.9990000128746033\n",
            "Epoch 7237/10000\n",
            "Step 0: Train Loss: 0.004075893200933933, Train Acc: 0.9990000128746033\n",
            "Epoch 7238/10000\n",
            "Step 0: Train Loss: 0.0034657446667551994, Train Acc: 0.9993000030517578\n",
            "Epoch 7239/10000\n",
            "Step 0: Train Loss: 0.004519970156252384, Train Acc: 0.9988999962806702\n",
            "Epoch 7240/10000\n",
            "Step 0: Train Loss: 0.0057057724334299564, Train Acc: 0.9988999962806702\n",
            "Epoch 7241/10000\n",
            "Step 0: Train Loss: 0.0049652112647891045, Train Acc: 0.9984999895095825\n",
            "Epoch 7242/10000\n",
            "Step 0: Train Loss: 0.004748859442770481, Train Acc: 0.9987999796867371\n",
            "Epoch 7243/10000\n",
            "Step 0: Train Loss: 0.005151089746505022, Train Acc: 0.9987999796867371\n",
            "Epoch 7244/10000\n",
            "Step 0: Train Loss: 0.003054714063182473, Train Acc: 0.9997000098228455\n",
            "Epoch 7245/10000\n",
            "Step 0: Train Loss: 0.004663713742047548, Train Acc: 0.9991999864578247\n",
            "Epoch 7246/10000\n",
            "Step 0: Train Loss: 0.0040349457412958145, Train Acc: 0.9986000061035156\n",
            "Epoch 7247/10000\n",
            "Step 0: Train Loss: 0.0041684857569634914, Train Acc: 0.9983999729156494\n",
            "Epoch 7248/10000\n",
            "Step 0: Train Loss: 0.004702347330749035, Train Acc: 0.9987999796867371\n",
            "Epoch 7249/10000\n",
            "Step 0: Train Loss: 0.0032475795596837997, Train Acc: 0.9991999864578247\n",
            "Epoch 7250/10000\n",
            "Step 0: Train Loss: 0.004325623624026775, Train Acc: 0.9984999895095825\n",
            "Epoch 7251/10000\n",
            "Step 0: Train Loss: 0.003957971464842558, Train Acc: 0.9991000294685364\n",
            "Epoch 7252/10000\n",
            "Step 0: Train Loss: 0.003863838268443942, Train Acc: 0.9991000294685364\n",
            "Epoch 7253/10000\n",
            "Step 0: Train Loss: 0.004498099908232689, Train Acc: 0.9986000061035156\n",
            "Epoch 7254/10000\n",
            "Step 0: Train Loss: 0.004346396308392286, Train Acc: 0.9990000128746033\n",
            "Epoch 7255/10000\n",
            "Step 0: Train Loss: 0.0037202315870672464, Train Acc: 0.9991000294685364\n",
            "Epoch 7256/10000\n",
            "Step 0: Train Loss: 0.003167059039697051, Train Acc: 0.9991000294685364\n",
            "Epoch 7257/10000\n",
            "Step 0: Train Loss: 0.004454453010112047, Train Acc: 0.9991000294685364\n",
            "Epoch 7258/10000\n",
            "Step 0: Train Loss: 0.0033172371331602335, Train Acc: 0.9993000030517578\n",
            "Epoch 7259/10000\n",
            "Step 0: Train Loss: 0.003453055629506707, Train Acc: 0.9987999796867371\n",
            "Epoch 7260/10000\n",
            "Step 0: Train Loss: 0.004597092047333717, Train Acc: 0.9990000128746033\n",
            "Epoch 7261/10000\n",
            "Step 0: Train Loss: 0.003978656604886055, Train Acc: 0.9990000128746033\n",
            "Epoch 7262/10000\n",
            "Step 0: Train Loss: 0.004076977726072073, Train Acc: 0.9986000061035156\n",
            "Epoch 7263/10000\n",
            "Step 0: Train Loss: 0.0026753670535981655, Train Acc: 0.9994999766349792\n",
            "Epoch 7264/10000\n",
            "Step 0: Train Loss: 0.003945883829146624, Train Acc: 0.9990000128746033\n",
            "Epoch 7265/10000\n",
            "Step 0: Train Loss: 0.005277138203382492, Train Acc: 0.9987999796867371\n",
            "Epoch 7266/10000\n",
            "Step 0: Train Loss: 0.004729069769382477, Train Acc: 0.9984999895095825\n",
            "Epoch 7267/10000\n",
            "Step 0: Train Loss: 0.0035980406682938337, Train Acc: 0.9991000294685364\n",
            "Epoch 7268/10000\n",
            "Step 0: Train Loss: 0.004765999037772417, Train Acc: 0.9987000226974487\n",
            "Epoch 7269/10000\n",
            "Step 0: Train Loss: 0.0035138484090566635, Train Acc: 0.9991999864578247\n",
            "Epoch 7270/10000\n",
            "Step 0: Train Loss: 0.002725905040279031, Train Acc: 0.9994999766349792\n",
            "Epoch 7271/10000\n",
            "Step 0: Train Loss: 0.0025215016212314367, Train Acc: 0.9995999932289124\n",
            "Epoch 7272/10000\n",
            "Step 0: Train Loss: 0.0040703569538891315, Train Acc: 0.9987000226974487\n",
            "Epoch 7273/10000\n",
            "Step 0: Train Loss: 0.004938640631735325, Train Acc: 0.9983999729156494\n",
            "Epoch 7274/10000\n",
            "Step 0: Train Loss: 0.004991625435650349, Train Acc: 0.9984999895095825\n",
            "Epoch 7275/10000\n",
            "Step 0: Train Loss: 0.0036377403885126114, Train Acc: 0.9991999864578247\n",
            "Epoch 7276/10000\n",
            "Step 0: Train Loss: 0.004504607990384102, Train Acc: 0.9987000226974487\n",
            "Epoch 7277/10000\n",
            "Step 0: Train Loss: 0.0027906212490051985, Train Acc: 0.9994999766349792\n",
            "Epoch 7278/10000\n",
            "Step 0: Train Loss: 0.004483900032937527, Train Acc: 0.9986000061035156\n",
            "Epoch 7279/10000\n",
            "Step 0: Train Loss: 0.0034715267829596996, Train Acc: 0.9991999864578247\n",
            "Epoch 7280/10000\n",
            "Step 0: Train Loss: 0.0035378336906433105, Train Acc: 0.9991000294685364\n",
            "Epoch 7281/10000\n",
            "Step 0: Train Loss: 0.0039016478694975376, Train Acc: 0.9990000128746033\n",
            "Epoch 7282/10000\n",
            "Step 0: Train Loss: 0.0037715451326221228, Train Acc: 0.9991000294685364\n",
            "Epoch 7283/10000\n",
            "Step 0: Train Loss: 0.0026755149010568857, Train Acc: 0.9993000030517578\n",
            "Epoch 7284/10000\n",
            "Step 0: Train Loss: 0.003483452135697007, Train Acc: 0.9988999962806702\n",
            "Epoch 7285/10000\n",
            "Step 0: Train Loss: 0.003478649538010359, Train Acc: 0.9993000030517578\n",
            "Epoch 7286/10000\n",
            "Step 0: Train Loss: 0.00286180735565722, Train Acc: 0.9993000030517578\n",
            "Epoch 7287/10000\n",
            "Step 0: Train Loss: 0.0035523544065654278, Train Acc: 0.9990000128746033\n",
            "Epoch 7288/10000\n",
            "Step 0: Train Loss: 0.0036553475074470043, Train Acc: 0.9991999864578247\n",
            "Epoch 7289/10000\n",
            "Step 0: Train Loss: 0.0039735836908221245, Train Acc: 0.9990000128746033\n",
            "Epoch 7290/10000\n",
            "Step 0: Train Loss: 0.0028895323630422354, Train Acc: 0.9991000294685364\n",
            "Epoch 7291/10000\n",
            "Step 0: Train Loss: 0.002405486535280943, Train Acc: 0.9994000196456909\n",
            "Epoch 7292/10000\n",
            "Step 0: Train Loss: 0.003232101909816265, Train Acc: 0.9991999864578247\n",
            "Epoch 7293/10000\n",
            "Step 0: Train Loss: 0.0034740532282739878, Train Acc: 0.9987999796867371\n",
            "Epoch 7294/10000\n",
            "Step 0: Train Loss: 0.0030204616487026215, Train Acc: 0.9990000128746033\n",
            "Epoch 7295/10000\n",
            "Step 0: Train Loss: 0.003486936679109931, Train Acc: 0.9990000128746033\n",
            "Epoch 7296/10000\n",
            "Step 0: Train Loss: 0.003555161179974675, Train Acc: 0.9988999962806702\n",
            "Epoch 7297/10000\n",
            "Step 0: Train Loss: 0.003288465552031994, Train Acc: 0.9991999864578247\n",
            "Epoch 7298/10000\n",
            "Step 0: Train Loss: 0.002595530590042472, Train Acc: 0.9995999932289124\n",
            "Epoch 7299/10000\n",
            "Step 0: Train Loss: 0.0031874908600002527, Train Acc: 0.9994000196456909\n",
            "Epoch 7300/10000\n",
            "Step 0: Train Loss: 0.004152725916355848, Train Acc: 0.9987000226974487\n",
            "Epoch 7301/10000\n",
            "Step 0: Train Loss: 0.0026606135070323944, Train Acc: 0.9994000196456909\n",
            "Epoch 7302/10000\n",
            "Step 0: Train Loss: 0.0025908646639436483, Train Acc: 0.9991000294685364\n",
            "Epoch 7303/10000\n",
            "Step 0: Train Loss: 0.003044630168005824, Train Acc: 0.9990000128746033\n",
            "Epoch 7304/10000\n",
            "Step 0: Train Loss: 0.0033846916630864143, Train Acc: 0.9994000196456909\n",
            "Epoch 7305/10000\n",
            "Step 0: Train Loss: 0.0024787024594843388, Train Acc: 0.9993000030517578\n",
            "Epoch 7306/10000\n",
            "Step 0: Train Loss: 0.0027572407852858305, Train Acc: 0.9991999864578247\n",
            "Epoch 7307/10000\n",
            "Step 0: Train Loss: 0.0023267760407179594, Train Acc: 0.9994000196456909\n",
            "Epoch 7308/10000\n",
            "Step 0: Train Loss: 0.0034155594184994698, Train Acc: 0.9994000196456909\n",
            "Epoch 7309/10000\n",
            "Step 0: Train Loss: 0.0029720787424594164, Train Acc: 0.9991999864578247\n",
            "Epoch 7310/10000\n",
            "Step 0: Train Loss: 0.002290574135258794, Train Acc: 0.9995999932289124\n",
            "Epoch 7311/10000\n",
            "Step 0: Train Loss: 0.0022228695452213287, Train Acc: 0.9993000030517578\n",
            "Epoch 7312/10000\n",
            "Step 0: Train Loss: 0.002349447924643755, Train Acc: 0.9991000294685364\n",
            "Epoch 7313/10000\n",
            "Step 0: Train Loss: 0.002364282263442874, Train Acc: 0.9993000030517578\n",
            "Epoch 7314/10000\n",
            "Step 0: Train Loss: 0.0023929306771606207, Train Acc: 0.9994000196456909\n",
            "Epoch 7315/10000\n",
            "Step 0: Train Loss: 0.0023176916874945164, Train Acc: 0.9995999932289124\n",
            "Epoch 7316/10000\n",
            "Step 0: Train Loss: 0.0031387722119688988, Train Acc: 0.9991999864578247\n",
            "Epoch 7317/10000\n",
            "Step 0: Train Loss: 0.0036786808632314205, Train Acc: 0.9988999962806702\n",
            "Epoch 7318/10000\n",
            "Step 0: Train Loss: 0.0022626265417784452, Train Acc: 0.9994000196456909\n",
            "Epoch 7319/10000\n",
            "Step 0: Train Loss: 0.0024254962336272, Train Acc: 0.9991000294685364\n",
            "Epoch 7320/10000\n",
            "Step 0: Train Loss: 0.0025220380630344152, Train Acc: 0.9991999864578247\n",
            "Epoch 7321/10000\n",
            "Step 0: Train Loss: 0.0028558087069541216, Train Acc: 0.9991999864578247\n",
            "Epoch 7322/10000\n",
            "Step 0: Train Loss: 0.0025379620492458344, Train Acc: 0.9993000030517578\n",
            "Epoch 7323/10000\n",
            "Step 0: Train Loss: 0.0028799022547900677, Train Acc: 0.9993000030517578\n",
            "Epoch 7324/10000\n",
            "Step 0: Train Loss: 0.0015340314712375402, Train Acc: 0.9994999766349792\n",
            "Epoch 7325/10000\n",
            "Step 0: Train Loss: 0.0029001059010624886, Train Acc: 0.9993000030517578\n",
            "Epoch 7326/10000\n",
            "Step 0: Train Loss: 0.002293278928846121, Train Acc: 0.9994000196456909\n",
            "Epoch 7327/10000\n",
            "Step 0: Train Loss: 0.002957443008199334, Train Acc: 0.9990000128746033\n",
            "Epoch 7328/10000\n",
            "Step 0: Train Loss: 0.0025431360118091106, Train Acc: 0.9994999766349792\n",
            "Epoch 7329/10000\n",
            "Step 0: Train Loss: 0.002541300607845187, Train Acc: 0.9991999864578247\n",
            "Epoch 7330/10000\n",
            "Step 0: Train Loss: 0.0022902966011315584, Train Acc: 0.9994000196456909\n",
            "Epoch 7331/10000\n",
            "Step 0: Train Loss: 0.0027911325450986624, Train Acc: 0.9991000294685364\n",
            "Epoch 7332/10000\n",
            "Step 0: Train Loss: 0.00173426594119519, Train Acc: 0.9994999766349792\n",
            "Epoch 7333/10000\n",
            "Step 0: Train Loss: 0.0017603865126147866, Train Acc: 0.9995999932289124\n",
            "Epoch 7334/10000\n",
            "Step 0: Train Loss: 0.002136436989530921, Train Acc: 0.9994000196456909\n",
            "Epoch 7335/10000\n",
            "Step 0: Train Loss: 0.0017652923706918955, Train Acc: 0.9995999932289124\n",
            "Epoch 7336/10000\n",
            "Step 0: Train Loss: 0.0024677731562405825, Train Acc: 0.9995999932289124\n",
            "Epoch 7337/10000\n",
            "Step 0: Train Loss: 0.0022958088666200638, Train Acc: 0.9991000294685364\n",
            "Epoch 7338/10000\n",
            "Step 0: Train Loss: 0.0025212890468537807, Train Acc: 0.9991999864578247\n",
            "Epoch 7339/10000\n",
            "Step 0: Train Loss: 0.001936139538884163, Train Acc: 0.9998000264167786\n",
            "Epoch 7340/10000\n",
            "Step 0: Train Loss: 0.003323338693007827, Train Acc: 0.9990000128746033\n",
            "Epoch 7341/10000\n",
            "Step 0: Train Loss: 0.002454054309055209, Train Acc: 0.9991999864578247\n",
            "Epoch 7342/10000\n",
            "Step 0: Train Loss: 0.002590900519862771, Train Acc: 0.9991999864578247\n",
            "Epoch 7343/10000\n",
            "Step 0: Train Loss: 0.0026981928385794163, Train Acc: 0.9990000128746033\n",
            "Epoch 7344/10000\n",
            "Step 0: Train Loss: 0.0022813223768025637, Train Acc: 0.9991999864578247\n",
            "Epoch 7345/10000\n",
            "Step 0: Train Loss: 0.001623480929993093, Train Acc: 0.9994999766349792\n",
            "Epoch 7346/10000\n",
            "Step 0: Train Loss: 0.0024918965063989162, Train Acc: 0.9991999864578247\n",
            "Epoch 7347/10000\n",
            "Step 0: Train Loss: 0.0021661643404513597, Train Acc: 0.9994000196456909\n",
            "Epoch 7348/10000\n",
            "Step 0: Train Loss: 0.0018122043693438172, Train Acc: 0.9995999932289124\n",
            "Epoch 7349/10000\n",
            "Step 0: Train Loss: 0.001813129405491054, Train Acc: 0.9998000264167786\n",
            "Epoch 7350/10000\n",
            "Step 0: Train Loss: 0.00196184404194355, Train Acc: 0.9995999932289124\n",
            "Epoch 7351/10000\n",
            "Step 0: Train Loss: 0.0022994072642177343, Train Acc: 0.9993000030517578\n",
            "Epoch 7352/10000\n",
            "Step 0: Train Loss: 0.001858544535934925, Train Acc: 0.9994999766349792\n",
            "Epoch 7353/10000\n",
            "Step 0: Train Loss: 0.0015965367201715708, Train Acc: 0.9994000196456909\n",
            "Epoch 7354/10000\n",
            "Step 0: Train Loss: 0.0014982754364609718, Train Acc: 0.9995999932289124\n",
            "Epoch 7355/10000\n",
            "Step 0: Train Loss: 0.002251504687592387, Train Acc: 0.9993000030517578\n",
            "Epoch 7356/10000\n",
            "Step 0: Train Loss: 0.0018231450812891126, Train Acc: 0.9995999932289124\n",
            "Epoch 7357/10000\n",
            "Step 0: Train Loss: 0.0014854709152132273, Train Acc: 0.9994999766349792\n",
            "Epoch 7358/10000\n",
            "Step 0: Train Loss: 0.002808265620842576, Train Acc: 0.9991999864578247\n",
            "Epoch 7359/10000\n",
            "Step 0: Train Loss: 0.002529327291995287, Train Acc: 0.9993000030517578\n",
            "Epoch 7360/10000\n",
            "Step 0: Train Loss: 0.0022206453140825033, Train Acc: 0.9994999766349792\n",
            "Epoch 7361/10000\n",
            "Step 0: Train Loss: 0.0013212304329499602, Train Acc: 0.9997000098228455\n",
            "Epoch 7362/10000\n",
            "Step 0: Train Loss: 0.001713830977678299, Train Acc: 0.9998000264167786\n",
            "Epoch 7363/10000\n",
            "Step 0: Train Loss: 0.002399860182777047, Train Acc: 0.9994000196456909\n",
            "Epoch 7364/10000\n",
            "Step 0: Train Loss: 0.001795626012608409, Train Acc: 0.9994000196456909\n",
            "Epoch 7365/10000\n",
            "Step 0: Train Loss: 0.002254818333312869, Train Acc: 0.9994000196456909\n",
            "Epoch 7366/10000\n",
            "Step 0: Train Loss: 0.00240373145788908, Train Acc: 0.9993000030517578\n",
            "Epoch 7367/10000\n",
            "Step 0: Train Loss: 0.002151172375306487, Train Acc: 0.9993000030517578\n",
            "Epoch 7368/10000\n",
            "Step 0: Train Loss: 0.0016883315984159708, Train Acc: 0.9994000196456909\n",
            "Epoch 7369/10000\n",
            "Step 0: Train Loss: 0.0013141538947820663, Train Acc: 0.9998000264167786\n",
            "Epoch 7370/10000\n",
            "Step 0: Train Loss: 0.0008598255226388574, Train Acc: 1.0\n",
            "Epoch 7371/10000\n",
            "Step 0: Train Loss: 0.0014442495303228498, Train Acc: 0.9997000098228455\n",
            "Epoch 7372/10000\n",
            "Step 0: Train Loss: 0.0028239504899829626, Train Acc: 0.9991999864578247\n",
            "Epoch 7373/10000\n",
            "Step 0: Train Loss: 0.0018515927949920297, Train Acc: 0.9995999932289124\n",
            "Epoch 7374/10000\n",
            "Step 0: Train Loss: 0.0010577596258372068, Train Acc: 0.9998000264167786\n",
            "Epoch 7375/10000\n",
            "Step 0: Train Loss: 0.00211323075927794, Train Acc: 0.9994999766349792\n",
            "Epoch 7376/10000\n",
            "Step 0: Train Loss: 0.001384193659760058, Train Acc: 0.9998000264167786\n",
            "Epoch 7377/10000\n",
            "Step 0: Train Loss: 0.0020706024952232838, Train Acc: 0.9994000196456909\n",
            "Epoch 7378/10000\n",
            "Step 0: Train Loss: 0.002255517290905118, Train Acc: 0.9995999932289124\n",
            "Epoch 7379/10000\n",
            "Step 0: Train Loss: 0.0021246010437607765, Train Acc: 0.9995999932289124\n",
            "Epoch 7380/10000\n",
            "Step 0: Train Loss: 0.0012990257237106562, Train Acc: 0.9998000264167786\n",
            "Epoch 7381/10000\n",
            "Step 0: Train Loss: 0.001601035357452929, Train Acc: 0.9995999932289124\n",
            "Epoch 7382/10000\n",
            "Step 0: Train Loss: 0.0017651289235800505, Train Acc: 0.9997000098228455\n",
            "Epoch 7383/10000\n",
            "Step 0: Train Loss: 0.0021081208251416683, Train Acc: 0.9995999932289124\n",
            "Epoch 7384/10000\n",
            "Step 0: Train Loss: 0.0015257259365171194, Train Acc: 0.9997000098228455\n",
            "Epoch 7385/10000\n",
            "Step 0: Train Loss: 0.001137545215897262, Train Acc: 0.9998999834060669\n",
            "Epoch 7386/10000\n",
            "Step 0: Train Loss: 0.002365908119827509, Train Acc: 0.9994000196456909\n",
            "Epoch 7387/10000\n",
            "Step 0: Train Loss: 0.0013273496879264712, Train Acc: 0.9998000264167786\n",
            "Epoch 7388/10000\n",
            "Step 0: Train Loss: 0.0011766183888539672, Train Acc: 0.9998999834060669\n",
            "Epoch 7389/10000\n",
            "Step 0: Train Loss: 0.0013963988749310374, Train Acc: 0.9998000264167786\n",
            "Epoch 7390/10000\n",
            "Step 0: Train Loss: 0.0019030352123081684, Train Acc: 0.9994999766349792\n",
            "Epoch 7391/10000\n",
            "Step 0: Train Loss: 0.001421758788637817, Train Acc: 0.9997000098228455\n",
            "Epoch 7392/10000\n",
            "Step 0: Train Loss: 0.0011428355937823653, Train Acc: 0.9998000264167786\n",
            "Epoch 7393/10000\n",
            "Step 0: Train Loss: 0.0012073072139173746, Train Acc: 0.9995999932289124\n",
            "Epoch 7394/10000\n",
            "Step 0: Train Loss: 0.0016680139815434813, Train Acc: 0.9994999766349792\n",
            "Epoch 7395/10000\n",
            "Step 0: Train Loss: 0.0015211323043331504, Train Acc: 0.9997000098228455\n",
            "Epoch 7396/10000\n",
            "Step 0: Train Loss: 0.0015241678338497877, Train Acc: 0.9997000098228455\n",
            "Epoch 7397/10000\n",
            "Step 0: Train Loss: 0.0013062539510428905, Train Acc: 0.9997000098228455\n",
            "Epoch 7398/10000\n",
            "Step 0: Train Loss: 0.0013295805547386408, Train Acc: 0.9998000264167786\n",
            "Epoch 7399/10000\n",
            "Step 0: Train Loss: 0.0021623806096613407, Train Acc: 0.9993000030517578\n",
            "Epoch 7400/10000\n",
            "Step 0: Train Loss: 0.0011604229221120477, Train Acc: 0.9998000264167786\n",
            "Epoch 7401/10000\n",
            "Step 0: Train Loss: 0.001733416342176497, Train Acc: 0.9997000098228455\n",
            "Epoch 7402/10000\n",
            "Step 0: Train Loss: 0.0011626484338194132, Train Acc: 0.9997000098228455\n",
            "Epoch 7403/10000\n",
            "Step 0: Train Loss: 0.0012733531184494495, Train Acc: 0.9997000098228455\n",
            "Epoch 7404/10000\n",
            "Step 0: Train Loss: 0.0022523466031998396, Train Acc: 0.9994000196456909\n",
            "Epoch 7405/10000\n",
            "Step 0: Train Loss: 0.001486230525188148, Train Acc: 0.9995999932289124\n",
            "Epoch 7406/10000\n",
            "Step 0: Train Loss: 0.0009293717448599637, Train Acc: 0.9998999834060669\n",
            "Epoch 7407/10000\n",
            "Step 0: Train Loss: 0.0012729830341413617, Train Acc: 0.9998000264167786\n",
            "Epoch 7408/10000\n",
            "Step 0: Train Loss: 0.0009731923346407712, Train Acc: 0.9998000264167786\n",
            "Epoch 7409/10000\n",
            "Step 0: Train Loss: 0.001320588868111372, Train Acc: 0.9997000098228455\n",
            "Epoch 7410/10000\n",
            "Step 0: Train Loss: 0.001272776979021728, Train Acc: 0.9998000264167786\n",
            "Epoch 7411/10000\n",
            "Step 0: Train Loss: 0.001598553266376257, Train Acc: 0.9994999766349792\n",
            "Epoch 7412/10000\n",
            "Step 0: Train Loss: 0.0011091866763308644, Train Acc: 0.9997000098228455\n",
            "Epoch 7413/10000\n",
            "Step 0: Train Loss: 0.0012372351484373212, Train Acc: 0.9997000098228455\n",
            "Epoch 7414/10000\n",
            "Step 0: Train Loss: 0.0015034251846373081, Train Acc: 0.9997000098228455\n",
            "Epoch 7415/10000\n",
            "Step 0: Train Loss: 0.0022025294601917267, Train Acc: 0.9994000196456909\n",
            "Epoch 7416/10000\n",
            "Step 0: Train Loss: 0.0011621484300121665, Train Acc: 0.9998000264167786\n",
            "Epoch 7417/10000\n",
            "Step 0: Train Loss: 0.0009502082830294967, Train Acc: 0.9998999834060669\n",
            "Epoch 7418/10000\n",
            "Step 0: Train Loss: 0.0014514398062601686, Train Acc: 0.9994000196456909\n",
            "Epoch 7419/10000\n",
            "Step 0: Train Loss: 0.0012273147003725171, Train Acc: 0.9997000098228455\n",
            "Epoch 7420/10000\n",
            "Step 0: Train Loss: 0.0008100633858703077, Train Acc: 0.9998999834060669\n",
            "Epoch 7421/10000\n",
            "Step 0: Train Loss: 0.0012370115146040916, Train Acc: 0.9997000098228455\n",
            "Epoch 7422/10000\n",
            "Step 0: Train Loss: 0.001858362928032875, Train Acc: 0.9994999766349792\n",
            "Epoch 7423/10000\n",
            "Step 0: Train Loss: 0.0012930697994306684, Train Acc: 0.9997000098228455\n",
            "Epoch 7424/10000\n",
            "Step 0: Train Loss: 0.0013137992937117815, Train Acc: 0.9997000098228455\n",
            "Epoch 7425/10000\n",
            "Step 0: Train Loss: 0.0014723115600645542, Train Acc: 0.9995999932289124\n",
            "Epoch 7426/10000\n",
            "Step 0: Train Loss: 0.0007131953025236726, Train Acc: 1.0\n",
            "Epoch 7427/10000\n",
            "Step 0: Train Loss: 0.0014110569609329104, Train Acc: 0.9997000098228455\n",
            "Epoch 7428/10000\n",
            "Step 0: Train Loss: 0.0013199102832004428, Train Acc: 0.9994999766349792\n",
            "Epoch 7429/10000\n",
            "Step 0: Train Loss: 0.0022001920733600855, Train Acc: 0.9994999766349792\n",
            "Epoch 7430/10000\n",
            "Step 0: Train Loss: 0.0008788261329755187, Train Acc: 0.9998000264167786\n",
            "Epoch 7431/10000\n",
            "Step 0: Train Loss: 0.0009500457672402263, Train Acc: 0.9998000264167786\n",
            "Epoch 7432/10000\n",
            "Step 0: Train Loss: 0.0010319423163309693, Train Acc: 0.9997000098228455\n",
            "Epoch 7433/10000\n",
            "Step 0: Train Loss: 0.0011173278326168656, Train Acc: 0.9998000264167786\n",
            "Epoch 7434/10000\n",
            "Step 0: Train Loss: 0.0011041638208553195, Train Acc: 0.9998000264167786\n",
            "Epoch 7435/10000\n",
            "Step 0: Train Loss: 0.0015542220789939165, Train Acc: 0.9995999932289124\n",
            "Epoch 7436/10000\n",
            "Step 0: Train Loss: 0.0011540644336491823, Train Acc: 0.9995999932289124\n",
            "Epoch 7437/10000\n",
            "Step 0: Train Loss: 0.0008587626507505774, Train Acc: 0.9998999834060669\n",
            "Epoch 7438/10000\n",
            "Step 0: Train Loss: 0.0010758418356999755, Train Acc: 0.9998999834060669\n",
            "Epoch 7439/10000\n",
            "Step 0: Train Loss: 0.0014036684297025204, Train Acc: 0.9995999932289124\n",
            "Epoch 7440/10000\n",
            "Step 0: Train Loss: 0.0010932604782283306, Train Acc: 0.9997000098228455\n",
            "Epoch 7441/10000\n",
            "Step 0: Train Loss: 0.0011650958331301808, Train Acc: 0.9997000098228455\n",
            "Epoch 7442/10000\n",
            "Step 0: Train Loss: 0.001271408749744296, Train Acc: 0.9998000264167786\n",
            "Epoch 7443/10000\n",
            "Step 0: Train Loss: 0.0008576706750318408, Train Acc: 0.9998999834060669\n",
            "Epoch 7444/10000\n",
            "Step 0: Train Loss: 0.0009796592639759183, Train Acc: 0.9998000264167786\n",
            "Epoch 7445/10000\n",
            "Step 0: Train Loss: 0.0016497558681294322, Train Acc: 0.9995999932289124\n",
            "Epoch 7446/10000\n",
            "Step 0: Train Loss: 0.002302800305187702, Train Acc: 0.9991000294685364\n",
            "Epoch 7447/10000\n",
            "Step 0: Train Loss: 0.0012795687653124332, Train Acc: 0.9997000098228455\n",
            "Epoch 7448/10000\n",
            "Step 0: Train Loss: 0.0009996813023462892, Train Acc: 0.9998000264167786\n",
            "Epoch 7449/10000\n",
            "Step 0: Train Loss: 0.0016645101131871343, Train Acc: 0.9995999932289124\n",
            "Epoch 7450/10000\n",
            "Step 0: Train Loss: 0.001966046169400215, Train Acc: 0.9994000196456909\n",
            "Epoch 7451/10000\n",
            "Step 0: Train Loss: 0.010272950865328312, Train Acc: 0.9977999925613403\n",
            "Epoch 7452/10000\n",
            "Step 0: Train Loss: 0.003987117204815149, Train Acc: 0.9990000128746033\n",
            "Epoch 7453/10000\n",
            "Step 0: Train Loss: 0.004754840862005949, Train Acc: 0.9991000294685364\n",
            "Epoch 7454/10000\n",
            "Step 0: Train Loss: 0.004406655207276344, Train Acc: 0.9991000294685364\n",
            "Epoch 7455/10000\n",
            "Step 0: Train Loss: 0.00281340884976089, Train Acc: 0.9993000030517578\n",
            "Epoch 7456/10000\n",
            "Step 0: Train Loss: 0.0038876552134752274, Train Acc: 0.9991000294685364\n",
            "Epoch 7457/10000\n",
            "Step 0: Train Loss: 0.002117778407409787, Train Acc: 0.9998000264167786\n",
            "Epoch 7458/10000\n",
            "Step 0: Train Loss: 0.0027429191395640373, Train Acc: 0.9994999766349792\n",
            "Epoch 7459/10000\n",
            "Step 0: Train Loss: 0.001522158388979733, Train Acc: 0.9997000098228455\n",
            "Epoch 7460/10000\n",
            "Step 0: Train Loss: 0.0024625761434435844, Train Acc: 0.9993000030517578\n",
            "Epoch 7461/10000\n",
            "Step 0: Train Loss: 0.0016323599265888333, Train Acc: 0.9998000264167786\n",
            "Epoch 7462/10000\n",
            "Step 0: Train Loss: 0.002467944985255599, Train Acc: 0.9994000196456909\n",
            "Epoch 7463/10000\n",
            "Step 0: Train Loss: 0.001591560896486044, Train Acc: 0.9995999932289124\n",
            "Epoch 7464/10000\n",
            "Step 0: Train Loss: 0.0014536493690684438, Train Acc: 0.9997000098228455\n",
            "Epoch 7465/10000\n",
            "Step 0: Train Loss: 0.0008146156324073672, Train Acc: 1.0\n",
            "Epoch 7466/10000\n",
            "Step 0: Train Loss: 0.0008557306136935949, Train Acc: 1.0\n",
            "Epoch 7467/10000\n",
            "Step 0: Train Loss: 0.0010925524402409792, Train Acc: 0.9998000264167786\n",
            "Epoch 7468/10000\n",
            "Step 0: Train Loss: 0.0015805951552465558, Train Acc: 0.9995999932289124\n",
            "Epoch 7469/10000\n",
            "Step 0: Train Loss: 0.0011969058541581035, Train Acc: 0.9997000098228455\n",
            "Epoch 7470/10000\n",
            "Step 0: Train Loss: 0.0008784253150224686, Train Acc: 0.9998999834060669\n",
            "Epoch 7471/10000\n",
            "Step 0: Train Loss: 0.001058516907505691, Train Acc: 0.9998000264167786\n",
            "Epoch 7472/10000\n",
            "Step 0: Train Loss: 0.0009293421753682196, Train Acc: 0.9998000264167786\n",
            "Epoch 7473/10000\n",
            "Step 0: Train Loss: 0.0008736206218600273, Train Acc: 0.9998000264167786\n",
            "Epoch 7474/10000\n",
            "Step 0: Train Loss: 0.0008094387012533844, Train Acc: 0.9998000264167786\n",
            "Epoch 7475/10000\n",
            "Step 0: Train Loss: 0.000871249649208039, Train Acc: 0.9997000098228455\n",
            "Epoch 7476/10000\n",
            "Step 0: Train Loss: 0.0011070483596995473, Train Acc: 0.9997000098228455\n",
            "Epoch 7477/10000\n",
            "Step 0: Train Loss: 0.0011615261901170015, Train Acc: 0.9998000264167786\n",
            "Epoch 7478/10000\n",
            "Step 0: Train Loss: 0.0009692222811281681, Train Acc: 0.9998999834060669\n",
            "Epoch 7479/10000\n",
            "Step 0: Train Loss: 0.0006559679168276489, Train Acc: 0.9998999834060669\n",
            "Epoch 7480/10000\n",
            "Step 0: Train Loss: 0.0011546415043994784, Train Acc: 0.9998000264167786\n",
            "Epoch 7481/10000\n",
            "Step 0: Train Loss: 0.00177336810156703, Train Acc: 0.9994999766349792\n",
            "Epoch 7482/10000\n",
            "Step 0: Train Loss: 0.0006710186717100441, Train Acc: 0.9998999834060669\n",
            "Epoch 7483/10000\n",
            "Step 0: Train Loss: 0.0009530208772048354, Train Acc: 0.9998000264167786\n",
            "Epoch 7484/10000\n",
            "Step 0: Train Loss: 0.0009620880009606481, Train Acc: 0.9998000264167786\n",
            "Epoch 7485/10000\n",
            "Step 0: Train Loss: 0.0011570133501663804, Train Acc: 0.9998000264167786\n",
            "Epoch 7486/10000\n",
            "Step 0: Train Loss: 0.0012225204845890403, Train Acc: 0.9998000264167786\n",
            "Epoch 7487/10000\n",
            "Step 0: Train Loss: 0.0015597783494740725, Train Acc: 0.9997000098228455\n",
            "Epoch 7488/10000\n",
            "Step 0: Train Loss: 0.0007644778816029429, Train Acc: 0.9998000264167786\n",
            "Epoch 7489/10000\n",
            "Step 0: Train Loss: 0.0012914142571389675, Train Acc: 0.9997000098228455\n",
            "Epoch 7490/10000\n",
            "Step 0: Train Loss: 0.0009933748515322804, Train Acc: 0.9998000264167786\n",
            "Epoch 7491/10000\n",
            "Step 0: Train Loss: 0.0006413741502910852, Train Acc: 0.9998999834060669\n",
            "Epoch 7492/10000\n",
            "Step 0: Train Loss: 0.0012662529479712248, Train Acc: 0.9995999932289124\n",
            "Epoch 7493/10000\n",
            "Step 0: Train Loss: 0.0011006318964064121, Train Acc: 0.9997000098228455\n",
            "Epoch 7494/10000\n",
            "Step 0: Train Loss: 0.0008820217335596681, Train Acc: 0.9998999834060669\n",
            "Epoch 7495/10000\n",
            "Step 0: Train Loss: 0.0004958563367836177, Train Acc: 1.0\n",
            "Epoch 7496/10000\n",
            "Step 0: Train Loss: 0.0006052990211173892, Train Acc: 0.9998999834060669\n",
            "Epoch 7497/10000\n",
            "Step 0: Train Loss: 0.0008305225637741387, Train Acc: 0.9998999834060669\n",
            "Epoch 7498/10000\n",
            "Step 0: Train Loss: 0.0008381782681681216, Train Acc: 0.9998000264167786\n",
            "Epoch 7499/10000\n",
            "Step 0: Train Loss: 0.0009815103840082884, Train Acc: 0.9998000264167786\n",
            "Epoch 7500/10000\n",
            "Step 0: Train Loss: 0.0010640450054779649, Train Acc: 0.9995999932289124\n",
            "Epoch 7501/10000\n",
            "Step 0: Train Loss: 0.001551070250570774, Train Acc: 0.9994999766349792\n",
            "Epoch index and hidden dimension and ratio: 7500 1024 0.8977971770555864\n",
            "Epoch index and hidden dimension and ratio: 7500 20 0.7565338287643161\n",
            "Epoch index and hidden dimension and ratio: 7500 20 1.4543891789912562\n",
            "Epoch index and hidden dimension and ratio: 7500 20 4.316958840833306\n",
            "MI(X;T): [10.074773356169317, 6.655453952780919, 5.2714638226722474, 1.9072711203518753], MI(Y;T): [2.1942449995870446, 3.2252382464326317, 3.0125544197786467, 2.7436108209951606]\n",
            "Epoch index and hidden dimension and ratio: 7500 1024 0.8978487932708902\n",
            "Epoch index and hidden dimension and ratio: 7500 20 0.7565851950867977\n",
            "Epoch index and hidden dimension and ratio: 7500 20 1.4546932468993325\n",
            "Epoch index and hidden dimension and ratio: 7500 20 4.3175580149699755\n",
            "MI(X;T): [10.073662270635472, 6.657419289840628, 5.270854457108961, 1.9075009811264232], MI(Y;T): [2.1938770453015946, 3.2259376954779952, 3.012248188448386, 2.7432262993510825]\n",
            "Epoch index and hidden dimension and ratio: 7500 1024 0.8978871199627658\n",
            "Epoch index and hidden dimension and ratio: 7500 20 0.7566252250192068\n",
            "Epoch index and hidden dimension and ratio: 7500 20 1.4549075544687826\n",
            "Epoch index and hidden dimension and ratio: 7500 20 4.317941341641706\n",
            "MI(X;T): [10.073760606951815, 6.658980977139754, 5.268885080214692, 1.9072224560226945], MI(Y;T): [2.1935148813752408, 3.2263248550327157, 3.012074854137977, 2.7430688152007434]\n",
            "Epoch index and hidden dimension and ratio: 7500 1024 0.8979136255868407\n",
            "Epoch index and hidden dimension and ratio: 7500 20 0.7566890667948781\n",
            "Epoch index and hidden dimension and ratio: 7500 20 1.4551137366327558\n",
            "Epoch index and hidden dimension and ratio: 7500 20 4.318216744580966\n",
            "MI(X;T): [10.073079357784106, 6.656577674466378, 5.268103801609062, 1.9077077104147735], MI(Y;T): [2.19343802968797, 3.2261461035862786, 3.012472297163633, 2.743221126324976]\n",
            "Epoch index and hidden dimension and ratio: 7500 1024 0.8979050351214204\n",
            "Epoch index and hidden dimension and ratio: 7500 20 0.7567263304311451\n",
            "Epoch index and hidden dimension and ratio: 7500 20 1.4552040048092243\n",
            "Epoch index and hidden dimension and ratio: 7500 20 4.318269390304122\n",
            "MI(X;T): [10.074080068616716, 6.656380470943491, 5.265405122855643, 1.9073730503472537], MI(Y;T): [2.1940498090573506, 3.226039175124664, 3.0118617865314965, 2.743371293945085]\n",
            "Epoch index and hidden dimension and ratio: 7500 1024 0.8978911582157413\n",
            "Epoch index and hidden dimension and ratio: 7500 20 0.756736961686907\n",
            "Epoch index and hidden dimension and ratio: 7500 20 1.4551433181870697\n",
            "Epoch index and hidden dimension and ratio: 7500 20 4.317889353990089\n",
            "MI(X;T): [10.074979279779082, 6.655927254904265, 5.265154942565553, 1.906937666957921], MI(Y;T): [2.194475271598092, 3.2255902509632772, 3.0120006838425333, 2.743615666095799]\n",
            "Epoch 7502/10000\n",
            "Step 0: Train Loss: 0.0011994687374681234, Train Acc: 0.9997000098228455\n",
            "Epoch 7503/10000\n",
            "Step 0: Train Loss: 0.0010689323535189033, Train Acc: 0.9998000264167786\n",
            "Epoch 7504/10000\n",
            "Step 0: Train Loss: 0.0005287786480039358, Train Acc: 1.0\n",
            "Epoch 7505/10000\n",
            "Step 0: Train Loss: 0.0011839307844638824, Train Acc: 0.9995999932289124\n",
            "Epoch 7506/10000\n",
            "Step 0: Train Loss: 0.0010866085067391396, Train Acc: 0.9998000264167786\n",
            "Epoch 7507/10000\n",
            "Step 0: Train Loss: 0.0008463888079859316, Train Acc: 0.9998000264167786\n",
            "Epoch 7508/10000\n",
            "Step 0: Train Loss: 0.0007656466332264245, Train Acc: 0.9997000098228455\n",
            "Epoch 7509/10000\n",
            "Step 0: Train Loss: 0.0006436108378693461, Train Acc: 1.0\n",
            "Epoch 7510/10000\n",
            "Step 0: Train Loss: 0.001432399032637477, Train Acc: 0.9997000098228455\n",
            "Epoch 7511/10000\n",
            "Step 0: Train Loss: 0.0009516356512904167, Train Acc: 0.9997000098228455\n",
            "Epoch 7512/10000\n",
            "Step 0: Train Loss: 0.0008098315447568893, Train Acc: 1.0\n",
            "Epoch 7513/10000\n",
            "Step 0: Train Loss: 0.000592721626162529, Train Acc: 1.0\n",
            "Epoch 7514/10000\n",
            "Step 0: Train Loss: 0.0007045611273497343, Train Acc: 0.9998999834060669\n",
            "Epoch 7515/10000\n",
            "Step 0: Train Loss: 0.000952620233874768, Train Acc: 0.9998999834060669\n",
            "Epoch 7516/10000\n",
            "Step 0: Train Loss: 0.000975941657088697, Train Acc: 0.9998000264167786\n",
            "Epoch 7517/10000\n",
            "Step 0: Train Loss: 0.000782085582613945, Train Acc: 0.9998999834060669\n",
            "Epoch 7518/10000\n",
            "Step 0: Train Loss: 0.0009815637022256851, Train Acc: 0.9998000264167786\n",
            "Epoch 7519/10000\n",
            "Step 0: Train Loss: 0.0011098890099674463, Train Acc: 0.9997000098228455\n",
            "Epoch 7520/10000\n",
            "Step 0: Train Loss: 0.0011808337876573205, Train Acc: 0.9997000098228455\n",
            "Epoch 7521/10000\n",
            "Step 0: Train Loss: 0.0005959284026175737, Train Acc: 0.9998000264167786\n",
            "Epoch 7522/10000\n",
            "Step 0: Train Loss: 0.0008135291864164174, Train Acc: 0.9998000264167786\n",
            "Epoch 7523/10000\n",
            "Step 0: Train Loss: 0.0006159932818263769, Train Acc: 1.0\n",
            "Epoch 7524/10000\n",
            "Step 0: Train Loss: 0.0015214674640446901, Train Acc: 0.9997000098228455\n",
            "Epoch 7525/10000\n",
            "Step 0: Train Loss: 0.0008652583346702158, Train Acc: 0.9998999834060669\n",
            "Epoch 7526/10000\n",
            "Step 0: Train Loss: 0.0008321189088746905, Train Acc: 0.9998000264167786\n",
            "Epoch 7527/10000\n",
            "Step 0: Train Loss: 0.0011540474370121956, Train Acc: 0.9995999932289124\n",
            "Epoch 7528/10000\n",
            "Step 0: Train Loss: 0.0006054482655599713, Train Acc: 0.9998999834060669\n",
            "Epoch 7529/10000\n",
            "Step 0: Train Loss: 0.0006365098524838686, Train Acc: 0.9998999834060669\n",
            "Epoch 7530/10000\n",
            "Step 0: Train Loss: 0.000836703518871218, Train Acc: 0.9998000264167786\n",
            "Epoch 7531/10000\n",
            "Step 0: Train Loss: 0.0005265398067422211, Train Acc: 0.9998000264167786\n",
            "Epoch 7532/10000\n",
            "Step 0: Train Loss: 0.001437665894627571, Train Acc: 0.9994999766349792\n",
            "Epoch 7533/10000\n",
            "Step 0: Train Loss: 0.0003667545970529318, Train Acc: 0.9998999834060669\n",
            "Epoch 7534/10000\n",
            "Step 0: Train Loss: 0.000510246551129967, Train Acc: 0.9998999834060669\n",
            "Epoch 7535/10000\n",
            "Step 0: Train Loss: 0.0006927709328010678, Train Acc: 0.9998999834060669\n",
            "Epoch 7536/10000\n",
            "Step 0: Train Loss: 0.0006616227910853922, Train Acc: 0.9998999834060669\n",
            "Epoch 7537/10000\n",
            "Step 0: Train Loss: 0.000939670018851757, Train Acc: 0.9998000264167786\n",
            "Epoch 7538/10000\n",
            "Step 0: Train Loss: 0.0009532073163427413, Train Acc: 0.9998999834060669\n",
            "Epoch 7539/10000\n",
            "Step 0: Train Loss: 0.0004037616599816829, Train Acc: 1.0\n",
            "Epoch 7540/10000\n",
            "Step 0: Train Loss: 0.00036969801294617355, Train Acc: 1.0\n",
            "Epoch 7541/10000\n",
            "Step 0: Train Loss: 0.000865229987539351, Train Acc: 0.9997000098228455\n",
            "Epoch 7542/10000\n",
            "Step 0: Train Loss: 0.001191888703033328, Train Acc: 0.9997000098228455\n",
            "Epoch 7543/10000\n",
            "Step 0: Train Loss: 0.0008472640765830874, Train Acc: 0.9998000264167786\n",
            "Epoch 7544/10000\n",
            "Step 0: Train Loss: 0.00029550789622589946, Train Acc: 1.0\n",
            "Epoch 7545/10000\n",
            "Step 0: Train Loss: 0.0009261459344998002, Train Acc: 0.9998999834060669\n",
            "Epoch 7546/10000\n",
            "Step 0: Train Loss: 0.00045318464981392026, Train Acc: 1.0\n",
            "Epoch 7547/10000\n",
            "Step 0: Train Loss: 0.0009973504347726703, Train Acc: 0.9998000264167786\n",
            "Epoch 7548/10000\n",
            "Step 0: Train Loss: 0.000442205899162218, Train Acc: 1.0\n",
            "Epoch 7549/10000\n",
            "Step 0: Train Loss: 0.0005914335488341749, Train Acc: 0.9998999834060669\n",
            "Epoch 7550/10000\n",
            "Step 0: Train Loss: 0.0005127373733557761, Train Acc: 1.0\n",
            "Epoch 7551/10000\n",
            "Step 0: Train Loss: 0.0006697305361740291, Train Acc: 0.9998999834060669\n",
            "Epoch 7552/10000\n",
            "Step 0: Train Loss: 0.0005860841483809054, Train Acc: 0.9998999834060669\n",
            "Epoch 7553/10000\n",
            "Step 0: Train Loss: 0.0005628072540275753, Train Acc: 1.0\n",
            "Epoch 7554/10000\n",
            "Step 0: Train Loss: 0.0008684237254783511, Train Acc: 0.9998000264167786\n",
            "Epoch 7555/10000\n",
            "Step 0: Train Loss: 0.0007606264553032815, Train Acc: 0.9997000098228455\n",
            "Epoch 7556/10000\n",
            "Step 0: Train Loss: 0.0005345690879039466, Train Acc: 1.0\n",
            "Epoch 7557/10000\n",
            "Step 0: Train Loss: 0.000933353032451123, Train Acc: 0.9998000264167786\n",
            "Epoch 7558/10000\n",
            "Step 0: Train Loss: 0.0008524585864506662, Train Acc: 0.9998999834060669\n",
            "Epoch 7559/10000\n",
            "Step 0: Train Loss: 0.00036892606294713914, Train Acc: 0.9998999834060669\n",
            "Epoch 7560/10000\n",
            "Step 0: Train Loss: 0.00047984119737520814, Train Acc: 0.9998999834060669\n",
            "Epoch 7561/10000\n",
            "Step 0: Train Loss: 0.0006603652145713568, Train Acc: 1.0\n",
            "Epoch 7562/10000\n",
            "Step 0: Train Loss: 0.0003830226487480104, Train Acc: 0.9998999834060669\n",
            "Epoch 7563/10000\n",
            "Step 0: Train Loss: 0.0007243851432576776, Train Acc: 0.9998000264167786\n",
            "Epoch 7564/10000\n",
            "Step 0: Train Loss: 0.00047944384277798235, Train Acc: 1.0\n",
            "Epoch 7565/10000\n",
            "Step 0: Train Loss: 0.0005832223105244339, Train Acc: 1.0\n",
            "Epoch 7566/10000\n",
            "Step 0: Train Loss: 0.0008681237231940031, Train Acc: 0.9998000264167786\n",
            "Epoch 7567/10000\n",
            "Step 0: Train Loss: 0.0005401761154644191, Train Acc: 0.9998999834060669\n",
            "Epoch 7568/10000\n",
            "Step 0: Train Loss: 0.0006793467910028994, Train Acc: 0.9998999834060669\n",
            "Epoch 7569/10000\n",
            "Step 0: Train Loss: 0.0004753918037749827, Train Acc: 1.0\n",
            "Epoch 7570/10000\n",
            "Step 0: Train Loss: 0.0006277284119278193, Train Acc: 0.9998999834060669\n",
            "Epoch 7571/10000\n",
            "Step 0: Train Loss: 0.000429487758083269, Train Acc: 1.0\n",
            "Epoch 7572/10000\n",
            "Step 0: Train Loss: 0.0006213976885192096, Train Acc: 0.9998999834060669\n",
            "Epoch 7573/10000\n",
            "Step 0: Train Loss: 0.0002715718874242157, Train Acc: 1.0\n",
            "Epoch 7574/10000\n",
            "Step 0: Train Loss: 0.0005349860875867307, Train Acc: 1.0\n",
            "Epoch 7575/10000\n",
            "Step 0: Train Loss: 0.0008018181542865932, Train Acc: 1.0\n",
            "Epoch 7576/10000\n",
            "Step 0: Train Loss: 0.00040335417725145817, Train Acc: 1.0\n",
            "Epoch 7577/10000\n",
            "Step 0: Train Loss: 0.00033399934181943536, Train Acc: 1.0\n",
            "Epoch 7578/10000\n",
            "Step 0: Train Loss: 0.0004567759169731289, Train Acc: 0.9998999834060669\n",
            "Epoch 7579/10000\n",
            "Step 0: Train Loss: 0.0002852963225450367, Train Acc: 1.0\n",
            "Epoch 7580/10000\n",
            "Step 0: Train Loss: 0.0005383386742323637, Train Acc: 1.0\n",
            "Epoch 7581/10000\n",
            "Step 0: Train Loss: 0.000787445402238518, Train Acc: 0.9998999834060669\n",
            "Epoch 7582/10000\n",
            "Step 0: Train Loss: 0.00040683522820472717, Train Acc: 1.0\n",
            "Epoch 7583/10000\n",
            "Step 0: Train Loss: 0.00041000221972353756, Train Acc: 1.0\n",
            "Epoch 7584/10000\n",
            "Step 0: Train Loss: 0.0004890639684163034, Train Acc: 0.9998000264167786\n",
            "Epoch 7585/10000\n",
            "Step 0: Train Loss: 0.0010705485474318266, Train Acc: 0.9998000264167786\n",
            "Epoch 7586/10000\n",
            "Step 0: Train Loss: 0.0004341698659118265, Train Acc: 1.0\n",
            "Epoch 7587/10000\n",
            "Step 0: Train Loss: 0.0008410123991779983, Train Acc: 0.9998999834060669\n",
            "Epoch 7588/10000\n",
            "Step 0: Train Loss: 0.0002870577445719391, Train Acc: 1.0\n",
            "Epoch 7589/10000\n",
            "Step 0: Train Loss: 0.0006397439865395427, Train Acc: 0.9998000264167786\n",
            "Epoch 7590/10000\n",
            "Step 0: Train Loss: 0.0003206398105248809, Train Acc: 1.0\n",
            "Epoch 7591/10000\n",
            "Step 0: Train Loss: 0.00028588087297976017, Train Acc: 1.0\n",
            "Epoch 7592/10000\n",
            "Step 0: Train Loss: 0.0003566672676242888, Train Acc: 1.0\n",
            "Epoch 7593/10000\n",
            "Step 0: Train Loss: 0.0003477210411801934, Train Acc: 1.0\n",
            "Epoch 7594/10000\n",
            "Step 0: Train Loss: 0.0003884801990352571, Train Acc: 0.9998999834060669\n",
            "Epoch 7595/10000\n",
            "Step 0: Train Loss: 0.000946548767387867, Train Acc: 0.9998000264167786\n",
            "Epoch 7596/10000\n",
            "Step 0: Train Loss: 0.0005376852350309491, Train Acc: 0.9998999834060669\n",
            "Epoch 7597/10000\n",
            "Step 0: Train Loss: 0.00045787871931679547, Train Acc: 1.0\n",
            "Epoch 7598/10000\n",
            "Step 0: Train Loss: 0.0006237904308363795, Train Acc: 1.0\n",
            "Epoch 7599/10000\n",
            "Step 0: Train Loss: 0.00037318948307074606, Train Acc: 0.9998999834060669\n",
            "Epoch 7600/10000\n",
            "Step 0: Train Loss: 0.0011283807689324021, Train Acc: 0.9998000264167786\n",
            "Epoch 7601/10000\n",
            "Step 0: Train Loss: 0.000453567219665274, Train Acc: 1.0\n",
            "Epoch 7602/10000\n",
            "Step 0: Train Loss: 0.0002469531027600169, Train Acc: 1.0\n",
            "Epoch 7603/10000\n",
            "Step 0: Train Loss: 0.00045346669503487647, Train Acc: 1.0\n",
            "Epoch 7604/10000\n",
            "Step 0: Train Loss: 0.0007232817588374019, Train Acc: 0.9998999834060669\n",
            "Epoch 7605/10000\n",
            "Step 0: Train Loss: 0.0008662876207381487, Train Acc: 0.9998000264167786\n",
            "Epoch 7606/10000\n",
            "Step 0: Train Loss: 0.000706054677721113, Train Acc: 0.9998999834060669\n",
            "Epoch 7607/10000\n",
            "Step 0: Train Loss: 0.00037677481304854155, Train Acc: 1.0\n",
            "Epoch 7608/10000\n",
            "Step 0: Train Loss: 0.0007074848981574178, Train Acc: 0.9998999834060669\n",
            "Epoch 7609/10000\n",
            "Step 0: Train Loss: 0.0005828156135976315, Train Acc: 1.0\n",
            "Epoch 7610/10000\n",
            "Step 0: Train Loss: 0.0005843640537932515, Train Acc: 0.9998000264167786\n",
            "Epoch 7611/10000\n",
            "Step 0: Train Loss: 0.00043605262180790305, Train Acc: 1.0\n",
            "Epoch 7612/10000\n",
            "Step 0: Train Loss: 0.0004614788922481239, Train Acc: 0.9998999834060669\n",
            "Epoch 7613/10000\n",
            "Step 0: Train Loss: 0.0006921102758497, Train Acc: 0.9998000264167786\n",
            "Epoch 7614/10000\n",
            "Step 0: Train Loss: 0.0006657414487563074, Train Acc: 0.9998999834060669\n",
            "Epoch 7615/10000\n",
            "Step 0: Train Loss: 0.0003624456294346601, Train Acc: 1.0\n",
            "Epoch 7616/10000\n",
            "Step 0: Train Loss: 0.0005450028111226857, Train Acc: 0.9998000264167786\n",
            "Epoch 7617/10000\n",
            "Step 0: Train Loss: 0.000936047756113112, Train Acc: 0.9998000264167786\n",
            "Epoch 7618/10000\n",
            "Step 0: Train Loss: 0.0006742655532434583, Train Acc: 0.9998999834060669\n",
            "Epoch 7619/10000\n",
            "Step 0: Train Loss: 0.0004172633634880185, Train Acc: 1.0\n",
            "Epoch 7620/10000\n",
            "Step 0: Train Loss: 0.0007726280600763857, Train Acc: 0.9998999834060669\n",
            "Epoch 7621/10000\n",
            "Step 0: Train Loss: 0.00020983093418180943, Train Acc: 1.0\n",
            "Epoch 7622/10000\n",
            "Step 0: Train Loss: 0.00068185810232535, Train Acc: 0.9998999834060669\n",
            "Epoch 7623/10000\n",
            "Step 0: Train Loss: 0.00039005849976092577, Train Acc: 1.0\n",
            "Epoch 7624/10000\n",
            "Step 0: Train Loss: 0.0007548430585302413, Train Acc: 0.9998000264167786\n",
            "Epoch 7625/10000\n",
            "Step 0: Train Loss: 0.0003584354417398572, Train Acc: 1.0\n",
            "Epoch 7626/10000\n",
            "Step 0: Train Loss: 0.0003691616002470255, Train Acc: 1.0\n",
            "Epoch 7627/10000\n",
            "Step 0: Train Loss: 0.00031908394885249436, Train Acc: 0.9998999834060669\n",
            "Epoch 7628/10000\n",
            "Step 0: Train Loss: 0.0003921037132386118, Train Acc: 0.9998999834060669\n",
            "Epoch 7629/10000\n",
            "Step 0: Train Loss: 0.000856894301250577, Train Acc: 0.9998999834060669\n",
            "Epoch 7630/10000\n",
            "Step 0: Train Loss: 0.00041620945557951927, Train Acc: 0.9998999834060669\n",
            "Epoch 7631/10000\n",
            "Step 0: Train Loss: 0.00027894010418094695, Train Acc: 1.0\n",
            "Epoch 7632/10000\n",
            "Step 0: Train Loss: 0.00032698072027415037, Train Acc: 1.0\n",
            "Epoch 7633/10000\n",
            "Step 0: Train Loss: 0.0004126442945562303, Train Acc: 1.0\n",
            "Epoch 7634/10000\n",
            "Step 0: Train Loss: 0.00048589048674330115, Train Acc: 1.0\n",
            "Epoch 7635/10000\n",
            "Step 0: Train Loss: 0.0003601828939281404, Train Acc: 1.0\n",
            "Epoch 7636/10000\n",
            "Step 0: Train Loss: 0.0008735088631510735, Train Acc: 0.9998000264167786\n",
            "Epoch 7637/10000\n",
            "Step 0: Train Loss: 0.0005146628245711327, Train Acc: 0.9998000264167786\n",
            "Epoch 7638/10000\n",
            "Step 0: Train Loss: 0.000789453973993659, Train Acc: 0.9998000264167786\n",
            "Epoch 7639/10000\n",
            "Step 0: Train Loss: 0.0008255518041551113, Train Acc: 0.9998000264167786\n",
            "Epoch 7640/10000\n",
            "Step 0: Train Loss: 0.00019883802451658994, Train Acc: 1.0\n",
            "Epoch 7641/10000\n",
            "Step 0: Train Loss: 0.00043094230932183564, Train Acc: 0.9998999834060669\n",
            "Epoch 7642/10000\n",
            "Step 0: Train Loss: 0.0005641264724545181, Train Acc: 0.9998000264167786\n",
            "Epoch 7643/10000\n",
            "Step 0: Train Loss: 0.00032869569258764386, Train Acc: 1.0\n",
            "Epoch 7644/10000\n",
            "Step 0: Train Loss: 0.0006182724609971046, Train Acc: 0.9997000098228455\n",
            "Epoch 7645/10000\n",
            "Step 0: Train Loss: 0.00023894949117675424, Train Acc: 1.0\n",
            "Epoch 7646/10000\n",
            "Step 0: Train Loss: 0.00045059662079438567, Train Acc: 0.9998999834060669\n",
            "Epoch 7647/10000\n",
            "Step 0: Train Loss: 0.00040049661765806377, Train Acc: 1.0\n",
            "Epoch 7648/10000\n",
            "Step 0: Train Loss: 0.00022310686472337693, Train Acc: 1.0\n",
            "Epoch 7649/10000\n",
            "Step 0: Train Loss: 0.00027500870055519044, Train Acc: 1.0\n",
            "Epoch 7650/10000\n",
            "Step 0: Train Loss: 0.00016968608542811126, Train Acc: 1.0\n",
            "Epoch 7651/10000\n",
            "Step 0: Train Loss: 0.0003615674504544586, Train Acc: 1.0\n",
            "Epoch 7652/10000\n",
            "Step 0: Train Loss: 0.0004158142546657473, Train Acc: 0.9998999834060669\n",
            "Epoch 7653/10000\n",
            "Step 0: Train Loss: 0.0004174073110334575, Train Acc: 0.9998999834060669\n",
            "Epoch 7654/10000\n",
            "Step 0: Train Loss: 0.0002086882886942476, Train Acc: 1.0\n",
            "Epoch 7655/10000\n",
            "Step 0: Train Loss: 0.00030448526376858354, Train Acc: 1.0\n",
            "Epoch 7656/10000\n",
            "Step 0: Train Loss: 0.0004313367826398462, Train Acc: 0.9998999834060669\n",
            "Epoch 7657/10000\n",
            "Step 0: Train Loss: 0.00017194761312566698, Train Acc: 1.0\n",
            "Epoch 7658/10000\n",
            "Step 0: Train Loss: 0.00048165646148845553, Train Acc: 1.0\n",
            "Epoch 7659/10000\n",
            "Step 0: Train Loss: 0.00031346373725682497, Train Acc: 1.0\n",
            "Epoch 7660/10000\n",
            "Step 0: Train Loss: 0.000587204413022846, Train Acc: 0.9998999834060669\n",
            "Epoch 7661/10000\n",
            "Step 0: Train Loss: 0.00023854525352362543, Train Acc: 1.0\n",
            "Epoch 7662/10000\n",
            "Step 0: Train Loss: 0.0006706768763251603, Train Acc: 0.9998000264167786\n",
            "Epoch 7663/10000\n",
            "Step 0: Train Loss: 0.00028409220976755023, Train Acc: 1.0\n",
            "Epoch 7664/10000\n",
            "Step 0: Train Loss: 0.00020932209736201912, Train Acc: 1.0\n",
            "Epoch 7665/10000\n",
            "Step 0: Train Loss: 0.0004781183379236609, Train Acc: 0.9998000264167786\n",
            "Epoch 7666/10000\n",
            "Step 0: Train Loss: 0.0006960338796488941, Train Acc: 0.9998999834060669\n",
            "Epoch 7667/10000\n",
            "Step 0: Train Loss: 0.0005482139531522989, Train Acc: 0.9998999834060669\n",
            "Epoch 7668/10000\n",
            "Step 0: Train Loss: 9.689249418443069e-05, Train Acc: 1.0\n",
            "Epoch 7669/10000\n",
            "Step 0: Train Loss: 0.0002763971278909594, Train Acc: 1.0\n",
            "Epoch 7670/10000\n",
            "Step 0: Train Loss: 0.0003986849042121321, Train Acc: 1.0\n",
            "Epoch 7671/10000\n",
            "Step 0: Train Loss: 0.0006296747014857829, Train Acc: 0.9998000264167786\n",
            "Epoch 7672/10000\n",
            "Step 0: Train Loss: 0.0002817267959471792, Train Acc: 1.0\n",
            "Epoch 7673/10000\n",
            "Step 0: Train Loss: 0.0003814035444520414, Train Acc: 1.0\n",
            "Epoch 7674/10000\n",
            "Step 0: Train Loss: 0.0007734273094683886, Train Acc: 0.9997000098228455\n",
            "Epoch 7675/10000\n",
            "Step 0: Train Loss: 0.0003707010764628649, Train Acc: 1.0\n",
            "Epoch 7676/10000\n",
            "Step 0: Train Loss: 0.0006591303390450776, Train Acc: 0.9998999834060669\n",
            "Epoch 7677/10000\n",
            "Step 0: Train Loss: 0.0002983557933475822, Train Acc: 1.0\n",
            "Epoch 7678/10000\n",
            "Step 0: Train Loss: 0.00039691582787781954, Train Acc: 0.9998999834060669\n",
            "Epoch 7679/10000\n",
            "Step 0: Train Loss: 0.0004340601444710046, Train Acc: 0.9998999834060669\n",
            "Epoch 7680/10000\n",
            "Step 0: Train Loss: 0.0004124827974010259, Train Acc: 0.9998999834060669\n",
            "Epoch 7681/10000\n",
            "Step 0: Train Loss: 0.0007784440531395376, Train Acc: 0.9998000264167786\n",
            "Epoch 7682/10000\n",
            "Step 0: Train Loss: 0.001356462831608951, Train Acc: 0.9995999932289124\n",
            "Epoch 7683/10000\n",
            "Step 0: Train Loss: 0.0005662486655637622, Train Acc: 0.9998000264167786\n",
            "Epoch 7684/10000\n",
            "Step 0: Train Loss: 0.0022516713943332434, Train Acc: 0.9991999864578247\n",
            "Epoch 7685/10000\n",
            "Step 0: Train Loss: 0.0012611903948709369, Train Acc: 0.9994999766349792\n",
            "Epoch 7686/10000\n",
            "Step 0: Train Loss: 0.0018785522552207112, Train Acc: 0.9995999932289124\n",
            "Epoch 7687/10000\n",
            "Step 0: Train Loss: 0.0009156519081443548, Train Acc: 0.9995999932289124\n",
            "Epoch 7688/10000\n",
            "Step 0: Train Loss: 0.0010818178998306394, Train Acc: 0.9998999834060669\n",
            "Epoch 7689/10000\n",
            "Step 0: Train Loss: 0.0013015297008678317, Train Acc: 0.9995999932289124\n",
            "Epoch 7690/10000\n",
            "Step 0: Train Loss: 0.0035472519230097532, Train Acc: 0.9994999766349792\n",
            "Epoch 7691/10000\n",
            "Step 0: Train Loss: 0.001511806738562882, Train Acc: 0.9994000196456909\n",
            "Epoch 7692/10000\n",
            "Step 0: Train Loss: 0.0013466859236359596, Train Acc: 0.9997000098228455\n",
            "Epoch 7693/10000\n",
            "Step 0: Train Loss: 0.0010576039785519242, Train Acc: 0.9997000098228455\n",
            "Epoch 7694/10000\n",
            "Step 0: Train Loss: 0.002570741344243288, Train Acc: 0.9991999864578247\n",
            "Epoch 7695/10000\n",
            "Step 0: Train Loss: 0.004389829467982054, Train Acc: 0.9987999796867371\n",
            "Epoch 7696/10000\n",
            "Step 0: Train Loss: 0.0018009524792432785, Train Acc: 0.9994999766349792\n",
            "Epoch 7697/10000\n",
            "Step 0: Train Loss: 0.002486293902620673, Train Acc: 0.9994999766349792\n",
            "Epoch 7698/10000\n",
            "Step 0: Train Loss: 0.0019691074267029762, Train Acc: 0.9994999766349792\n",
            "Epoch 7699/10000\n",
            "Step 0: Train Loss: 0.0016327962512150407, Train Acc: 0.9995999932289124\n",
            "Epoch 7700/10000\n",
            "Step 0: Train Loss: 0.002281661843881011, Train Acc: 0.9995999932289124\n",
            "Epoch 7701/10000\n",
            "Step 0: Train Loss: 0.0011620413279160857, Train Acc: 0.9997000098228455\n",
            "Epoch 7702/10000\n",
            "Step 0: Train Loss: 0.0013051878195255995, Train Acc: 0.9997000098228455\n",
            "Epoch 7703/10000\n",
            "Step 0: Train Loss: 0.0007435677107423544, Train Acc: 1.0\n",
            "Epoch 7704/10000\n",
            "Step 0: Train Loss: 0.0016365114133805037, Train Acc: 0.9997000098228455\n",
            "Epoch 7705/10000\n",
            "Step 0: Train Loss: 0.0008195036207325757, Train Acc: 0.9998999834060669\n",
            "Epoch 7706/10000\n",
            "Step 0: Train Loss: 0.0007520706858485937, Train Acc: 1.0\n",
            "Epoch 7707/10000\n",
            "Step 0: Train Loss: 0.001645401818677783, Train Acc: 0.9997000098228455\n",
            "Epoch 7708/10000\n",
            "Step 0: Train Loss: 0.0004750143561977893, Train Acc: 1.0\n",
            "Epoch 7709/10000\n",
            "Step 0: Train Loss: 0.0005303766229189932, Train Acc: 1.0\n",
            "Epoch 7710/10000\n",
            "Step 0: Train Loss: 0.0005518719553947449, Train Acc: 0.9998000264167786\n",
            "Epoch 7711/10000\n",
            "Step 0: Train Loss: 0.0007656934903934598, Train Acc: 0.9998999834060669\n",
            "Epoch 7712/10000\n",
            "Step 0: Train Loss: 0.0004062310326844454, Train Acc: 1.0\n",
            "Epoch 7713/10000\n",
            "Step 0: Train Loss: 0.0003897069254890084, Train Acc: 0.9998999834060669\n",
            "Epoch 7714/10000\n",
            "Step 0: Train Loss: 0.0004472402506507933, Train Acc: 0.9998999834060669\n",
            "Epoch 7715/10000\n",
            "Step 0: Train Loss: 0.0004736450209748, Train Acc: 0.9998000264167786\n",
            "Epoch 7716/10000\n",
            "Step 0: Train Loss: 0.000629962538369, Train Acc: 0.9998999834060669\n",
            "Epoch 7717/10000\n",
            "Step 0: Train Loss: 0.00046778202522546053, Train Acc: 0.9998999834060669\n",
            "Epoch 7718/10000\n",
            "Step 0: Train Loss: 0.0005326030077412724, Train Acc: 1.0\n",
            "Epoch 7719/10000\n",
            "Step 0: Train Loss: 0.000271375582087785, Train Acc: 1.0\n",
            "Epoch 7720/10000\n",
            "Step 0: Train Loss: 0.0002622214669827372, Train Acc: 1.0\n",
            "Epoch 7721/10000\n",
            "Step 0: Train Loss: 0.00025650099269114435, Train Acc: 1.0\n",
            "Epoch 7722/10000\n",
            "Step 0: Train Loss: 0.00031030841637402773, Train Acc: 0.9998999834060669\n",
            "Epoch 7723/10000\n",
            "Step 0: Train Loss: 0.00024196073354687542, Train Acc: 1.0\n",
            "Epoch 7724/10000\n",
            "Step 0: Train Loss: 0.00024352686887141317, Train Acc: 1.0\n",
            "Epoch 7725/10000\n",
            "Step 0: Train Loss: 0.0004064505046699196, Train Acc: 0.9998999834060669\n",
            "Epoch 7726/10000\n",
            "Step 0: Train Loss: 0.0002749903069343418, Train Acc: 1.0\n",
            "Epoch 7727/10000\n",
            "Step 0: Train Loss: 0.0005386066623032093, Train Acc: 0.9998999834060669\n",
            "Epoch 7728/10000\n",
            "Step 0: Train Loss: 0.00032491376623511314, Train Acc: 1.0\n",
            "Epoch 7729/10000\n",
            "Step 0: Train Loss: 0.00023305165814235806, Train Acc: 1.0\n",
            "Epoch 7730/10000\n",
            "Step 0: Train Loss: 0.0007013218128122389, Train Acc: 0.9998000264167786\n",
            "Epoch 7731/10000\n",
            "Step 0: Train Loss: 0.00037213426548987627, Train Acc: 1.0\n",
            "Epoch 7732/10000\n",
            "Step 0: Train Loss: 0.0004743931640405208, Train Acc: 0.9998999834060669\n",
            "Epoch 7733/10000\n",
            "Step 0: Train Loss: 0.00031873464467935264, Train Acc: 1.0\n",
            "Epoch 7734/10000\n",
            "Step 0: Train Loss: 0.0003501409664750099, Train Acc: 0.9998999834060669\n",
            "Epoch 7735/10000\n",
            "Step 0: Train Loss: 0.00031598511850461364, Train Acc: 1.0\n",
            "Epoch 7736/10000\n",
            "Step 0: Train Loss: 0.0006762639968656003, Train Acc: 0.9998999834060669\n",
            "Epoch 7737/10000\n",
            "Step 0: Train Loss: 0.0002718691830523312, Train Acc: 1.0\n",
            "Epoch 7738/10000\n",
            "Step 0: Train Loss: 0.00032519010710529983, Train Acc: 0.9998999834060669\n",
            "Epoch 7739/10000\n",
            "Step 0: Train Loss: 0.0001384557835990563, Train Acc: 1.0\n",
            "Epoch 7740/10000\n",
            "Step 0: Train Loss: 0.00016243044228758663, Train Acc: 1.0\n",
            "Epoch 7741/10000\n",
            "Step 0: Train Loss: 0.00013820822641719133, Train Acc: 1.0\n",
            "Epoch 7742/10000\n",
            "Step 0: Train Loss: 0.0005471311742439866, Train Acc: 0.9998000264167786\n",
            "Epoch 7743/10000\n",
            "Step 0: Train Loss: 0.0002699728065636009, Train Acc: 1.0\n",
            "Epoch 7744/10000\n",
            "Step 0: Train Loss: 0.0004002270579803735, Train Acc: 1.0\n",
            "Epoch 7745/10000\n",
            "Step 0: Train Loss: 0.00019436070579104125, Train Acc: 1.0\n",
            "Epoch 7746/10000\n",
            "Step 0: Train Loss: 0.0005959905683994293, Train Acc: 0.9998999834060669\n",
            "Epoch 7747/10000\n",
            "Step 0: Train Loss: 0.00036322022788226604, Train Acc: 0.9998999834060669\n",
            "Epoch 7748/10000\n",
            "Step 0: Train Loss: 0.00010918537736870348, Train Acc: 1.0\n",
            "Epoch 7749/10000\n",
            "Step 0: Train Loss: 0.00047786501818336546, Train Acc: 0.9998999834060669\n",
            "Epoch 7750/10000\n",
            "Step 0: Train Loss: 0.00023478319053538144, Train Acc: 1.0\n",
            "Epoch 7751/10000\n",
            "Step 0: Train Loss: 0.0002016907383222133, Train Acc: 1.0\n",
            "Epoch 7752/10000\n",
            "Step 0: Train Loss: 0.0003127769741695374, Train Acc: 1.0\n",
            "Epoch 7753/10000\n",
            "Step 0: Train Loss: 0.00011119201371911913, Train Acc: 1.0\n",
            "Epoch 7754/10000\n",
            "Step 0: Train Loss: 0.0004110425943508744, Train Acc: 1.0\n",
            "Epoch 7755/10000\n",
            "Step 0: Train Loss: 0.00023237038112711161, Train Acc: 1.0\n",
            "Epoch 7756/10000\n",
            "Step 0: Train Loss: 0.00021899915009271353, Train Acc: 1.0\n",
            "Epoch 7757/10000\n",
            "Step 0: Train Loss: 0.00033818359952419996, Train Acc: 0.9998999834060669\n",
            "Epoch 7758/10000\n",
            "Step 0: Train Loss: 0.00030839882674627006, Train Acc: 0.9998999834060669\n",
            "Epoch 7759/10000\n",
            "Step 0: Train Loss: 0.0005628226790577173, Train Acc: 0.9998999834060669\n",
            "Epoch 7760/10000\n",
            "Step 0: Train Loss: 0.00012767070438712835, Train Acc: 1.0\n",
            "Epoch 7761/10000\n",
            "Step 0: Train Loss: 0.0001243373117176816, Train Acc: 1.0\n",
            "Epoch 7762/10000\n",
            "Step 0: Train Loss: 0.0005576098337769508, Train Acc: 0.9998999834060669\n",
            "Epoch 7763/10000\n",
            "Step 0: Train Loss: 0.0001919358765007928, Train Acc: 1.0\n",
            "Epoch 7764/10000\n",
            "Step 0: Train Loss: 0.00033103147870860994, Train Acc: 1.0\n",
            "Epoch 7765/10000\n",
            "Step 0: Train Loss: 0.00017134692461695522, Train Acc: 1.0\n",
            "Epoch 7766/10000\n",
            "Step 0: Train Loss: 0.00041616379166953266, Train Acc: 0.9998999834060669\n",
            "Epoch 7767/10000\n",
            "Step 0: Train Loss: 0.00038358019082807004, Train Acc: 1.0\n",
            "Epoch 7768/10000\n",
            "Step 0: Train Loss: 0.00032568766619078815, Train Acc: 1.0\n",
            "Epoch 7769/10000\n",
            "Step 0: Train Loss: 0.0004945944529026747, Train Acc: 0.9998999834060669\n",
            "Epoch 7770/10000\n",
            "Step 0: Train Loss: 0.0003004017344210297, Train Acc: 0.9998999834060669\n",
            "Epoch 7771/10000\n",
            "Step 0: Train Loss: 0.00047141386312432587, Train Acc: 0.9998999834060669\n",
            "Epoch 7772/10000\n",
            "Step 0: Train Loss: 0.00010692489013308659, Train Acc: 1.0\n",
            "Epoch 7773/10000\n",
            "Step 0: Train Loss: 0.0003209123096894473, Train Acc: 0.9998999834060669\n",
            "Epoch 7774/10000\n",
            "Step 0: Train Loss: 0.00023005384718999267, Train Acc: 1.0\n",
            "Epoch 7775/10000\n",
            "Step 0: Train Loss: 0.00012091963435523212, Train Acc: 1.0\n",
            "Epoch 7776/10000\n",
            "Step 0: Train Loss: 0.0001962951646419242, Train Acc: 1.0\n",
            "Epoch 7777/10000\n",
            "Step 0: Train Loss: 0.0004103513201698661, Train Acc: 0.9998999834060669\n",
            "Epoch 7778/10000\n",
            "Step 0: Train Loss: 0.00043668574653565884, Train Acc: 0.9998999834060669\n",
            "Epoch 7779/10000\n",
            "Step 0: Train Loss: 0.0001223390136146918, Train Acc: 1.0\n",
            "Epoch 7780/10000\n",
            "Step 0: Train Loss: 0.00028337628464214504, Train Acc: 1.0\n",
            "Epoch 7781/10000\n",
            "Step 0: Train Loss: 0.00021227517572697252, Train Acc: 1.0\n",
            "Epoch 7782/10000\n",
            "Step 0: Train Loss: 0.00015991591499187052, Train Acc: 1.0\n",
            "Epoch 7783/10000\n",
            "Step 0: Train Loss: 0.0001813896669773385, Train Acc: 1.0\n",
            "Epoch 7784/10000\n",
            "Step 0: Train Loss: 0.00042281803325749934, Train Acc: 1.0\n",
            "Epoch 7785/10000\n",
            "Step 0: Train Loss: 0.00020523353305179626, Train Acc: 0.9998999834060669\n",
            "Epoch 7786/10000\n",
            "Step 0: Train Loss: 0.0002455129288136959, Train Acc: 1.0\n",
            "Epoch 7787/10000\n",
            "Step 0: Train Loss: 0.0003196600009687245, Train Acc: 1.0\n",
            "Epoch 7788/10000\n",
            "Step 0: Train Loss: 0.0004116592463105917, Train Acc: 0.9998999834060669\n",
            "Epoch 7789/10000\n",
            "Step 0: Train Loss: 0.0001405344228260219, Train Acc: 1.0\n",
            "Epoch 7790/10000\n",
            "Step 0: Train Loss: 0.00013322135782800615, Train Acc: 1.0\n",
            "Epoch 7791/10000\n",
            "Step 0: Train Loss: 0.00018851201457437128, Train Acc: 1.0\n",
            "Epoch 7792/10000\n",
            "Step 0: Train Loss: 0.000235677624004893, Train Acc: 0.9998999834060669\n",
            "Epoch 7793/10000\n",
            "Step 0: Train Loss: 0.0003873209352605045, Train Acc: 0.9998999834060669\n",
            "Epoch 7794/10000\n",
            "Step 0: Train Loss: 0.0002408222935628146, Train Acc: 0.9998999834060669\n",
            "Epoch 7795/10000\n",
            "Step 0: Train Loss: 0.00031110519194044173, Train Acc: 1.0\n",
            "Epoch 7796/10000\n",
            "Step 0: Train Loss: 0.00014974898658692837, Train Acc: 1.0\n",
            "Epoch 7797/10000\n",
            "Step 0: Train Loss: 0.00026933045592159033, Train Acc: 1.0\n",
            "Epoch 7798/10000\n",
            "Step 0: Train Loss: 0.0004388598317746073, Train Acc: 0.9998999834060669\n",
            "Epoch 7799/10000\n",
            "Step 0: Train Loss: 0.00010354062396800146, Train Acc: 1.0\n",
            "Epoch 7800/10000\n",
            "Step 0: Train Loss: 0.00022004950733389705, Train Acc: 1.0\n",
            "Epoch 7801/10000\n",
            "Step 0: Train Loss: 0.0006176194874569774, Train Acc: 0.9998000264167786\n",
            "Epoch 7802/10000\n",
            "Step 0: Train Loss: 0.00026242213789373636, Train Acc: 1.0\n",
            "Epoch 7803/10000\n",
            "Step 0: Train Loss: 0.00017338593897875398, Train Acc: 1.0\n",
            "Epoch 7804/10000\n",
            "Step 0: Train Loss: 0.000353048526449129, Train Acc: 1.0\n",
            "Epoch 7805/10000\n",
            "Step 0: Train Loss: 0.00010134244803339243, Train Acc: 1.0\n",
            "Epoch 7806/10000\n",
            "Step 0: Train Loss: 0.00017078212113119662, Train Acc: 1.0\n",
            "Epoch 7807/10000\n",
            "Step 0: Train Loss: 0.00018104814807884395, Train Acc: 1.0\n",
            "Epoch 7808/10000\n",
            "Step 0: Train Loss: 0.00019237758533563465, Train Acc: 1.0\n",
            "Epoch 7809/10000\n",
            "Step 0: Train Loss: 0.00022967749100644141, Train Acc: 1.0\n",
            "Epoch 7810/10000\n",
            "Step 0: Train Loss: 0.00044745253399014473, Train Acc: 0.9998999834060669\n",
            "Epoch 7811/10000\n",
            "Step 0: Train Loss: 0.0003119090979453176, Train Acc: 0.9998999834060669\n",
            "Epoch 7812/10000\n",
            "Step 0: Train Loss: 0.00017319012840744108, Train Acc: 1.0\n",
            "Epoch 7813/10000\n",
            "Step 0: Train Loss: 0.00020148782641626894, Train Acc: 1.0\n",
            "Epoch 7814/10000\n",
            "Step 0: Train Loss: 0.00024457264225929976, Train Acc: 1.0\n",
            "Epoch 7815/10000\n",
            "Step 0: Train Loss: 0.0005771415890194476, Train Acc: 0.9998000264167786\n",
            "Epoch 7816/10000\n",
            "Step 0: Train Loss: 0.0002563705202192068, Train Acc: 1.0\n",
            "Epoch 7817/10000\n",
            "Step 0: Train Loss: 0.0003099467430729419, Train Acc: 0.9998999834060669\n",
            "Epoch 7818/10000\n",
            "Step 0: Train Loss: 0.0002217651781393215, Train Acc: 1.0\n",
            "Epoch 7819/10000\n",
            "Step 0: Train Loss: 9.48761953623034e-05, Train Acc: 1.0\n",
            "Epoch 7820/10000\n",
            "Step 0: Train Loss: 0.0006158538744784892, Train Acc: 0.9998000264167786\n",
            "Epoch 7821/10000\n",
            "Step 0: Train Loss: 0.00016424889327026904, Train Acc: 1.0\n",
            "Epoch 7822/10000\n",
            "Step 0: Train Loss: 0.00020570291962940246, Train Acc: 1.0\n",
            "Epoch 7823/10000\n",
            "Step 0: Train Loss: 0.00010988493158947676, Train Acc: 1.0\n",
            "Epoch 7824/10000\n",
            "Step 0: Train Loss: 0.00012170871195849031, Train Acc: 1.0\n",
            "Epoch 7825/10000\n",
            "Step 0: Train Loss: 0.0005162684246897697, Train Acc: 0.9998999834060669\n",
            "Epoch 7826/10000\n",
            "Step 0: Train Loss: 0.0002486951998434961, Train Acc: 1.0\n",
            "Epoch 7827/10000\n",
            "Step 0: Train Loss: 0.0002126024046447128, Train Acc: 1.0\n",
            "Epoch 7828/10000\n",
            "Step 0: Train Loss: 0.00014102573913987726, Train Acc: 1.0\n",
            "Epoch 7829/10000\n",
            "Step 0: Train Loss: 0.00038354567368514836, Train Acc: 0.9998999834060669\n",
            "Epoch 7830/10000\n",
            "Step 0: Train Loss: 0.0003049182705581188, Train Acc: 1.0\n",
            "Epoch 7831/10000\n",
            "Step 0: Train Loss: 0.0002478142559994012, Train Acc: 0.9998999834060669\n",
            "Epoch 7832/10000\n",
            "Step 0: Train Loss: 0.00018104231276083738, Train Acc: 0.9998999834060669\n",
            "Epoch 7833/10000\n",
            "Step 0: Train Loss: 0.00040249552694149315, Train Acc: 0.9998999834060669\n",
            "Epoch 7834/10000\n",
            "Step 0: Train Loss: 0.0002170287334593013, Train Acc: 1.0\n",
            "Epoch 7835/10000\n",
            "Step 0: Train Loss: 0.00013066070096101612, Train Acc: 1.0\n",
            "Epoch 7836/10000\n",
            "Step 0: Train Loss: 0.0005577611154876649, Train Acc: 0.9998999834060669\n",
            "Epoch 7837/10000\n",
            "Step 0: Train Loss: 0.0001499732316005975, Train Acc: 1.0\n",
            "Epoch 7838/10000\n",
            "Step 0: Train Loss: 0.00026567562599666417, Train Acc: 0.9998999834060669\n",
            "Epoch 7839/10000\n",
            "Step 0: Train Loss: 0.00033001418341882527, Train Acc: 0.9998999834060669\n",
            "Epoch 7840/10000\n",
            "Step 0: Train Loss: 0.00015985456411726773, Train Acc: 1.0\n",
            "Epoch 7841/10000\n",
            "Step 0: Train Loss: 0.00024042220320552588, Train Acc: 1.0\n",
            "Epoch 7842/10000\n",
            "Step 0: Train Loss: 0.0002882259141188115, Train Acc: 0.9998999834060669\n",
            "Epoch 7843/10000\n",
            "Step 0: Train Loss: 0.00011505477596074343, Train Acc: 1.0\n",
            "Epoch 7844/10000\n",
            "Step 0: Train Loss: 0.00012965101632289588, Train Acc: 1.0\n",
            "Epoch 7845/10000\n",
            "Step 0: Train Loss: 0.00020308308012317866, Train Acc: 0.9998999834060669\n",
            "Epoch 7846/10000\n",
            "Step 0: Train Loss: 0.00019257047097198665, Train Acc: 0.9998999834060669\n",
            "Epoch 7847/10000\n",
            "Step 0: Train Loss: 0.0005079819820821285, Train Acc: 0.9998999834060669\n",
            "Epoch 7848/10000\n",
            "Step 0: Train Loss: 0.00031662240508012474, Train Acc: 1.0\n",
            "Epoch 7849/10000\n",
            "Step 0: Train Loss: 0.00013453126302920282, Train Acc: 1.0\n",
            "Epoch 7850/10000\n",
            "Step 0: Train Loss: 0.0001686640753177926, Train Acc: 1.0\n",
            "Epoch 7851/10000\n",
            "Step 0: Train Loss: 8.782595978118479e-05, Train Acc: 1.0\n",
            "Epoch 7852/10000\n",
            "Step 0: Train Loss: 0.0002936851524282247, Train Acc: 1.0\n",
            "Epoch 7853/10000\n",
            "Step 0: Train Loss: 0.00035122502595186234, Train Acc: 0.9998999834060669\n",
            "Epoch 7854/10000\n",
            "Step 0: Train Loss: 0.00013673170178662986, Train Acc: 1.0\n",
            "Epoch 7855/10000\n",
            "Step 0: Train Loss: 0.000568213639780879, Train Acc: 0.9998999834060669\n",
            "Epoch 7856/10000\n",
            "Step 0: Train Loss: 0.000108507061668206, Train Acc: 1.0\n",
            "Epoch 7857/10000\n",
            "Step 0: Train Loss: 0.00013201603724155575, Train Acc: 1.0\n",
            "Epoch 7858/10000\n",
            "Step 0: Train Loss: 0.00020201198640279472, Train Acc: 0.9998999834060669\n",
            "Epoch 7859/10000\n",
            "Step 0: Train Loss: 0.00011831497977254912, Train Acc: 1.0\n",
            "Epoch 7860/10000\n",
            "Step 0: Train Loss: 0.0001648289180593565, Train Acc: 1.0\n",
            "Epoch 7861/10000\n",
            "Step 0: Train Loss: 0.0001507633860455826, Train Acc: 1.0\n",
            "Epoch 7862/10000\n",
            "Step 0: Train Loss: 0.000420267489971593, Train Acc: 0.9998999834060669\n",
            "Epoch 7863/10000\n",
            "Step 0: Train Loss: 0.0001920202048495412, Train Acc: 1.0\n",
            "Epoch 7864/10000\n",
            "Step 0: Train Loss: 0.00011945192818529904, Train Acc: 1.0\n",
            "Epoch 7865/10000\n",
            "Step 0: Train Loss: 0.00016908325778786093, Train Acc: 1.0\n",
            "Epoch 7866/10000\n",
            "Step 0: Train Loss: 0.0001413733552908525, Train Acc: 1.0\n",
            "Epoch 7867/10000\n",
            "Step 0: Train Loss: 8.925999281927943e-05, Train Acc: 1.0\n",
            "Epoch 7868/10000\n",
            "Step 0: Train Loss: 0.00019496111781336367, Train Acc: 0.9998999834060669\n",
            "Epoch 7869/10000\n",
            "Step 0: Train Loss: 0.00034360421705059707, Train Acc: 0.9998999834060669\n",
            "Epoch 7870/10000\n",
            "Step 0: Train Loss: 0.00034787674667313695, Train Acc: 0.9998999834060669\n",
            "Epoch 7871/10000\n",
            "Step 0: Train Loss: 0.00012409694318193942, Train Acc: 1.0\n",
            "Epoch 7872/10000\n",
            "Step 0: Train Loss: 0.00036872271448373795, Train Acc: 0.9998999834060669\n",
            "Epoch 7873/10000\n",
            "Step 0: Train Loss: 0.00014155761164147407, Train Acc: 1.0\n",
            "Epoch 7874/10000\n",
            "Step 0: Train Loss: 0.00010549768194323406, Train Acc: 1.0\n",
            "Epoch 7875/10000\n",
            "Step 0: Train Loss: 0.0001641400158405304, Train Acc: 1.0\n",
            "Epoch 7876/10000\n",
            "Step 0: Train Loss: 7.409244426526129e-05, Train Acc: 1.0\n",
            "Epoch 7877/10000\n",
            "Step 0: Train Loss: 0.00012977700680494308, Train Acc: 1.0\n",
            "Epoch 7878/10000\n",
            "Step 0: Train Loss: 0.00013411391410045326, Train Acc: 1.0\n",
            "Epoch 7879/10000\n",
            "Step 0: Train Loss: 5.8527195506030694e-05, Train Acc: 1.0\n",
            "Epoch 7880/10000\n",
            "Step 0: Train Loss: 6.666883564321324e-05, Train Acc: 1.0\n",
            "Epoch 7881/10000\n",
            "Step 0: Train Loss: 0.00021162648044992238, Train Acc: 1.0\n",
            "Epoch 7882/10000\n",
            "Step 0: Train Loss: 6.751069304300472e-05, Train Acc: 1.0\n",
            "Epoch 7883/10000\n",
            "Step 0: Train Loss: 9.499399311607704e-05, Train Acc: 1.0\n",
            "Epoch 7884/10000\n",
            "Step 0: Train Loss: 0.0002934465301223099, Train Acc: 0.9998999834060669\n",
            "Epoch 7885/10000\n",
            "Step 0: Train Loss: 0.0001940427755471319, Train Acc: 1.0\n",
            "Epoch 7886/10000\n",
            "Step 0: Train Loss: 0.000262125744484365, Train Acc: 1.0\n",
            "Epoch 7887/10000\n",
            "Step 0: Train Loss: 0.00012143770436523482, Train Acc: 1.0\n",
            "Epoch 7888/10000\n",
            "Step 0: Train Loss: 7.873308641137555e-05, Train Acc: 1.0\n",
            "Epoch 7889/10000\n",
            "Step 0: Train Loss: 0.00011406252451706678, Train Acc: 1.0\n",
            "Epoch 7890/10000\n",
            "Step 0: Train Loss: 0.0001924809766933322, Train Acc: 1.0\n",
            "Epoch 7891/10000\n",
            "Step 0: Train Loss: 0.0001833145070122555, Train Acc: 1.0\n",
            "Epoch 7892/10000\n",
            "Step 0: Train Loss: 5.529036570806056e-05, Train Acc: 1.0\n",
            "Epoch 7893/10000\n",
            "Step 0: Train Loss: 0.00012074132246198133, Train Acc: 1.0\n",
            "Epoch 7894/10000\n",
            "Step 0: Train Loss: 0.0002765469835139811, Train Acc: 1.0\n",
            "Epoch 7895/10000\n",
            "Step 0: Train Loss: 0.00014014440239407122, Train Acc: 1.0\n",
            "Epoch 7896/10000\n",
            "Step 0: Train Loss: 8.795048779575154e-05, Train Acc: 1.0\n",
            "Epoch 7897/10000\n",
            "Step 0: Train Loss: 9.479536674916744e-05, Train Acc: 1.0\n",
            "Epoch 7898/10000\n",
            "Step 0: Train Loss: 0.00018382928101345897, Train Acc: 1.0\n",
            "Epoch 7899/10000\n",
            "Step 0: Train Loss: 8.157900447258726e-05, Train Acc: 1.0\n",
            "Epoch 7900/10000\n",
            "Step 0: Train Loss: 0.00013050159031990916, Train Acc: 1.0\n",
            "Epoch 7901/10000\n",
            "Step 0: Train Loss: 8.189093205146492e-05, Train Acc: 1.0\n",
            "Epoch 7902/10000\n",
            "Step 0: Train Loss: 0.00026123246061615646, Train Acc: 0.9998999834060669\n",
            "Epoch 7903/10000\n",
            "Step 0: Train Loss: 0.00017721128824632615, Train Acc: 0.9998999834060669\n",
            "Epoch 7904/10000\n",
            "Step 0: Train Loss: 0.00014043327246326953, Train Acc: 1.0\n",
            "Epoch 7905/10000\n",
            "Step 0: Train Loss: 0.00022827446809969842, Train Acc: 1.0\n",
            "Epoch 7906/10000\n",
            "Step 0: Train Loss: 0.0002476991212461144, Train Acc: 1.0\n",
            "Epoch 7907/10000\n",
            "Step 0: Train Loss: 0.0001368712546536699, Train Acc: 1.0\n",
            "Epoch 7908/10000\n",
            "Step 0: Train Loss: 9.754698112374172e-05, Train Acc: 1.0\n",
            "Epoch 7909/10000\n",
            "Step 0: Train Loss: 6.26474866294302e-05, Train Acc: 1.0\n",
            "Epoch 7910/10000\n",
            "Step 0: Train Loss: 0.00020143236906733364, Train Acc: 1.0\n",
            "Epoch 7911/10000\n",
            "Step 0: Train Loss: 9.866235632216558e-05, Train Acc: 1.0\n",
            "Epoch 7912/10000\n",
            "Step 0: Train Loss: 0.00011032656766474247, Train Acc: 1.0\n",
            "Epoch 7913/10000\n",
            "Step 0: Train Loss: 0.00011261260806350037, Train Acc: 1.0\n",
            "Epoch 7914/10000\n",
            "Step 0: Train Loss: 0.0001430974662071094, Train Acc: 1.0\n",
            "Epoch 7915/10000\n",
            "Step 0: Train Loss: 0.00013547988783102483, Train Acc: 1.0\n",
            "Epoch 7916/10000\n",
            "Step 0: Train Loss: 0.00025667407317087054, Train Acc: 1.0\n",
            "Epoch 7917/10000\n",
            "Step 0: Train Loss: 9.748776210471988e-05, Train Acc: 1.0\n",
            "Epoch 7918/10000\n",
            "Step 0: Train Loss: 7.62927666073665e-05, Train Acc: 1.0\n",
            "Epoch 7919/10000\n",
            "Step 0: Train Loss: 3.779236431000754e-05, Train Acc: 1.0\n",
            "Epoch 7920/10000\n",
            "Step 0: Train Loss: 0.00010703270527301356, Train Acc: 1.0\n",
            "Epoch 7921/10000\n",
            "Step 0: Train Loss: 5.7212120736949146e-05, Train Acc: 1.0\n",
            "Epoch 7922/10000\n",
            "Step 0: Train Loss: 0.00019340730796102434, Train Acc: 1.0\n",
            "Epoch 7923/10000\n",
            "Step 0: Train Loss: 0.0002488337049726397, Train Acc: 1.0\n",
            "Epoch 7924/10000\n",
            "Step 0: Train Loss: 0.00016634677012916654, Train Acc: 1.0\n",
            "Epoch 7925/10000\n",
            "Step 0: Train Loss: 0.00012311551836319268, Train Acc: 1.0\n",
            "Epoch 7926/10000\n",
            "Step 0: Train Loss: 7.620635005878285e-05, Train Acc: 1.0\n",
            "Epoch 7927/10000\n",
            "Step 0: Train Loss: 0.00019518348562996835, Train Acc: 1.0\n",
            "Epoch 7928/10000\n",
            "Step 0: Train Loss: 7.082430965965614e-05, Train Acc: 1.0\n",
            "Epoch 7929/10000\n",
            "Step 0: Train Loss: 0.00013147981371730566, Train Acc: 1.0\n",
            "Epoch 7930/10000\n",
            "Step 0: Train Loss: 0.0001443425426259637, Train Acc: 1.0\n",
            "Epoch 7931/10000\n",
            "Step 0: Train Loss: 0.00010415224096504971, Train Acc: 1.0\n",
            "Epoch 7932/10000\n",
            "Step 0: Train Loss: 0.0001075437176041305, Train Acc: 1.0\n",
            "Epoch 7933/10000\n",
            "Step 0: Train Loss: 0.00012925655755680054, Train Acc: 1.0\n",
            "Epoch 7934/10000\n",
            "Step 0: Train Loss: 5.398106077336706e-05, Train Acc: 1.0\n",
            "Epoch 7935/10000\n",
            "Step 0: Train Loss: 7.073734741425142e-05, Train Acc: 1.0\n",
            "Epoch 7936/10000\n",
            "Step 0: Train Loss: 0.00016961473738774657, Train Acc: 1.0\n",
            "Epoch 7937/10000\n",
            "Step 0: Train Loss: 0.0001998732186621055, Train Acc: 1.0\n",
            "Epoch 7938/10000\n",
            "Step 0: Train Loss: 9.031469380715862e-05, Train Acc: 1.0\n",
            "Epoch 7939/10000\n",
            "Step 0: Train Loss: 5.5311476899078116e-05, Train Acc: 1.0\n",
            "Epoch 7940/10000\n",
            "Step 0: Train Loss: 9.048599895322695e-05, Train Acc: 1.0\n",
            "Epoch 7941/10000\n",
            "Step 0: Train Loss: 0.00013904350635129958, Train Acc: 1.0\n",
            "Epoch 7942/10000\n",
            "Step 0: Train Loss: 0.0001671419304329902, Train Acc: 1.0\n",
            "Epoch 7943/10000\n",
            "Step 0: Train Loss: 7.871424168115482e-05, Train Acc: 1.0\n",
            "Epoch 7944/10000\n",
            "Step 0: Train Loss: 0.0001260547578567639, Train Acc: 1.0\n",
            "Epoch 7945/10000\n",
            "Step 0: Train Loss: 0.00012119723396608606, Train Acc: 1.0\n",
            "Epoch 7946/10000\n",
            "Step 0: Train Loss: 6.929101073183119e-05, Train Acc: 1.0\n",
            "Epoch 7947/10000\n",
            "Step 0: Train Loss: 0.0001351063692709431, Train Acc: 1.0\n",
            "Epoch 7948/10000\n",
            "Step 0: Train Loss: 3.8120873796287924e-05, Train Acc: 1.0\n",
            "Epoch 7949/10000\n",
            "Step 0: Train Loss: 8.109586633509025e-05, Train Acc: 1.0\n",
            "Epoch 7950/10000\n",
            "Step 0: Train Loss: 0.00012131898256484419, Train Acc: 1.0\n",
            "Epoch 7951/10000\n",
            "Step 0: Train Loss: 0.00011716803419403732, Train Acc: 1.0\n",
            "Epoch 7952/10000\n",
            "Step 0: Train Loss: 8.43395318952389e-05, Train Acc: 1.0\n",
            "Epoch 7953/10000\n",
            "Step 0: Train Loss: 5.797397534479387e-05, Train Acc: 1.0\n",
            "Epoch 7954/10000\n",
            "Step 0: Train Loss: 8.3969920524396e-05, Train Acc: 1.0\n",
            "Epoch 7955/10000\n",
            "Step 0: Train Loss: 9.274741023546085e-05, Train Acc: 1.0\n",
            "Epoch 7956/10000\n",
            "Step 0: Train Loss: 0.00011337845353409648, Train Acc: 1.0\n",
            "Epoch 7957/10000\n",
            "Step 0: Train Loss: 0.00011093553621321917, Train Acc: 1.0\n",
            "Epoch 7958/10000\n",
            "Step 0: Train Loss: 9.325332939624786e-05, Train Acc: 1.0\n",
            "Epoch 7959/10000\n",
            "Step 0: Train Loss: 0.00010011145786847919, Train Acc: 1.0\n",
            "Epoch 7960/10000\n",
            "Step 0: Train Loss: 0.00010188491432927549, Train Acc: 1.0\n",
            "Epoch 7961/10000\n",
            "Step 0: Train Loss: 0.00013201918045524508, Train Acc: 1.0\n",
            "Epoch 7962/10000\n",
            "Step 0: Train Loss: 6.727169966325164e-05, Train Acc: 1.0\n",
            "Epoch 7963/10000\n",
            "Step 0: Train Loss: 0.00018979293236043304, Train Acc: 1.0\n",
            "Epoch 7964/10000\n",
            "Step 0: Train Loss: 9.676433546701446e-05, Train Acc: 1.0\n",
            "Epoch 7965/10000\n",
            "Step 0: Train Loss: 9.387492173118517e-05, Train Acc: 1.0\n",
            "Epoch 7966/10000\n",
            "Step 0: Train Loss: 0.00010532893793424591, Train Acc: 1.0\n",
            "Epoch 7967/10000\n",
            "Step 0: Train Loss: 9.733980550663546e-05, Train Acc: 1.0\n",
            "Epoch 7968/10000\n",
            "Step 0: Train Loss: 0.00011468456796137616, Train Acc: 1.0\n",
            "Epoch 7969/10000\n",
            "Step 0: Train Loss: 0.00019928278925362974, Train Acc: 1.0\n",
            "Epoch 7970/10000\n",
            "Step 0: Train Loss: 0.00011021408863598481, Train Acc: 1.0\n",
            "Epoch 7971/10000\n",
            "Step 0: Train Loss: 6.796561501687393e-05, Train Acc: 1.0\n",
            "Epoch 7972/10000\n",
            "Step 0: Train Loss: 4.9858386773848906e-05, Train Acc: 1.0\n",
            "Epoch 7973/10000\n",
            "Step 0: Train Loss: 9.175477316603065e-05, Train Acc: 1.0\n",
            "Epoch 7974/10000\n",
            "Step 0: Train Loss: 0.00018486054614186287, Train Acc: 1.0\n",
            "Epoch 7975/10000\n",
            "Step 0: Train Loss: 0.00018578124581836164, Train Acc: 1.0\n",
            "Epoch 7976/10000\n",
            "Step 0: Train Loss: 6.877221312606707e-05, Train Acc: 1.0\n",
            "Epoch 7977/10000\n",
            "Step 0: Train Loss: 5.6037413742160425e-05, Train Acc: 1.0\n",
            "Epoch 7978/10000\n",
            "Step 0: Train Loss: 3.0735780455870554e-05, Train Acc: 1.0\n",
            "Epoch 7979/10000\n",
            "Step 0: Train Loss: 0.00012887667980976403, Train Acc: 1.0\n",
            "Epoch 7980/10000\n",
            "Step 0: Train Loss: 0.00010166436550207436, Train Acc: 1.0\n",
            "Epoch 7981/10000\n",
            "Step 0: Train Loss: 4.0239472582470626e-05, Train Acc: 1.0\n",
            "Epoch 7982/10000\n",
            "Step 0: Train Loss: 7.699233538005501e-05, Train Acc: 1.0\n",
            "Epoch 7983/10000\n",
            "Step 0: Train Loss: 5.5090105888666585e-05, Train Acc: 1.0\n",
            "Epoch 7984/10000\n",
            "Step 0: Train Loss: 7.42132033337839e-05, Train Acc: 1.0\n",
            "Epoch 7985/10000\n",
            "Step 0: Train Loss: 8.764009544393048e-05, Train Acc: 1.0\n",
            "Epoch 7986/10000\n",
            "Step 0: Train Loss: 6.907352508278564e-05, Train Acc: 1.0\n",
            "Epoch 7987/10000\n",
            "Step 0: Train Loss: 8.891923789633438e-05, Train Acc: 1.0\n",
            "Epoch 7988/10000\n",
            "Step 0: Train Loss: 0.0001628043391974643, Train Acc: 1.0\n",
            "Epoch 7989/10000\n",
            "Step 0: Train Loss: 4.819537571165711e-05, Train Acc: 1.0\n",
            "Epoch 7990/10000\n",
            "Step 0: Train Loss: 6.225998367881402e-05, Train Acc: 1.0\n",
            "Epoch 7991/10000\n",
            "Step 0: Train Loss: 6.828876939835027e-05, Train Acc: 1.0\n",
            "Epoch 7992/10000\n",
            "Step 0: Train Loss: 5.915627480135299e-05, Train Acc: 1.0\n",
            "Epoch 7993/10000\n",
            "Step 0: Train Loss: 7.949956489028409e-05, Train Acc: 1.0\n",
            "Epoch 7994/10000\n",
            "Step 0: Train Loss: 4.864477159571834e-05, Train Acc: 1.0\n",
            "Epoch 7995/10000\n",
            "Step 0: Train Loss: 3.521937833284028e-05, Train Acc: 1.0\n",
            "Epoch 7996/10000\n",
            "Step 0: Train Loss: 7.19398885848932e-05, Train Acc: 1.0\n",
            "Epoch 7997/10000\n",
            "Step 0: Train Loss: 5.099346526549198e-05, Train Acc: 1.0\n",
            "Epoch 7998/10000\n",
            "Step 0: Train Loss: 4.486661418923177e-05, Train Acc: 1.0\n",
            "Epoch 7999/10000\n",
            "Step 0: Train Loss: 0.00010868949902942404, Train Acc: 1.0\n",
            "Epoch 8000/10000\n",
            "Step 0: Train Loss: 0.0001229595363838598, Train Acc: 1.0\n",
            "Epoch 8001/10000\n",
            "Step 0: Train Loss: 3.3906686439877376e-05, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 8000 1024 0.9286715300448591\n",
            "Epoch index and hidden dimension and ratio: 8000 20 0.8396074918676317\n",
            "Epoch index and hidden dimension and ratio: 8000 20 1.6312338047338104\n",
            "Epoch index and hidden dimension and ratio: 8000 20 4.880438848167514\n",
            "MI(X;T): [10.140310554475967, 6.934959890005551, 5.435716185857464, 2.1053398220900084], MI(Y;T): [2.2161170863262876, 3.246209288391497, 3.0665972896341613, 2.7354203594292685]\n",
            "Epoch index and hidden dimension and ratio: 8000 1024 0.9286846727227245\n",
            "Epoch index and hidden dimension and ratio: 8000 20 0.8396081427608416\n",
            "Epoch index and hidden dimension and ratio: 8000 20 1.631206762368708\n",
            "Epoch index and hidden dimension and ratio: 8000 20 4.880387847623206\n",
            "MI(X;T): [10.140262499775613, 6.934778443725371, 5.436298593012118, 2.105727833486255], MI(Y;T): [2.2160939166186755, 3.246019806406784, 3.066450514568135, 2.7351408345254384]\n",
            "Epoch index and hidden dimension and ratio: 8000 1024 0.9286985496284036\n",
            "Epoch index and hidden dimension and ratio: 8000 20 0.8396064070456153\n",
            "Epoch index and hidden dimension and ratio: 8000 20 1.6312044770984178\n",
            "Epoch index and hidden dimension and ratio: 8000 20 4.880424699629415\n",
            "MI(X;T): [10.140486408800504, 6.935079937261079, 5.436195588913526, 2.1051039904207856], MI(Y;T): [2.216031640789757, 3.24625064574745, 3.066122494309133, 2.7347013294368194]\n",
            "Epoch index and hidden dimension and ratio: 8000 1024 0.9287036158003182\n",
            "Epoch index and hidden dimension and ratio: 8000 20 0.8396070579388252\n",
            "Epoch index and hidden dimension and ratio: 8000 20 1.631217173044475\n",
            "Epoch index and hidden dimension and ratio: 8000 20 4.8805628946527\n",
            "MI(X;T): [10.140246616450295, 6.935777541365982, 5.435923969048173, 2.1051464497045496], MI(Y;T): [2.21596031649767, 3.246295387244444, 3.0666702315122327, 2.7350210396946775]\n",
            "Epoch index and hidden dimension and ratio: 8000 1024 0.9287113251923622\n",
            "Epoch index and hidden dimension and ratio: 8000 20 0.8396169298191754\n",
            "Epoch index and hidden dimension and ratio: 8000 20 1.6312463737204073\n",
            "Epoch index and hidden dimension and ratio: 8000 20 4.8807596580429955\n",
            "MI(X;T): [10.140026852038986, 6.935901181513756, 5.436335178192678, 2.1048513310418553], MI(Y;T): [2.2156926471594884, 3.246146437537811, 3.066701192517325, 2.7347403320280055]\n",
            "Epoch index and hidden dimension and ratio: 8000 1024 0.9287189611616248\n",
            "Epoch index and hidden dimension and ratio: 8000 20 0.8396313579519948\n",
            "Epoch index and hidden dimension and ratio: 8000 20 1.6312702420989953\n",
            "Epoch index and hidden dimension and ratio: 8000 20 4.880881401277794\n",
            "MI(X;T): [10.139792941526508, 6.937614802101217, 5.436874045021963, 2.1050242542066235], MI(Y;T): [2.215529485435151, 3.246264949511526, 3.06687106373853, 2.7350357174864626]\n",
            "Epoch 8002/10000\n",
            "Step 0: Train Loss: 5.879188029211946e-05, Train Acc: 1.0\n",
            "Epoch 8003/10000\n",
            "Step 0: Train Loss: 4.1960101953009143e-05, Train Acc: 1.0\n",
            "Epoch 8004/10000\n",
            "Step 0: Train Loss: 4.7790374082978815e-05, Train Acc: 1.0\n",
            "Epoch 8005/10000\n",
            "Step 0: Train Loss: 4.483988232095726e-05, Train Acc: 1.0\n",
            "Epoch 8006/10000\n",
            "Step 0: Train Loss: 7.930886931717396e-05, Train Acc: 1.0\n",
            "Epoch 8007/10000\n",
            "Step 0: Train Loss: 3.1387247872771695e-05, Train Acc: 1.0\n",
            "Epoch 8008/10000\n",
            "Step 0: Train Loss: 0.00021803920390084386, Train Acc: 1.0\n",
            "Epoch 8009/10000\n",
            "Step 0: Train Loss: 0.0001340721792075783, Train Acc: 1.0\n",
            "Epoch 8010/10000\n",
            "Step 0: Train Loss: 0.00015010380593594164, Train Acc: 1.0\n",
            "Epoch 8011/10000\n",
            "Step 0: Train Loss: 3.42017192451749e-05, Train Acc: 1.0\n",
            "Epoch 8012/10000\n",
            "Step 0: Train Loss: 5.501579653355293e-05, Train Acc: 1.0\n",
            "Epoch 8013/10000\n",
            "Step 0: Train Loss: 5.061107731307857e-05, Train Acc: 1.0\n",
            "Epoch 8014/10000\n",
            "Step 0: Train Loss: 6.749064050382003e-05, Train Acc: 1.0\n",
            "Epoch 8015/10000\n",
            "Step 0: Train Loss: 8.167256601154804e-05, Train Acc: 1.0\n",
            "Epoch 8016/10000\n",
            "Step 0: Train Loss: 7.740099681541324e-05, Train Acc: 1.0\n",
            "Epoch 8017/10000\n",
            "Step 0: Train Loss: 2.7017680622520857e-05, Train Acc: 1.0\n",
            "Epoch 8018/10000\n",
            "Step 0: Train Loss: 9.034139657160267e-05, Train Acc: 1.0\n",
            "Epoch 8019/10000\n",
            "Step 0: Train Loss: 0.00010419355385238305, Train Acc: 1.0\n",
            "Epoch 8020/10000\n",
            "Step 0: Train Loss: 3.6009001632919535e-05, Train Acc: 1.0\n",
            "Epoch 8021/10000\n",
            "Step 0: Train Loss: 9.609945846023038e-05, Train Acc: 1.0\n",
            "Epoch 8022/10000\n",
            "Step 0: Train Loss: 5.4875737987458706e-05, Train Acc: 1.0\n",
            "Epoch 8023/10000\n",
            "Step 0: Train Loss: 4.654060467146337e-05, Train Acc: 1.0\n",
            "Epoch 8024/10000\n",
            "Step 0: Train Loss: 0.00011762001668103039, Train Acc: 1.0\n",
            "Epoch 8025/10000\n",
            "Step 0: Train Loss: 6.239450158318505e-05, Train Acc: 1.0\n",
            "Epoch 8026/10000\n",
            "Step 0: Train Loss: 0.0001307681086473167, Train Acc: 1.0\n",
            "Epoch 8027/10000\n",
            "Step 0: Train Loss: 2.723547186178621e-05, Train Acc: 1.0\n",
            "Epoch 8028/10000\n",
            "Step 0: Train Loss: 3.0241495551308617e-05, Train Acc: 1.0\n",
            "Epoch 8029/10000\n",
            "Step 0: Train Loss: 4.3371928768465295e-05, Train Acc: 1.0\n",
            "Epoch 8030/10000\n",
            "Step 0: Train Loss: 0.00010024360381066799, Train Acc: 1.0\n",
            "Epoch 8031/10000\n",
            "Step 0: Train Loss: 8.548394544050097e-05, Train Acc: 1.0\n",
            "Epoch 8032/10000\n",
            "Step 0: Train Loss: 0.00010297458356944844, Train Acc: 1.0\n",
            "Epoch 8033/10000\n",
            "Step 0: Train Loss: 5.7743167417356744e-05, Train Acc: 1.0\n",
            "Epoch 8034/10000\n",
            "Step 0: Train Loss: 9.576311276759952e-05, Train Acc: 1.0\n",
            "Epoch 8035/10000\n",
            "Step 0: Train Loss: 2.6911771783488803e-05, Train Acc: 1.0\n",
            "Epoch 8036/10000\n",
            "Step 0: Train Loss: 8.056058868533e-05, Train Acc: 1.0\n",
            "Epoch 8037/10000\n",
            "Step 0: Train Loss: 3.055860361200757e-05, Train Acc: 1.0\n",
            "Epoch 8038/10000\n",
            "Step 0: Train Loss: 5.2009199862368405e-05, Train Acc: 1.0\n",
            "Epoch 8039/10000\n",
            "Step 0: Train Loss: 9.299875819124281e-05, Train Acc: 1.0\n",
            "Epoch 8040/10000\n",
            "Step 0: Train Loss: 0.00012668759154621512, Train Acc: 1.0\n",
            "Epoch 8041/10000\n",
            "Step 0: Train Loss: 2.68669864453841e-05, Train Acc: 1.0\n",
            "Epoch 8042/10000\n",
            "Step 0: Train Loss: 4.692647053161636e-05, Train Acc: 1.0\n",
            "Epoch 8043/10000\n",
            "Step 0: Train Loss: 2.753163244051393e-05, Train Acc: 1.0\n",
            "Epoch 8044/10000\n",
            "Step 0: Train Loss: 8.317123865708709e-05, Train Acc: 1.0\n",
            "Epoch 8045/10000\n",
            "Step 0: Train Loss: 7.833700510673225e-05, Train Acc: 1.0\n",
            "Epoch 8046/10000\n",
            "Step 0: Train Loss: 9.467559721088037e-05, Train Acc: 1.0\n",
            "Epoch 8047/10000\n",
            "Step 0: Train Loss: 4.186385194770992e-05, Train Acc: 1.0\n",
            "Epoch 8048/10000\n",
            "Step 0: Train Loss: 1.995819911826402e-05, Train Acc: 1.0\n",
            "Epoch 8049/10000\n",
            "Step 0: Train Loss: 0.0001098528882721439, Train Acc: 1.0\n",
            "Epoch 8050/10000\n",
            "Step 0: Train Loss: 0.0001520862861070782, Train Acc: 1.0\n",
            "Epoch 8051/10000\n",
            "Step 0: Train Loss: 7.318733696592972e-05, Train Acc: 1.0\n",
            "Epoch 8052/10000\n",
            "Step 0: Train Loss: 6.857290281914175e-05, Train Acc: 1.0\n",
            "Epoch 8053/10000\n",
            "Step 0: Train Loss: 2.938180841738358e-05, Train Acc: 1.0\n",
            "Epoch 8054/10000\n",
            "Step 0: Train Loss: 5.311968925525434e-05, Train Acc: 1.0\n",
            "Epoch 8055/10000\n",
            "Step 0: Train Loss: 3.649194695753977e-05, Train Acc: 1.0\n",
            "Epoch 8056/10000\n",
            "Step 0: Train Loss: 3.095047941314988e-05, Train Acc: 1.0\n",
            "Epoch 8057/10000\n",
            "Step 0: Train Loss: 6.831345672253519e-05, Train Acc: 1.0\n",
            "Epoch 8058/10000\n",
            "Step 0: Train Loss: 4.749241270474158e-05, Train Acc: 1.0\n",
            "Epoch 8059/10000\n",
            "Step 0: Train Loss: 2.5962888685171492e-05, Train Acc: 1.0\n",
            "Epoch 8060/10000\n",
            "Step 0: Train Loss: 3.423107409616932e-05, Train Acc: 1.0\n",
            "Epoch 8061/10000\n",
            "Step 0: Train Loss: 2.508142642909661e-05, Train Acc: 1.0\n",
            "Epoch 8062/10000\n",
            "Step 0: Train Loss: 9.565841901348904e-05, Train Acc: 1.0\n",
            "Epoch 8063/10000\n",
            "Step 0: Train Loss: 2.3225980839924887e-05, Train Acc: 1.0\n",
            "Epoch 8064/10000\n",
            "Step 0: Train Loss: 4.1957158828154206e-05, Train Acc: 1.0\n",
            "Epoch 8065/10000\n",
            "Step 0: Train Loss: 3.960017784265801e-05, Train Acc: 1.0\n",
            "Epoch 8066/10000\n",
            "Step 0: Train Loss: 2.1534589905058965e-05, Train Acc: 1.0\n",
            "Epoch 8067/10000\n",
            "Step 0: Train Loss: 4.630438343156129e-05, Train Acc: 1.0\n",
            "Epoch 8068/10000\n",
            "Step 0: Train Loss: 0.0001275677204830572, Train Acc: 1.0\n",
            "Epoch 8069/10000\n",
            "Step 0: Train Loss: 2.3446535124094225e-05, Train Acc: 1.0\n",
            "Epoch 8070/10000\n",
            "Step 0: Train Loss: 5.555333336815238e-05, Train Acc: 1.0\n",
            "Epoch 8071/10000\n",
            "Step 0: Train Loss: 8.000346133485436e-05, Train Acc: 1.0\n",
            "Epoch 8072/10000\n",
            "Step 0: Train Loss: 4.507619087235071e-05, Train Acc: 1.0\n",
            "Epoch 8073/10000\n",
            "Step 0: Train Loss: 1.6080872228485532e-05, Train Acc: 1.0\n",
            "Epoch 8074/10000\n",
            "Step 0: Train Loss: 4.876661841990426e-05, Train Acc: 1.0\n",
            "Epoch 8075/10000\n",
            "Step 0: Train Loss: 4.777393405674957e-05, Train Acc: 1.0\n",
            "Epoch 8076/10000\n",
            "Step 0: Train Loss: 4.291719960747287e-05, Train Acc: 1.0\n",
            "Epoch 8077/10000\n",
            "Step 0: Train Loss: 7.765248301438987e-05, Train Acc: 1.0\n",
            "Epoch 8078/10000\n",
            "Step 0: Train Loss: 2.56212406384293e-05, Train Acc: 1.0\n",
            "Epoch 8079/10000\n",
            "Step 0: Train Loss: 5.2010749641340226e-05, Train Acc: 1.0\n",
            "Epoch 8080/10000\n",
            "Step 0: Train Loss: 0.0001170551186078228, Train Acc: 1.0\n",
            "Epoch 8081/10000\n",
            "Step 0: Train Loss: 0.00011298501340206712, Train Acc: 1.0\n",
            "Epoch 8082/10000\n",
            "Step 0: Train Loss: 2.025916546699591e-05, Train Acc: 1.0\n",
            "Epoch 8083/10000\n",
            "Step 0: Train Loss: 3.143849244224839e-05, Train Acc: 1.0\n",
            "Epoch 8084/10000\n",
            "Step 0: Train Loss: 2.3985887310118414e-05, Train Acc: 1.0\n",
            "Epoch 8085/10000\n",
            "Step 0: Train Loss: 2.7238113034400158e-05, Train Acc: 1.0\n",
            "Epoch 8086/10000\n",
            "Step 0: Train Loss: 6.0759502957807854e-05, Train Acc: 1.0\n",
            "Epoch 8087/10000\n",
            "Step 0: Train Loss: 3.218251731595956e-05, Train Acc: 1.0\n",
            "Epoch 8088/10000\n",
            "Step 0: Train Loss: 6.998054595896974e-05, Train Acc: 1.0\n",
            "Epoch 8089/10000\n",
            "Step 0: Train Loss: 3.443806781433523e-05, Train Acc: 1.0\n",
            "Epoch 8090/10000\n",
            "Step 0: Train Loss: 6.127899541752413e-05, Train Acc: 1.0\n",
            "Epoch 8091/10000\n",
            "Step 0: Train Loss: 3.0828545277472585e-05, Train Acc: 1.0\n",
            "Epoch 8092/10000\n",
            "Step 0: Train Loss: 1.8410830307402648e-05, Train Acc: 1.0\n",
            "Epoch 8093/10000\n",
            "Step 0: Train Loss: 1.8617742171045393e-05, Train Acc: 1.0\n",
            "Epoch 8094/10000\n",
            "Step 0: Train Loss: 7.596945215482265e-05, Train Acc: 1.0\n",
            "Epoch 8095/10000\n",
            "Step 0: Train Loss: 4.373400588519871e-05, Train Acc: 1.0\n",
            "Epoch 8096/10000\n",
            "Step 0: Train Loss: 3.673823448480107e-05, Train Acc: 1.0\n",
            "Epoch 8097/10000\n",
            "Step 0: Train Loss: 3.244054823881015e-05, Train Acc: 1.0\n",
            "Epoch 8098/10000\n",
            "Step 0: Train Loss: 4.6359156840480864e-05, Train Acc: 1.0\n",
            "Epoch 8099/10000\n",
            "Step 0: Train Loss: 2.9490629458450712e-05, Train Acc: 1.0\n",
            "Epoch 8100/10000\n",
            "Step 0: Train Loss: 1.853892536018975e-05, Train Acc: 1.0\n",
            "Epoch 8101/10000\n",
            "Step 0: Train Loss: 2.131527435267344e-05, Train Acc: 1.0\n",
            "Epoch 8102/10000\n",
            "Step 0: Train Loss: 8.253765554400161e-05, Train Acc: 1.0\n",
            "Epoch 8103/10000\n",
            "Step 0: Train Loss: 4.0015973354456946e-05, Train Acc: 1.0\n",
            "Epoch 8104/10000\n",
            "Step 0: Train Loss: 6.34171228739433e-05, Train Acc: 1.0\n",
            "Epoch 8105/10000\n",
            "Step 0: Train Loss: 5.2992243581684306e-05, Train Acc: 1.0\n",
            "Epoch 8106/10000\n",
            "Step 0: Train Loss: 6.0817317717010155e-05, Train Acc: 1.0\n",
            "Epoch 8107/10000\n",
            "Step 0: Train Loss: 6.255892367335036e-05, Train Acc: 1.0\n",
            "Epoch 8108/10000\n",
            "Step 0: Train Loss: 2.1349036614992656e-05, Train Acc: 1.0\n",
            "Epoch 8109/10000\n",
            "Step 0: Train Loss: 3.437366467551328e-05, Train Acc: 1.0\n",
            "Epoch 8110/10000\n",
            "Step 0: Train Loss: 5.29849057784304e-05, Train Acc: 1.0\n",
            "Epoch 8111/10000\n",
            "Step 0: Train Loss: 3.483034379314631e-05, Train Acc: 1.0\n",
            "Epoch 8112/10000\n",
            "Step 0: Train Loss: 5.102957584313117e-05, Train Acc: 1.0\n",
            "Epoch 8113/10000\n",
            "Step 0: Train Loss: 2.6720959795056842e-05, Train Acc: 1.0\n",
            "Epoch 8114/10000\n",
            "Step 0: Train Loss: 1.829552638810128e-05, Train Acc: 1.0\n",
            "Epoch 8115/10000\n",
            "Step 0: Train Loss: 7.326111517613754e-05, Train Acc: 1.0\n",
            "Epoch 8116/10000\n",
            "Step 0: Train Loss: 2.1056779587524943e-05, Train Acc: 1.0\n",
            "Epoch 8117/10000\n",
            "Step 0: Train Loss: 6.831085920566693e-05, Train Acc: 1.0\n",
            "Epoch 8118/10000\n",
            "Step 0: Train Loss: 3.697374268085696e-05, Train Acc: 1.0\n",
            "Epoch 8119/10000\n",
            "Step 0: Train Loss: 7.009951514191926e-05, Train Acc: 1.0\n",
            "Epoch 8120/10000\n",
            "Step 0: Train Loss: 2.3742544726701453e-05, Train Acc: 1.0\n",
            "Epoch 8121/10000\n",
            "Step 0: Train Loss: 6.386063614627346e-05, Train Acc: 1.0\n",
            "Epoch 8122/10000\n",
            "Step 0: Train Loss: 2.7964604669250548e-05, Train Acc: 1.0\n",
            "Epoch 8123/10000\n",
            "Step 0: Train Loss: 6.937569560250267e-05, Train Acc: 1.0\n",
            "Epoch 8124/10000\n",
            "Step 0: Train Loss: 3.705822382471524e-05, Train Acc: 1.0\n",
            "Epoch 8125/10000\n",
            "Step 0: Train Loss: 7.555642514489591e-05, Train Acc: 1.0\n",
            "Epoch 8126/10000\n",
            "Step 0: Train Loss: 3.5028384445467964e-05, Train Acc: 1.0\n",
            "Epoch 8127/10000\n",
            "Step 0: Train Loss: 5.795248216600157e-05, Train Acc: 1.0\n",
            "Epoch 8128/10000\n",
            "Step 0: Train Loss: 3.039637886104174e-05, Train Acc: 1.0\n",
            "Epoch 8129/10000\n",
            "Step 0: Train Loss: 6.654958997387439e-05, Train Acc: 1.0\n",
            "Epoch 8130/10000\n",
            "Step 0: Train Loss: 3.095325155300088e-05, Train Acc: 1.0\n",
            "Epoch 8131/10000\n",
            "Step 0: Train Loss: 6.220907380338758e-05, Train Acc: 1.0\n",
            "Epoch 8132/10000\n",
            "Step 0: Train Loss: 2.5849569283309393e-05, Train Acc: 1.0\n",
            "Epoch 8133/10000\n",
            "Step 0: Train Loss: 8.817543857730925e-05, Train Acc: 1.0\n",
            "Epoch 8134/10000\n",
            "Step 0: Train Loss: 4.5007956941844895e-05, Train Acc: 1.0\n",
            "Epoch 8135/10000\n",
            "Step 0: Train Loss: 7.407584780594334e-05, Train Acc: 1.0\n",
            "Epoch 8136/10000\n",
            "Step 0: Train Loss: 2.119850614690222e-05, Train Acc: 1.0\n",
            "Epoch 8137/10000\n",
            "Step 0: Train Loss: 6.309054879238829e-05, Train Acc: 1.0\n",
            "Epoch 8138/10000\n",
            "Step 0: Train Loss: 8.580745634390041e-05, Train Acc: 1.0\n",
            "Epoch 8139/10000\n",
            "Step 0: Train Loss: 2.845486960723065e-05, Train Acc: 1.0\n",
            "Epoch 8140/10000\n",
            "Step 0: Train Loss: 3.5693901736522093e-05, Train Acc: 1.0\n",
            "Epoch 8141/10000\n",
            "Step 0: Train Loss: 2.2940168491913937e-05, Train Acc: 1.0\n",
            "Epoch 8142/10000\n",
            "Step 0: Train Loss: 5.3159092203713953e-05, Train Acc: 1.0\n",
            "Epoch 8143/10000\n",
            "Step 0: Train Loss: 2.6044368496513925e-05, Train Acc: 1.0\n",
            "Epoch 8144/10000\n",
            "Step 0: Train Loss: 3.4309701732126996e-05, Train Acc: 1.0\n",
            "Epoch 8145/10000\n",
            "Step 0: Train Loss: 2.755985588009935e-05, Train Acc: 1.0\n",
            "Epoch 8146/10000\n",
            "Step 0: Train Loss: 2.4352773834834807e-05, Train Acc: 1.0\n",
            "Epoch 8147/10000\n",
            "Step 0: Train Loss: 2.1266230760375038e-05, Train Acc: 1.0\n",
            "Epoch 8148/10000\n",
            "Step 0: Train Loss: 3.5715165722649544e-05, Train Acc: 1.0\n",
            "Epoch 8149/10000\n",
            "Step 0: Train Loss: 4.170183456153609e-05, Train Acc: 1.0\n",
            "Epoch 8150/10000\n",
            "Step 0: Train Loss: 9.883038728730753e-05, Train Acc: 1.0\n",
            "Epoch 8151/10000\n",
            "Step 0: Train Loss: 2.6798310500453226e-05, Train Acc: 1.0\n",
            "Epoch 8152/10000\n",
            "Step 0: Train Loss: 3.8898782804608345e-05, Train Acc: 1.0\n",
            "Epoch 8153/10000\n",
            "Step 0: Train Loss: 4.6479315642500296e-05, Train Acc: 1.0\n",
            "Epoch 8154/10000\n",
            "Step 0: Train Loss: 2.4928815037128516e-05, Train Acc: 1.0\n",
            "Epoch 8155/10000\n",
            "Step 0: Train Loss: 2.37252297665691e-05, Train Acc: 1.0\n",
            "Epoch 8156/10000\n",
            "Step 0: Train Loss: 4.345121851656586e-05, Train Acc: 1.0\n",
            "Epoch 8157/10000\n",
            "Step 0: Train Loss: 6.299435335677117e-05, Train Acc: 1.0\n",
            "Epoch 8158/10000\n",
            "Step 0: Train Loss: 1.4457440556725487e-05, Train Acc: 1.0\n",
            "Epoch 8159/10000\n",
            "Step 0: Train Loss: 1.6620790120214224e-05, Train Acc: 1.0\n",
            "Epoch 8160/10000\n",
            "Step 0: Train Loss: 1.5257575796567835e-05, Train Acc: 1.0\n",
            "Epoch 8161/10000\n",
            "Step 0: Train Loss: 5.4127187468111515e-05, Train Acc: 1.0\n",
            "Epoch 8162/10000\n",
            "Step 0: Train Loss: 6.538646266562864e-05, Train Acc: 1.0\n",
            "Epoch 8163/10000\n",
            "Step 0: Train Loss: 2.7464287995826453e-05, Train Acc: 1.0\n",
            "Epoch 8164/10000\n",
            "Step 0: Train Loss: 2.164072975574527e-05, Train Acc: 1.0\n",
            "Epoch 8165/10000\n",
            "Step 0: Train Loss: 2.2711516066920012e-05, Train Acc: 1.0\n",
            "Epoch 8166/10000\n",
            "Step 0: Train Loss: 3.1647072319174185e-05, Train Acc: 1.0\n",
            "Epoch 8167/10000\n",
            "Step 0: Train Loss: 5.434225022327155e-05, Train Acc: 1.0\n",
            "Epoch 8168/10000\n",
            "Step 0: Train Loss: 7.041915523586795e-05, Train Acc: 1.0\n",
            "Epoch 8169/10000\n",
            "Step 0: Train Loss: 2.5368428396177478e-05, Train Acc: 1.0\n",
            "Epoch 8170/10000\n",
            "Step 0: Train Loss: 2.2012431145412847e-05, Train Acc: 1.0\n",
            "Epoch 8171/10000\n",
            "Step 0: Train Loss: 3.963935887441039e-05, Train Acc: 1.0\n",
            "Epoch 8172/10000\n",
            "Step 0: Train Loss: 7.028250547591597e-05, Train Acc: 1.0\n",
            "Epoch 8173/10000\n",
            "Step 0: Train Loss: 1.8112765246769413e-05, Train Acc: 1.0\n",
            "Epoch 8174/10000\n",
            "Step 0: Train Loss: 1.5856758182053454e-05, Train Acc: 1.0\n",
            "Epoch 8175/10000\n",
            "Step 0: Train Loss: 1.8251541405334137e-05, Train Acc: 1.0\n",
            "Epoch 8176/10000\n",
            "Step 0: Train Loss: 5.724652874050662e-05, Train Acc: 1.0\n",
            "Epoch 8177/10000\n",
            "Step 0: Train Loss: 2.633043914102018e-05, Train Acc: 1.0\n",
            "Epoch 8178/10000\n",
            "Step 0: Train Loss: 2.4882261641323566e-05, Train Acc: 1.0\n",
            "Epoch 8179/10000\n",
            "Step 0: Train Loss: 1.759936276357621e-05, Train Acc: 1.0\n",
            "Epoch 8180/10000\n",
            "Step 0: Train Loss: 1.8227450709673576e-05, Train Acc: 1.0\n",
            "Epoch 8181/10000\n",
            "Step 0: Train Loss: 1.617017187527381e-05, Train Acc: 1.0\n",
            "Epoch 8182/10000\n",
            "Step 0: Train Loss: 3.986620140494779e-05, Train Acc: 1.0\n",
            "Epoch 8183/10000\n",
            "Step 0: Train Loss: 2.550608587625902e-05, Train Acc: 1.0\n",
            "Epoch 8184/10000\n",
            "Step 0: Train Loss: 1.6167221474461257e-05, Train Acc: 1.0\n",
            "Epoch 8185/10000\n",
            "Step 0: Train Loss: 4.401503247208893e-05, Train Acc: 1.0\n",
            "Epoch 8186/10000\n",
            "Step 0: Train Loss: 5.638164293486625e-05, Train Acc: 1.0\n",
            "Epoch 8187/10000\n",
            "Step 0: Train Loss: 3.51063790731132e-05, Train Acc: 1.0\n",
            "Epoch 8188/10000\n",
            "Step 0: Train Loss: 2.4172872144845314e-05, Train Acc: 1.0\n",
            "Epoch 8189/10000\n",
            "Step 0: Train Loss: 3.216542609152384e-05, Train Acc: 1.0\n",
            "Epoch 8190/10000\n",
            "Step 0: Train Loss: 2.0459197912714444e-05, Train Acc: 1.0\n",
            "Epoch 8191/10000\n",
            "Step 0: Train Loss: 1.6315929315169342e-05, Train Acc: 1.0\n",
            "Epoch 8192/10000\n",
            "Step 0: Train Loss: 2.620931263663806e-05, Train Acc: 1.0\n",
            "Epoch 8193/10000\n",
            "Step 0: Train Loss: 4.0290771721629426e-05, Train Acc: 1.0\n",
            "Epoch 8194/10000\n",
            "Step 0: Train Loss: 1.7266111171920784e-05, Train Acc: 1.0\n",
            "Epoch 8195/10000\n",
            "Step 0: Train Loss: 1.6100495486170985e-05, Train Acc: 1.0\n",
            "Epoch 8196/10000\n",
            "Step 0: Train Loss: 1.8598952010506764e-05, Train Acc: 1.0\n",
            "Epoch 8197/10000\n",
            "Step 0: Train Loss: 3.068304431508295e-05, Train Acc: 1.0\n",
            "Epoch 8198/10000\n",
            "Step 0: Train Loss: 2.1485670004040003e-05, Train Acc: 1.0\n",
            "Epoch 8199/10000\n",
            "Step 0: Train Loss: 2.133937414328102e-05, Train Acc: 1.0\n",
            "Epoch 8200/10000\n",
            "Step 0: Train Loss: 2.6695906854001805e-05, Train Acc: 1.0\n",
            "Epoch 8201/10000\n",
            "Step 0: Train Loss: 2.5366553018102422e-05, Train Acc: 1.0\n",
            "Epoch 8202/10000\n",
            "Step 0: Train Loss: 3.654017564258538e-05, Train Acc: 1.0\n",
            "Epoch 8203/10000\n",
            "Step 0: Train Loss: 2.3256303393281996e-05, Train Acc: 1.0\n",
            "Epoch 8204/10000\n",
            "Step 0: Train Loss: 2.8321781428530812e-05, Train Acc: 1.0\n",
            "Epoch 8205/10000\n",
            "Step 0: Train Loss: 2.9117378289811313e-05, Train Acc: 1.0\n",
            "Epoch 8206/10000\n",
            "Step 0: Train Loss: 2.3116439479053952e-05, Train Acc: 1.0\n",
            "Epoch 8207/10000\n",
            "Step 0: Train Loss: 2.1440220734803006e-05, Train Acc: 1.0\n",
            "Epoch 8208/10000\n",
            "Step 0: Train Loss: 2.342110383324325e-05, Train Acc: 1.0\n",
            "Epoch 8209/10000\n",
            "Step 0: Train Loss: 4.208115205983631e-05, Train Acc: 1.0\n",
            "Epoch 8210/10000\n",
            "Step 0: Train Loss: 2.349764145037625e-05, Train Acc: 1.0\n",
            "Epoch 8211/10000\n",
            "Step 0: Train Loss: 1.884356970549561e-05, Train Acc: 1.0\n",
            "Epoch 8212/10000\n",
            "Step 0: Train Loss: 2.067143032036256e-05, Train Acc: 1.0\n",
            "Epoch 8213/10000\n",
            "Step 0: Train Loss: 1.2956312275491655e-05, Train Acc: 1.0\n",
            "Epoch 8214/10000\n",
            "Step 0: Train Loss: 1.6442994819954038e-05, Train Acc: 1.0\n",
            "Epoch 8215/10000\n",
            "Step 0: Train Loss: 1.8363960407441482e-05, Train Acc: 1.0\n",
            "Epoch 8216/10000\n",
            "Step 0: Train Loss: 3.90483983210288e-05, Train Acc: 1.0\n",
            "Epoch 8217/10000\n",
            "Step 0: Train Loss: 1.6304751625284553e-05, Train Acc: 1.0\n",
            "Epoch 8218/10000\n",
            "Step 0: Train Loss: 1.2221265933476388e-05, Train Acc: 1.0\n",
            "Epoch 8219/10000\n",
            "Step 0: Train Loss: 2.1238938643364236e-05, Train Acc: 1.0\n",
            "Epoch 8220/10000\n",
            "Step 0: Train Loss: 1.7468481019022875e-05, Train Acc: 1.0\n",
            "Epoch 8221/10000\n",
            "Step 0: Train Loss: 1.793844421627e-05, Train Acc: 1.0\n",
            "Epoch 8222/10000\n",
            "Step 0: Train Loss: 2.4619977921247482e-05, Train Acc: 1.0\n",
            "Epoch 8223/10000\n",
            "Step 0: Train Loss: 2.8065189326298423e-05, Train Acc: 1.0\n",
            "Epoch 8224/10000\n",
            "Step 0: Train Loss: 5.8030273066833615e-05, Train Acc: 1.0\n",
            "Epoch 8225/10000\n",
            "Step 0: Train Loss: 1.5241504115692805e-05, Train Acc: 1.0\n",
            "Epoch 8226/10000\n",
            "Step 0: Train Loss: 6.750594184268266e-05, Train Acc: 1.0\n",
            "Epoch 8227/10000\n",
            "Step 0: Train Loss: 5.184649853617884e-05, Train Acc: 1.0\n",
            "Epoch 8228/10000\n",
            "Step 0: Train Loss: 5.740438791690394e-05, Train Acc: 1.0\n",
            "Epoch 8229/10000\n",
            "Step 0: Train Loss: 1.8615128283272497e-05, Train Acc: 1.0\n",
            "Epoch 8230/10000\n",
            "Step 0: Train Loss: 1.7653143004281446e-05, Train Acc: 1.0\n",
            "Epoch 8231/10000\n",
            "Step 0: Train Loss: 1.87144451047061e-05, Train Acc: 1.0\n",
            "Epoch 8232/10000\n",
            "Step 0: Train Loss: 1.2296064596739598e-05, Train Acc: 1.0\n",
            "Epoch 8233/10000\n",
            "Step 0: Train Loss: 1.4470481801254209e-05, Train Acc: 1.0\n",
            "Epoch 8234/10000\n",
            "Step 0: Train Loss: 2.3123044229578227e-05, Train Acc: 1.0\n",
            "Epoch 8235/10000\n",
            "Step 0: Train Loss: 7.982459464983549e-06, Train Acc: 1.0\n",
            "Epoch 8236/10000\n",
            "Step 0: Train Loss: 1.3645880244439468e-05, Train Acc: 1.0\n",
            "Epoch 8237/10000\n",
            "Step 0: Train Loss: 1.673216866038274e-05, Train Acc: 1.0\n",
            "Epoch 8238/10000\n",
            "Step 0: Train Loss: 4.5790333388140425e-05, Train Acc: 1.0\n",
            "Epoch 8239/10000\n",
            "Step 0: Train Loss: 1.4691772776131984e-05, Train Acc: 1.0\n",
            "Epoch 8240/10000\n",
            "Step 0: Train Loss: 4.6583641960751265e-05, Train Acc: 1.0\n",
            "Epoch 8241/10000\n",
            "Step 0: Train Loss: 1.4920798093953636e-05, Train Acc: 1.0\n",
            "Epoch 8242/10000\n",
            "Step 0: Train Loss: 4.456430542632006e-05, Train Acc: 1.0\n",
            "Epoch 8243/10000\n",
            "Step 0: Train Loss: 5.8206151152262464e-05, Train Acc: 1.0\n",
            "Epoch 8244/10000\n",
            "Step 0: Train Loss: 1.3827937436872162e-05, Train Acc: 1.0\n",
            "Epoch 8245/10000\n",
            "Step 0: Train Loss: 1.2988021808268968e-05, Train Acc: 1.0\n",
            "Epoch 8246/10000\n",
            "Step 0: Train Loss: 8.233090738940518e-06, Train Acc: 1.0\n",
            "Epoch 8247/10000\n",
            "Step 0: Train Loss: 1.3257138562039472e-05, Train Acc: 1.0\n",
            "Epoch 8248/10000\n",
            "Step 0: Train Loss: 1.8168913811678067e-05, Train Acc: 1.0\n",
            "Epoch 8249/10000\n",
            "Step 0: Train Loss: 1.293008608627133e-05, Train Acc: 1.0\n",
            "Epoch 8250/10000\n",
            "Step 0: Train Loss: 2.2609345251112245e-05, Train Acc: 1.0\n",
            "Epoch 8251/10000\n",
            "Step 0: Train Loss: 1.2193269867566414e-05, Train Acc: 1.0\n",
            "Epoch 8252/10000\n",
            "Step 0: Train Loss: 2.0195091565256007e-05, Train Acc: 1.0\n",
            "Epoch 8253/10000\n",
            "Step 0: Train Loss: 1.1156677828694228e-05, Train Acc: 1.0\n",
            "Epoch 8254/10000\n",
            "Step 0: Train Loss: 2.2577438357984647e-05, Train Acc: 1.0\n",
            "Epoch 8255/10000\n",
            "Step 0: Train Loss: 4.132794128963724e-05, Train Acc: 1.0\n",
            "Epoch 8256/10000\n",
            "Step 0: Train Loss: 2.3102747945813462e-05, Train Acc: 1.0\n",
            "Epoch 8257/10000\n",
            "Step 0: Train Loss: 2.309467708982993e-05, Train Acc: 1.0\n",
            "Epoch 8258/10000\n",
            "Step 0: Train Loss: 1.216851069330005e-05, Train Acc: 1.0\n",
            "Epoch 8259/10000\n",
            "Step 0: Train Loss: 4.446858656592667e-05, Train Acc: 1.0\n",
            "Epoch 8260/10000\n",
            "Step 0: Train Loss: 4.3355790694477037e-05, Train Acc: 1.0\n",
            "Epoch 8261/10000\n",
            "Step 0: Train Loss: 1.0922715773631353e-05, Train Acc: 1.0\n",
            "Epoch 8262/10000\n",
            "Step 0: Train Loss: 2.1599245883408003e-05, Train Acc: 1.0\n",
            "Epoch 8263/10000\n",
            "Step 0: Train Loss: 4.451698623597622e-05, Train Acc: 1.0\n",
            "Epoch 8264/10000\n",
            "Step 0: Train Loss: 1.2231852451805025e-05, Train Acc: 1.0\n",
            "Epoch 8265/10000\n",
            "Step 0: Train Loss: 2.399529876129236e-05, Train Acc: 1.0\n",
            "Epoch 8266/10000\n",
            "Step 0: Train Loss: 1.1666897080431227e-05, Train Acc: 1.0\n",
            "Epoch 8267/10000\n",
            "Step 0: Train Loss: 1.5320920283556916e-05, Train Acc: 1.0\n",
            "Epoch 8268/10000\n",
            "Step 0: Train Loss: 1.0813738299475517e-05, Train Acc: 1.0\n",
            "Epoch 8269/10000\n",
            "Step 0: Train Loss: 2.643015977810137e-05, Train Acc: 1.0\n",
            "Epoch 8270/10000\n",
            "Step 0: Train Loss: 1.1404623364796862e-05, Train Acc: 1.0\n",
            "Epoch 8271/10000\n",
            "Step 0: Train Loss: 1.3532875527744181e-05, Train Acc: 1.0\n",
            "Epoch 8272/10000\n",
            "Step 0: Train Loss: 1.3798877262161113e-05, Train Acc: 1.0\n",
            "Epoch 8273/10000\n",
            "Step 0: Train Loss: 2.5758581614354625e-05, Train Acc: 1.0\n",
            "Epoch 8274/10000\n",
            "Step 0: Train Loss: 1.3533489436667878e-05, Train Acc: 1.0\n",
            "Epoch 8275/10000\n",
            "Step 0: Train Loss: 1.2944144145876635e-05, Train Acc: 1.0\n",
            "Epoch 8276/10000\n",
            "Step 0: Train Loss: 1.4314561667561065e-05, Train Acc: 1.0\n",
            "Epoch 8277/10000\n",
            "Step 0: Train Loss: 9.471768862567842e-06, Train Acc: 1.0\n",
            "Epoch 8278/10000\n",
            "Step 0: Train Loss: 1.0053009646071587e-05, Train Acc: 1.0\n",
            "Epoch 8279/10000\n",
            "Step 0: Train Loss: 1.3565406334237196e-05, Train Acc: 1.0\n",
            "Epoch 8280/10000\n",
            "Step 0: Train Loss: 1.1094418368884362e-05, Train Acc: 1.0\n",
            "Epoch 8281/10000\n",
            "Step 0: Train Loss: 1.760422674124129e-05, Train Acc: 1.0\n",
            "Epoch 8282/10000\n",
            "Step 0: Train Loss: 4.085633918293752e-05, Train Acc: 1.0\n",
            "Epoch 8283/10000\n",
            "Step 0: Train Loss: 2.2584403268410824e-05, Train Acc: 1.0\n",
            "Epoch 8284/10000\n",
            "Step 0: Train Loss: 9.052251698449254e-06, Train Acc: 1.0\n",
            "Epoch 8285/10000\n",
            "Step 0: Train Loss: 1.4678210391139146e-05, Train Acc: 1.0\n",
            "Epoch 8286/10000\n",
            "Step 0: Train Loss: 1.3901487363909837e-05, Train Acc: 1.0\n",
            "Epoch 8287/10000\n",
            "Step 0: Train Loss: 3.884835678036325e-05, Train Acc: 1.0\n",
            "Epoch 8288/10000\n",
            "Step 0: Train Loss: 1.1620722034422215e-05, Train Acc: 1.0\n",
            "Epoch 8289/10000\n",
            "Step 0: Train Loss: 1.3637729352922179e-05, Train Acc: 1.0\n",
            "Epoch 8290/10000\n",
            "Step 0: Train Loss: 1.006501497613499e-05, Train Acc: 1.0\n",
            "Epoch 8291/10000\n",
            "Step 0: Train Loss: 1.4146276953397319e-05, Train Acc: 1.0\n",
            "Epoch 8292/10000\n",
            "Step 0: Train Loss: 4.8631754907546565e-05, Train Acc: 1.0\n",
            "Epoch 8293/10000\n",
            "Step 0: Train Loss: 4.848275420954451e-05, Train Acc: 1.0\n",
            "Epoch 8294/10000\n",
            "Step 0: Train Loss: 1.3723756637773477e-05, Train Acc: 1.0\n",
            "Epoch 8295/10000\n",
            "Step 0: Train Loss: 1.0973790267598815e-05, Train Acc: 1.0\n",
            "Epoch 8296/10000\n",
            "Step 0: Train Loss: 8.149242603394669e-06, Train Acc: 1.0\n",
            "Epoch 8297/10000\n",
            "Step 0: Train Loss: 1.7155434761662036e-05, Train Acc: 1.0\n",
            "Epoch 8298/10000\n",
            "Step 0: Train Loss: 2.0073794075869955e-05, Train Acc: 1.0\n",
            "Epoch 8299/10000\n",
            "Step 0: Train Loss: 8.542640898667742e-06, Train Acc: 1.0\n",
            "Epoch 8300/10000\n",
            "Step 0: Train Loss: 1.2112725016777404e-05, Train Acc: 1.0\n",
            "Epoch 8301/10000\n",
            "Step 0: Train Loss: 6.974016287131235e-06, Train Acc: 1.0\n",
            "Epoch 8302/10000\n",
            "Step 0: Train Loss: 3.779980761464685e-05, Train Acc: 1.0\n",
            "Epoch 8303/10000\n",
            "Step 0: Train Loss: 4.0980983612826094e-05, Train Acc: 1.0\n",
            "Epoch 8304/10000\n",
            "Step 0: Train Loss: 2.052938361885026e-05, Train Acc: 1.0\n",
            "Epoch 8305/10000\n",
            "Step 0: Train Loss: 3.374953303136863e-05, Train Acc: 1.0\n",
            "Epoch 8306/10000\n",
            "Step 0: Train Loss: 1.222555056301644e-05, Train Acc: 1.0\n",
            "Epoch 8307/10000\n",
            "Step 0: Train Loss: 3.577578900149092e-05, Train Acc: 1.0\n",
            "Epoch 8308/10000\n",
            "Step 0: Train Loss: 1.2956087630300317e-05, Train Acc: 1.0\n",
            "Epoch 8309/10000\n",
            "Step 0: Train Loss: 8.324144801008515e-06, Train Acc: 1.0\n",
            "Epoch 8310/10000\n",
            "Step 0: Train Loss: 1.2065880582667887e-05, Train Acc: 1.0\n",
            "Epoch 8311/10000\n",
            "Step 0: Train Loss: 5.728398264182033e-06, Train Acc: 1.0\n",
            "Epoch 8312/10000\n",
            "Step 0: Train Loss: 9.681299161456991e-06, Train Acc: 1.0\n",
            "Epoch 8313/10000\n",
            "Step 0: Train Loss: 7.018676569714444e-06, Train Acc: 1.0\n",
            "Epoch 8314/10000\n",
            "Step 0: Train Loss: 2.7789747036877088e-05, Train Acc: 1.0\n",
            "Epoch 8315/10000\n",
            "Step 0: Train Loss: 8.396132216148544e-06, Train Acc: 1.0\n",
            "Epoch 8316/10000\n",
            "Step 0: Train Loss: 1.0609891432977747e-05, Train Acc: 1.0\n",
            "Epoch 8317/10000\n",
            "Step 0: Train Loss: 1.3295425560500007e-05, Train Acc: 1.0\n",
            "Epoch 8318/10000\n",
            "Step 0: Train Loss: 3.246970663894899e-05, Train Acc: 1.0\n",
            "Epoch 8319/10000\n",
            "Step 0: Train Loss: 9.897648851620033e-06, Train Acc: 1.0\n",
            "Epoch 8320/10000\n",
            "Step 0: Train Loss: 8.404658728977665e-06, Train Acc: 1.0\n",
            "Epoch 8321/10000\n",
            "Step 0: Train Loss: 9.052604582393542e-06, Train Acc: 1.0\n",
            "Epoch 8322/10000\n",
            "Step 0: Train Loss: 1.321904801443452e-05, Train Acc: 1.0\n",
            "Epoch 8323/10000\n",
            "Step 0: Train Loss: 1.760947452567052e-05, Train Acc: 1.0\n",
            "Epoch 8324/10000\n",
            "Step 0: Train Loss: 9.529092494631186e-06, Train Acc: 1.0\n",
            "Epoch 8325/10000\n",
            "Step 0: Train Loss: 1.8803917555487715e-05, Train Acc: 1.0\n",
            "Epoch 8326/10000\n",
            "Step 0: Train Loss: 2.657857294252608e-05, Train Acc: 1.0\n",
            "Epoch 8327/10000\n",
            "Step 0: Train Loss: 8.871153113432229e-06, Train Acc: 1.0\n",
            "Epoch 8328/10000\n",
            "Step 0: Train Loss: 1.6133013559738174e-05, Train Acc: 1.0\n",
            "Epoch 8329/10000\n",
            "Step 0: Train Loss: 1.006160346150864e-05, Train Acc: 1.0\n",
            "Epoch 8330/10000\n",
            "Step 0: Train Loss: 1.1492426892800722e-05, Train Acc: 1.0\n",
            "Epoch 8331/10000\n",
            "Step 0: Train Loss: 8.28222073323559e-06, Train Acc: 1.0\n",
            "Epoch 8332/10000\n",
            "Step 0: Train Loss: 1.4081555491429754e-05, Train Acc: 1.0\n",
            "Epoch 8333/10000\n",
            "Step 0: Train Loss: 8.384427928831428e-06, Train Acc: 1.0\n",
            "Epoch 8334/10000\n",
            "Step 0: Train Loss: 8.300478839373682e-06, Train Acc: 1.0\n",
            "Epoch 8335/10000\n",
            "Step 0: Train Loss: 1.4009467122377828e-05, Train Acc: 1.0\n",
            "Epoch 8336/10000\n",
            "Step 0: Train Loss: 3.502173058222979e-05, Train Acc: 1.0\n",
            "Epoch 8337/10000\n",
            "Step 0: Train Loss: 1.151707510871347e-05, Train Acc: 1.0\n",
            "Epoch 8338/10000\n",
            "Step 0: Train Loss: 1.6755511751398444e-05, Train Acc: 1.0\n",
            "Epoch 8339/10000\n",
            "Step 0: Train Loss: 1.0387345355411526e-05, Train Acc: 1.0\n",
            "Epoch 8340/10000\n",
            "Step 0: Train Loss: 9.642800250730943e-06, Train Acc: 1.0\n",
            "Epoch 8341/10000\n",
            "Step 0: Train Loss: 1.3492965990735684e-05, Train Acc: 1.0\n",
            "Epoch 8342/10000\n",
            "Step 0: Train Loss: 1.0683087566576432e-05, Train Acc: 1.0\n",
            "Epoch 8343/10000\n",
            "Step 0: Train Loss: 7.750159056740813e-06, Train Acc: 1.0\n",
            "Epoch 8344/10000\n",
            "Step 0: Train Loss: 3.65670639439486e-05, Train Acc: 1.0\n",
            "Epoch 8345/10000\n",
            "Step 0: Train Loss: 1.580178468429949e-05, Train Acc: 1.0\n",
            "Epoch 8346/10000\n",
            "Step 0: Train Loss: 8.414209332840983e-06, Train Acc: 1.0\n",
            "Epoch 8347/10000\n",
            "Step 0: Train Loss: 9.534396667731926e-06, Train Acc: 1.0\n",
            "Epoch 8348/10000\n",
            "Step 0: Train Loss: 1.2486601917771623e-05, Train Acc: 1.0\n",
            "Epoch 8349/10000\n",
            "Step 0: Train Loss: 7.219889084808528e-06, Train Acc: 1.0\n",
            "Epoch 8350/10000\n",
            "Step 0: Train Loss: 3.186817411915399e-05, Train Acc: 1.0\n",
            "Epoch 8351/10000\n",
            "Step 0: Train Loss: 1.4857419955660589e-05, Train Acc: 1.0\n",
            "Epoch 8352/10000\n",
            "Step 0: Train Loss: 7.719376299064606e-06, Train Acc: 1.0\n",
            "Epoch 8353/10000\n",
            "Step 0: Train Loss: 6.738322099408833e-06, Train Acc: 1.0\n",
            "Epoch 8354/10000\n",
            "Step 0: Train Loss: 9.874514944385737e-06, Train Acc: 1.0\n",
            "Epoch 8355/10000\n",
            "Step 0: Train Loss: 3.134779763058759e-05, Train Acc: 1.0\n",
            "Epoch 8356/10000\n",
            "Step 0: Train Loss: 6.6850466282630805e-06, Train Acc: 1.0\n",
            "Epoch 8357/10000\n",
            "Step 0: Train Loss: 1.0312511221854948e-05, Train Acc: 1.0\n",
            "Epoch 8358/10000\n",
            "Step 0: Train Loss: 1.012771281239111e-05, Train Acc: 1.0\n",
            "Epoch 8359/10000\n",
            "Step 0: Train Loss: 7.777034625178203e-06, Train Acc: 1.0\n",
            "Epoch 8360/10000\n",
            "Step 0: Train Loss: 3.101053152931854e-05, Train Acc: 1.0\n",
            "Epoch 8361/10000\n",
            "Step 0: Train Loss: 1.024034463625867e-05, Train Acc: 1.0\n",
            "Epoch 8362/10000\n",
            "Step 0: Train Loss: 1.2190525922051165e-05, Train Acc: 1.0\n",
            "Epoch 8363/10000\n",
            "Step 0: Train Loss: 9.160749868897256e-06, Train Acc: 1.0\n",
            "Epoch 8364/10000\n",
            "Step 0: Train Loss: 7.053460194583749e-06, Train Acc: 1.0\n",
            "Epoch 8365/10000\n",
            "Step 0: Train Loss: 1.1975116649409756e-05, Train Acc: 1.0\n",
            "Epoch 8366/10000\n",
            "Step 0: Train Loss: 1.1298413483018521e-05, Train Acc: 1.0\n",
            "Epoch 8367/10000\n",
            "Step 0: Train Loss: 1.0945304893539287e-05, Train Acc: 1.0\n",
            "Epoch 8368/10000\n",
            "Step 0: Train Loss: 1.318485738011077e-05, Train Acc: 1.0\n",
            "Epoch 8369/10000\n",
            "Step 0: Train Loss: 7.184074092947412e-06, Train Acc: 1.0\n",
            "Epoch 8370/10000\n",
            "Step 0: Train Loss: 1.0923305126198102e-05, Train Acc: 1.0\n",
            "Epoch 8371/10000\n",
            "Step 0: Train Loss: 1.4901623217156157e-05, Train Acc: 1.0\n",
            "Epoch 8372/10000\n",
            "Step 0: Train Loss: 8.89517195901135e-06, Train Acc: 1.0\n",
            "Epoch 8373/10000\n",
            "Step 0: Train Loss: 5.554772997129476e-06, Train Acc: 1.0\n",
            "Epoch 8374/10000\n",
            "Step 0: Train Loss: 1.6701755157555453e-05, Train Acc: 1.0\n",
            "Epoch 8375/10000\n",
            "Step 0: Train Loss: 8.212878128688317e-06, Train Acc: 1.0\n",
            "Epoch 8376/10000\n",
            "Step 0: Train Loss: 8.403584615734871e-06, Train Acc: 1.0\n",
            "Epoch 8377/10000\n",
            "Step 0: Train Loss: 7.5099060268257745e-06, Train Acc: 1.0\n",
            "Epoch 8378/10000\n",
            "Step 0: Train Loss: 7.613285106344847e-06, Train Acc: 1.0\n",
            "Epoch 8379/10000\n",
            "Step 0: Train Loss: 8.295848601846956e-06, Train Acc: 1.0\n",
            "Epoch 8380/10000\n",
            "Step 0: Train Loss: 2.4032380679273047e-05, Train Acc: 1.0\n",
            "Epoch 8381/10000\n",
            "Step 0: Train Loss: 7.259966423589503e-06, Train Acc: 1.0\n",
            "Epoch 8382/10000\n",
            "Step 0: Train Loss: 6.157306415843777e-06, Train Acc: 1.0\n",
            "Epoch 8383/10000\n",
            "Step 0: Train Loss: 4.270580120646628e-06, Train Acc: 1.0\n",
            "Epoch 8384/10000\n",
            "Step 0: Train Loss: 1.1709346836141776e-05, Train Acc: 1.0\n",
            "Epoch 8385/10000\n",
            "Step 0: Train Loss: 5.889859494345728e-06, Train Acc: 1.0\n",
            "Epoch 8386/10000\n",
            "Step 0: Train Loss: 8.841180715535302e-06, Train Acc: 1.0\n",
            "Epoch 8387/10000\n",
            "Step 0: Train Loss: 2.5844634365057573e-05, Train Acc: 1.0\n",
            "Epoch 8388/10000\n",
            "Step 0: Train Loss: 2.9517217626562342e-05, Train Acc: 1.0\n",
            "Epoch 8389/10000\n",
            "Step 0: Train Loss: 1.1501435437821783e-05, Train Acc: 1.0\n",
            "Epoch 8390/10000\n",
            "Step 0: Train Loss: 9.934175068337936e-06, Train Acc: 1.0\n",
            "Epoch 8391/10000\n",
            "Step 0: Train Loss: 7.931444997666404e-06, Train Acc: 1.0\n",
            "Epoch 8392/10000\n",
            "Step 0: Train Loss: 9.745645911607426e-06, Train Acc: 1.0\n",
            "Epoch 8393/10000\n",
            "Step 0: Train Loss: 6.144223789306125e-06, Train Acc: 1.0\n",
            "Epoch 8394/10000\n",
            "Step 0: Train Loss: 1.0554363143455703e-05, Train Acc: 1.0\n",
            "Epoch 8395/10000\n",
            "Step 0: Train Loss: 2.7282176233711652e-05, Train Acc: 1.0\n",
            "Epoch 8396/10000\n",
            "Step 0: Train Loss: 6.940332696103724e-06, Train Acc: 1.0\n",
            "Epoch 8397/10000\n",
            "Step 0: Train Loss: 6.0831393966509495e-06, Train Acc: 1.0\n",
            "Epoch 8398/10000\n",
            "Step 0: Train Loss: 8.922910637920722e-06, Train Acc: 1.0\n",
            "Epoch 8399/10000\n",
            "Step 0: Train Loss: 1.1548783731996082e-05, Train Acc: 1.0\n",
            "Epoch 8400/10000\n",
            "Step 0: Train Loss: 1.3745046089752577e-05, Train Acc: 1.0\n",
            "Epoch 8401/10000\n",
            "Step 0: Train Loss: 1.2732082723232452e-05, Train Acc: 1.0\n",
            "Epoch 8402/10000\n",
            "Step 0: Train Loss: 7.931143045425415e-06, Train Acc: 1.0\n",
            "Epoch 8403/10000\n",
            "Step 0: Train Loss: 2.679496719792951e-05, Train Acc: 1.0\n",
            "Epoch 8404/10000\n",
            "Step 0: Train Loss: 1.0395358003734145e-05, Train Acc: 1.0\n",
            "Epoch 8405/10000\n",
            "Step 0: Train Loss: 8.250450264313258e-06, Train Acc: 1.0\n",
            "Epoch 8406/10000\n",
            "Step 0: Train Loss: 1.0797618415381294e-05, Train Acc: 1.0\n",
            "Epoch 8407/10000\n",
            "Step 0: Train Loss: 2.378333556407597e-05, Train Acc: 1.0\n",
            "Epoch 8408/10000\n",
            "Step 0: Train Loss: 6.199178642418701e-06, Train Acc: 1.0\n",
            "Epoch 8409/10000\n",
            "Step 0: Train Loss: 7.4148633757431526e-06, Train Acc: 1.0\n",
            "Epoch 8410/10000\n",
            "Step 0: Train Loss: 2.4890012355172075e-05, Train Acc: 1.0\n",
            "Epoch 8411/10000\n",
            "Step 0: Train Loss: 1.192286072182469e-05, Train Acc: 1.0\n",
            "Epoch 8412/10000\n",
            "Step 0: Train Loss: 6.995701824052958e-06, Train Acc: 1.0\n",
            "Epoch 8413/10000\n",
            "Step 0: Train Loss: 1.1176231055287644e-05, Train Acc: 1.0\n",
            "Epoch 8414/10000\n",
            "Step 0: Train Loss: 9.226734619005583e-06, Train Acc: 1.0\n",
            "Epoch 8415/10000\n",
            "Step 0: Train Loss: 5.553494702326134e-06, Train Acc: 1.0\n",
            "Epoch 8416/10000\n",
            "Step 0: Train Loss: 6.409089110093191e-06, Train Acc: 1.0\n",
            "Epoch 8417/10000\n",
            "Step 0: Train Loss: 7.073668257362442e-06, Train Acc: 1.0\n",
            "Epoch 8418/10000\n",
            "Step 0: Train Loss: 7.266241937031737e-06, Train Acc: 1.0\n",
            "Epoch 8419/10000\n",
            "Step 0: Train Loss: 2.159279029001482e-05, Train Acc: 1.0\n",
            "Epoch 8420/10000\n",
            "Step 0: Train Loss: 7.1983681664278265e-06, Train Acc: 1.0\n",
            "Epoch 8421/10000\n",
            "Step 0: Train Loss: 6.7117057369614486e-06, Train Acc: 1.0\n",
            "Epoch 8422/10000\n",
            "Step 0: Train Loss: 2.0249168301234022e-05, Train Acc: 1.0\n",
            "Epoch 8423/10000\n",
            "Step 0: Train Loss: 6.173169822432101e-06, Train Acc: 1.0\n",
            "Epoch 8424/10000\n",
            "Step 0: Train Loss: 4.5954598135722335e-06, Train Acc: 1.0\n",
            "Epoch 8425/10000\n",
            "Step 0: Train Loss: 4.842325324716512e-06, Train Acc: 1.0\n",
            "Epoch 8426/10000\n",
            "Step 0: Train Loss: 5.794621756649576e-06, Train Acc: 1.0\n",
            "Epoch 8427/10000\n",
            "Step 0: Train Loss: 4.841829195356695e-06, Train Acc: 1.0\n",
            "Epoch 8428/10000\n",
            "Step 0: Train Loss: 7.253579951793654e-06, Train Acc: 1.0\n",
            "Epoch 8429/10000\n",
            "Step 0: Train Loss: 4.755243480758509e-06, Train Acc: 1.0\n",
            "Epoch 8430/10000\n",
            "Step 0: Train Loss: 6.9138141043367796e-06, Train Acc: 1.0\n",
            "Epoch 8431/10000\n",
            "Step 0: Train Loss: 5.443895133794285e-06, Train Acc: 1.0\n",
            "Epoch 8432/10000\n",
            "Step 0: Train Loss: 9.819586921366863e-06, Train Acc: 1.0\n",
            "Epoch 8433/10000\n",
            "Step 0: Train Loss: 2.161323936888948e-05, Train Acc: 1.0\n",
            "Epoch 8434/10000\n",
            "Step 0: Train Loss: 2.1367251974879764e-05, Train Acc: 1.0\n",
            "Epoch 8435/10000\n",
            "Step 0: Train Loss: 1.0461645615578163e-05, Train Acc: 1.0\n",
            "Epoch 8436/10000\n",
            "Step 0: Train Loss: 2.5591880330466665e-05, Train Acc: 1.0\n",
            "Epoch 8437/10000\n",
            "Step 0: Train Loss: 5.7657980505609885e-06, Train Acc: 1.0\n",
            "Epoch 8438/10000\n",
            "Step 0: Train Loss: 5.264877927402267e-06, Train Acc: 1.0\n",
            "Epoch 8439/10000\n",
            "Step 0: Train Loss: 6.872466201457428e-06, Train Acc: 1.0\n",
            "Epoch 8440/10000\n",
            "Step 0: Train Loss: 4.9650548135105055e-06, Train Acc: 1.0\n",
            "Epoch 8441/10000\n",
            "Step 0: Train Loss: 7.349350198637694e-06, Train Acc: 1.0\n",
            "Epoch 8442/10000\n",
            "Step 0: Train Loss: 4.743987119582016e-06, Train Acc: 1.0\n",
            "Epoch 8443/10000\n",
            "Step 0: Train Loss: 4.890237050858559e-06, Train Acc: 1.0\n",
            "Epoch 8444/10000\n",
            "Step 0: Train Loss: 8.25708411866799e-06, Train Acc: 1.0\n",
            "Epoch 8445/10000\n",
            "Step 0: Train Loss: 2.3043956389301457e-05, Train Acc: 1.0\n",
            "Epoch 8446/10000\n",
            "Step 0: Train Loss: 7.7022905315971e-06, Train Acc: 1.0\n",
            "Epoch 8447/10000\n",
            "Step 0: Train Loss: 4.882620487478562e-06, Train Acc: 1.0\n",
            "Epoch 8448/10000\n",
            "Step 0: Train Loss: 7.470349828508915e-06, Train Acc: 1.0\n",
            "Epoch 8449/10000\n",
            "Step 0: Train Loss: 8.653646546008531e-06, Train Acc: 1.0\n",
            "Epoch 8450/10000\n",
            "Step 0: Train Loss: 7.493345492548542e-06, Train Acc: 1.0\n",
            "Epoch 8451/10000\n",
            "Step 0: Train Loss: 4.417919171828544e-06, Train Acc: 1.0\n",
            "Epoch 8452/10000\n",
            "Step 0: Train Loss: 4.422267920745071e-06, Train Acc: 1.0\n",
            "Epoch 8453/10000\n",
            "Step 0: Train Loss: 7.258578534674598e-06, Train Acc: 1.0\n",
            "Epoch 8454/10000\n",
            "Step 0: Train Loss: 5.255947144178208e-06, Train Acc: 1.0\n",
            "Epoch 8455/10000\n",
            "Step 0: Train Loss: 1.062820683728205e-05, Train Acc: 1.0\n",
            "Epoch 8456/10000\n",
            "Step 0: Train Loss: 4.946611625200603e-06, Train Acc: 1.0\n",
            "Epoch 8457/10000\n",
            "Step 0: Train Loss: 8.853282452037092e-06, Train Acc: 1.0\n",
            "Epoch 8458/10000\n",
            "Step 0: Train Loss: 6.791266969230492e-06, Train Acc: 1.0\n",
            "Epoch 8459/10000\n",
            "Step 0: Train Loss: 4.917812475468963e-06, Train Acc: 1.0\n",
            "Epoch 8460/10000\n",
            "Step 0: Train Loss: 4.452434495760826e-06, Train Acc: 1.0\n",
            "Epoch 8461/10000\n",
            "Step 0: Train Loss: 6.216223027877277e-06, Train Acc: 1.0\n",
            "Epoch 8462/10000\n",
            "Step 0: Train Loss: 4.7709568207210395e-06, Train Acc: 1.0\n",
            "Epoch 8463/10000\n",
            "Step 0: Train Loss: 2.0005523765576072e-05, Train Acc: 1.0\n",
            "Epoch 8464/10000\n",
            "Step 0: Train Loss: 6.335487796604866e-06, Train Acc: 1.0\n",
            "Epoch 8465/10000\n",
            "Step 0: Train Loss: 5.127343683852814e-06, Train Acc: 1.0\n",
            "Epoch 8466/10000\n",
            "Step 0: Train Loss: 9.581693120708223e-06, Train Acc: 1.0\n",
            "Epoch 8467/10000\n",
            "Step 0: Train Loss: 5.610410426015733e-06, Train Acc: 1.0\n",
            "Epoch 8468/10000\n",
            "Step 0: Train Loss: 9.935718480846845e-06, Train Acc: 1.0\n",
            "Epoch 8469/10000\n",
            "Step 0: Train Loss: 7.865973202569876e-06, Train Acc: 1.0\n",
            "Epoch 8470/10000\n",
            "Step 0: Train Loss: 5.218753358349204e-06, Train Acc: 1.0\n",
            "Epoch 8471/10000\n",
            "Step 0: Train Loss: 5.873594545846572e-06, Train Acc: 1.0\n",
            "Epoch 8472/10000\n",
            "Step 0: Train Loss: 8.142612387018744e-06, Train Acc: 1.0\n",
            "Epoch 8473/10000\n",
            "Step 0: Train Loss: 2.3281592802959494e-05, Train Acc: 1.0\n",
            "Epoch 8474/10000\n",
            "Step 0: Train Loss: 7.00334157954785e-06, Train Acc: 1.0\n",
            "Epoch 8475/10000\n",
            "Step 0: Train Loss: 6.674393262073863e-06, Train Acc: 1.0\n",
            "Epoch 8476/10000\n",
            "Step 0: Train Loss: 6.108285560912918e-06, Train Acc: 1.0\n",
            "Epoch 8477/10000\n",
            "Step 0: Train Loss: 4.4226262616575696e-06, Train Acc: 1.0\n",
            "Epoch 8478/10000\n",
            "Step 0: Train Loss: 4.115525825909572e-06, Train Acc: 1.0\n",
            "Epoch 8479/10000\n",
            "Step 0: Train Loss: 6.148115971882362e-06, Train Acc: 1.0\n",
            "Epoch 8480/10000\n",
            "Step 0: Train Loss: 4.051479209010722e-06, Train Acc: 1.0\n",
            "Epoch 8481/10000\n",
            "Step 0: Train Loss: 1.7288059098063968e-05, Train Acc: 1.0\n",
            "Epoch 8482/10000\n",
            "Step 0: Train Loss: 4.3827176341437735e-06, Train Acc: 1.0\n",
            "Epoch 8483/10000\n",
            "Step 0: Train Loss: 6.340351774269948e-06, Train Acc: 1.0\n",
            "Epoch 8484/10000\n",
            "Step 0: Train Loss: 7.068782906571869e-06, Train Acc: 1.0\n",
            "Epoch 8485/10000\n",
            "Step 0: Train Loss: 7.701994945819024e-06, Train Acc: 1.0\n",
            "Epoch 8486/10000\n",
            "Step 0: Train Loss: 4.926328529109014e-06, Train Acc: 1.0\n",
            "Epoch 8487/10000\n",
            "Step 0: Train Loss: 3.7123759284440894e-06, Train Acc: 1.0\n",
            "Epoch 8488/10000\n",
            "Step 0: Train Loss: 5.338437858881662e-06, Train Acc: 1.0\n",
            "Epoch 8489/10000\n",
            "Step 0: Train Loss: 4.1304824662802275e-06, Train Acc: 1.0\n",
            "Epoch 8490/10000\n",
            "Step 0: Train Loss: 5.429242264654022e-06, Train Acc: 1.0\n",
            "Epoch 8491/10000\n",
            "Step 0: Train Loss: 5.050850631960202e-06, Train Acc: 1.0\n",
            "Epoch 8492/10000\n",
            "Step 0: Train Loss: 5.301317742123501e-06, Train Acc: 1.0\n",
            "Epoch 8493/10000\n",
            "Step 0: Train Loss: 5.257195880403742e-06, Train Acc: 1.0\n",
            "Epoch 8494/10000\n",
            "Step 0: Train Loss: 5.109601261210628e-06, Train Acc: 1.0\n",
            "Epoch 8495/10000\n",
            "Step 0: Train Loss: 7.044950962153962e-06, Train Acc: 1.0\n",
            "Epoch 8496/10000\n",
            "Step 0: Train Loss: 1.7585436580702662e-05, Train Acc: 1.0\n",
            "Epoch 8497/10000\n",
            "Step 0: Train Loss: 4.384995918371715e-06, Train Acc: 1.0\n",
            "Epoch 8498/10000\n",
            "Step 0: Train Loss: 7.87201861385256e-06, Train Acc: 1.0\n",
            "Epoch 8499/10000\n",
            "Step 0: Train Loss: 3.0443889045272954e-06, Train Acc: 1.0\n",
            "Epoch 8500/10000\n",
            "Step 0: Train Loss: 1.8186759916716255e-05, Train Acc: 1.0\n",
            "Epoch 8501/10000\n",
            "Step 0: Train Loss: 1.7006432244670577e-05, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 8500 1024 0.9503255969951286\n",
            "Epoch index and hidden dimension and ratio: 8500 20 0.8868663527481037\n",
            "Epoch index and hidden dimension and ratio: 8500 20 1.7373354764090279\n",
            "Epoch index and hidden dimension and ratio: 8500 20 5.268360833457476\n",
            "MI(X;T): [10.1864099547196, 7.036581842464326, 5.368578374356948, 2.24953928447212], MI(Y;T): [2.2275028234798127, 3.25353525966264, 3.069009212276269, 2.747637751856141]\n",
            "Epoch index and hidden dimension and ratio: 8500 1024 0.9503376383312735\n",
            "Epoch index and hidden dimension and ratio: 8500 20 0.886878990924596\n",
            "Epoch index and hidden dimension and ratio: 8500 20 1.7373391582333846\n",
            "Epoch index and hidden dimension and ratio: 8500 20 5.268454608651847\n",
            "MI(X;T): [10.186308218651256, 7.036669294476088, 5.368651710620192, 2.2495933737465785], MI(Y;T): [2.227693089917699, 3.2535107484128565, 3.0685407772458975, 2.7474610030036444]\n",
            "Epoch index and hidden dimension and ratio: 8500 1024 0.9503496796674183\n",
            "Epoch index and hidden dimension and ratio: 8500 20 0.8868927139231048\n",
            "Epoch index and hidden dimension and ratio: 8500 20 1.7373540124902718\n",
            "Epoch index and hidden dimension and ratio: 8500 20 5.26857075827856\n",
            "MI(X;T): [10.18563416864319, 7.036073087053156, 5.368685723747406, 2.2503009520472688], MI(Y;T): [2.227356124963502, 3.253747930993508, 3.068769506351331, 2.747636411063685]\n",
            "Epoch index and hidden dimension and ratio: 8500 1024 0.950364070532567\n",
            "Epoch index and hidden dimension and ratio: 8500 20 0.8869119695138977\n",
            "Epoch index and hidden dimension and ratio: 8500 20 1.7373848636391913\n",
            "Epoch index and hidden dimension and ratio: 8500 20 5.268842541824354\n",
            "MI(X;T): [10.18437825436509, 7.036937330871858, 5.368306048281527, 2.250451243216154], MI(Y;T): [2.2273696007609196, 3.253747874009915, 3.0684955662569893, 2.747379976779993]\n",
            "Epoch index and hidden dimension and ratio: 8500 1024 0.9503775069015579\n",
            "Epoch index and hidden dimension and ratio: 8500 20 0.8869259094768098\n",
            "Epoch index and hidden dimension and ratio: 8500 20 1.7374223166800609\n",
            "Epoch index and hidden dimension and ratio: 8500 20 5.269096557438582\n",
            "MI(X;T): [10.18446458778093, 7.036977938802476, 5.366067578486511, 2.250565233475222], MI(Y;T): [2.227286832708387, 3.2539630677535727, 3.0688490974425977, 2.747291680858793]\n",
            "Epoch index and hidden dimension and ratio: 8500 1024 0.9503884468959821\n",
            "Epoch index and hidden dimension and ratio: 8500 20 0.8869407172973351\n",
            "Epoch index and hidden dimension and ratio: 8500 20 1.7374620549912208\n",
            "Epoch index and hidden dimension and ratio: 8500 20 5.269342676194336\n",
            "MI(X;T): [10.184962409198285, 7.037212027021175, 5.367198250678202, 2.250481417181277], MI(Y;T): [2.2273144295581035, 3.253988214263323, 3.0685344899592693, 2.7472813124474076]\n",
            "Epoch 8502/10000\n",
            "Step 0: Train Loss: 4.3738336898968555e-06, Train Acc: 1.0\n",
            "Epoch 8503/10000\n",
            "Step 0: Train Loss: 4.423302016220987e-06, Train Acc: 1.0\n",
            "Epoch 8504/10000\n",
            "Step 0: Train Loss: 2.0524841602309607e-05, Train Acc: 1.0\n",
            "Epoch 8505/10000\n",
            "Step 0: Train Loss: 1.526311825728044e-05, Train Acc: 1.0\n",
            "Epoch 8506/10000\n",
            "Step 0: Train Loss: 1.501530823588837e-05, Train Acc: 1.0\n",
            "Epoch 8507/10000\n",
            "Step 0: Train Loss: 3.3472795166744618e-06, Train Acc: 1.0\n",
            "Epoch 8508/10000\n",
            "Step 0: Train Loss: 5.725704340875382e-06, Train Acc: 1.0\n",
            "Epoch 8509/10000\n",
            "Step 0: Train Loss: 4.880404958385043e-06, Train Acc: 1.0\n",
            "Epoch 8510/10000\n",
            "Step 0: Train Loss: 4.396933491079835e-06, Train Acc: 1.0\n",
            "Epoch 8511/10000\n",
            "Step 0: Train Loss: 3.4897900604846654e-06, Train Acc: 1.0\n",
            "Epoch 8512/10000\n",
            "Step 0: Train Loss: 3.0768253509450005e-06, Train Acc: 1.0\n",
            "Epoch 8513/10000\n",
            "Step 0: Train Loss: 6.1291166275623254e-06, Train Acc: 1.0\n",
            "Epoch 8514/10000\n",
            "Step 0: Train Loss: 5.297809821058763e-06, Train Acc: 1.0\n",
            "Epoch 8515/10000\n",
            "Step 0: Train Loss: 4.4175121729495e-06, Train Acc: 1.0\n",
            "Epoch 8516/10000\n",
            "Step 0: Train Loss: 4.2846595533774234e-06, Train Acc: 1.0\n",
            "Epoch 8517/10000\n",
            "Step 0: Train Loss: 4.853568498219829e-06, Train Acc: 1.0\n",
            "Epoch 8518/10000\n",
            "Step 0: Train Loss: 5.285464794724248e-06, Train Acc: 1.0\n",
            "Epoch 8519/10000\n",
            "Step 0: Train Loss: 3.154526666548918e-06, Train Acc: 1.0\n",
            "Epoch 8520/10000\n",
            "Step 0: Train Loss: 1.736978992994409e-05, Train Acc: 1.0\n",
            "Epoch 8521/10000\n",
            "Step 0: Train Loss: 6.351865977194393e-06, Train Acc: 1.0\n",
            "Epoch 8522/10000\n",
            "Step 0: Train Loss: 3.385597892702208e-06, Train Acc: 1.0\n",
            "Epoch 8523/10000\n",
            "Step 0: Train Loss: 4.151163466303842e-06, Train Acc: 1.0\n",
            "Epoch 8524/10000\n",
            "Step 0: Train Loss: 1.4329079021990765e-05, Train Acc: 1.0\n",
            "Epoch 8525/10000\n",
            "Step 0: Train Loss: 3.7526442611124367e-06, Train Acc: 1.0\n",
            "Epoch 8526/10000\n",
            "Step 0: Train Loss: 5.56832856091205e-06, Train Acc: 1.0\n",
            "Epoch 8527/10000\n",
            "Step 0: Train Loss: 4.816809450858273e-06, Train Acc: 1.0\n",
            "Epoch 8528/10000\n",
            "Step 0: Train Loss: 5.5796990636736155e-06, Train Acc: 1.0\n",
            "Epoch 8529/10000\n",
            "Step 0: Train Loss: 5.729167241952382e-06, Train Acc: 1.0\n",
            "Epoch 8530/10000\n",
            "Step 0: Train Loss: 3.7654895095329266e-06, Train Acc: 1.0\n",
            "Epoch 8531/10000\n",
            "Step 0: Train Loss: 1.795221760403365e-05, Train Acc: 1.0\n",
            "Epoch 8532/10000\n",
            "Step 0: Train Loss: 3.222921122869593e-06, Train Acc: 1.0\n",
            "Epoch 8533/10000\n",
            "Step 0: Train Loss: 4.194577286398271e-06, Train Acc: 1.0\n",
            "Epoch 8534/10000\n",
            "Step 0: Train Loss: 5.501188752532471e-06, Train Acc: 1.0\n",
            "Epoch 8535/10000\n",
            "Step 0: Train Loss: 7.001488029345637e-06, Train Acc: 1.0\n",
            "Epoch 8536/10000\n",
            "Step 0: Train Loss: 4.888343937636819e-06, Train Acc: 1.0\n",
            "Epoch 8537/10000\n",
            "Step 0: Train Loss: 4.64254026155686e-06, Train Acc: 1.0\n",
            "Epoch 8538/10000\n",
            "Step 0: Train Loss: 1.5281977539416403e-05, Train Acc: 1.0\n",
            "Epoch 8539/10000\n",
            "Step 0: Train Loss: 1.6637228327454068e-05, Train Acc: 1.0\n",
            "Epoch 8540/10000\n",
            "Step 0: Train Loss: 3.179624627591693e-06, Train Acc: 1.0\n",
            "Epoch 8541/10000\n",
            "Step 0: Train Loss: 5.906185833737254e-06, Train Acc: 1.0\n",
            "Epoch 8542/10000\n",
            "Step 0: Train Loss: 6.242312338144984e-06, Train Acc: 1.0\n",
            "Epoch 8543/10000\n",
            "Step 0: Train Loss: 3.5973876038042363e-06, Train Acc: 1.0\n",
            "Epoch 8544/10000\n",
            "Step 0: Train Loss: 3.5085809031443205e-06, Train Acc: 1.0\n",
            "Epoch 8545/10000\n",
            "Step 0: Train Loss: 4.6745008148718625e-06, Train Acc: 1.0\n",
            "Epoch 8546/10000\n",
            "Step 0: Train Loss: 4.5255228542373516e-06, Train Acc: 1.0\n",
            "Epoch 8547/10000\n",
            "Step 0: Train Loss: 5.635736215481302e-06, Train Acc: 1.0\n",
            "Epoch 8548/10000\n",
            "Step 0: Train Loss: 1.776768476702273e-05, Train Acc: 1.0\n",
            "Epoch 8549/10000\n",
            "Step 0: Train Loss: 4.5365459300228395e-06, Train Acc: 1.0\n",
            "Epoch 8550/10000\n",
            "Step 0: Train Loss: 3.7182721825956833e-06, Train Acc: 1.0\n",
            "Epoch 8551/10000\n",
            "Step 0: Train Loss: 5.764008164987899e-06, Train Acc: 1.0\n",
            "Epoch 8552/10000\n",
            "Step 0: Train Loss: 3.308072336949408e-06, Train Acc: 1.0\n",
            "Epoch 8553/10000\n",
            "Step 0: Train Loss: 3.6176163575873943e-06, Train Acc: 1.0\n",
            "Epoch 8554/10000\n",
            "Step 0: Train Loss: 2.894673116315971e-06, Train Acc: 1.0\n",
            "Epoch 8555/10000\n",
            "Step 0: Train Loss: 1.4718239071953576e-05, Train Acc: 1.0\n",
            "Epoch 8556/10000\n",
            "Step 0: Train Loss: 3.389765424799407e-06, Train Acc: 1.0\n",
            "Epoch 8557/10000\n",
            "Step 0: Train Loss: 1.3461595699482132e-05, Train Acc: 1.0\n",
            "Epoch 8558/10000\n",
            "Step 0: Train Loss: 3.662012886707089e-06, Train Acc: 1.0\n",
            "Epoch 8559/10000\n",
            "Step 0: Train Loss: 2.7615574254014064e-06, Train Acc: 1.0\n",
            "Epoch 8560/10000\n",
            "Step 0: Train Loss: 5.10404652231955e-06, Train Acc: 1.0\n",
            "Epoch 8561/10000\n",
            "Step 0: Train Loss: 1.2629254342755303e-05, Train Acc: 1.0\n",
            "Epoch 8562/10000\n",
            "Step 0: Train Loss: 5.880281605641358e-06, Train Acc: 1.0\n",
            "Epoch 8563/10000\n",
            "Step 0: Train Loss: 1.2215368769830093e-05, Train Acc: 1.0\n",
            "Epoch 8564/10000\n",
            "Step 0: Train Loss: 3.096535238000797e-06, Train Acc: 1.0\n",
            "Epoch 8565/10000\n",
            "Step 0: Train Loss: 4.614107183442684e-06, Train Acc: 1.0\n",
            "Epoch 8566/10000\n",
            "Step 0: Train Loss: 4.414000159158604e-06, Train Acc: 1.0\n",
            "Epoch 8567/10000\n",
            "Step 0: Train Loss: 1.3027008208155166e-05, Train Acc: 1.0\n",
            "Epoch 8568/10000\n",
            "Step 0: Train Loss: 4.22733637606143e-06, Train Acc: 1.0\n",
            "Epoch 8569/10000\n",
            "Step 0: Train Loss: 2.787091943901032e-06, Train Acc: 1.0\n",
            "Epoch 8570/10000\n",
            "Step 0: Train Loss: 3.735112386493711e-06, Train Acc: 1.0\n",
            "Epoch 8571/10000\n",
            "Step 0: Train Loss: 3.199111688445555e-06, Train Acc: 1.0\n",
            "Epoch 8572/10000\n",
            "Step 0: Train Loss: 4.619842457032064e-06, Train Acc: 1.0\n",
            "Epoch 8573/10000\n",
            "Step 0: Train Loss: 1.425707796443021e-05, Train Acc: 1.0\n",
            "Epoch 8574/10000\n",
            "Step 0: Train Loss: 1.3865308574168012e-05, Train Acc: 1.0\n",
            "Epoch 8575/10000\n",
            "Step 0: Train Loss: 3.874155936500756e-06, Train Acc: 1.0\n",
            "Epoch 8576/10000\n",
            "Step 0: Train Loss: 3.5183641102776164e-06, Train Acc: 1.0\n",
            "Epoch 8577/10000\n",
            "Step 0: Train Loss: 1.3235825463198125e-05, Train Acc: 1.0\n",
            "Epoch 8578/10000\n",
            "Step 0: Train Loss: 3.0458354558504652e-06, Train Acc: 1.0\n",
            "Epoch 8579/10000\n",
            "Step 0: Train Loss: 3.3220351269847015e-06, Train Acc: 1.0\n",
            "Epoch 8580/10000\n",
            "Step 0: Train Loss: 2.663940449565416e-06, Train Acc: 1.0\n",
            "Epoch 8581/10000\n",
            "Step 0: Train Loss: 4.806104243471054e-06, Train Acc: 1.0\n",
            "Epoch 8582/10000\n",
            "Step 0: Train Loss: 5.2465043154370505e-06, Train Acc: 1.0\n",
            "Epoch 8583/10000\n",
            "Step 0: Train Loss: 2.2224746771826176e-06, Train Acc: 1.0\n",
            "Epoch 8584/10000\n",
            "Step 0: Train Loss: 2.0195366232655942e-06, Train Acc: 1.0\n",
            "Epoch 8585/10000\n",
            "Step 0: Train Loss: 2.878853365473333e-06, Train Acc: 1.0\n",
            "Epoch 8586/10000\n",
            "Step 0: Train Loss: 5.061241608927958e-06, Train Acc: 1.0\n",
            "Epoch 8587/10000\n",
            "Step 0: Train Loss: 3.2241682674793992e-06, Train Acc: 1.0\n",
            "Epoch 8588/10000\n",
            "Step 0: Train Loss: 3.328023012727499e-06, Train Acc: 1.0\n",
            "Epoch 8589/10000\n",
            "Step 0: Train Loss: 4.1823514038696885e-06, Train Acc: 1.0\n",
            "Epoch 8590/10000\n",
            "Step 0: Train Loss: 4.340728992247023e-06, Train Acc: 1.0\n",
            "Epoch 8591/10000\n",
            "Step 0: Train Loss: 4.401530077302596e-06, Train Acc: 1.0\n",
            "Epoch 8592/10000\n",
            "Step 0: Train Loss: 1.3885151929571293e-05, Train Acc: 1.0\n",
            "Epoch 8593/10000\n",
            "Step 0: Train Loss: 3.3606156648602337e-06, Train Acc: 1.0\n",
            "Epoch 8594/10000\n",
            "Step 0: Train Loss: 2.2559447643288877e-06, Train Acc: 1.0\n",
            "Epoch 8595/10000\n",
            "Step 0: Train Loss: 3.0158512345224153e-06, Train Acc: 1.0\n",
            "Epoch 8596/10000\n",
            "Step 0: Train Loss: 3.621180439949967e-06, Train Acc: 1.0\n",
            "Epoch 8597/10000\n",
            "Step 0: Train Loss: 6.557371307280846e-06, Train Acc: 1.0\n",
            "Epoch 8598/10000\n",
            "Step 0: Train Loss: 5.791699095425429e-06, Train Acc: 1.0\n",
            "Epoch 8599/10000\n",
            "Step 0: Train Loss: 4.6732275222893804e-06, Train Acc: 1.0\n",
            "Epoch 8600/10000\n",
            "Step 0: Train Loss: 2.5744182039488805e-06, Train Acc: 1.0\n",
            "Epoch 8601/10000\n",
            "Step 0: Train Loss: 2.6582868031255202e-06, Train Acc: 1.0\n",
            "Epoch 8602/10000\n",
            "Step 0: Train Loss: 1.193121715914458e-05, Train Acc: 1.0\n",
            "Epoch 8603/10000\n",
            "Step 0: Train Loss: 3.4579745715745958e-06, Train Acc: 1.0\n",
            "Epoch 8604/10000\n",
            "Step 0: Train Loss: 2.735489715632866e-06, Train Acc: 1.0\n",
            "Epoch 8605/10000\n",
            "Step 0: Train Loss: 2.8718043267872417e-06, Train Acc: 1.0\n",
            "Epoch 8606/10000\n",
            "Step 0: Train Loss: 3.205091616109712e-06, Train Acc: 1.0\n",
            "Epoch 8607/10000\n",
            "Step 0: Train Loss: 2.336189936613664e-06, Train Acc: 1.0\n",
            "Epoch 8608/10000\n",
            "Step 0: Train Loss: 2.564731175880297e-06, Train Acc: 1.0\n",
            "Epoch 8609/10000\n",
            "Step 0: Train Loss: 3.5547509469324723e-06, Train Acc: 1.0\n",
            "Epoch 8610/10000\n",
            "Step 0: Train Loss: 3.839221335510956e-06, Train Acc: 1.0\n",
            "Epoch 8611/10000\n",
            "Step 0: Train Loss: 2.8150177513452945e-06, Train Acc: 1.0\n",
            "Epoch 8612/10000\n",
            "Step 0: Train Loss: 2.6757095383800333e-06, Train Acc: 1.0\n",
            "Epoch 8613/10000\n",
            "Step 0: Train Loss: 1.220437388838036e-05, Train Acc: 1.0\n",
            "Epoch 8614/10000\n",
            "Step 0: Train Loss: 1.8092714526574127e-06, Train Acc: 1.0\n",
            "Epoch 8615/10000\n",
            "Step 0: Train Loss: 1.0987589121214114e-05, Train Acc: 1.0\n",
            "Epoch 8616/10000\n",
            "Step 0: Train Loss: 2.77072899734776e-06, Train Acc: 1.0\n",
            "Epoch 8617/10000\n",
            "Step 0: Train Loss: 2.804411906254245e-06, Train Acc: 1.0\n",
            "Epoch 8618/10000\n",
            "Step 0: Train Loss: 1.2444128515198827e-05, Train Acc: 1.0\n",
            "Epoch 8619/10000\n",
            "Step 0: Train Loss: 3.2940324672381394e-06, Train Acc: 1.0\n",
            "Epoch 8620/10000\n",
            "Step 0: Train Loss: 2.368868990743067e-06, Train Acc: 1.0\n",
            "Epoch 8621/10000\n",
            "Step 0: Train Loss: 3.150468955936958e-06, Train Acc: 1.0\n",
            "Epoch 8622/10000\n",
            "Step 0: Train Loss: 1.0838756679731887e-05, Train Acc: 1.0\n",
            "Epoch 8623/10000\n",
            "Step 0: Train Loss: 4.223987616569502e-06, Train Acc: 1.0\n",
            "Epoch 8624/10000\n",
            "Step 0: Train Loss: 3.983921942563029e-06, Train Acc: 1.0\n",
            "Epoch 8625/10000\n",
            "Step 0: Train Loss: 4.246293428877834e-06, Train Acc: 1.0\n",
            "Epoch 8626/10000\n",
            "Step 0: Train Loss: 2.991738028867985e-06, Train Acc: 1.0\n",
            "Epoch 8627/10000\n",
            "Step 0: Train Loss: 2.3502498152083717e-06, Train Acc: 1.0\n",
            "Epoch 8628/10000\n",
            "Step 0: Train Loss: 2.7134415176988114e-06, Train Acc: 1.0\n",
            "Epoch 8629/10000\n",
            "Step 0: Train Loss: 2.2668355086352676e-06, Train Acc: 1.0\n",
            "Epoch 8630/10000\n",
            "Step 0: Train Loss: 3.3560052088432712e-06, Train Acc: 1.0\n",
            "Epoch 8631/10000\n",
            "Step 0: Train Loss: 3.4217491702293046e-06, Train Acc: 1.0\n",
            "Epoch 8632/10000\n",
            "Step 0: Train Loss: 2.7281007533019874e-06, Train Acc: 1.0\n",
            "Epoch 8633/10000\n",
            "Step 0: Train Loss: 9.992475497710984e-06, Train Acc: 1.0\n",
            "Epoch 8634/10000\n",
            "Step 0: Train Loss: 1.0140580343431793e-05, Train Acc: 1.0\n",
            "Epoch 8635/10000\n",
            "Step 0: Train Loss: 1.0825936442415696e-05, Train Acc: 1.0\n",
            "Epoch 8636/10000\n",
            "Step 0: Train Loss: 2.9024411105638137e-06, Train Acc: 1.0\n",
            "Epoch 8637/10000\n",
            "Step 0: Train Loss: 2.4801229301374406e-06, Train Acc: 1.0\n",
            "Epoch 8638/10000\n",
            "Step 0: Train Loss: 3.239457782910904e-06, Train Acc: 1.0\n",
            "Epoch 8639/10000\n",
            "Step 0: Train Loss: 1.0098719940287992e-05, Train Acc: 1.0\n",
            "Epoch 8640/10000\n",
            "Step 0: Train Loss: 2.51970755016373e-06, Train Acc: 1.0\n",
            "Epoch 8641/10000\n",
            "Step 0: Train Loss: 3.1962797493179096e-06, Train Acc: 1.0\n",
            "Epoch 8642/10000\n",
            "Step 0: Train Loss: 1.909754473672365e-06, Train Acc: 1.0\n",
            "Epoch 8643/10000\n",
            "Step 0: Train Loss: 2.8676402052951744e-06, Train Acc: 1.0\n",
            "Epoch 8644/10000\n",
            "Step 0: Train Loss: 1.9340511698828777e-06, Train Acc: 1.0\n",
            "Epoch 8645/10000\n",
            "Step 0: Train Loss: 3.872837623930536e-06, Train Acc: 1.0\n",
            "Epoch 8646/10000\n",
            "Step 0: Train Loss: 3.2887614906940144e-06, Train Acc: 1.0\n",
            "Epoch 8647/10000\n",
            "Step 0: Train Loss: 3.869718511850806e-06, Train Acc: 1.0\n",
            "Epoch 8648/10000\n",
            "Step 0: Train Loss: 1.7824830820245552e-06, Train Acc: 1.0\n",
            "Epoch 8649/10000\n",
            "Step 0: Train Loss: 1.114085807785159e-05, Train Acc: 1.0\n",
            "Epoch 8650/10000\n",
            "Step 0: Train Loss: 1.0142710380023345e-05, Train Acc: 1.0\n",
            "Epoch 8651/10000\n",
            "Step 0: Train Loss: 3.5851112443197053e-06, Train Acc: 1.0\n",
            "Epoch 8652/10000\n",
            "Step 0: Train Loss: 4.039558461954584e-06, Train Acc: 1.0\n",
            "Epoch 8653/10000\n",
            "Step 0: Train Loss: 3.6353058021632023e-06, Train Acc: 1.0\n",
            "Epoch 8654/10000\n",
            "Step 0: Train Loss: 1.6517240055691218e-06, Train Acc: 1.0\n",
            "Epoch 8655/10000\n",
            "Step 0: Train Loss: 3.7169784263824113e-06, Train Acc: 1.0\n",
            "Epoch 8656/10000\n",
            "Step 0: Train Loss: 9.69836582953576e-06, Train Acc: 1.0\n",
            "Epoch 8657/10000\n",
            "Step 0: Train Loss: 2.6081195301230764e-06, Train Acc: 1.0\n",
            "Epoch 8658/10000\n",
            "Step 0: Train Loss: 3.950577138311928e-06, Train Acc: 1.0\n",
            "Epoch 8659/10000\n",
            "Step 0: Train Loss: 2.4394919364567613e-06, Train Acc: 1.0\n",
            "Epoch 8660/10000\n",
            "Step 0: Train Loss: 9.34120180318132e-06, Train Acc: 1.0\n",
            "Epoch 8661/10000\n",
            "Step 0: Train Loss: 3.489746177365305e-06, Train Acc: 1.0\n",
            "Epoch 8662/10000\n",
            "Step 0: Train Loss: 1.7372483398503391e-06, Train Acc: 1.0\n",
            "Epoch 8663/10000\n",
            "Step 0: Train Loss: 4.260056357452413e-06, Train Acc: 1.0\n",
            "Epoch 8664/10000\n",
            "Step 0: Train Loss: 1.8968881931868964e-06, Train Acc: 1.0\n",
            "Epoch 8665/10000\n",
            "Step 0: Train Loss: 9.372463864565361e-06, Train Acc: 1.0\n",
            "Epoch 8666/10000\n",
            "Step 0: Train Loss: 2.119684495482943e-06, Train Acc: 1.0\n",
            "Epoch 8667/10000\n",
            "Step 0: Train Loss: 2.7391347430238966e-06, Train Acc: 1.0\n",
            "Epoch 8668/10000\n",
            "Step 0: Train Loss: 2.676205440366175e-06, Train Acc: 1.0\n",
            "Epoch 8669/10000\n",
            "Step 0: Train Loss: 1.825807089517184e-06, Train Acc: 1.0\n",
            "Epoch 8670/10000\n",
            "Step 0: Train Loss: 1.8285334135725861e-06, Train Acc: 1.0\n",
            "Epoch 8671/10000\n",
            "Step 0: Train Loss: 2.371527216382674e-06, Train Acc: 1.0\n",
            "Epoch 8672/10000\n",
            "Step 0: Train Loss: 1.8868513507186435e-06, Train Acc: 1.0\n",
            "Epoch 8673/10000\n",
            "Step 0: Train Loss: 2.0776255951204803e-06, Train Acc: 1.0\n",
            "Epoch 8674/10000\n",
            "Step 0: Train Loss: 4.879386779066408e-06, Train Acc: 1.0\n",
            "Epoch 8675/10000\n",
            "Step 0: Train Loss: 3.0080416308919666e-06, Train Acc: 1.0\n",
            "Epoch 8676/10000\n",
            "Step 0: Train Loss: 1.695443643257022e-06, Train Acc: 1.0\n",
            "Epoch 8677/10000\n",
            "Step 0: Train Loss: 2.3471034182875883e-06, Train Acc: 1.0\n",
            "Epoch 8678/10000\n",
            "Step 0: Train Loss: 9.717857210489456e-06, Train Acc: 1.0\n",
            "Epoch 8679/10000\n",
            "Step 0: Train Loss: 2.040083700194373e-06, Train Acc: 1.0\n",
            "Epoch 8680/10000\n",
            "Step 0: Train Loss: 2.7059861622547032e-06, Train Acc: 1.0\n",
            "Epoch 8681/10000\n",
            "Step 0: Train Loss: 1.8793076606016257e-06, Train Acc: 1.0\n",
            "Epoch 8682/10000\n",
            "Step 0: Train Loss: 2.2997649011813337e-06, Train Acc: 1.0\n",
            "Epoch 8683/10000\n",
            "Step 0: Train Loss: 2.788550318655325e-06, Train Acc: 1.0\n",
            "Epoch 8684/10000\n",
            "Step 0: Train Loss: 3.0428714126173873e-06, Train Acc: 1.0\n",
            "Epoch 8685/10000\n",
            "Step 0: Train Loss: 1.9769320260820678e-06, Train Acc: 1.0\n",
            "Epoch 8686/10000\n",
            "Step 0: Train Loss: 1.709209755063057e-06, Train Acc: 1.0\n",
            "Epoch 8687/10000\n",
            "Step 0: Train Loss: 2.4862818008841714e-06, Train Acc: 1.0\n",
            "Epoch 8688/10000\n",
            "Step 0: Train Loss: 2.9201221423136303e-06, Train Acc: 1.0\n",
            "Epoch 8689/10000\n",
            "Step 0: Train Loss: 8.307501957460772e-06, Train Acc: 1.0\n",
            "Epoch 8690/10000\n",
            "Step 0: Train Loss: 2.2170045212988043e-06, Train Acc: 1.0\n",
            "Epoch 8691/10000\n",
            "Step 0: Train Loss: 2.560631855885731e-06, Train Acc: 1.0\n",
            "Epoch 8692/10000\n",
            "Step 0: Train Loss: 2.5315728180430597e-06, Train Acc: 1.0\n",
            "Epoch 8693/10000\n",
            "Step 0: Train Loss: 2.8726988148264354e-06, Train Acc: 1.0\n",
            "Epoch 8694/10000\n",
            "Step 0: Train Loss: 1.910542550831451e-06, Train Acc: 1.0\n",
            "Epoch 8695/10000\n",
            "Step 0: Train Loss: 2.2978733795753215e-06, Train Acc: 1.0\n",
            "Epoch 8696/10000\n",
            "Step 0: Train Loss: 2.3954000880621606e-06, Train Acc: 1.0\n",
            "Epoch 8697/10000\n",
            "Step 0: Train Loss: 1.0092646334669553e-05, Train Acc: 1.0\n",
            "Epoch 8698/10000\n",
            "Step 0: Train Loss: 2.0112329366384074e-06, Train Acc: 1.0\n",
            "Epoch 8699/10000\n",
            "Step 0: Train Loss: 2.117525809808285e-06, Train Acc: 1.0\n",
            "Epoch 8700/10000\n",
            "Step 0: Train Loss: 1.9422311652306234e-06, Train Acc: 1.0\n",
            "Epoch 8701/10000\n",
            "Step 0: Train Loss: 8.060730579018127e-06, Train Acc: 1.0\n",
            "Epoch 8702/10000\n",
            "Step 0: Train Loss: 2.123920694430126e-06, Train Acc: 1.0\n",
            "Epoch 8703/10000\n",
            "Step 0: Train Loss: 2.997512638103217e-06, Train Acc: 1.0\n",
            "Epoch 8704/10000\n",
            "Step 0: Train Loss: 1.4888282748870552e-06, Train Acc: 1.0\n",
            "Epoch 8705/10000\n",
            "Step 0: Train Loss: 7.90057129052002e-06, Train Acc: 1.0\n",
            "Epoch 8706/10000\n",
            "Step 0: Train Loss: 2.559169843152631e-06, Train Acc: 1.0\n",
            "Epoch 8707/10000\n",
            "Step 0: Train Loss: 8.366111615032423e-06, Train Acc: 1.0\n",
            "Epoch 8708/10000\n",
            "Step 0: Train Loss: 2.5488225219305605e-06, Train Acc: 1.0\n",
            "Epoch 8709/10000\n",
            "Step 0: Train Loss: 2.1174587345740292e-06, Train Acc: 1.0\n",
            "Epoch 8710/10000\n",
            "Step 0: Train Loss: 3.3396634080418153e-06, Train Acc: 1.0\n",
            "Epoch 8711/10000\n",
            "Step 0: Train Loss: 2.122462774423184e-06, Train Acc: 1.0\n",
            "Epoch 8712/10000\n",
            "Step 0: Train Loss: 1.1891373787875636e-06, Train Acc: 1.0\n",
            "Epoch 8713/10000\n",
            "Step 0: Train Loss: 2.357360472160508e-06, Train Acc: 1.0\n",
            "Epoch 8714/10000\n",
            "Step 0: Train Loss: 2.5880062821670435e-06, Train Acc: 1.0\n",
            "Epoch 8715/10000\n",
            "Step 0: Train Loss: 1.7920724530995358e-06, Train Acc: 1.0\n",
            "Epoch 8716/10000\n",
            "Step 0: Train Loss: 2.410637080174638e-06, Train Acc: 1.0\n",
            "Epoch 8717/10000\n",
            "Step 0: Train Loss: 3.2478239972988376e-06, Train Acc: 1.0\n",
            "Epoch 8718/10000\n",
            "Step 0: Train Loss: 2.1609753275697585e-06, Train Acc: 1.0\n",
            "Epoch 8719/10000\n",
            "Step 0: Train Loss: 3.338713213452138e-06, Train Acc: 1.0\n",
            "Epoch 8720/10000\n",
            "Step 0: Train Loss: 1.9483768483041786e-06, Train Acc: 1.0\n",
            "Epoch 8721/10000\n",
            "Step 0: Train Loss: 2.140237484127283e-06, Train Acc: 1.0\n",
            "Epoch 8722/10000\n",
            "Step 0: Train Loss: 7.82064762461232e-06, Train Acc: 1.0\n",
            "Epoch 8723/10000\n",
            "Step 0: Train Loss: 3.244952949899016e-06, Train Acc: 1.0\n",
            "Epoch 8724/10000\n",
            "Step 0: Train Loss: 1.7690083495836006e-06, Train Acc: 1.0\n",
            "Epoch 8725/10000\n",
            "Step 0: Train Loss: 7.540874321421143e-06, Train Acc: 1.0\n",
            "Epoch 8726/10000\n",
            "Step 0: Train Loss: 1.4790225577598903e-06, Train Acc: 1.0\n",
            "Epoch 8727/10000\n",
            "Step 0: Train Loss: 2.0648326426453423e-06, Train Acc: 1.0\n",
            "Epoch 8728/10000\n",
            "Step 0: Train Loss: 2.09642917070596e-06, Train Acc: 1.0\n",
            "Epoch 8729/10000\n",
            "Step 0: Train Loss: 2.5759957225091057e-06, Train Acc: 1.0\n",
            "Epoch 8730/10000\n",
            "Step 0: Train Loss: 2.9264326713018818e-06, Train Acc: 1.0\n",
            "Epoch 8731/10000\n",
            "Step 0: Train Loss: 3.522527322274982e-06, Train Acc: 1.0\n",
            "Epoch 8732/10000\n",
            "Step 0: Train Loss: 7.669262231502216e-06, Train Acc: 1.0\n",
            "Epoch 8733/10000\n",
            "Step 0: Train Loss: 1.5775416386532015e-06, Train Acc: 1.0\n",
            "Epoch 8734/10000\n",
            "Step 0: Train Loss: 2.0745303572766716e-06, Train Acc: 1.0\n",
            "Epoch 8735/10000\n",
            "Step 0: Train Loss: 1.952613956746063e-06, Train Acc: 1.0\n",
            "Epoch 8736/10000\n",
            "Step 0: Train Loss: 8.004331903066486e-06, Train Acc: 1.0\n",
            "Epoch 8737/10000\n",
            "Step 0: Train Loss: 2.353487388973008e-06, Train Acc: 1.0\n",
            "Epoch 8738/10000\n",
            "Step 0: Train Loss: 2.0968668650311884e-06, Train Acc: 1.0\n",
            "Epoch 8739/10000\n",
            "Step 0: Train Loss: 2.0080638023500796e-06, Train Acc: 1.0\n",
            "Epoch 8740/10000\n",
            "Step 0: Train Loss: 1.3219275842857314e-06, Train Acc: 1.0\n",
            "Epoch 8741/10000\n",
            "Step 0: Train Loss: 1.5285824019883876e-06, Train Acc: 1.0\n",
            "Epoch 8742/10000\n",
            "Step 0: Train Loss: 2.711180059122853e-06, Train Acc: 1.0\n",
            "Epoch 8743/10000\n",
            "Step 0: Train Loss: 8.395214535994455e-06, Train Acc: 1.0\n",
            "Epoch 8744/10000\n",
            "Step 0: Train Loss: 2.024589548454969e-06, Train Acc: 1.0\n",
            "Epoch 8745/10000\n",
            "Step 0: Train Loss: 1.6585599951213226e-06, Train Acc: 1.0\n",
            "Epoch 8746/10000\n",
            "Step 0: Train Loss: 1.8375218360233703e-06, Train Acc: 1.0\n",
            "Epoch 8747/10000\n",
            "Step 0: Train Loss: 3.2857749374670675e-06, Train Acc: 1.0\n",
            "Epoch 8748/10000\n",
            "Step 0: Train Loss: 1.7663787730270997e-06, Train Acc: 1.0\n",
            "Epoch 8749/10000\n",
            "Step 0: Train Loss: 8.22250876808539e-06, Train Acc: 1.0\n",
            "Epoch 8750/10000\n",
            "Step 0: Train Loss: 1.713568394734466e-06, Train Acc: 1.0\n",
            "Epoch 8751/10000\n",
            "Step 0: Train Loss: 2.781753664748976e-06, Train Acc: 1.0\n",
            "Epoch 8752/10000\n",
            "Step 0: Train Loss: 1.4017448393133236e-06, Train Acc: 1.0\n",
            "Epoch 8753/10000\n",
            "Step 0: Train Loss: 7.319005362660391e-06, Train Acc: 1.0\n",
            "Epoch 8754/10000\n",
            "Step 0: Train Loss: 1.3464671155816177e-06, Train Acc: 1.0\n",
            "Epoch 8755/10000\n",
            "Step 0: Train Loss: 3.0662924928037683e-06, Train Acc: 1.0\n",
            "Epoch 8756/10000\n",
            "Step 0: Train Loss: 2.6472894205653574e-06, Train Acc: 1.0\n",
            "Epoch 8757/10000\n",
            "Step 0: Train Loss: 1.4265525578593952e-06, Train Acc: 1.0\n",
            "Epoch 8758/10000\n",
            "Step 0: Train Loss: 1.8452001313562505e-06, Train Acc: 1.0\n",
            "Epoch 8759/10000\n",
            "Step 0: Train Loss: 2.5521692350594094e-06, Train Acc: 1.0\n",
            "Epoch 8760/10000\n",
            "Step 0: Train Loss: 2.0391180441947654e-06, Train Acc: 1.0\n",
            "Epoch 8761/10000\n",
            "Step 0: Train Loss: 6.845992629678221e-06, Train Acc: 1.0\n",
            "Epoch 8762/10000\n",
            "Step 0: Train Loss: 1.5268785773514537e-06, Train Acc: 1.0\n",
            "Epoch 8763/10000\n",
            "Step 0: Train Loss: 6.996714546403382e-06, Train Acc: 1.0\n",
            "Epoch 8764/10000\n",
            "Step 0: Train Loss: 2.5279196051997133e-06, Train Acc: 1.0\n",
            "Epoch 8765/10000\n",
            "Step 0: Train Loss: 2.227650384156732e-06, Train Acc: 1.0\n",
            "Epoch 8766/10000\n",
            "Step 0: Train Loss: 7.1980557549977675e-06, Train Acc: 1.0\n",
            "Epoch 8767/10000\n",
            "Step 0: Train Loss: 3.6368273867992684e-06, Train Acc: 1.0\n",
            "Epoch 8768/10000\n",
            "Step 0: Train Loss: 1.3509875316231046e-06, Train Acc: 1.0\n",
            "Epoch 8769/10000\n",
            "Step 0: Train Loss: 1.915502707561245e-06, Train Acc: 1.0\n",
            "Epoch 8770/10000\n",
            "Step 0: Train Loss: 1.8823441223503323e-06, Train Acc: 1.0\n",
            "Epoch 8771/10000\n",
            "Step 0: Train Loss: 1.5963593114065588e-06, Train Acc: 1.0\n",
            "Epoch 8772/10000\n",
            "Step 0: Train Loss: 2.1946561901131645e-06, Train Acc: 1.0\n",
            "Epoch 8773/10000\n",
            "Step 0: Train Loss: 2.5686458684504032e-06, Train Acc: 1.0\n",
            "Epoch 8774/10000\n",
            "Step 0: Train Loss: 2.0235761439835187e-06, Train Acc: 1.0\n",
            "Epoch 8775/10000\n",
            "Step 0: Train Loss: 1.115668624152022e-06, Train Acc: 1.0\n",
            "Epoch 8776/10000\n",
            "Step 0: Train Loss: 7.269881734828232e-06, Train Acc: 1.0\n",
            "Epoch 8777/10000\n",
            "Step 0: Train Loss: 2.040136678260751e-06, Train Acc: 1.0\n",
            "Epoch 8778/10000\n",
            "Step 0: Train Loss: 1.6661472272971878e-06, Train Acc: 1.0\n",
            "Epoch 8779/10000\n",
            "Step 0: Train Loss: 5.67196275369497e-06, Train Acc: 1.0\n",
            "Epoch 8780/10000\n",
            "Step 0: Train Loss: 1.362440798402531e-06, Train Acc: 1.0\n",
            "Epoch 8781/10000\n",
            "Step 0: Train Loss: 1.6758683614170877e-06, Train Acc: 1.0\n",
            "Epoch 8782/10000\n",
            "Step 0: Train Loss: 1.8110982864527614e-06, Train Acc: 1.0\n",
            "Epoch 8783/10000\n",
            "Step 0: Train Loss: 2.392097712800023e-06, Train Acc: 1.0\n",
            "Epoch 8784/10000\n",
            "Step 0: Train Loss: 6.1470582295442e-06, Train Acc: 1.0\n",
            "Epoch 8785/10000\n",
            "Step 0: Train Loss: 1.8959541421281756e-06, Train Acc: 1.0\n",
            "Epoch 8786/10000\n",
            "Step 0: Train Loss: 1.4786178326176014e-06, Train Acc: 1.0\n",
            "Epoch 8787/10000\n",
            "Step 0: Train Loss: 2.127661446138518e-06, Train Acc: 1.0\n",
            "Epoch 8788/10000\n",
            "Step 0: Train Loss: 2.2082952000346268e-06, Train Acc: 1.0\n",
            "Epoch 8789/10000\n",
            "Step 0: Train Loss: 1.9723545392480446e-06, Train Acc: 1.0\n",
            "Epoch 8790/10000\n",
            "Step 0: Train Loss: 1.4775192767046974e-06, Train Acc: 1.0\n",
            "Epoch 8791/10000\n",
            "Step 0: Train Loss: 1.4854102801109548e-06, Train Acc: 1.0\n",
            "Epoch 8792/10000\n",
            "Step 0: Train Loss: 2.2758088107366348e-06, Train Acc: 1.0\n",
            "Epoch 8793/10000\n",
            "Step 0: Train Loss: 1.9942185645049904e-06, Train Acc: 1.0\n",
            "Epoch 8794/10000\n",
            "Step 0: Train Loss: 6.92379717293079e-06, Train Acc: 1.0\n",
            "Epoch 8795/10000\n",
            "Step 0: Train Loss: 6.407673481589882e-06, Train Acc: 1.0\n",
            "Epoch 8796/10000\n",
            "Step 0: Train Loss: 6.2318244999914896e-06, Train Acc: 1.0\n",
            "Epoch 8797/10000\n",
            "Step 0: Train Loss: 1.6398945490436745e-06, Train Acc: 1.0\n",
            "Epoch 8798/10000\n",
            "Step 0: Train Loss: 5.773020347987767e-06, Train Acc: 1.0\n",
            "Epoch 8799/10000\n",
            "Step 0: Train Loss: 5.459109615912894e-06, Train Acc: 1.0\n",
            "Epoch 8800/10000\n",
            "Step 0: Train Loss: 1.4524378002533922e-06, Train Acc: 1.0\n",
            "Epoch 8801/10000\n",
            "Step 0: Train Loss: 2.287343022544519e-06, Train Acc: 1.0\n",
            "Epoch 8802/10000\n",
            "Step 0: Train Loss: 1.5333944247686304e-06, Train Acc: 1.0\n",
            "Epoch 8803/10000\n",
            "Step 0: Train Loss: 1.8737338223218103e-06, Train Acc: 1.0\n",
            "Epoch 8804/10000\n",
            "Step 0: Train Loss: 2.4729959022806725e-06, Train Acc: 1.0\n",
            "Epoch 8805/10000\n",
            "Step 0: Train Loss: 1.7171073523059022e-06, Train Acc: 1.0\n",
            "Epoch 8806/10000\n",
            "Step 0: Train Loss: 1.1168968967467663e-06, Train Acc: 1.0\n",
            "Epoch 8807/10000\n",
            "Step 0: Train Loss: 1.3964272511657327e-06, Train Acc: 1.0\n",
            "Epoch 8808/10000\n",
            "Step 0: Train Loss: 1.52388338392484e-06, Train Acc: 1.0\n",
            "Epoch 8809/10000\n",
            "Step 0: Train Loss: 1.7756613033270696e-06, Train Acc: 1.0\n",
            "Epoch 8810/10000\n",
            "Step 0: Train Loss: 9.989346381189534e-07, Train Acc: 1.0\n",
            "Epoch 8811/10000\n",
            "Step 0: Train Loss: 1.047108298735111e-06, Train Acc: 1.0\n",
            "Epoch 8812/10000\n",
            "Step 0: Train Loss: 1.2485245406423928e-06, Train Acc: 1.0\n",
            "Epoch 8813/10000\n",
            "Step 0: Train Loss: 1.1854539252453833e-06, Train Acc: 1.0\n",
            "Epoch 8814/10000\n",
            "Step 0: Train Loss: 1.4351315940075438e-06, Train Acc: 1.0\n",
            "Epoch 8815/10000\n",
            "Step 0: Train Loss: 1.2380518228383153e-06, Train Acc: 1.0\n",
            "Epoch 8816/10000\n",
            "Step 0: Train Loss: 1.2956388673046604e-06, Train Acc: 1.0\n",
            "Epoch 8817/10000\n",
            "Step 0: Train Loss: 1.1325522564220591e-06, Train Acc: 1.0\n",
            "Epoch 8818/10000\n",
            "Step 0: Train Loss: 1.0848254987649852e-06, Train Acc: 1.0\n",
            "Epoch 8819/10000\n",
            "Step 0: Train Loss: 1.8057230590784457e-06, Train Acc: 1.0\n",
            "Epoch 8820/10000\n",
            "Step 0: Train Loss: 1.9683495793287875e-06, Train Acc: 1.0\n",
            "Epoch 8821/10000\n",
            "Step 0: Train Loss: 1.1952689646932413e-06, Train Acc: 1.0\n",
            "Epoch 8822/10000\n",
            "Step 0: Train Loss: 1.619096678950882e-06, Train Acc: 1.0\n",
            "Epoch 8823/10000\n",
            "Step 0: Train Loss: 1.4572120790035115e-06, Train Acc: 1.0\n",
            "Epoch 8824/10000\n",
            "Step 0: Train Loss: 1.4639404071203899e-06, Train Acc: 1.0\n",
            "Epoch 8825/10000\n",
            "Step 0: Train Loss: 2.5103374809987145e-06, Train Acc: 1.0\n",
            "Epoch 8826/10000\n",
            "Step 0: Train Loss: 1.8601933788886527e-06, Train Acc: 1.0\n",
            "Epoch 8827/10000\n",
            "Step 0: Train Loss: 1.4470334690486197e-06, Train Acc: 1.0\n",
            "Epoch 8828/10000\n",
            "Step 0: Train Loss: 2.1888122319069225e-06, Train Acc: 1.0\n",
            "Epoch 8829/10000\n",
            "Step 0: Train Loss: 2.256956122437259e-06, Train Acc: 1.0\n",
            "Epoch 8830/10000\n",
            "Step 0: Train Loss: 1.2863911251770332e-06, Train Acc: 1.0\n",
            "Epoch 8831/10000\n",
            "Step 0: Train Loss: 1.5160670727709658e-06, Train Acc: 1.0\n",
            "Epoch 8832/10000\n",
            "Step 0: Train Loss: 1.3518360901798587e-06, Train Acc: 1.0\n",
            "Epoch 8833/10000\n",
            "Step 0: Train Loss: 1.4354006907524308e-06, Train Acc: 1.0\n",
            "Epoch 8834/10000\n",
            "Step 0: Train Loss: 5.24472898177919e-06, Train Acc: 1.0\n",
            "Epoch 8835/10000\n",
            "Step 0: Train Loss: 2.535130988690071e-06, Train Acc: 1.0\n",
            "Epoch 8836/10000\n",
            "Step 0: Train Loss: 5.213704298512312e-06, Train Acc: 1.0\n",
            "Epoch 8837/10000\n",
            "Step 0: Train Loss: 4.731356057163794e-06, Train Acc: 1.0\n",
            "Epoch 8838/10000\n",
            "Step 0: Train Loss: 5.0355001803836785e-06, Train Acc: 1.0\n",
            "Epoch 8839/10000\n",
            "Step 0: Train Loss: 1.6559737332499935e-06, Train Acc: 1.0\n",
            "Epoch 8840/10000\n",
            "Step 0: Train Loss: 1.3958459703644621e-06, Train Acc: 1.0\n",
            "Epoch 8841/10000\n",
            "Step 0: Train Loss: 1.5616077462254907e-06, Train Acc: 1.0\n",
            "Epoch 8842/10000\n",
            "Step 0: Train Loss: 1.4624390587414382e-06, Train Acc: 1.0\n",
            "Epoch 8843/10000\n",
            "Step 0: Train Loss: 1.0000346719607478e-06, Train Acc: 1.0\n",
            "Epoch 8844/10000\n",
            "Step 0: Train Loss: 1.337183221039595e-06, Train Acc: 1.0\n",
            "Epoch 8845/10000\n",
            "Step 0: Train Loss: 1.0501099723114748e-06, Train Acc: 1.0\n",
            "Epoch 8846/10000\n",
            "Step 0: Train Loss: 1.1356912636983907e-06, Train Acc: 1.0\n",
            "Epoch 8847/10000\n",
            "Step 0: Train Loss: 1.3762698927166639e-06, Train Acc: 1.0\n",
            "Epoch 8848/10000\n",
            "Step 0: Train Loss: 1.2448524557839846e-06, Train Acc: 1.0\n",
            "Epoch 8849/10000\n",
            "Step 0: Train Loss: 1.2330592653597705e-06, Train Acc: 1.0\n",
            "Epoch 8850/10000\n",
            "Step 0: Train Loss: 1.0260046110488474e-06, Train Acc: 1.0\n",
            "Epoch 8851/10000\n",
            "Step 0: Train Loss: 1.2349091775831766e-06, Train Acc: 1.0\n",
            "Epoch 8852/10000\n",
            "Step 0: Train Loss: 1.368910830024106e-06, Train Acc: 1.0\n",
            "Epoch 8853/10000\n",
            "Step 0: Train Loss: 1.971098527064896e-06, Train Acc: 1.0\n",
            "Epoch 8854/10000\n",
            "Step 0: Train Loss: 4.91892387799453e-06, Train Acc: 1.0\n",
            "Epoch 8855/10000\n",
            "Step 0: Train Loss: 5.081866675027413e-06, Train Acc: 1.0\n",
            "Epoch 8856/10000\n",
            "Step 0: Train Loss: 1.621983983568498e-06, Train Acc: 1.0\n",
            "Epoch 8857/10000\n",
            "Step 0: Train Loss: 1.3081457836960908e-06, Train Acc: 1.0\n",
            "Epoch 8858/10000\n",
            "Step 0: Train Loss: 1.2895693544123787e-06, Train Acc: 1.0\n",
            "Epoch 8859/10000\n",
            "Step 0: Train Loss: 8.760480341152288e-07, Train Acc: 1.0\n",
            "Epoch 8860/10000\n",
            "Step 0: Train Loss: 9.171231454274675e-07, Train Acc: 1.0\n",
            "Epoch 8861/10000\n",
            "Step 0: Train Loss: 1.3042217688052915e-06, Train Acc: 1.0\n",
            "Epoch 8862/10000\n",
            "Step 0: Train Loss: 1.0041428595286561e-06, Train Acc: 1.0\n",
            "Epoch 8863/10000\n",
            "Step 0: Train Loss: 1.2745899766741786e-06, Train Acc: 1.0\n",
            "Epoch 8864/10000\n",
            "Step 0: Train Loss: 4.83604026157991e-06, Train Acc: 1.0\n",
            "Epoch 8865/10000\n",
            "Step 0: Train Loss: 1.3008990435992018e-06, Train Acc: 1.0\n",
            "Epoch 8866/10000\n",
            "Step 0: Train Loss: 1.6158630842255661e-06, Train Acc: 1.0\n",
            "Epoch 8867/10000\n",
            "Step 0: Train Loss: 1.961313046194846e-06, Train Acc: 1.0\n",
            "Epoch 8868/10000\n",
            "Step 0: Train Loss: 1.940674110301188e-06, Train Acc: 1.0\n",
            "Epoch 8869/10000\n",
            "Step 0: Train Loss: 1.5041327969811391e-06, Train Acc: 1.0\n",
            "Epoch 8870/10000\n",
            "Step 0: Train Loss: 4.612327302311314e-06, Train Acc: 1.0\n",
            "Epoch 8871/10000\n",
            "Step 0: Train Loss: 9.810055416892283e-07, Train Acc: 1.0\n",
            "Epoch 8872/10000\n",
            "Step 0: Train Loss: 1.1138064337501419e-06, Train Acc: 1.0\n",
            "Epoch 8873/10000\n",
            "Step 0: Train Loss: 1.6031474388000788e-06, Train Acc: 1.0\n",
            "Epoch 8874/10000\n",
            "Step 0: Train Loss: 1.3071302191747236e-06, Train Acc: 1.0\n",
            "Epoch 8875/10000\n",
            "Step 0: Train Loss: 1.08399694909167e-06, Train Acc: 1.0\n",
            "Epoch 8876/10000\n",
            "Step 0: Train Loss: 8.447048003290547e-07, Train Acc: 1.0\n",
            "Epoch 8877/10000\n",
            "Step 0: Train Loss: 5.12601809532498e-06, Train Acc: 1.0\n",
            "Epoch 8878/10000\n",
            "Step 0: Train Loss: 1.2472694379539462e-06, Train Acc: 1.0\n",
            "Epoch 8879/10000\n",
            "Step 0: Train Loss: 5.035869889979949e-06, Train Acc: 1.0\n",
            "Epoch 8880/10000\n",
            "Step 0: Train Loss: 9.151399922302517e-07, Train Acc: 1.0\n",
            "Epoch 8881/10000\n",
            "Step 0: Train Loss: 2.2335839275910985e-06, Train Acc: 1.0\n",
            "Epoch 8882/10000\n",
            "Step 0: Train Loss: 9.216273042511602e-07, Train Acc: 1.0\n",
            "Epoch 8883/10000\n",
            "Step 0: Train Loss: 9.65773438110773e-07, Train Acc: 1.0\n",
            "Epoch 8884/10000\n",
            "Step 0: Train Loss: 4.138419171795249e-06, Train Acc: 1.0\n",
            "Epoch 8885/10000\n",
            "Step 0: Train Loss: 1.0552106459726929e-06, Train Acc: 1.0\n",
            "Epoch 8886/10000\n",
            "Step 0: Train Loss: 4.688186436396791e-06, Train Acc: 1.0\n",
            "Epoch 8887/10000\n",
            "Step 0: Train Loss: 4.773293767357245e-06, Train Acc: 1.0\n",
            "Epoch 8888/10000\n",
            "Step 0: Train Loss: 1.0445916132084676e-06, Train Acc: 1.0\n",
            "Epoch 8889/10000\n",
            "Step 0: Train Loss: 1.006858269647637e-06, Train Acc: 1.0\n",
            "Epoch 8890/10000\n",
            "Step 0: Train Loss: 4.67248355562333e-06, Train Acc: 1.0\n",
            "Epoch 8891/10000\n",
            "Step 0: Train Loss: 1.4277445643529063e-06, Train Acc: 1.0\n",
            "Epoch 8892/10000\n",
            "Step 0: Train Loss: 4.304994945414364e-06, Train Acc: 1.0\n",
            "Epoch 8893/10000\n",
            "Step 0: Train Loss: 9.315921261077165e-07, Train Acc: 1.0\n",
            "Epoch 8894/10000\n",
            "Step 0: Train Loss: 1.2945301932631992e-06, Train Acc: 1.0\n",
            "Epoch 8895/10000\n",
            "Step 0: Train Loss: 8.768171255724155e-07, Train Acc: 1.0\n",
            "Epoch 8896/10000\n",
            "Step 0: Train Loss: 1.0931828455795767e-06, Train Acc: 1.0\n",
            "Epoch 8897/10000\n",
            "Step 0: Train Loss: 1.5971957054716768e-06, Train Acc: 1.0\n",
            "Epoch 8898/10000\n",
            "Step 0: Train Loss: 1.153608423010155e-06, Train Acc: 1.0\n",
            "Epoch 8899/10000\n",
            "Step 0: Train Loss: 1.6603596577624558e-06, Train Acc: 1.0\n",
            "Epoch 8900/10000\n",
            "Step 0: Train Loss: 1.0245140629194793e-06, Train Acc: 1.0\n",
            "Epoch 8901/10000\n",
            "Step 0: Train Loss: 1.214202484334237e-06, Train Acc: 1.0\n",
            "Epoch 8902/10000\n",
            "Step 0: Train Loss: 9.710262247608625e-07, Train Acc: 1.0\n",
            "Epoch 8903/10000\n",
            "Step 0: Train Loss: 7.182661647675559e-07, Train Acc: 1.0\n",
            "Epoch 8904/10000\n",
            "Step 0: Train Loss: 9.904576927510789e-07, Train Acc: 1.0\n",
            "Epoch 8905/10000\n",
            "Step 0: Train Loss: 8.93991000339156e-07, Train Acc: 1.0\n",
            "Epoch 8906/10000\n",
            "Step 0: Train Loss: 1.0685412235034164e-06, Train Acc: 1.0\n",
            "Epoch 8907/10000\n",
            "Step 0: Train Loss: 1.312115159635141e-06, Train Acc: 1.0\n",
            "Epoch 8908/10000\n",
            "Step 0: Train Loss: 1.0138199968423578e-06, Train Acc: 1.0\n",
            "Epoch 8909/10000\n",
            "Step 0: Train Loss: 4.217614787194179e-06, Train Acc: 1.0\n",
            "Epoch 8910/10000\n",
            "Step 0: Train Loss: 1.0779687045214814e-06, Train Acc: 1.0\n",
            "Epoch 8911/10000\n",
            "Step 0: Train Loss: 8.070385320024798e-07, Train Acc: 1.0\n",
            "Epoch 8912/10000\n",
            "Step 0: Train Loss: 9.120809068008384e-07, Train Acc: 1.0\n",
            "Epoch 8913/10000\n",
            "Step 0: Train Loss: 1.7110536418840638e-06, Train Acc: 1.0\n",
            "Epoch 8914/10000\n",
            "Step 0: Train Loss: 8.1113165606439e-07, Train Acc: 1.0\n",
            "Epoch 8915/10000\n",
            "Step 0: Train Loss: 7.359374762927473e-07, Train Acc: 1.0\n",
            "Epoch 8916/10000\n",
            "Step 0: Train Loss: 1.0597992741168127e-06, Train Acc: 1.0\n",
            "Epoch 8917/10000\n",
            "Step 0: Train Loss: 1.1796054195656325e-06, Train Acc: 1.0\n",
            "Epoch 8918/10000\n",
            "Step 0: Train Loss: 1.070018925020122e-06, Train Acc: 1.0\n",
            "Epoch 8919/10000\n",
            "Step 0: Train Loss: 1.2391914196996368e-06, Train Acc: 1.0\n",
            "Epoch 8920/10000\n",
            "Step 0: Train Loss: 1.4670629298052518e-06, Train Acc: 1.0\n",
            "Epoch 8921/10000\n",
            "Step 0: Train Loss: 1.35680636503821e-06, Train Acc: 1.0\n",
            "Epoch 8922/10000\n",
            "Step 0: Train Loss: 7.037709224277933e-07, Train Acc: 1.0\n",
            "Epoch 8923/10000\n",
            "Step 0: Train Loss: 3.5696591567102587e-06, Train Acc: 1.0\n",
            "Epoch 8924/10000\n",
            "Step 0: Train Loss: 9.133437401942501e-07, Train Acc: 1.0\n",
            "Epoch 8925/10000\n",
            "Step 0: Train Loss: 4.368908776086755e-06, Train Acc: 1.0\n",
            "Epoch 8926/10000\n",
            "Step 0: Train Loss: 1.313392203883268e-06, Train Acc: 1.0\n",
            "Epoch 8927/10000\n",
            "Step 0: Train Loss: 9.954495681085973e-07, Train Acc: 1.0\n",
            "Epoch 8928/10000\n",
            "Step 0: Train Loss: 9.906060540743056e-07, Train Acc: 1.0\n",
            "Epoch 8929/10000\n",
            "Step 0: Train Loss: 1.379072159579664e-06, Train Acc: 1.0\n",
            "Epoch 8930/10000\n",
            "Step 0: Train Loss: 1.8829181271939888e-06, Train Acc: 1.0\n",
            "Epoch 8931/10000\n",
            "Step 0: Train Loss: 8.623929943496478e-07, Train Acc: 1.0\n",
            "Epoch 8932/10000\n",
            "Step 0: Train Loss: 1.0067874427477363e-06, Train Acc: 1.0\n",
            "Epoch 8933/10000\n",
            "Step 0: Train Loss: 8.57313466440246e-07, Train Acc: 1.0\n",
            "Epoch 8934/10000\n",
            "Step 0: Train Loss: 9.236745768248511e-07, Train Acc: 1.0\n",
            "Epoch 8935/10000\n",
            "Step 0: Train Loss: 7.528753371843777e-07, Train Acc: 1.0\n",
            "Epoch 8936/10000\n",
            "Step 0: Train Loss: 7.735303597655729e-07, Train Acc: 1.0\n",
            "Epoch 8937/10000\n",
            "Step 0: Train Loss: 8.414953072133358e-07, Train Acc: 1.0\n",
            "Epoch 8938/10000\n",
            "Step 0: Train Loss: 1.3732084198636585e-06, Train Acc: 1.0\n",
            "Epoch 8939/10000\n",
            "Step 0: Train Loss: 1.2750225550917094e-06, Train Acc: 1.0\n",
            "Epoch 8940/10000\n",
            "Step 0: Train Loss: 8.774982802606246e-07, Train Acc: 1.0\n",
            "Epoch 8941/10000\n",
            "Step 0: Train Loss: 1.1034643421226065e-06, Train Acc: 1.0\n",
            "Epoch 8942/10000\n",
            "Step 0: Train Loss: 1.057350118571776e-06, Train Acc: 1.0\n",
            "Epoch 8943/10000\n",
            "Step 0: Train Loss: 8.208338044823904e-07, Train Acc: 1.0\n",
            "Epoch 8944/10000\n",
            "Step 0: Train Loss: 8.31199088224821e-07, Train Acc: 1.0\n",
            "Epoch 8945/10000\n",
            "Step 0: Train Loss: 8.338282100339711e-07, Train Acc: 1.0\n",
            "Epoch 8946/10000\n",
            "Step 0: Train Loss: 7.490590405723196e-07, Train Acc: 1.0\n",
            "Epoch 8947/10000\n",
            "Step 0: Train Loss: 6.232464215827349e-07, Train Acc: 1.0\n",
            "Epoch 8948/10000\n",
            "Step 0: Train Loss: 6.340237632684875e-07, Train Acc: 1.0\n",
            "Epoch 8949/10000\n",
            "Step 0: Train Loss: 8.317290962622792e-07, Train Acc: 1.0\n",
            "Epoch 8950/10000\n",
            "Step 0: Train Loss: 1.1956789194300654e-06, Train Acc: 1.0\n",
            "Epoch 8951/10000\n",
            "Step 0: Train Loss: 1.0505351610845537e-06, Train Acc: 1.0\n",
            "Epoch 8952/10000\n",
            "Step 0: Train Loss: 1.0198348263656953e-06, Train Acc: 1.0\n",
            "Epoch 8953/10000\n",
            "Step 0: Train Loss: 7.784621516293555e-07, Train Acc: 1.0\n",
            "Epoch 8954/10000\n",
            "Step 0: Train Loss: 9.012594546220498e-07, Train Acc: 1.0\n",
            "Epoch 8955/10000\n",
            "Step 0: Train Loss: 8.952437724474294e-07, Train Acc: 1.0\n",
            "Epoch 8956/10000\n",
            "Step 0: Train Loss: 7.50781055103289e-07, Train Acc: 1.0\n",
            "Epoch 8957/10000\n",
            "Step 0: Train Loss: 7.05233446751663e-07, Train Acc: 1.0\n",
            "Epoch 8958/10000\n",
            "Step 0: Train Loss: 6.874480504848179e-07, Train Acc: 1.0\n",
            "Epoch 8959/10000\n",
            "Step 0: Train Loss: 1.2907103155157529e-06, Train Acc: 1.0\n",
            "Epoch 8960/10000\n",
            "Step 0: Train Loss: 8.328726721629209e-07, Train Acc: 1.0\n",
            "Epoch 8961/10000\n",
            "Step 0: Train Loss: 6.908431373631174e-07, Train Acc: 1.0\n",
            "Epoch 8962/10000\n",
            "Step 0: Train Loss: 9.800393172554323e-07, Train Acc: 1.0\n",
            "Epoch 8963/10000\n",
            "Step 0: Train Loss: 1.2391118389132316e-06, Train Acc: 1.0\n",
            "Epoch 8964/10000\n",
            "Step 0: Train Loss: 8.608383836872235e-07, Train Acc: 1.0\n",
            "Epoch 8965/10000\n",
            "Step 0: Train Loss: 1.151368110186013e-06, Train Acc: 1.0\n",
            "Epoch 8966/10000\n",
            "Step 0: Train Loss: 7.201501261988597e-07, Train Acc: 1.0\n",
            "Epoch 8967/10000\n",
            "Step 0: Train Loss: 8.213009436985885e-07, Train Acc: 1.0\n",
            "Epoch 8968/10000\n",
            "Step 0: Train Loss: 7.487973903153033e-07, Train Acc: 1.0\n",
            "Epoch 8969/10000\n",
            "Step 0: Train Loss: 1.1465865554782795e-06, Train Acc: 1.0\n",
            "Epoch 8970/10000\n",
            "Step 0: Train Loss: 7.808279747223423e-07, Train Acc: 1.0\n",
            "Epoch 8971/10000\n",
            "Step 0: Train Loss: 1.038913751472137e-06, Train Acc: 1.0\n",
            "Epoch 8972/10000\n",
            "Step 0: Train Loss: 7.733879101579078e-07, Train Acc: 1.0\n",
            "Epoch 8973/10000\n",
            "Step 0: Train Loss: 1.1181277841387782e-06, Train Acc: 1.0\n",
            "Epoch 8974/10000\n",
            "Step 0: Train Loss: 9.348805178888142e-07, Train Acc: 1.0\n",
            "Epoch 8975/10000\n",
            "Step 0: Train Loss: 1.0878947023229557e-06, Train Acc: 1.0\n",
            "Epoch 8976/10000\n",
            "Step 0: Train Loss: 7.056478921185771e-07, Train Acc: 1.0\n",
            "Epoch 8977/10000\n",
            "Step 0: Train Loss: 8.979961307886697e-07, Train Acc: 1.0\n",
            "Epoch 8978/10000\n",
            "Step 0: Train Loss: 8.232547088482534e-07, Train Acc: 1.0\n",
            "Epoch 8979/10000\n",
            "Step 0: Train Loss: 9.906076456900337e-07, Train Acc: 1.0\n",
            "Epoch 8980/10000\n",
            "Step 0: Train Loss: 9.128041824624233e-07, Train Acc: 1.0\n",
            "Epoch 8981/10000\n",
            "Step 0: Train Loss: 7.848584004932491e-07, Train Acc: 1.0\n",
            "Epoch 8982/10000\n",
            "Step 0: Train Loss: 9.232212505594362e-07, Train Acc: 1.0\n",
            "Epoch 8983/10000\n",
            "Step 0: Train Loss: 7.563994586234912e-07, Train Acc: 1.0\n",
            "Epoch 8984/10000\n",
            "Step 0: Train Loss: 1.1123797776235733e-06, Train Acc: 1.0\n",
            "Epoch 8985/10000\n",
            "Step 0: Train Loss: 1.2315355206737877e-06, Train Acc: 1.0\n",
            "Epoch 8986/10000\n",
            "Step 0: Train Loss: 6.849077749393473e-07, Train Acc: 1.0\n",
            "Epoch 8987/10000\n",
            "Step 0: Train Loss: 6.498682409983303e-07, Train Acc: 1.0\n",
            "Epoch 8988/10000\n",
            "Step 0: Train Loss: 8.417399612881127e-07, Train Acc: 1.0\n",
            "Epoch 8989/10000\n",
            "Step 0: Train Loss: 8.09632865639287e-07, Train Acc: 1.0\n",
            "Epoch 8990/10000\n",
            "Step 0: Train Loss: 6.306502768893552e-07, Train Acc: 1.0\n",
            "Epoch 8991/10000\n",
            "Step 0: Train Loss: 5.92960020640021e-07, Train Acc: 1.0\n",
            "Epoch 8992/10000\n",
            "Step 0: Train Loss: 6.48866603114584e-07, Train Acc: 1.0\n",
            "Epoch 8993/10000\n",
            "Step 0: Train Loss: 8.448605512967333e-07, Train Acc: 1.0\n",
            "Epoch 8994/10000\n",
            "Step 0: Train Loss: 6.054558525647735e-07, Train Acc: 1.0\n",
            "Epoch 8995/10000\n",
            "Step 0: Train Loss: 6.506178920062666e-07, Train Acc: 1.0\n",
            "Epoch 8996/10000\n",
            "Step 0: Train Loss: 8.526079113835294e-07, Train Acc: 1.0\n",
            "Epoch 8997/10000\n",
            "Step 0: Train Loss: 9.931002296070801e-07, Train Acc: 1.0\n",
            "Epoch 8998/10000\n",
            "Step 0: Train Loss: 9.256649491362623e-07, Train Acc: 1.0\n",
            "Epoch 8999/10000\n",
            "Step 0: Train Loss: 7.222838576126378e-07, Train Acc: 1.0\n",
            "Epoch 9000/10000\n",
            "Step 0: Train Loss: 6.694169201182376e-07, Train Acc: 1.0\n",
            "Epoch 9001/10000\n",
            "Step 0: Train Loss: 8.477526876049524e-07, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 9000 1024 0.966495789827582\n",
            "Epoch index and hidden dimension and ratio: 9000 20 0.9215544588246041\n",
            "Epoch index and hidden dimension and ratio: 9000 20 1.8161138216955943\n",
            "Epoch index and hidden dimension and ratio: 9000 20 5.5562348864616835\n",
            "MI(X;T): [10.231177975955784, 7.150975662010165, 5.385248477624451, 2.3076693554337386], MI(Y;T): [2.227982070026677, 3.2579813210049657, 3.072758470267684, 2.7655638362231887]\n",
            "Epoch index and hidden dimension and ratio: 9000 1024 0.9665040866018769\n",
            "Epoch index and hidden dimension and ratio: 9000 20 0.9215746365141111\n",
            "Epoch index and hidden dimension and ratio: 9000 20 1.8161757779123548\n",
            "Epoch index and hidden dimension and ratio: 9000 20 5.556427701422742\n",
            "MI(X;T): [10.231604099095343, 7.1507856924908895, 5.385148123366419, 2.306963819094736], MI(Y;T): [2.2279881488926754, 3.2579813210049657, 3.072771391949693, 2.765719394171568]\n",
            "Epoch index and hidden dimension and ratio: 9000 1024 0.9665078311637268\n",
            "Epoch index and hidden dimension and ratio: 9000 20 0.9215903664333503\n",
            "Epoch index and hidden dimension and ratio: 9000 20 1.8162322748723105\n",
            "Epoch index and hidden dimension and ratio: 9000 20 5.556618542169183\n",
            "MI(X;T): [10.231538130196306, 7.151373678712667, 5.384130544027194, 2.307084356076747], MI(Y;T): [2.22783020796635, 3.258141484430859, 3.0725377520687047, 2.765494190515567]\n",
            "Epoch index and hidden dimension and ratio: 9000 1024 0.966509446464917\n",
            "Epoch index and hidden dimension and ratio: 9000 20 0.9216008892069105\n",
            "Epoch index and hidden dimension and ratio: 9000 20 1.8162904223052536\n",
            "Epoch index and hidden dimension and ratio: 9000 20 5.556843602635675\n",
            "MI(X;T): [10.23127021635655, 7.151298501511452, 5.384344042242422, 2.307750254240733], MI(Y;T): [2.227692512453645, 3.2580232580320287, 3.0723203994846044, 2.7655133252642763]\n",
            "Epoch index and hidden dimension and ratio: 9000 1024 0.9665078311637268\n",
            "Epoch index and hidden dimension and ratio: 9000 20 0.9215968753654494\n",
            "Epoch index and hidden dimension and ratio: 9000 20 1.8162998173053362\n",
            "Epoch index and hidden dimension and ratio: 9000 20 5.556929151935804\n",
            "MI(X;T): [10.23160989300105, 7.150384027747845, 5.384150890812434, 2.309046367648107], MI(Y;T): [2.227908716134774, 3.2581610024071375, 3.0725452878950072, 2.7655131581244277]\n",
            "Epoch index and hidden dimension and ratio: 9000 1024 0.9665077577409454\n",
            "Epoch index and hidden dimension and ratio: 9000 20 0.9215949226858197\n",
            "Epoch index and hidden dimension and ratio: 9000 20 1.816330033656953\n",
            "Epoch index and hidden dimension and ratio: 9000 20 5.557058792029076\n",
            "MI(X;T): [10.231876299634468, 7.150326765396459, 5.384063398928625, 2.308781622350093], MI(Y;T): [2.227781839490416, 3.2581610024071375, 3.072709328044086, 2.765959922362235]\n",
            "Epoch 9002/10000\n",
            "Step 0: Train Loss: 8.018889730010414e-07, Train Acc: 1.0\n",
            "Epoch 9003/10000\n",
            "Step 0: Train Loss: 4.162518223438383e-07, Train Acc: 1.0\n",
            "Epoch 9004/10000\n",
            "Step 0: Train Loss: 8.380977192246064e-07, Train Acc: 1.0\n",
            "Epoch 9005/10000\n",
            "Step 0: Train Loss: 6.604404916288331e-07, Train Acc: 1.0\n",
            "Epoch 9006/10000\n",
            "Step 0: Train Loss: 4.750368134409655e-07, Train Acc: 1.0\n",
            "Epoch 9007/10000\n",
            "Step 0: Train Loss: 7.860438699935912e-07, Train Acc: 1.0\n",
            "Epoch 9008/10000\n",
            "Step 0: Train Loss: 9.927211976901162e-07, Train Acc: 1.0\n",
            "Epoch 9009/10000\n",
            "Step 0: Train Loss: 9.92020545709238e-07, Train Acc: 1.0\n",
            "Epoch 9010/10000\n",
            "Step 0: Train Loss: 1.0844051985259284e-06, Train Acc: 1.0\n",
            "Epoch 9011/10000\n",
            "Step 0: Train Loss: 8.498649322064011e-07, Train Acc: 1.0\n",
            "Epoch 9012/10000\n",
            "Step 0: Train Loss: 6.566164074683911e-07, Train Acc: 1.0\n",
            "Epoch 9013/10000\n",
            "Step 0: Train Loss: 6.44419344553171e-07, Train Acc: 1.0\n",
            "Epoch 9014/10000\n",
            "Step 0: Train Loss: 7.843115099603892e-07, Train Acc: 1.0\n",
            "Epoch 9015/10000\n",
            "Step 0: Train Loss: 5.753115601692116e-07, Train Acc: 1.0\n",
            "Epoch 9016/10000\n",
            "Step 0: Train Loss: 6.594106594093319e-07, Train Acc: 1.0\n",
            "Epoch 9017/10000\n",
            "Step 0: Train Loss: 7.138698379094421e-07, Train Acc: 1.0\n",
            "Epoch 9018/10000\n",
            "Step 0: Train Loss: 4.777380127052311e-07, Train Acc: 1.0\n",
            "Epoch 9019/10000\n",
            "Step 0: Train Loss: 3.866178985845181e-07, Train Acc: 1.0\n",
            "Epoch 9020/10000\n",
            "Step 0: Train Loss: 5.608300739368133e-07, Train Acc: 1.0\n",
            "Epoch 9021/10000\n",
            "Step 0: Train Loss: 9.000638669931504e-07, Train Acc: 1.0\n",
            "Epoch 9022/10000\n",
            "Step 0: Train Loss: 6.486660026894242e-07, Train Acc: 1.0\n",
            "Epoch 9023/10000\n",
            "Step 0: Train Loss: 6.464438229158986e-07, Train Acc: 1.0\n",
            "Epoch 9024/10000\n",
            "Step 0: Train Loss: 5.406733407653519e-07, Train Acc: 1.0\n",
            "Epoch 9025/10000\n",
            "Step 0: Train Loss: 5.422505182650639e-07, Train Acc: 1.0\n",
            "Epoch 9026/10000\n",
            "Step 0: Train Loss: 7.06680339135346e-07, Train Acc: 1.0\n",
            "Epoch 9027/10000\n",
            "Step 0: Train Loss: 6.335107514132687e-07, Train Acc: 1.0\n",
            "Epoch 9028/10000\n",
            "Step 0: Train Loss: 5.678262482433638e-07, Train Acc: 1.0\n",
            "Epoch 9029/10000\n",
            "Step 0: Train Loss: 5.657144015458471e-07, Train Acc: 1.0\n",
            "Epoch 9030/10000\n",
            "Step 0: Train Loss: 8.688276693646912e-07, Train Acc: 1.0\n",
            "Epoch 9031/10000\n",
            "Step 0: Train Loss: 5.075428362033563e-07, Train Acc: 1.0\n",
            "Epoch 9032/10000\n",
            "Step 0: Train Loss: 6.181585376907606e-07, Train Acc: 1.0\n",
            "Epoch 9033/10000\n",
            "Step 0: Train Loss: 8.811733778202324e-07, Train Acc: 1.0\n",
            "Epoch 9034/10000\n",
            "Step 0: Train Loss: 6.57164093809115e-07, Train Acc: 1.0\n",
            "Epoch 9035/10000\n",
            "Step 0: Train Loss: 6.550485522893723e-07, Train Acc: 1.0\n",
            "Epoch 9036/10000\n",
            "Step 0: Train Loss: 7.908950578894292e-07, Train Acc: 1.0\n",
            "Epoch 9037/10000\n",
            "Step 0: Train Loss: 5.346453804122575e-07, Train Acc: 1.0\n",
            "Epoch 9038/10000\n",
            "Step 0: Train Loss: 6.585825076399487e-07, Train Acc: 1.0\n",
            "Epoch 9039/10000\n",
            "Step 0: Train Loss: 5.897380788155715e-07, Train Acc: 1.0\n",
            "Epoch 9040/10000\n",
            "Step 0: Train Loss: 8.297213298646966e-07, Train Acc: 1.0\n",
            "Epoch 9041/10000\n",
            "Step 0: Train Loss: 3.8649389466627326e-07, Train Acc: 1.0\n",
            "Epoch 9042/10000\n",
            "Step 0: Train Loss: 7.195858415798284e-07, Train Acc: 1.0\n",
            "Epoch 9043/10000\n",
            "Step 0: Train Loss: 5.317207865118689e-07, Train Acc: 1.0\n",
            "Epoch 9044/10000\n",
            "Step 0: Train Loss: 7.89855789662397e-07, Train Acc: 1.0\n",
            "Epoch 9045/10000\n",
            "Step 0: Train Loss: 7.559781920463138e-07, Train Acc: 1.0\n",
            "Epoch 9046/10000\n",
            "Step 0: Train Loss: 7.832222763681784e-07, Train Acc: 1.0\n",
            "Epoch 9047/10000\n",
            "Step 0: Train Loss: 5.181360052119999e-07, Train Acc: 1.0\n",
            "Epoch 9048/10000\n",
            "Step 0: Train Loss: 6.871711661915469e-07, Train Acc: 1.0\n",
            "Epoch 9049/10000\n",
            "Step 0: Train Loss: 5.116920078762632e-07, Train Acc: 1.0\n",
            "Epoch 9050/10000\n",
            "Step 0: Train Loss: 5.61801300591469e-07, Train Acc: 1.0\n",
            "Epoch 9051/10000\n",
            "Step 0: Train Loss: 5.765897412857157e-07, Train Acc: 1.0\n",
            "Epoch 9052/10000\n",
            "Step 0: Train Loss: 8.608211032878899e-07, Train Acc: 1.0\n",
            "Epoch 9053/10000\n",
            "Step 0: Train Loss: 6.879442366880539e-07, Train Acc: 1.0\n",
            "Epoch 9054/10000\n",
            "Step 0: Train Loss: 4.7175834083645896e-07, Train Acc: 1.0\n",
            "Epoch 9055/10000\n",
            "Step 0: Train Loss: 4.877837227468262e-07, Train Acc: 1.0\n",
            "Epoch 9056/10000\n",
            "Step 0: Train Loss: 4.288778256977821e-07, Train Acc: 1.0\n",
            "Epoch 9057/10000\n",
            "Step 0: Train Loss: 6.783775461371988e-07, Train Acc: 1.0\n",
            "Epoch 9058/10000\n",
            "Step 0: Train Loss: 7.567155080323573e-07, Train Acc: 1.0\n",
            "Epoch 9059/10000\n",
            "Step 0: Train Loss: 5.562133083003573e-07, Train Acc: 1.0\n",
            "Epoch 9060/10000\n",
            "Step 0: Train Loss: 6.815148481109645e-07, Train Acc: 1.0\n",
            "Epoch 9061/10000\n",
            "Step 0: Train Loss: 8.358681498066289e-07, Train Acc: 1.0\n",
            "Epoch 9062/10000\n",
            "Step 0: Train Loss: 1.0594512787065469e-06, Train Acc: 1.0\n",
            "Epoch 9063/10000\n",
            "Step 0: Train Loss: 6.145602355900337e-07, Train Acc: 1.0\n",
            "Epoch 9064/10000\n",
            "Step 0: Train Loss: 7.145752078940859e-07, Train Acc: 1.0\n",
            "Epoch 9065/10000\n",
            "Step 0: Train Loss: 6.311313427431742e-07, Train Acc: 1.0\n",
            "Epoch 9066/10000\n",
            "Step 0: Train Loss: 8.099524393401225e-07, Train Acc: 1.0\n",
            "Epoch 9067/10000\n",
            "Step 0: Train Loss: 6.510350658572861e-07, Train Acc: 1.0\n",
            "Epoch 9068/10000\n",
            "Step 0: Train Loss: 5.349976390789379e-07, Train Acc: 1.0\n",
            "Epoch 9069/10000\n",
            "Step 0: Train Loss: 6.794406317567336e-07, Train Acc: 1.0\n",
            "Epoch 9070/10000\n",
            "Step 0: Train Loss: 5.327041208147421e-07, Train Acc: 1.0\n",
            "Epoch 9071/10000\n",
            "Step 0: Train Loss: 6.737619742125389e-07, Train Acc: 1.0\n",
            "Epoch 9072/10000\n",
            "Step 0: Train Loss: 4.560949662391067e-07, Train Acc: 1.0\n",
            "Epoch 9073/10000\n",
            "Step 0: Train Loss: 4.00846971615465e-07, Train Acc: 1.0\n",
            "Epoch 9074/10000\n",
            "Step 0: Train Loss: 4.5272381044014764e-07, Train Acc: 1.0\n",
            "Epoch 9075/10000\n",
            "Step 0: Train Loss: 4.938890469929902e-07, Train Acc: 1.0\n",
            "Epoch 9076/10000\n",
            "Step 0: Train Loss: 6.852207548035949e-07, Train Acc: 1.0\n",
            "Epoch 9077/10000\n",
            "Step 0: Train Loss: 4.3197161403440987e-07, Train Acc: 1.0\n",
            "Epoch 9078/10000\n",
            "Step 0: Train Loss: 4.901297643300495e-07, Train Acc: 1.0\n",
            "Epoch 9079/10000\n",
            "Step 0: Train Loss: 4.617806155238213e-07, Train Acc: 1.0\n",
            "Epoch 9080/10000\n",
            "Step 0: Train Loss: 4.837230562770856e-07, Train Acc: 1.0\n",
            "Epoch 9081/10000\n",
            "Step 0: Train Loss: 5.958804649708327e-07, Train Acc: 1.0\n",
            "Epoch 9082/10000\n",
            "Step 0: Train Loss: 4.919565981253982e-07, Train Acc: 1.0\n",
            "Epoch 9083/10000\n",
            "Step 0: Train Loss: 7.538232011938817e-07, Train Acc: 1.0\n",
            "Epoch 9084/10000\n",
            "Step 0: Train Loss: 4.122917403037718e-07, Train Acc: 1.0\n",
            "Epoch 9085/10000\n",
            "Step 0: Train Loss: 4.0805088019624236e-07, Train Acc: 1.0\n",
            "Epoch 9086/10000\n",
            "Step 0: Train Loss: 4.1829423480521655e-07, Train Acc: 1.0\n",
            "Epoch 9087/10000\n",
            "Step 0: Train Loss: 4.476858350699331e-07, Train Acc: 1.0\n",
            "Epoch 9088/10000\n",
            "Step 0: Train Loss: 7.546158826698957e-07, Train Acc: 1.0\n",
            "Epoch 9089/10000\n",
            "Step 0: Train Loss: 5.623715537694807e-07, Train Acc: 1.0\n",
            "Epoch 9090/10000\n",
            "Step 0: Train Loss: 4.2816415657398466e-07, Train Acc: 1.0\n",
            "Epoch 9091/10000\n",
            "Step 0: Train Loss: 4.1965876107497024e-07, Train Acc: 1.0\n",
            "Epoch 9092/10000\n",
            "Step 0: Train Loss: 5.700372298633738e-07, Train Acc: 1.0\n",
            "Epoch 9093/10000\n",
            "Step 0: Train Loss: 5.497967094925116e-07, Train Acc: 1.0\n",
            "Epoch 9094/10000\n",
            "Step 0: Train Loss: 4.616678666025109e-07, Train Acc: 1.0\n",
            "Epoch 9095/10000\n",
            "Step 0: Train Loss: 6.488444341812283e-07, Train Acc: 1.0\n",
            "Epoch 9096/10000\n",
            "Step 0: Train Loss: 6.018140652486181e-07, Train Acc: 1.0\n",
            "Epoch 9097/10000\n",
            "Step 0: Train Loss: 6.522456033053459e-07, Train Acc: 1.0\n",
            "Epoch 9098/10000\n",
            "Step 0: Train Loss: 5.663327442562149e-07, Train Acc: 1.0\n",
            "Epoch 9099/10000\n",
            "Step 0: Train Loss: 5.242229690338718e-07, Train Acc: 1.0\n",
            "Epoch 9100/10000\n",
            "Step 0: Train Loss: 2.6447361278769677e-07, Train Acc: 1.0\n",
            "Epoch 9101/10000\n",
            "Step 0: Train Loss: 4.5397447934192314e-07, Train Acc: 1.0\n",
            "Epoch 9102/10000\n",
            "Step 0: Train Loss: 3.917400590580655e-07, Train Acc: 1.0\n",
            "Epoch 9103/10000\n",
            "Step 0: Train Loss: 5.263307230052305e-07, Train Acc: 1.0\n",
            "Epoch 9104/10000\n",
            "Step 0: Train Loss: 4.3570744878707046e-07, Train Acc: 1.0\n",
            "Epoch 9105/10000\n",
            "Step 0: Train Loss: 4.903396302324836e-07, Train Acc: 1.0\n",
            "Epoch 9106/10000\n",
            "Step 0: Train Loss: 4.6201199666029424e-07, Train Acc: 1.0\n",
            "Epoch 9107/10000\n",
            "Step 0: Train Loss: 7.585379648844537e-07, Train Acc: 1.0\n",
            "Epoch 9108/10000\n",
            "Step 0: Train Loss: 3.614632362314296e-07, Train Acc: 1.0\n",
            "Epoch 9109/10000\n",
            "Step 0: Train Loss: 4.351864220097923e-07, Train Acc: 1.0\n",
            "Epoch 9110/10000\n",
            "Step 0: Train Loss: 4.636933113033592e-07, Train Acc: 1.0\n",
            "Epoch 9111/10000\n",
            "Step 0: Train Loss: 5.134717753207951e-07, Train Acc: 1.0\n",
            "Epoch 9112/10000\n",
            "Step 0: Train Loss: 6.870420179438952e-07, Train Acc: 1.0\n",
            "Epoch 9113/10000\n",
            "Step 0: Train Loss: 3.86088487402958e-07, Train Acc: 1.0\n",
            "Epoch 9114/10000\n",
            "Step 0: Train Loss: 4.4187311232235515e-07, Train Acc: 1.0\n",
            "Epoch 9115/10000\n",
            "Step 0: Train Loss: 4.06908583272525e-07, Train Acc: 1.0\n",
            "Epoch 9116/10000\n",
            "Step 0: Train Loss: 3.0607233725277183e-07, Train Acc: 1.0\n",
            "Epoch 9117/10000\n",
            "Step 0: Train Loss: 6.943253652025305e-07, Train Acc: 1.0\n",
            "Epoch 9118/10000\n",
            "Step 0: Train Loss: 4.855420456806314e-07, Train Acc: 1.0\n",
            "Epoch 9119/10000\n",
            "Step 0: Train Loss: 3.588625929751288e-07, Train Acc: 1.0\n",
            "Epoch 9120/10000\n",
            "Step 0: Train Loss: 3.357568232331687e-07, Train Acc: 1.0\n",
            "Epoch 9121/10000\n",
            "Step 0: Train Loss: 4.7021825366755365e-07, Train Acc: 1.0\n",
            "Epoch 9122/10000\n",
            "Step 0: Train Loss: 3.756748014893674e-07, Train Acc: 1.0\n",
            "Epoch 9123/10000\n",
            "Step 0: Train Loss: 6.304796897893539e-07, Train Acc: 1.0\n",
            "Epoch 9124/10000\n",
            "Step 0: Train Loss: 4.6985348944872385e-07, Train Acc: 1.0\n",
            "Epoch 9125/10000\n",
            "Step 0: Train Loss: 4.2947399947479425e-07, Train Acc: 1.0\n",
            "Epoch 9126/10000\n",
            "Step 0: Train Loss: 6.484043524324079e-07, Train Acc: 1.0\n",
            "Epoch 9127/10000\n",
            "Step 0: Train Loss: 4.274954221727967e-07, Train Acc: 1.0\n",
            "Epoch 9128/10000\n",
            "Step 0: Train Loss: 4.978210768058489e-07, Train Acc: 1.0\n",
            "Epoch 9129/10000\n",
            "Step 0: Train Loss: 4.75170963909477e-07, Train Acc: 1.0\n",
            "Epoch 9130/10000\n",
            "Step 0: Train Loss: 6.955769435990078e-07, Train Acc: 1.0\n",
            "Epoch 9131/10000\n",
            "Step 0: Train Loss: 5.021905735702603e-07, Train Acc: 1.0\n",
            "Epoch 9132/10000\n",
            "Step 0: Train Loss: 4.0029209458225523e-07, Train Acc: 1.0\n",
            "Epoch 9133/10000\n",
            "Step 0: Train Loss: 6.77258924497437e-07, Train Acc: 1.0\n",
            "Epoch 9134/10000\n",
            "Step 0: Train Loss: 5.396464644036314e-07, Train Acc: 1.0\n",
            "Epoch 9135/10000\n",
            "Step 0: Train Loss: 4.294302868856903e-07, Train Acc: 1.0\n",
            "Epoch 9136/10000\n",
            "Step 0: Train Loss: 3.551287193204189e-07, Train Acc: 1.0\n",
            "Epoch 9137/10000\n",
            "Step 0: Train Loss: 6.12068163263757e-07, Train Acc: 1.0\n",
            "Epoch 9138/10000\n",
            "Step 0: Train Loss: 3.6398412817106873e-07, Train Acc: 1.0\n",
            "Epoch 9139/10000\n",
            "Step 0: Train Loss: 4.111645637294714e-07, Train Acc: 1.0\n",
            "Epoch 9140/10000\n",
            "Step 0: Train Loss: 4.6206034198803536e-07, Train Acc: 1.0\n",
            "Epoch 9141/10000\n",
            "Step 0: Train Loss: 5.517708814295474e-07, Train Acc: 1.0\n",
            "Epoch 9142/10000\n",
            "Step 0: Train Loss: 5.395777407102287e-07, Train Acc: 1.0\n",
            "Epoch 9143/10000\n",
            "Step 0: Train Loss: 3.642862225206045e-07, Train Acc: 1.0\n",
            "Epoch 9144/10000\n",
            "Step 0: Train Loss: 3.096006366831716e-07, Train Acc: 1.0\n",
            "Epoch 9145/10000\n",
            "Step 0: Train Loss: 4.767138648276159e-07, Train Acc: 1.0\n",
            "Epoch 9146/10000\n",
            "Step 0: Train Loss: 3.8794524925833684e-07, Train Acc: 1.0\n",
            "Epoch 9147/10000\n",
            "Step 0: Train Loss: 4.013327554730495e-07, Train Acc: 1.0\n",
            "Epoch 9148/10000\n",
            "Step 0: Train Loss: 5.045365583100647e-07, Train Acc: 1.0\n",
            "Epoch 9149/10000\n",
            "Step 0: Train Loss: 4.142920033700648e-07, Train Acc: 1.0\n",
            "Epoch 9150/10000\n",
            "Step 0: Train Loss: 3.839871567379305e-07, Train Acc: 1.0\n",
            "Epoch 9151/10000\n",
            "Step 0: Train Loss: 5.160860041542037e-07, Train Acc: 1.0\n",
            "Epoch 9152/10000\n",
            "Step 0: Train Loss: 3.408900397516845e-07, Train Acc: 1.0\n",
            "Epoch 9153/10000\n",
            "Step 0: Train Loss: 3.157984735935315e-07, Train Acc: 1.0\n",
            "Epoch 9154/10000\n",
            "Step 0: Train Loss: 5.312248845257272e-07, Train Acc: 1.0\n",
            "Epoch 9155/10000\n",
            "Step 0: Train Loss: 3.9460729794882354e-07, Train Acc: 1.0\n",
            "Epoch 9156/10000\n",
            "Step 0: Train Loss: 3.213123704881582e-07, Train Acc: 1.0\n",
            "Epoch 9157/10000\n",
            "Step 0: Train Loss: 3.480538453004556e-07, Train Acc: 1.0\n",
            "Epoch 9158/10000\n",
            "Step 0: Train Loss: 3.951331564167049e-07, Train Acc: 1.0\n",
            "Epoch 9159/10000\n",
            "Step 0: Train Loss: 3.3895426554408914e-07, Train Acc: 1.0\n",
            "Epoch 9160/10000\n",
            "Step 0: Train Loss: 3.7607205172207614e-07, Train Acc: 1.0\n",
            "Epoch 9161/10000\n",
            "Step 0: Train Loss: 2.9759772246507055e-07, Train Acc: 1.0\n",
            "Epoch 9162/10000\n",
            "Step 0: Train Loss: 4.369387909264333e-07, Train Acc: 1.0\n",
            "Epoch 9163/10000\n",
            "Step 0: Train Loss: 4.5742373799839697e-07, Train Acc: 1.0\n",
            "Epoch 9164/10000\n",
            "Step 0: Train Loss: 4.954118253408524e-07, Train Acc: 1.0\n",
            "Epoch 9165/10000\n",
            "Step 0: Train Loss: 4.41347992818919e-07, Train Acc: 1.0\n",
            "Epoch 9166/10000\n",
            "Step 0: Train Loss: 2.849352540579275e-07, Train Acc: 1.0\n",
            "Epoch 9167/10000\n",
            "Step 0: Train Loss: 4.7281250203923264e-07, Train Acc: 1.0\n",
            "Epoch 9168/10000\n",
            "Step 0: Train Loss: 3.2891261980694253e-07, Train Acc: 1.0\n",
            "Epoch 9169/10000\n",
            "Step 0: Train Loss: 5.175456294637115e-07, Train Acc: 1.0\n",
            "Epoch 9170/10000\n",
            "Step 0: Train Loss: 2.2666128529635898e-07, Train Acc: 1.0\n",
            "Epoch 9171/10000\n",
            "Step 0: Train Loss: 3.618934840687871e-07, Train Acc: 1.0\n",
            "Epoch 9172/10000\n",
            "Step 0: Train Loss: 3.4722373243312177e-07, Train Acc: 1.0\n",
            "Epoch 9173/10000\n",
            "Step 0: Train Loss: 3.172221454406099e-07, Train Acc: 1.0\n",
            "Epoch 9174/10000\n",
            "Step 0: Train Loss: 3.6305644357526035e-07, Train Acc: 1.0\n",
            "Epoch 9175/10000\n",
            "Step 0: Train Loss: 3.164838346947363e-07, Train Acc: 1.0\n",
            "Epoch 9176/10000\n",
            "Step 0: Train Loss: 5.37632502073393e-07, Train Acc: 1.0\n",
            "Epoch 9177/10000\n",
            "Step 0: Train Loss: 4.7172653694360633e-07, Train Acc: 1.0\n",
            "Epoch 9178/10000\n",
            "Step 0: Train Loss: 3.2680671324669675e-07, Train Acc: 1.0\n",
            "Epoch 9179/10000\n",
            "Step 0: Train Loss: 5.954545372333087e-07, Train Acc: 1.0\n",
            "Epoch 9180/10000\n",
            "Step 0: Train Loss: 4.48025247123951e-07, Train Acc: 1.0\n",
            "Epoch 9181/10000\n",
            "Step 0: Train Loss: 3.3842067637124273e-07, Train Acc: 1.0\n",
            "Epoch 9182/10000\n",
            "Step 0: Train Loss: 3.193083557562204e-07, Train Acc: 1.0\n",
            "Epoch 9183/10000\n",
            "Step 0: Train Loss: 5.564318144024583e-07, Train Acc: 1.0\n",
            "Epoch 9184/10000\n",
            "Step 0: Train Loss: 2.85713838366064e-07, Train Acc: 1.0\n",
            "Epoch 9185/10000\n",
            "Step 0: Train Loss: 3.1664504263062554e-07, Train Acc: 1.0\n",
            "Epoch 9186/10000\n",
            "Step 0: Train Loss: 3.3390708154001913e-07, Train Acc: 1.0\n",
            "Epoch 9187/10000\n",
            "Step 0: Train Loss: 3.230910294860223e-07, Train Acc: 1.0\n",
            "Epoch 9188/10000\n",
            "Step 0: Train Loss: 4.469097518722265e-07, Train Acc: 1.0\n",
            "Epoch 9189/10000\n",
            "Step 0: Train Loss: 4.3631806079247326e-07, Train Acc: 1.0\n",
            "Epoch 9190/10000\n",
            "Step 0: Train Loss: 3.736301437129441e-07, Train Acc: 1.0\n",
            "Epoch 9191/10000\n",
            "Step 0: Train Loss: 4.13548661981622e-07, Train Acc: 1.0\n",
            "Epoch 9192/10000\n",
            "Step 0: Train Loss: 3.467140174961969e-07, Train Acc: 1.0\n",
            "Epoch 9193/10000\n",
            "Step 0: Train Loss: 3.133523307496944e-07, Train Acc: 1.0\n",
            "Epoch 9194/10000\n",
            "Step 0: Train Loss: 3.4415802474541124e-07, Train Acc: 1.0\n",
            "Epoch 9195/10000\n",
            "Step 0: Train Loss: 3.498868181850412e-07, Train Acc: 1.0\n",
            "Epoch 9196/10000\n",
            "Step 0: Train Loss: 3.029464323844877e-07, Train Acc: 1.0\n",
            "Epoch 9197/10000\n",
            "Step 0: Train Loss: 4.773450541506463e-07, Train Acc: 1.0\n",
            "Epoch 9198/10000\n",
            "Step 0: Train Loss: 3.347930430663837e-07, Train Acc: 1.0\n",
            "Epoch 9199/10000\n",
            "Step 0: Train Loss: 3.803348249675764e-07, Train Acc: 1.0\n",
            "Epoch 9200/10000\n",
            "Step 0: Train Loss: 3.2799576388242713e-07, Train Acc: 1.0\n",
            "Epoch 9201/10000\n",
            "Step 0: Train Loss: 4.5604198817272845e-07, Train Acc: 1.0\n",
            "Epoch 9202/10000\n",
            "Step 0: Train Loss: 3.385657407761755e-07, Train Acc: 1.0\n",
            "Epoch 9203/10000\n",
            "Step 0: Train Loss: 3.8354625075953663e-07, Train Acc: 1.0\n",
            "Epoch 9204/10000\n",
            "Step 0: Train Loss: 3.748612584786315e-07, Train Acc: 1.0\n",
            "Epoch 9205/10000\n",
            "Step 0: Train Loss: 3.26793326621555e-07, Train Acc: 1.0\n",
            "Epoch 9206/10000\n",
            "Step 0: Train Loss: 4.1248117099712545e-07, Train Acc: 1.0\n",
            "Epoch 9207/10000\n",
            "Step 0: Train Loss: 3.7818790588062257e-07, Train Acc: 1.0\n",
            "Epoch 9208/10000\n",
            "Step 0: Train Loss: 2.988460323649633e-07, Train Acc: 1.0\n",
            "Epoch 9209/10000\n",
            "Step 0: Train Loss: 3.718598122759431e-07, Train Acc: 1.0\n",
            "Epoch 9210/10000\n",
            "Step 0: Train Loss: 4.2942733102790953e-07, Train Acc: 1.0\n",
            "Epoch 9211/10000\n",
            "Step 0: Train Loss: 4.861554430135584e-07, Train Acc: 1.0\n",
            "Epoch 9212/10000\n",
            "Step 0: Train Loss: 3.351193811340636e-07, Train Acc: 1.0\n",
            "Epoch 9213/10000\n",
            "Step 0: Train Loss: 2.9131166456863866e-07, Train Acc: 1.0\n",
            "Epoch 9214/10000\n",
            "Step 0: Train Loss: 4.2997518789889e-07, Train Acc: 1.0\n",
            "Epoch 9215/10000\n",
            "Step 0: Train Loss: 4.0457408090333047e-07, Train Acc: 1.0\n",
            "Epoch 9216/10000\n",
            "Step 0: Train Loss: 3.2295685059580137e-07, Train Acc: 1.0\n",
            "Epoch 9217/10000\n",
            "Step 0: Train Loss: 3.4454515684956277e-07, Train Acc: 1.0\n",
            "Epoch 9218/10000\n",
            "Step 0: Train Loss: 3.546537641341274e-07, Train Acc: 1.0\n",
            "Epoch 9219/10000\n",
            "Step 0: Train Loss: 3.023400267920806e-07, Train Acc: 1.0\n",
            "Epoch 9220/10000\n",
            "Step 0: Train Loss: 3.394832504000078e-07, Train Acc: 1.0\n",
            "Epoch 9221/10000\n",
            "Step 0: Train Loss: 3.559499646144104e-07, Train Acc: 1.0\n",
            "Epoch 9222/10000\n",
            "Step 0: Train Loss: 3.7290848808879673e-07, Train Acc: 1.0\n",
            "Epoch 9223/10000\n",
            "Step 0: Train Loss: 2.5163942041217524e-07, Train Acc: 1.0\n",
            "Epoch 9224/10000\n",
            "Step 0: Train Loss: 2.8422979880815546e-07, Train Acc: 1.0\n",
            "Epoch 9225/10000\n",
            "Step 0: Train Loss: 2.9080962349326e-07, Train Acc: 1.0\n",
            "Epoch 9226/10000\n",
            "Step 0: Train Loss: 2.793133546674653e-07, Train Acc: 1.0\n",
            "Epoch 9227/10000\n",
            "Step 0: Train Loss: 3.2566393315391906e-07, Train Acc: 1.0\n",
            "Epoch 9228/10000\n",
            "Step 0: Train Loss: 4.3336819999240106e-07, Train Acc: 1.0\n",
            "Epoch 9229/10000\n",
            "Step 0: Train Loss: 2.956480784632731e-07, Train Acc: 1.0\n",
            "Epoch 9230/10000\n",
            "Step 0: Train Loss: 2.579626823262515e-07, Train Acc: 1.0\n",
            "Epoch 9231/10000\n",
            "Step 0: Train Loss: 3.896730049746111e-07, Train Acc: 1.0\n",
            "Epoch 9232/10000\n",
            "Step 0: Train Loss: 3.2756588552729227e-07, Train Acc: 1.0\n",
            "Epoch 9233/10000\n",
            "Step 0: Train Loss: 3.8247668499025167e-07, Train Acc: 1.0\n",
            "Epoch 9234/10000\n",
            "Step 0: Train Loss: 3.40088604389166e-07, Train Acc: 1.0\n",
            "Epoch 9235/10000\n",
            "Step 0: Train Loss: 3.254635316807253e-07, Train Acc: 1.0\n",
            "Epoch 9236/10000\n",
            "Step 0: Train Loss: 2.60533852269873e-07, Train Acc: 1.0\n",
            "Epoch 9237/10000\n",
            "Step 0: Train Loss: 3.811119029251131e-07, Train Acc: 1.0\n",
            "Epoch 9238/10000\n",
            "Step 0: Train Loss: 4.3465783505780564e-07, Train Acc: 1.0\n",
            "Epoch 9239/10000\n",
            "Step 0: Train Loss: 3.491324491733394e-07, Train Acc: 1.0\n",
            "Epoch 9240/10000\n",
            "Step 0: Train Loss: 3.1694270319349016e-07, Train Acc: 1.0\n",
            "Epoch 9241/10000\n",
            "Step 0: Train Loss: 3.098909644450032e-07, Train Acc: 1.0\n",
            "Epoch 9242/10000\n",
            "Step 0: Train Loss: 3.5050149449489254e-07, Train Acc: 1.0\n",
            "Epoch 9243/10000\n",
            "Step 0: Train Loss: 4.241782107783365e-07, Train Acc: 1.0\n",
            "Epoch 9244/10000\n",
            "Step 0: Train Loss: 2.796723777009902e-07, Train Acc: 1.0\n",
            "Epoch 9245/10000\n",
            "Step 0: Train Loss: 3.7994669810359483e-07, Train Acc: 1.0\n",
            "Epoch 9246/10000\n",
            "Step 0: Train Loss: 3.65803145996324e-07, Train Acc: 1.0\n",
            "Epoch 9247/10000\n",
            "Step 0: Train Loss: 3.8076120745245134e-07, Train Acc: 1.0\n",
            "Epoch 9248/10000\n",
            "Step 0: Train Loss: 3.791491849369777e-07, Train Acc: 1.0\n",
            "Epoch 9249/10000\n",
            "Step 0: Train Loss: 2.642293566168519e-07, Train Acc: 1.0\n",
            "Epoch 9250/10000\n",
            "Step 0: Train Loss: 2.756263768333156e-07, Train Acc: 1.0\n",
            "Epoch 9251/10000\n",
            "Step 0: Train Loss: 2.818236453094869e-07, Train Acc: 1.0\n",
            "Epoch 9252/10000\n",
            "Step 0: Train Loss: 3.6370883549352584e-07, Train Acc: 1.0\n",
            "Epoch 9253/10000\n",
            "Step 0: Train Loss: 3.6451191931519134e-07, Train Acc: 1.0\n",
            "Epoch 9254/10000\n",
            "Step 0: Train Loss: 2.9068843332424876e-07, Train Acc: 1.0\n",
            "Epoch 9255/10000\n",
            "Step 0: Train Loss: 2.7849915795741254e-07, Train Acc: 1.0\n",
            "Epoch 9256/10000\n",
            "Step 0: Train Loss: 2.208788885127433e-07, Train Acc: 1.0\n",
            "Epoch 9257/10000\n",
            "Step 0: Train Loss: 3.352752457885799e-07, Train Acc: 1.0\n",
            "Epoch 9258/10000\n",
            "Step 0: Train Loss: 2.4141337462424417e-07, Train Acc: 1.0\n",
            "Epoch 9259/10000\n",
            "Step 0: Train Loss: 3.263099017658533e-07, Train Acc: 1.0\n",
            "Epoch 9260/10000\n",
            "Step 0: Train Loss: 2.4716882762731984e-07, Train Acc: 1.0\n",
            "Epoch 9261/10000\n",
            "Step 0: Train Loss: 3.977676499289373e-07, Train Acc: 1.0\n",
            "Epoch 9262/10000\n",
            "Step 0: Train Loss: 2.3558423833947018e-07, Train Acc: 1.0\n",
            "Epoch 9263/10000\n",
            "Step 0: Train Loss: 2.6861775381803454e-07, Train Acc: 1.0\n",
            "Epoch 9264/10000\n",
            "Step 0: Train Loss: 3.6323086760603474e-07, Train Acc: 1.0\n",
            "Epoch 9265/10000\n",
            "Step 0: Train Loss: 2.4930568542913534e-07, Train Acc: 1.0\n",
            "Epoch 9266/10000\n",
            "Step 0: Train Loss: 3.0872689649186214e-07, Train Acc: 1.0\n",
            "Epoch 9267/10000\n",
            "Step 0: Train Loss: 2.1419799622890423e-07, Train Acc: 1.0\n",
            "Epoch 9268/10000\n",
            "Step 0: Train Loss: 1.6522105283911515e-07, Train Acc: 1.0\n",
            "Epoch 9269/10000\n",
            "Step 0: Train Loss: 2.444864719564066e-07, Train Acc: 1.0\n",
            "Epoch 9270/10000\n",
            "Step 0: Train Loss: 2.975813231387292e-07, Train Acc: 1.0\n",
            "Epoch 9271/10000\n",
            "Step 0: Train Loss: 3.3230270446438226e-07, Train Acc: 1.0\n",
            "Epoch 9272/10000\n",
            "Step 0: Train Loss: 3.173311426962755e-07, Train Acc: 1.0\n",
            "Epoch 9273/10000\n",
            "Step 0: Train Loss: 2.2508416464006586e-07, Train Acc: 1.0\n",
            "Epoch 9274/10000\n",
            "Step 0: Train Loss: 4.650426888019865e-07, Train Acc: 1.0\n",
            "Epoch 9275/10000\n",
            "Step 0: Train Loss: 2.7590976969804615e-07, Train Acc: 1.0\n",
            "Epoch 9276/10000\n",
            "Step 0: Train Loss: 2.4830171696521575e-07, Train Acc: 1.0\n",
            "Epoch 9277/10000\n",
            "Step 0: Train Loss: 1.870833870043498e-07, Train Acc: 1.0\n",
            "Epoch 9278/10000\n",
            "Step 0: Train Loss: 2.138437764642731e-07, Train Acc: 1.0\n",
            "Epoch 9279/10000\n",
            "Step 0: Train Loss: 2.515810706427146e-07, Train Acc: 1.0\n",
            "Epoch 9280/10000\n",
            "Step 0: Train Loss: 2.245020880309312e-07, Train Acc: 1.0\n",
            "Epoch 9281/10000\n",
            "Step 0: Train Loss: 3.1679564926889725e-07, Train Acc: 1.0\n",
            "Epoch 9282/10000\n",
            "Step 0: Train Loss: 3.765460689919564e-07, Train Acc: 1.0\n",
            "Epoch 9283/10000\n",
            "Step 0: Train Loss: 2.7630031240732933e-07, Train Acc: 1.0\n",
            "Epoch 9284/10000\n",
            "Step 0: Train Loss: 2.590678036540339e-07, Train Acc: 1.0\n",
            "Epoch 9285/10000\n",
            "Step 0: Train Loss: 2.1079385703615117e-07, Train Acc: 1.0\n",
            "Epoch 9286/10000\n",
            "Step 0: Train Loss: 2.438606259147491e-07, Train Acc: 1.0\n",
            "Epoch 9287/10000\n",
            "Step 0: Train Loss: 3.6980878803660744e-07, Train Acc: 1.0\n",
            "Epoch 9288/10000\n",
            "Step 0: Train Loss: 2.2350901929257816e-07, Train Acc: 1.0\n",
            "Epoch 9289/10000\n",
            "Step 0: Train Loss: 2.178382629836051e-07, Train Acc: 1.0\n",
            "Epoch 9290/10000\n",
            "Step 0: Train Loss: 3.253056775065488e-07, Train Acc: 1.0\n",
            "Epoch 9291/10000\n",
            "Step 0: Train Loss: 2.782897183806199e-07, Train Acc: 1.0\n",
            "Epoch 9292/10000\n",
            "Step 0: Train Loss: 1.9377024784716923e-07, Train Acc: 1.0\n",
            "Epoch 9293/10000\n",
            "Step 0: Train Loss: 3.1527565624855924e-07, Train Acc: 1.0\n",
            "Epoch 9294/10000\n",
            "Step 0: Train Loss: 2.325585057860735e-07, Train Acc: 1.0\n",
            "Epoch 9295/10000\n",
            "Step 0: Train Loss: 3.953151974656066e-07, Train Acc: 1.0\n",
            "Epoch 9296/10000\n",
            "Step 0: Train Loss: 2.3499042356434074e-07, Train Acc: 1.0\n",
            "Epoch 9297/10000\n",
            "Step 0: Train Loss: 2.424160925329488e-07, Train Acc: 1.0\n",
            "Epoch 9298/10000\n",
            "Step 0: Train Loss: 1.7075304015179427e-07, Train Acc: 1.0\n",
            "Epoch 9299/10000\n",
            "Step 0: Train Loss: 3.247821780405502e-07, Train Acc: 1.0\n",
            "Epoch 9300/10000\n",
            "Step 0: Train Loss: 2.6430092248119763e-07, Train Acc: 1.0\n",
            "Epoch 9301/10000\n",
            "Step 0: Train Loss: 3.2661534987710183e-07, Train Acc: 1.0\n",
            "Epoch 9302/10000\n",
            "Step 0: Train Loss: 2.4017631972128584e-07, Train Acc: 1.0\n",
            "Epoch 9303/10000\n",
            "Step 0: Train Loss: 2.423468004053575e-07, Train Acc: 1.0\n",
            "Epoch 9304/10000\n",
            "Step 0: Train Loss: 2.732655559611885e-07, Train Acc: 1.0\n",
            "Epoch 9305/10000\n",
            "Step 0: Train Loss: 2.3598049381234887e-07, Train Acc: 1.0\n",
            "Epoch 9306/10000\n",
            "Step 0: Train Loss: 2.826577656378504e-07, Train Acc: 1.0\n",
            "Epoch 9307/10000\n",
            "Step 0: Train Loss: 3.161708548304887e-07, Train Acc: 1.0\n",
            "Epoch 9308/10000\n",
            "Step 0: Train Loss: 2.2356947226853663e-07, Train Acc: 1.0\n",
            "Epoch 9309/10000\n",
            "Step 0: Train Loss: 1.9511774951297411e-07, Train Acc: 1.0\n",
            "Epoch 9310/10000\n",
            "Step 0: Train Loss: 2.2816010414317134e-07, Train Acc: 1.0\n",
            "Epoch 9311/10000\n",
            "Step 0: Train Loss: 2.924141995208629e-07, Train Acc: 1.0\n",
            "Epoch 9312/10000\n",
            "Step 0: Train Loss: 2.0093533237286465e-07, Train Acc: 1.0\n",
            "Epoch 9313/10000\n",
            "Step 0: Train Loss: 2.2555008172275848e-07, Train Acc: 1.0\n",
            "Epoch 9314/10000\n",
            "Step 0: Train Loss: 2.542726349474833e-07, Train Acc: 1.0\n",
            "Epoch 9315/10000\n",
            "Step 0: Train Loss: 1.8952702873775706e-07, Train Acc: 1.0\n",
            "Epoch 9316/10000\n",
            "Step 0: Train Loss: 3.2087379508993763e-07, Train Acc: 1.0\n",
            "Epoch 9317/10000\n",
            "Step 0: Train Loss: 2.8626868697756436e-07, Train Acc: 1.0\n",
            "Epoch 9318/10000\n",
            "Step 0: Train Loss: 2.3179278230145428e-07, Train Acc: 1.0\n",
            "Epoch 9319/10000\n",
            "Step 0: Train Loss: 2.4850504587448086e-07, Train Acc: 1.0\n",
            "Epoch 9320/10000\n",
            "Step 0: Train Loss: 2.281465043552089e-07, Train Acc: 1.0\n",
            "Epoch 9321/10000\n",
            "Step 0: Train Loss: 2.405575116881664e-07, Train Acc: 1.0\n",
            "Epoch 9322/10000\n",
            "Step 0: Train Loss: 3.3551083333804854e-07, Train Acc: 1.0\n",
            "Epoch 9323/10000\n",
            "Step 0: Train Loss: 2.078356260426517e-07, Train Acc: 1.0\n",
            "Epoch 9324/10000\n",
            "Step 0: Train Loss: 2.3689777606250573e-07, Train Acc: 1.0\n",
            "Epoch 9325/10000\n",
            "Step 0: Train Loss: 2.0313846960107185e-07, Train Acc: 1.0\n",
            "Epoch 9326/10000\n",
            "Step 0: Train Loss: 2.433568511150952e-07, Train Acc: 1.0\n",
            "Epoch 9327/10000\n",
            "Step 0: Train Loss: 3.060979167912592e-07, Train Acc: 1.0\n",
            "Epoch 9328/10000\n",
            "Step 0: Train Loss: 2.038972013451712e-07, Train Acc: 1.0\n",
            "Epoch 9329/10000\n",
            "Step 0: Train Loss: 1.9235149295582232e-07, Train Acc: 1.0\n",
            "Epoch 9330/10000\n",
            "Step 0: Train Loss: 2.825262583883159e-07, Train Acc: 1.0\n",
            "Epoch 9331/10000\n",
            "Step 0: Train Loss: 1.8846647265036154e-07, Train Acc: 1.0\n",
            "Epoch 9332/10000\n",
            "Step 0: Train Loss: 1.8513786415041977e-07, Train Acc: 1.0\n",
            "Epoch 9333/10000\n",
            "Step 0: Train Loss: 1.9226740732847247e-07, Train Acc: 1.0\n",
            "Epoch 9334/10000\n",
            "Step 0: Train Loss: 2.2181644965257874e-07, Train Acc: 1.0\n",
            "Epoch 9335/10000\n",
            "Step 0: Train Loss: 2.91540430907844e-07, Train Acc: 1.0\n",
            "Epoch 9336/10000\n",
            "Step 0: Train Loss: 1.8474709406746115e-07, Train Acc: 1.0\n",
            "Epoch 9337/10000\n",
            "Step 0: Train Loss: 2.0744171536080103e-07, Train Acc: 1.0\n",
            "Epoch 9338/10000\n",
            "Step 0: Train Loss: 1.8497097187264444e-07, Train Acc: 1.0\n",
            "Epoch 9339/10000\n",
            "Step 0: Train Loss: 1.7852242706339894e-07, Train Acc: 1.0\n",
            "Epoch 9340/10000\n",
            "Step 0: Train Loss: 1.6577978101395274e-07, Train Acc: 1.0\n",
            "Epoch 9341/10000\n",
            "Step 0: Train Loss: 1.8063198581330653e-07, Train Acc: 1.0\n",
            "Epoch 9342/10000\n",
            "Step 0: Train Loss: 2.1019471319050353e-07, Train Acc: 1.0\n",
            "Epoch 9343/10000\n",
            "Step 0: Train Loss: 2.7715307737707917e-07, Train Acc: 1.0\n",
            "Epoch 9344/10000\n",
            "Step 0: Train Loss: 2.2609653171912214e-07, Train Acc: 1.0\n",
            "Epoch 9345/10000\n",
            "Step 0: Train Loss: 3.1527602573078184e-07, Train Acc: 1.0\n",
            "Epoch 9346/10000\n",
            "Step 0: Train Loss: 2.6443450451552053e-07, Train Acc: 1.0\n",
            "Epoch 9347/10000\n",
            "Step 0: Train Loss: 1.6798607305190671e-07, Train Acc: 1.0\n",
            "Epoch 9348/10000\n",
            "Step 0: Train Loss: 1.9167217146787152e-07, Train Acc: 1.0\n",
            "Epoch 9349/10000\n",
            "Step 0: Train Loss: 2.1904064340105833e-07, Train Acc: 1.0\n",
            "Epoch 9350/10000\n",
            "Step 0: Train Loss: 2.2700319846080674e-07, Train Acc: 1.0\n",
            "Epoch 9351/10000\n",
            "Step 0: Train Loss: 2.522871227483847e-07, Train Acc: 1.0\n",
            "Epoch 9352/10000\n",
            "Step 0: Train Loss: 1.3812602617235825e-07, Train Acc: 1.0\n",
            "Epoch 9353/10000\n",
            "Step 0: Train Loss: 1.497727737387322e-07, Train Acc: 1.0\n",
            "Epoch 9354/10000\n",
            "Step 0: Train Loss: 3.0440756404459535e-07, Train Acc: 1.0\n",
            "Epoch 9355/10000\n",
            "Step 0: Train Loss: 1.8206536367415538e-07, Train Acc: 1.0\n",
            "Epoch 9356/10000\n",
            "Step 0: Train Loss: 1.7742560487477022e-07, Train Acc: 1.0\n",
            "Epoch 9357/10000\n",
            "Step 0: Train Loss: 1.8326878148400283e-07, Train Acc: 1.0\n",
            "Epoch 9358/10000\n",
            "Step 0: Train Loss: 1.7508834559976094e-07, Train Acc: 1.0\n",
            "Epoch 9359/10000\n",
            "Step 0: Train Loss: 2.1375792869093857e-07, Train Acc: 1.0\n",
            "Epoch 9360/10000\n",
            "Step 0: Train Loss: 1.558042015403771e-07, Train Acc: 1.0\n",
            "Epoch 9361/10000\n",
            "Step 0: Train Loss: 1.7532796903196868e-07, Train Acc: 1.0\n",
            "Epoch 9362/10000\n",
            "Step 0: Train Loss: 1.9337360868121323e-07, Train Acc: 1.0\n",
            "Epoch 9363/10000\n",
            "Step 0: Train Loss: 2.4080864591269346e-07, Train Acc: 1.0\n",
            "Epoch 9364/10000\n",
            "Step 0: Train Loss: 1.4363141076501051e-07, Train Acc: 1.0\n",
            "Epoch 9365/10000\n",
            "Step 0: Train Loss: 1.5126157393297035e-07, Train Acc: 1.0\n",
            "Epoch 9366/10000\n",
            "Step 0: Train Loss: 1.489974295054708e-07, Train Acc: 1.0\n",
            "Epoch 9367/10000\n",
            "Step 0: Train Loss: 2.582596323463804e-07, Train Acc: 1.0\n",
            "Epoch 9368/10000\n",
            "Step 0: Train Loss: 1.9810740070624888e-07, Train Acc: 1.0\n",
            "Epoch 9369/10000\n",
            "Step 0: Train Loss: 1.7447997890940314e-07, Train Acc: 1.0\n",
            "Epoch 9370/10000\n",
            "Step 0: Train Loss: 2.0968364822238073e-07, Train Acc: 1.0\n",
            "Epoch 9371/10000\n",
            "Step 0: Train Loss: 2.4832237954797165e-07, Train Acc: 1.0\n",
            "Epoch 9372/10000\n",
            "Step 0: Train Loss: 2.500122775472846e-07, Train Acc: 1.0\n",
            "Epoch 9373/10000\n",
            "Step 0: Train Loss: 2.0280522505800036e-07, Train Acc: 1.0\n",
            "Epoch 9374/10000\n",
            "Step 0: Train Loss: 1.6165635940978973e-07, Train Acc: 1.0\n",
            "Epoch 9375/10000\n",
            "Step 0: Train Loss: 1.7928633155861462e-07, Train Acc: 1.0\n",
            "Epoch 9376/10000\n",
            "Step 0: Train Loss: 1.880719509017581e-07, Train Acc: 1.0\n",
            "Epoch 9377/10000\n",
            "Step 0: Train Loss: 3.033515554307087e-07, Train Acc: 1.0\n",
            "Epoch 9378/10000\n",
            "Step 0: Train Loss: 1.957708803956848e-07, Train Acc: 1.0\n",
            "Epoch 9379/10000\n",
            "Step 0: Train Loss: 2.0650038834446605e-07, Train Acc: 1.0\n",
            "Epoch 9380/10000\n",
            "Step 0: Train Loss: 2.1387850779319706e-07, Train Acc: 1.0\n",
            "Epoch 9381/10000\n",
            "Step 0: Train Loss: 1.672579372780092e-07, Train Acc: 1.0\n",
            "Epoch 9382/10000\n",
            "Step 0: Train Loss: 1.9528377492861182e-07, Train Acc: 1.0\n",
            "Epoch 9383/10000\n",
            "Step 0: Train Loss: 2.3543772442735644e-07, Train Acc: 1.0\n",
            "Epoch 9384/10000\n",
            "Step 0: Train Loss: 1.506185611788169e-07, Train Acc: 1.0\n",
            "Epoch 9385/10000\n",
            "Step 0: Train Loss: 1.497475210499033e-07, Train Acc: 1.0\n",
            "Epoch 9386/10000\n",
            "Step 0: Train Loss: 1.5389706220503285e-07, Train Acc: 1.0\n",
            "Epoch 9387/10000\n",
            "Step 0: Train Loss: 1.6559066295940283e-07, Train Acc: 1.0\n",
            "Epoch 9388/10000\n",
            "Step 0: Train Loss: 1.5800755193140503e-07, Train Acc: 1.0\n",
            "Epoch 9389/10000\n",
            "Step 0: Train Loss: 2.005502324209374e-07, Train Acc: 1.0\n",
            "Epoch 9390/10000\n",
            "Step 0: Train Loss: 1.6187028961667238e-07, Train Acc: 1.0\n",
            "Epoch 9391/10000\n",
            "Step 0: Train Loss: 2.785401420624112e-07, Train Acc: 1.0\n",
            "Epoch 9392/10000\n",
            "Step 0: Train Loss: 1.810864489470987e-07, Train Acc: 1.0\n",
            "Epoch 9393/10000\n",
            "Step 0: Train Loss: 1.6408790770583437e-07, Train Acc: 1.0\n",
            "Epoch 9394/10000\n",
            "Step 0: Train Loss: 1.7012041553243762e-07, Train Acc: 1.0\n",
            "Epoch 9395/10000\n",
            "Step 0: Train Loss: 1.512109548684748e-07, Train Acc: 1.0\n",
            "Epoch 9396/10000\n",
            "Step 0: Train Loss: 1.3209401572567003e-07, Train Acc: 1.0\n",
            "Epoch 9397/10000\n",
            "Step 0: Train Loss: 1.6175133055185142e-07, Train Acc: 1.0\n",
            "Epoch 9398/10000\n",
            "Step 0: Train Loss: 2.0735240013891598e-07, Train Acc: 1.0\n",
            "Epoch 9399/10000\n",
            "Step 0: Train Loss: 1.400912026383594e-07, Train Acc: 1.0\n",
            "Epoch 9400/10000\n",
            "Step 0: Train Loss: 1.8399430246063275e-07, Train Acc: 1.0\n",
            "Epoch 9401/10000\n",
            "Step 0: Train Loss: 1.7296925136633945e-07, Train Acc: 1.0\n",
            "Epoch 9402/10000\n",
            "Step 0: Train Loss: 1.4293016192823416e-07, Train Acc: 1.0\n",
            "Epoch 9403/10000\n",
            "Step 0: Train Loss: 1.5392105012779211e-07, Train Acc: 1.0\n",
            "Epoch 9404/10000\n",
            "Step 0: Train Loss: 2.543854407122126e-07, Train Acc: 1.0\n",
            "Epoch 9405/10000\n",
            "Step 0: Train Loss: 1.6851052464517124e-07, Train Acc: 1.0\n",
            "Epoch 9406/10000\n",
            "Step 0: Train Loss: 2.1853996656773234e-07, Train Acc: 1.0\n",
            "Epoch 9407/10000\n",
            "Step 0: Train Loss: 1.8371366650171694e-07, Train Acc: 1.0\n",
            "Epoch 9408/10000\n",
            "Step 0: Train Loss: 2.0991465987663105e-07, Train Acc: 1.0\n",
            "Epoch 9409/10000\n",
            "Step 0: Train Loss: 1.4384590940608177e-07, Train Acc: 1.0\n",
            "Epoch 9410/10000\n",
            "Step 0: Train Loss: 2.3267006099558785e-07, Train Acc: 1.0\n",
            "Epoch 9411/10000\n",
            "Step 0: Train Loss: 1.5195242042409518e-07, Train Acc: 1.0\n",
            "Epoch 9412/10000\n",
            "Step 0: Train Loss: 1.5735149361262302e-07, Train Acc: 1.0\n",
            "Epoch 9413/10000\n",
            "Step 0: Train Loss: 1.465040497805603e-07, Train Acc: 1.0\n",
            "Epoch 9414/10000\n",
            "Step 0: Train Loss: 2.427449317110586e-07, Train Acc: 1.0\n",
            "Epoch 9415/10000\n",
            "Step 0: Train Loss: 2.031374890520965e-07, Train Acc: 1.0\n",
            "Epoch 9416/10000\n",
            "Step 0: Train Loss: 1.4600399822484178e-07, Train Acc: 1.0\n",
            "Epoch 9417/10000\n",
            "Step 0: Train Loss: 1.9380385651857068e-07, Train Acc: 1.0\n",
            "Epoch 9418/10000\n",
            "Step 0: Train Loss: 1.7746272362728632e-07, Train Acc: 1.0\n",
            "Epoch 9419/10000\n",
            "Step 0: Train Loss: 1.9348235014149395e-07, Train Acc: 1.0\n",
            "Epoch 9420/10000\n",
            "Step 0: Train Loss: 1.2134212568071234e-07, Train Acc: 1.0\n",
            "Epoch 9421/10000\n",
            "Step 0: Train Loss: 1.6132177904637501e-07, Train Acc: 1.0\n",
            "Epoch 9422/10000\n",
            "Step 0: Train Loss: 2.0167045988728205e-07, Train Acc: 1.0\n",
            "Epoch 9423/10000\n",
            "Step 0: Train Loss: 1.6203809138914949e-07, Train Acc: 1.0\n",
            "Epoch 9424/10000\n",
            "Step 0: Train Loss: 1.914794012236598e-07, Train Acc: 1.0\n",
            "Epoch 9425/10000\n",
            "Step 0: Train Loss: 1.5276320652901632e-07, Train Acc: 1.0\n",
            "Epoch 9426/10000\n",
            "Step 0: Train Loss: 1.66127009038064e-07, Train Acc: 1.0\n",
            "Epoch 9427/10000\n",
            "Step 0: Train Loss: 1.7812794794735964e-07, Train Acc: 1.0\n",
            "Epoch 9428/10000\n",
            "Step 0: Train Loss: 1.4287007843449828e-07, Train Acc: 1.0\n",
            "Epoch 9429/10000\n",
            "Step 0: Train Loss: 1.3656359953984065e-07, Train Acc: 1.0\n",
            "Epoch 9430/10000\n",
            "Step 0: Train Loss: 1.8754600716874847e-07, Train Acc: 1.0\n",
            "Epoch 9431/10000\n",
            "Step 0: Train Loss: 1.7454232192903874e-07, Train Acc: 1.0\n",
            "Epoch 9432/10000\n",
            "Step 0: Train Loss: 1.303302070709833e-07, Train Acc: 1.0\n",
            "Epoch 9433/10000\n",
            "Step 0: Train Loss: 1.489004120003301e-07, Train Acc: 1.0\n",
            "Epoch 9434/10000\n",
            "Step 0: Train Loss: 1.3291653999658593e-07, Train Acc: 1.0\n",
            "Epoch 9435/10000\n",
            "Step 0: Train Loss: 1.706675334389729e-07, Train Acc: 1.0\n",
            "Epoch 9436/10000\n",
            "Step 0: Train Loss: 1.597251326757032e-07, Train Acc: 1.0\n",
            "Epoch 9437/10000\n",
            "Step 0: Train Loss: 1.64445495443033e-07, Train Acc: 1.0\n",
            "Epoch 9438/10000\n",
            "Step 0: Train Loss: 1.972617980072755e-07, Train Acc: 1.0\n",
            "Epoch 9439/10000\n",
            "Step 0: Train Loss: 1.365754229709637e-07, Train Acc: 1.0\n",
            "Epoch 9440/10000\n",
            "Step 0: Train Loss: 1.3040101976002916e-07, Train Acc: 1.0\n",
            "Epoch 9441/10000\n",
            "Step 0: Train Loss: 1.7127494800206478e-07, Train Acc: 1.0\n",
            "Epoch 9442/10000\n",
            "Step 0: Train Loss: 1.5891485816155182e-07, Train Acc: 1.0\n",
            "Epoch 9443/10000\n",
            "Step 0: Train Loss: 1.8284536906776339e-07, Train Acc: 1.0\n",
            "Epoch 9444/10000\n",
            "Step 0: Train Loss: 2.056238770364871e-07, Train Acc: 1.0\n",
            "Epoch 9445/10000\n",
            "Step 0: Train Loss: 1.8034626236840268e-07, Train Acc: 1.0\n",
            "Epoch 9446/10000\n",
            "Step 0: Train Loss: 1.2197341447972576e-07, Train Acc: 1.0\n",
            "Epoch 9447/10000\n",
            "Step 0: Train Loss: 1.7291004894559592e-07, Train Acc: 1.0\n",
            "Epoch 9448/10000\n",
            "Step 0: Train Loss: 1.631679964475552e-07, Train Acc: 1.0\n",
            "Epoch 9449/10000\n",
            "Step 0: Train Loss: 1.5222673255266272e-07, Train Acc: 1.0\n",
            "Epoch 9450/10000\n",
            "Step 0: Train Loss: 1.0094519353742726e-07, Train Acc: 1.0\n",
            "Epoch 9451/10000\n",
            "Step 0: Train Loss: 1.7060763468634832e-07, Train Acc: 1.0\n",
            "Epoch 9452/10000\n",
            "Step 0: Train Loss: 1.3166511791951052e-07, Train Acc: 1.0\n",
            "Epoch 9453/10000\n",
            "Step 0: Train Loss: 1.9594966715885676e-07, Train Acc: 1.0\n",
            "Epoch 9454/10000\n",
            "Step 0: Train Loss: 1.1970745106282266e-07, Train Acc: 1.0\n",
            "Epoch 9455/10000\n",
            "Step 0: Train Loss: 1.3476240212639823e-07, Train Acc: 1.0\n",
            "Epoch 9456/10000\n",
            "Step 0: Train Loss: 1.4198229791873018e-07, Train Acc: 1.0\n",
            "Epoch 9457/10000\n",
            "Step 0: Train Loss: 1.288988329406493e-07, Train Acc: 1.0\n",
            "Epoch 9458/10000\n",
            "Step 0: Train Loss: 1.8513868837999325e-07, Train Acc: 1.0\n",
            "Epoch 9459/10000\n",
            "Step 0: Train Loss: 1.5396801700262586e-07, Train Acc: 1.0\n",
            "Epoch 9460/10000\n",
            "Step 0: Train Loss: 1.8431565251830762e-07, Train Acc: 1.0\n",
            "Epoch 9461/10000\n",
            "Step 0: Train Loss: 1.0901565872245556e-07, Train Acc: 1.0\n",
            "Epoch 9462/10000\n",
            "Step 0: Train Loss: 1.5327451308166928e-07, Train Acc: 1.0\n",
            "Epoch 9463/10000\n",
            "Step 0: Train Loss: 1.600926964329119e-07, Train Acc: 1.0\n",
            "Epoch 9464/10000\n",
            "Step 0: Train Loss: 1.394356274886377e-07, Train Acc: 1.0\n",
            "Epoch 9465/10000\n",
            "Step 0: Train Loss: 1.1582223180539586e-07, Train Acc: 1.0\n",
            "Epoch 9466/10000\n",
            "Step 0: Train Loss: 1.5117267082587205e-07, Train Acc: 1.0\n",
            "Epoch 9467/10000\n",
            "Step 0: Train Loss: 1.4307278206615592e-07, Train Acc: 1.0\n",
            "Epoch 9468/10000\n",
            "Step 0: Train Loss: 1.5072509995661676e-07, Train Acc: 1.0\n",
            "Epoch 9469/10000\n",
            "Step 0: Train Loss: 1.403021059331877e-07, Train Acc: 1.0\n",
            "Epoch 9470/10000\n",
            "Step 0: Train Loss: 1.2597780596479424e-07, Train Acc: 1.0\n",
            "Epoch 9471/10000\n",
            "Step 0: Train Loss: 1.4685068094877352e-07, Train Acc: 1.0\n",
            "Epoch 9472/10000\n",
            "Step 0: Train Loss: 1.2434509244485525e-07, Train Acc: 1.0\n",
            "Epoch 9473/10000\n",
            "Step 0: Train Loss: 1.517975363185542e-07, Train Acc: 1.0\n",
            "Epoch 9474/10000\n",
            "Step 0: Train Loss: 1.2175706842754153e-07, Train Acc: 1.0\n",
            "Epoch 9475/10000\n",
            "Step 0: Train Loss: 1.1743171768330285e-07, Train Acc: 1.0\n",
            "Epoch 9476/10000\n",
            "Step 0: Train Loss: 1.4221389221802383e-07, Train Acc: 1.0\n",
            "Epoch 9477/10000\n",
            "Step 0: Train Loss: 1.4213092924819648e-07, Train Acc: 1.0\n",
            "Epoch 9478/10000\n",
            "Step 0: Train Loss: 1.2967318241408066e-07, Train Acc: 1.0\n",
            "Epoch 9479/10000\n",
            "Step 0: Train Loss: 1.284701340864558e-07, Train Acc: 1.0\n",
            "Epoch 9480/10000\n",
            "Step 0: Train Loss: 1.8130795353954454e-07, Train Acc: 1.0\n",
            "Epoch 9481/10000\n",
            "Step 0: Train Loss: 1.3462070569403295e-07, Train Acc: 1.0\n",
            "Epoch 9482/10000\n",
            "Step 0: Train Loss: 1.2781433156305866e-07, Train Acc: 1.0\n",
            "Epoch 9483/10000\n",
            "Step 0: Train Loss: 1.6161915539214533e-07, Train Acc: 1.0\n",
            "Epoch 9484/10000\n",
            "Step 0: Train Loss: 1.3208077120907546e-07, Train Acc: 1.0\n",
            "Epoch 9485/10000\n",
            "Step 0: Train Loss: 1.762185775078251e-07, Train Acc: 1.0\n",
            "Epoch 9486/10000\n",
            "Step 0: Train Loss: 1.5035503508897818e-07, Train Acc: 1.0\n",
            "Epoch 9487/10000\n",
            "Step 0: Train Loss: 9.968173486640808e-08, Train Acc: 1.0\n",
            "Epoch 9488/10000\n",
            "Step 0: Train Loss: 1.3878120341814792e-07, Train Acc: 1.0\n",
            "Epoch 9489/10000\n",
            "Step 0: Train Loss: 8.03823638761969e-08, Train Acc: 1.0\n",
            "Epoch 9490/10000\n",
            "Step 0: Train Loss: 1.611203117590776e-07, Train Acc: 1.0\n",
            "Epoch 9491/10000\n",
            "Step 0: Train Loss: 1.1439145453095989e-07, Train Acc: 1.0\n",
            "Epoch 9492/10000\n",
            "Step 0: Train Loss: 1.9730654798877367e-07, Train Acc: 1.0\n",
            "Epoch 9493/10000\n",
            "Step 0: Train Loss: 1.215440050827965e-07, Train Acc: 1.0\n",
            "Epoch 9494/10000\n",
            "Step 0: Train Loss: 1.1386675424773784e-07, Train Acc: 1.0\n",
            "Epoch 9495/10000\n",
            "Step 0: Train Loss: 1.151306534552532e-07, Train Acc: 1.0\n",
            "Epoch 9496/10000\n",
            "Step 0: Train Loss: 1.680446644058975e-07, Train Acc: 1.0\n",
            "Epoch 9497/10000\n",
            "Step 0: Train Loss: 1.3761176376192452e-07, Train Acc: 1.0\n",
            "Epoch 9498/10000\n",
            "Step 0: Train Loss: 1.4802726866491867e-07, Train Acc: 1.0\n",
            "Epoch 9499/10000\n",
            "Step 0: Train Loss: 1.3667961695773556e-07, Train Acc: 1.0\n",
            "Epoch 9500/10000\n",
            "Step 0: Train Loss: 1.4101821932399616e-07, Train Acc: 1.0\n",
            "Epoch 9501/10000\n",
            "Step 0: Train Loss: 1.1024355472954994e-07, Train Acc: 1.0\n",
            "Epoch index and hidden dimension and ratio: 9500 1024 0.9772860752006406\n",
            "Epoch index and hidden dimension and ratio: 9500 20 0.9484186657947591\n",
            "Epoch index and hidden dimension and ratio: 9500 20 1.882856917957527\n",
            "Epoch index and hidden dimension and ratio: 9500 20 5.790068104646207\n",
            "MI(X;T): [10.256590682598777, 7.272622052532474, 5.420395150441301, 2.2942137315219053], MI(Y;T): [2.2344994968412903, 3.261017588337962, 3.0794567193880784, 2.77260199365535]\n",
            "Epoch index and hidden dimension and ratio: 9500 1024 0.9772822572160093\n",
            "Epoch index and hidden dimension and ratio: 9500 20 0.948409661772022\n",
            "Epoch index and hidden dimension and ratio: 9500 20 1.8828316530248725\n",
            "Epoch index and hidden dimension and ratio: 9500 20 5.7900161169945905\n",
            "MI(X;T): [10.257462793149825, 7.2728355961699105, 5.420730800331189, 2.294237201561892], MI(Y;T): [2.234513269904543, 3.2610420995877463, 3.0794567317961072, 2.7725118873130707]\n",
            "Epoch index and hidden dimension and ratio: 9500 1024 0.9772845333222319\n",
            "Epoch index and hidden dimension and ratio: 9500 20 0.9484102041830303\n",
            "Epoch index and hidden dimension and ratio: 9500 20 1.8828405401871129\n",
            "Epoch index and hidden dimension and ratio: 9500 20 5.790081924148535\n",
            "MI(X;T): [10.257488429556817, 7.272541865664243, 5.420064562597306, 2.2933041160346153], MI(Y;T): [2.234577587268997, 3.2610420995877467, 3.079267390021154, 2.7726691697125756]\n",
            "Epoch index and hidden dimension and ratio: 9500 1024 0.9772887184207699\n",
            "Epoch index and hidden dimension and ratio: 9500 20 0.9484187742769608\n",
            "Epoch index and hidden dimension and ratio: 9500 20 1.8828639007278587\n",
            "Epoch index and hidden dimension and ratio: 9500 20 5.790191822095624\n",
            "MI(X;T): [10.256389534239572, 7.2729371488310335, 5.419896529218857, 2.29234703748456], MI(Y;T): [2.2345739924194152, 3.2610420995877463, 3.0791065635775476, 2.772731022734213]\n",
            "Epoch index and hidden dimension and ratio: 9500 1024 0.9772970151950648\n",
            "Epoch index and hidden dimension and ratio: 9500 20 0.9484507765264476\n",
            "Epoch index and hidden dimension and ratio: 9500 20 1.882930681404121\n",
            "Epoch index and hidden dimension and ratio: 9500 20 5.790428727849826\n",
            "MI(X;T): [10.256418116186978, 7.273282527510931, 5.419614600718592, 2.292682337537747], MI(Y;T): [2.234480248963385, 3.261045068894418, 3.0791564552926407, 2.77288562100306]\n",
            "Epoch index and hidden dimension and ratio: 9500 1024 0.9773051651237971\n",
            "Epoch index and hidden dimension and ratio: 9500 20 0.9484832127047411\n",
            "Epoch index and hidden dimension and ratio: 9500 20 1.8830030482966484\n",
            "Epoch index and hidden dimension and ratio: 9500 20 5.790669582033265\n",
            "MI(X;T): [10.255873578980042, 7.274407579185393, 5.420557011860837, 2.2921101581057353], MI(Y;T): [2.2341180441476842, 3.2610450688944175, 3.0789949310407416, 2.7729079460328503]\n",
            "Epoch 9502/10000\n",
            "Step 0: Train Loss: 1.1154254053735713e-07, Train Acc: 1.0\n",
            "Epoch 9503/10000\n",
            "Step 0: Train Loss: 1.807843261758535e-07, Train Acc: 1.0\n",
            "Epoch 9504/10000\n",
            "Step 0: Train Loss: 1.5703001565725572e-07, Train Acc: 1.0\n",
            "Epoch 9505/10000\n",
            "Step 0: Train Loss: 1.386851806728373e-07, Train Acc: 1.0\n",
            "Epoch 9506/10000\n",
            "Step 0: Train Loss: 1.0853868559479452e-07, Train Acc: 1.0\n",
            "Epoch 9507/10000\n",
            "Step 0: Train Loss: 1.2404751714711892e-07, Train Acc: 1.0\n",
            "Epoch 9508/10000\n",
            "Step 0: Train Loss: 1.0256628968363657e-07, Train Acc: 1.0\n",
            "Epoch 9509/10000\n",
            "Step 0: Train Loss: 1.0567786290494041e-07, Train Acc: 1.0\n",
            "Epoch 9510/10000\n",
            "Step 0: Train Loss: 1.5236902584092604e-07, Train Acc: 1.0\n",
            "Epoch 9511/10000\n",
            "Step 0: Train Loss: 1.4432288253374281e-07, Train Acc: 1.0\n",
            "Epoch 9512/10000\n",
            "Step 0: Train Loss: 9.839396142297119e-08, Train Acc: 1.0\n",
            "Epoch 9513/10000\n",
            "Step 0: Train Loss: 1.2964918028046668e-07, Train Acc: 1.0\n",
            "Epoch 9514/10000\n",
            "Step 0: Train Loss: 1.3849458468939702e-07, Train Acc: 1.0\n",
            "Epoch 9515/10000\n",
            "Step 0: Train Loss: 8.872665091530507e-08, Train Acc: 1.0\n",
            "Epoch 9516/10000\n",
            "Step 0: Train Loss: 1.4499192957373452e-07, Train Acc: 1.0\n",
            "Epoch 9517/10000\n",
            "Step 0: Train Loss: 1.0988450327431565e-07, Train Acc: 1.0\n",
            "Epoch 9518/10000\n",
            "Step 0: Train Loss: 1.5347913517871348e-07, Train Acc: 1.0\n",
            "Epoch 9519/10000\n",
            "Step 0: Train Loss: 1.31342403619783e-07, Train Acc: 1.0\n",
            "Epoch 9520/10000\n",
            "Step 0: Train Loss: 1.4148653804113565e-07, Train Acc: 1.0\n",
            "Epoch 9521/10000\n",
            "Step 0: Train Loss: 9.852556814848867e-08, Train Acc: 1.0\n",
            "Epoch 9522/10000\n",
            "Step 0: Train Loss: 1.1012378564601022e-07, Train Acc: 1.0\n",
            "Epoch 9523/10000\n",
            "Step 0: Train Loss: 1.2186237086098117e-07, Train Acc: 1.0\n",
            "Epoch 9524/10000\n",
            "Step 0: Train Loss: 1.2897019985302904e-07, Train Acc: 1.0\n",
            "Epoch 9525/10000\n",
            "Step 0: Train Loss: 1.0320925269979853e-07, Train Acc: 1.0\n",
            "Epoch 9526/10000\n",
            "Step 0: Train Loss: 1.3136359200416337e-07, Train Acc: 1.0\n",
            "Epoch 9527/10000\n",
            "Step 0: Train Loss: 9.690373303783417e-08, Train Acc: 1.0\n",
            "Epoch 9528/10000\n",
            "Step 0: Train Loss: 1.188243388128285e-07, Train Acc: 1.0\n",
            "Epoch 9529/10000\n",
            "Step 0: Train Loss: 7.967894077864912e-08, Train Acc: 1.0\n",
            "Epoch 9530/10000\n",
            "Step 0: Train Loss: 9.648670129536185e-08, Train Acc: 1.0\n",
            "Epoch 9531/10000\n",
            "Step 0: Train Loss: 1.1861111204325425e-07, Train Acc: 1.0\n",
            "Epoch 9532/10000\n",
            "Step 0: Train Loss: 8.143143759298255e-08, Train Acc: 1.0\n",
            "Epoch 9533/10000\n",
            "Step 0: Train Loss: 1.0924136972789711e-07, Train Acc: 1.0\n",
            "Epoch 9534/10000\n",
            "Step 0: Train Loss: 1.516637837539747e-07, Train Acc: 1.0\n",
            "Epoch 9535/10000\n",
            "Step 0: Train Loss: 1.0840788178256844e-07, Train Acc: 1.0\n",
            "Epoch 9536/10000\n",
            "Step 0: Train Loss: 1.1328284443834491e-07, Train Acc: 1.0\n",
            "Epoch 9537/10000\n",
            "Step 0: Train Loss: 1.2713387320673064e-07, Train Acc: 1.0\n",
            "Epoch 9538/10000\n",
            "Step 0: Train Loss: 1.3004341781197581e-07, Train Acc: 1.0\n",
            "Epoch 9539/10000\n",
            "Step 0: Train Loss: 1.565741030162826e-07, Train Acc: 1.0\n",
            "Epoch 9540/10000\n",
            "Step 0: Train Loss: 1.3527355235964933e-07, Train Acc: 1.0\n",
            "Epoch 9541/10000\n",
            "Step 0: Train Loss: 9.369770737066574e-08, Train Acc: 1.0\n",
            "Epoch 9542/10000\n",
            "Step 0: Train Loss: 8.121659078597077e-08, Train Acc: 1.0\n",
            "Epoch 9543/10000\n",
            "Step 0: Train Loss: 9.47345100144048e-08, Train Acc: 1.0\n",
            "Epoch 9544/10000\n",
            "Step 0: Train Loss: 1.4186568364493723e-07, Train Acc: 1.0\n",
            "Epoch 9545/10000\n",
            "Step 0: Train Loss: 1.5999520996956562e-07, Train Acc: 1.0\n",
            "Epoch 9546/10000\n",
            "Step 0: Train Loss: 1.5396429375869047e-07, Train Acc: 1.0\n",
            "Epoch 9547/10000\n",
            "Step 0: Train Loss: 1.429381484285841e-07, Train Acc: 1.0\n",
            "Epoch 9548/10000\n",
            "Step 0: Train Loss: 1.2092380075046094e-07, Train Acc: 1.0\n",
            "Epoch 9549/10000\n",
            "Step 0: Train Loss: 8.382698268860622e-08, Train Acc: 1.0\n",
            "Epoch 9550/10000\n",
            "Step 0: Train Loss: 1.4311909524167277e-07, Train Acc: 1.0\n",
            "Epoch 9551/10000\n",
            "Step 0: Train Loss: 1.303260006579876e-07, Train Acc: 1.0\n",
            "Epoch 9552/10000\n",
            "Step 0: Train Loss: 1.3995797587540437e-07, Train Acc: 1.0\n",
            "Epoch 9553/10000\n",
            "Step 0: Train Loss: 1.2019619077818788e-07, Train Acc: 1.0\n",
            "Epoch 9554/10000\n",
            "Step 0: Train Loss: 9.434101144734086e-08, Train Acc: 1.0\n",
            "Epoch 9555/10000\n",
            "Step 0: Train Loss: 1.0237598502271794e-07, Train Acc: 1.0\n",
            "Epoch 9556/10000\n",
            "Step 0: Train Loss: 1.1321063198010961e-07, Train Acc: 1.0\n",
            "Epoch 9557/10000\n",
            "Step 0: Train Loss: 1.0154078466939609e-07, Train Acc: 1.0\n",
            "Epoch 9558/10000\n",
            "Step 0: Train Loss: 1.2875537436229934e-07, Train Acc: 1.0\n",
            "Epoch 9559/10000\n",
            "Step 0: Train Loss: 1.1602426042145453e-07, Train Acc: 1.0\n",
            "Epoch 9560/10000\n",
            "Step 0: Train Loss: 8.950146934694203e-08, Train Acc: 1.0\n",
            "Epoch 9561/10000\n",
            "Step 0: Train Loss: 8.668789774901597e-08, Train Acc: 1.0\n",
            "Epoch 9562/10000\n",
            "Step 0: Train Loss: 1.2690735218257032e-07, Train Acc: 1.0\n",
            "Epoch 9563/10000\n",
            "Step 0: Train Loss: 1.1792852205871895e-07, Train Acc: 1.0\n",
            "Epoch 9564/10000\n",
            "Step 0: Train Loss: 1.1440354796832253e-07, Train Acc: 1.0\n",
            "Epoch 9565/10000\n",
            "Step 0: Train Loss: 1.0083757473466903e-07, Train Acc: 1.0\n",
            "Epoch 9566/10000\n",
            "Step 0: Train Loss: 6.810378749833035e-08, Train Acc: 1.0\n",
            "Epoch 9567/10000\n",
            "Step 0: Train Loss: 1.154262889713209e-07, Train Acc: 1.0\n",
            "Epoch 9568/10000\n",
            "Step 0: Train Loss: 1.343559290489793e-07, Train Acc: 1.0\n",
            "Epoch 9569/10000\n",
            "Step 0: Train Loss: 1.119952912631561e-07, Train Acc: 1.0\n",
            "Epoch 9570/10000\n",
            "Step 0: Train Loss: 1.0014615270392824e-07, Train Acc: 1.0\n",
            "Epoch 9571/10000\n",
            "Step 0: Train Loss: 9.916822563127425e-08, Train Acc: 1.0\n",
            "Epoch 9572/10000\n",
            "Step 0: Train Loss: 9.121803401512807e-08, Train Acc: 1.0\n",
            "Epoch 9573/10000\n",
            "Step 0: Train Loss: 1.2316266406742216e-07, Train Acc: 1.0\n",
            "Epoch 9574/10000\n",
            "Step 0: Train Loss: 7.436221238776852e-08, Train Acc: 1.0\n",
            "Epoch 9575/10000\n",
            "Step 0: Train Loss: 7.598357143479006e-08, Train Acc: 1.0\n",
            "Epoch 9576/10000\n",
            "Step 0: Train Loss: 1.2160091955593089e-07, Train Acc: 1.0\n",
            "Epoch 9577/10000\n",
            "Step 0: Train Loss: 1.1605835936734366e-07, Train Acc: 1.0\n",
            "Epoch 9578/10000\n",
            "Step 0: Train Loss: 8.928557093668132e-08, Train Acc: 1.0\n",
            "Epoch 9579/10000\n",
            "Step 0: Train Loss: 8.331453926757604e-08, Train Acc: 1.0\n",
            "Epoch 9580/10000\n",
            "Step 0: Train Loss: 1.0136211159306185e-07, Train Acc: 1.0\n",
            "Epoch 9581/10000\n",
            "Step 0: Train Loss: 7.440965532623522e-08, Train Acc: 1.0\n",
            "Epoch 9582/10000\n",
            "Step 0: Train Loss: 9.025217906355465e-08, Train Acc: 1.0\n",
            "Epoch 9583/10000\n",
            "Step 0: Train Loss: 9.015710134008259e-08, Train Acc: 1.0\n",
            "Epoch 9584/10000\n",
            "Step 0: Train Loss: 7.358750053754193e-08, Train Acc: 1.0\n",
            "Epoch 9585/10000\n",
            "Step 0: Train Loss: 8.678329521671913e-08, Train Acc: 1.0\n",
            "Epoch 9586/10000\n",
            "Step 0: Train Loss: 7.488682030043492e-08, Train Acc: 1.0\n",
            "Epoch 9587/10000\n",
            "Step 0: Train Loss: 6.434882493522309e-08, Train Acc: 1.0\n",
            "Epoch 9588/10000\n",
            "Step 0: Train Loss: 8.33025808333332e-08, Train Acc: 1.0\n",
            "Epoch 9589/10000\n",
            "Step 0: Train Loss: 9.401884426551987e-08, Train Acc: 1.0\n",
            "Epoch 9590/10000\n",
            "Step 0: Train Loss: 1.3068643056612927e-07, Train Acc: 1.0\n",
            "Epoch 9591/10000\n",
            "Step 0: Train Loss: 1.1230481078428056e-07, Train Acc: 1.0\n",
            "Epoch 9592/10000\n",
            "Step 0: Train Loss: 7.213294139774007e-08, Train Acc: 1.0\n",
            "Epoch 9593/10000\n",
            "Step 0: Train Loss: 1.183610507382582e-07, Train Acc: 1.0\n",
            "Epoch 9594/10000\n",
            "Step 0: Train Loss: 7.351551545298207e-08, Train Acc: 1.0\n",
            "Epoch 9595/10000\n",
            "Step 0: Train Loss: 7.545668978536924e-08, Train Acc: 1.0\n",
            "Epoch 9596/10000\n",
            "Step 0: Train Loss: 8.860735789539831e-08, Train Acc: 1.0\n",
            "Epoch 9597/10000\n",
            "Step 0: Train Loss: 9.282705093482946e-08, Train Acc: 1.0\n",
            "Epoch 9598/10000\n",
            "Step 0: Train Loss: 1.1487914974850355e-07, Train Acc: 1.0\n",
            "Epoch 9599/10000\n",
            "Step 0: Train Loss: 7.328914364279626e-08, Train Acc: 1.0\n",
            "Epoch 9600/10000\n",
            "Step 0: Train Loss: 8.225320158317118e-08, Train Acc: 1.0\n",
            "Epoch 9601/10000\n",
            "Step 0: Train Loss: 9.593842520416729e-08, Train Acc: 1.0\n",
            "Epoch 9602/10000\n",
            "Step 0: Train Loss: 8.535283058108689e-08, Train Acc: 1.0\n",
            "Epoch 9603/10000\n",
            "Step 0: Train Loss: 9.821475543958513e-08, Train Acc: 1.0\n",
            "Epoch 9604/10000\n",
            "Step 0: Train Loss: 8.531714001946966e-08, Train Acc: 1.0\n",
            "Epoch 9605/10000\n",
            "Step 0: Train Loss: 6.172608379984013e-08, Train Acc: 1.0\n",
            "Epoch 9606/10000\n",
            "Step 0: Train Loss: 8.362391668015334e-08, Train Acc: 1.0\n",
            "Epoch 9607/10000\n",
            "Step 0: Train Loss: 8.710532028999296e-08, Train Acc: 1.0\n",
            "Epoch 9608/10000\n",
            "Step 0: Train Loss: 8.426809472439345e-08, Train Acc: 1.0\n",
            "Epoch 9609/10000\n",
            "Step 0: Train Loss: 9.963340374952168e-08, Train Acc: 1.0\n",
            "Epoch 9610/10000\n",
            "Step 0: Train Loss: 8.658068395561713e-08, Train Acc: 1.0\n",
            "Epoch 9611/10000\n",
            "Step 0: Train Loss: 9.106082643484115e-08, Train Acc: 1.0\n",
            "Epoch 9612/10000\n",
            "Step 0: Train Loss: 7.481502706241372e-08, Train Acc: 1.0\n",
            "Epoch 9613/10000\n",
            "Step 0: Train Loss: 9.785710375354029e-08, Train Acc: 1.0\n",
            "Epoch 9614/10000\n",
            "Step 0: Train Loss: 6.365729632307193e-08, Train Acc: 1.0\n",
            "Epoch 9615/10000\n",
            "Step 0: Train Loss: 8.480404289912258e-08, Train Acc: 1.0\n",
            "Epoch 9616/10000\n",
            "Step 0: Train Loss: 9.540177359212976e-08, Train Acc: 1.0\n",
            "Epoch 9617/10000\n",
            "Step 0: Train Loss: 1.2158983508925303e-07, Train Acc: 1.0\n",
            "Epoch 9618/10000\n",
            "Step 0: Train Loss: 8.040595389502414e-08, Train Acc: 1.0\n",
            "Epoch 9619/10000\n",
            "Step 0: Train Loss: 8.337354984178091e-08, Train Acc: 1.0\n",
            "Epoch 9620/10000\n",
            "Step 0: Train Loss: 1.0129060967756232e-07, Train Acc: 1.0\n",
            "Epoch 9621/10000\n",
            "Step 0: Train Loss: 9.523433419644789e-08, Train Acc: 1.0\n",
            "Epoch 9622/10000\n",
            "Step 0: Train Loss: 1.1207815475700045e-07, Train Acc: 1.0\n",
            "Epoch 9623/10000\n",
            "Step 0: Train Loss: 7.281245473222953e-08, Train Acc: 1.0\n",
            "Epoch 9624/10000\n",
            "Step 0: Train Loss: 9.553243529580868e-08, Train Acc: 1.0\n",
            "Epoch 9625/10000\n",
            "Step 0: Train Loss: 1.2296061413508141e-07, Train Acc: 1.0\n",
            "Epoch 9626/10000\n",
            "Step 0: Train Loss: 9.606802109374257e-08, Train Acc: 1.0\n",
            "Epoch 9627/10000\n",
            "Step 0: Train Loss: 1.0261351945928254e-07, Train Acc: 1.0\n",
            "Epoch 9628/10000\n",
            "Step 0: Train Loss: 9.281455248810744e-08, Train Acc: 1.0\n",
            "Epoch 9629/10000\n",
            "Step 0: Train Loss: 7.363480136746148e-08, Train Acc: 1.0\n",
            "Epoch 9630/10000\n",
            "Step 0: Train Loss: 7.268125301607142e-08, Train Acc: 1.0\n",
            "Epoch 9631/10000\n",
            "Step 0: Train Loss: 6.350248327180452e-08, Train Acc: 1.0\n",
            "Epoch 9632/10000\n",
            "Step 0: Train Loss: 6.189300449932489e-08, Train Acc: 1.0\n",
            "Epoch 9633/10000\n",
            "Step 0: Train Loss: 9.910864662288077e-08, Train Acc: 1.0\n",
            "Epoch 9634/10000\n",
            "Step 0: Train Loss: 7.715124894502878e-08, Train Acc: 1.0\n",
            "Epoch 9635/10000\n",
            "Step 0: Train Loss: 7.494547560327192e-08, Train Acc: 1.0\n",
            "Epoch 9636/10000\n",
            "Step 0: Train Loss: 9.040709159080507e-08, Train Acc: 1.0\n",
            "Epoch 9637/10000\n",
            "Step 0: Train Loss: 9.514975118918301e-08, Train Acc: 1.0\n",
            "Epoch 9638/10000\n",
            "Step 0: Train Loss: 8.571024068260158e-08, Train Acc: 1.0\n",
            "Epoch 9639/10000\n",
            "Step 0: Train Loss: 1.0348215084832191e-07, Train Acc: 1.0\n",
            "Epoch 9640/10000\n",
            "Step 0: Train Loss: 9.837021508474209e-08, Train Acc: 1.0\n",
            "Epoch 9641/10000\n",
            "Step 0: Train Loss: 6.1261090422704e-08, Train Acc: 1.0\n",
            "Epoch 9642/10000\n",
            "Step 0: Train Loss: 6.974850919050368e-08, Train Acc: 1.0\n",
            "Epoch 9643/10000\n",
            "Step 0: Train Loss: 8.381451266359363e-08, Train Acc: 1.0\n",
            "Epoch 9644/10000\n",
            "Step 0: Train Loss: 7.837933679866183e-08, Train Acc: 1.0\n",
            "Epoch 9645/10000\n",
            "Step 0: Train Loss: 7.412345581769841e-08, Train Acc: 1.0\n",
            "Epoch 9646/10000\n",
            "Step 0: Train Loss: 7.970207605012547e-08, Train Acc: 1.0\n",
            "Epoch 9647/10000\n",
            "Step 0: Train Loss: 1.0345839029923809e-07, Train Acc: 1.0\n",
            "Epoch 9648/10000\n",
            "Step 0: Train Loss: 7.805719093312291e-08, Train Acc: 1.0\n",
            "Epoch 9649/10000\n",
            "Step 0: Train Loss: 8.337375589917428e-08, Train Acc: 1.0\n",
            "Epoch 9650/10000\n",
            "Step 0: Train Loss: 8.169319443140921e-08, Train Acc: 1.0\n",
            "Epoch 9651/10000\n",
            "Step 0: Train Loss: 7.640021237875771e-08, Train Acc: 1.0\n",
            "Epoch 9652/10000\n",
            "Step 0: Train Loss: 6.214313685859452e-08, Train Acc: 1.0\n",
            "Epoch 9653/10000\n",
            "Step 0: Train Loss: 7.227589549074764e-08, Train Acc: 1.0\n",
            "Epoch 9654/10000\n",
            "Step 0: Train Loss: 6.716173572840489e-08, Train Acc: 1.0\n",
            "Epoch 9655/10000\n",
            "Step 0: Train Loss: 9.40302697927109e-08, Train Acc: 1.0\n",
            "Epoch 9656/10000\n",
            "Step 0: Train Loss: 7.70322117205069e-08, Train Acc: 1.0\n",
            "Epoch 9657/10000\n",
            "Step 0: Train Loss: 6.862772039539777e-08, Train Acc: 1.0\n",
            "Epoch 9658/10000\n",
            "Step 0: Train Loss: 6.111833528166244e-08, Train Acc: 1.0\n",
            "Epoch 9659/10000\n",
            "Step 0: Train Loss: 7.390906375803752e-08, Train Acc: 1.0\n",
            "Epoch 9660/10000\n",
            "Step 0: Train Loss: 6.368134819467741e-08, Train Acc: 1.0\n",
            "Epoch 9661/10000\n",
            "Step 0: Train Loss: 6.72334934392893e-08, Train Acc: 1.0\n",
            "Epoch 9662/10000\n",
            "Step 0: Train Loss: 6.668442864565804e-08, Train Acc: 1.0\n",
            "Epoch 9663/10000\n",
            "Step 0: Train Loss: 7.954704273060997e-08, Train Acc: 1.0\n",
            "Epoch 9664/10000\n",
            "Step 0: Train Loss: 8.041715915396708e-08, Train Acc: 1.0\n",
            "Epoch 9665/10000\n",
            "Step 0: Train Loss: 5.261863833538882e-08, Train Acc: 1.0\n",
            "Epoch 9666/10000\n",
            "Step 0: Train Loss: 6.277514330577105e-08, Train Acc: 1.0\n",
            "Epoch 9667/10000\n",
            "Step 0: Train Loss: 6.591029944047477e-08, Train Acc: 1.0\n",
            "Epoch 9668/10000\n",
            "Step 0: Train Loss: 8.311116062031942e-08, Train Acc: 1.0\n",
            "Epoch 9669/10000\n",
            "Step 0: Train Loss: 8.435036846776711e-08, Train Acc: 1.0\n",
            "Epoch 9670/10000\n",
            "Step 0: Train Loss: 7.691181025393234e-08, Train Acc: 1.0\n",
            "Epoch 9671/10000\n",
            "Step 0: Train Loss: 8.287207720059087e-08, Train Acc: 1.0\n",
            "Epoch 9672/10000\n",
            "Step 0: Train Loss: 8.755762337386841e-08, Train Acc: 1.0\n",
            "Epoch 9673/10000\n",
            "Step 0: Train Loss: 5.828092142223795e-08, Train Acc: 1.0\n",
            "Epoch 9674/10000\n",
            "Step 0: Train Loss: 7.296699067182999e-08, Train Acc: 1.0\n",
            "Epoch 9675/10000\n",
            "Step 0: Train Loss: 7.213289165974857e-08, Train Acc: 1.0\n",
            "Epoch 9676/10000\n",
            "Step 0: Train Loss: 6.356176385224899e-08, Train Acc: 1.0\n",
            "Epoch 9677/10000\n",
            "Step 0: Train Loss: 6.591011469936348e-08, Train Acc: 1.0\n",
            "Epoch 9678/10000\n",
            "Step 0: Train Loss: 7.461122208951565e-08, Train Acc: 1.0\n",
            "Epoch 9679/10000\n",
            "Step 0: Train Loss: 7.133397872394198e-08, Train Acc: 1.0\n",
            "Epoch 9680/10000\n",
            "Step 0: Train Loss: 5.2845244624677434e-08, Train Acc: 1.0\n",
            "Epoch 9681/10000\n",
            "Step 0: Train Loss: 6.730375190500126e-08, Train Acc: 1.0\n",
            "Epoch 9682/10000\n",
            "Step 0: Train Loss: 5.6206651777301886e-08, Train Acc: 1.0\n",
            "Epoch 9683/10000\n",
            "Step 0: Train Loss: 7.171557570018194e-08, Train Acc: 1.0\n",
            "Epoch 9684/10000\n",
            "Step 0: Train Loss: 7.368260668272342e-08, Train Acc: 1.0\n",
            "Epoch 9685/10000\n",
            "Step 0: Train Loss: 6.400280483376264e-08, Train Acc: 1.0\n",
            "Epoch 9686/10000\n",
            "Step 0: Train Loss: 5.977095440812263e-08, Train Acc: 1.0\n",
            "Epoch 9687/10000\n",
            "Step 0: Train Loss: 6.02837033625292e-08, Train Acc: 1.0\n",
            "Epoch 9688/10000\n",
            "Step 0: Train Loss: 6.551704956336835e-08, Train Acc: 1.0\n",
            "Epoch 9689/10000\n",
            "Step 0: Train Loss: 5.798296598413799e-08, Train Acc: 1.0\n",
            "Epoch 9690/10000\n",
            "Step 0: Train Loss: 1.1216089745857971e-07, Train Acc: 1.0\n",
            "Epoch 9691/10000\n",
            "Step 0: Train Loss: 8.680621732537475e-08, Train Acc: 1.0\n",
            "Epoch 9692/10000\n",
            "Step 0: Train Loss: 7.612599262074582e-08, Train Acc: 1.0\n",
            "Epoch 9693/10000\n",
            "Step 0: Train Loss: 9.029847802821678e-08, Train Acc: 1.0\n",
            "Epoch 9694/10000\n",
            "Step 0: Train Loss: 7.310987371056399e-08, Train Acc: 1.0\n",
            "Epoch 9695/10000\n",
            "Step 0: Train Loss: 6.79487115462507e-08, Train Acc: 1.0\n",
            "Epoch 9696/10000\n",
            "Step 0: Train Loss: 5.758957755119809e-08, Train Acc: 1.0\n",
            "Epoch 9697/10000\n",
            "Step 0: Train Loss: 6.998612178676922e-08, Train Acc: 1.0\n",
            "Epoch 9698/10000\n",
            "Step 0: Train Loss: 5.46809744150778e-08, Train Acc: 1.0\n",
            "Epoch 9699/10000\n",
            "Step 0: Train Loss: 7.115431088777768e-08, Train Acc: 1.0\n",
            "Epoch 9700/10000\n",
            "Step 0: Train Loss: 4.7087443277860075e-08, Train Acc: 1.0\n",
            "Epoch 9701/10000\n",
            "Step 0: Train Loss: 8.291960540418586e-08, Train Acc: 1.0\n",
            "Epoch 9702/10000\n",
            "Step 0: Train Loss: 6.73884272828218e-08, Train Acc: 1.0\n",
            "Epoch 9703/10000\n",
            "Step 0: Train Loss: 4.943576570326513e-08, Train Acc: 1.0\n",
            "Epoch 9704/10000\n",
            "Step 0: Train Loss: 7.407595603581285e-08, Train Acc: 1.0\n",
            "Epoch 9705/10000\n",
            "Step 0: Train Loss: 7.635266285888065e-08, Train Acc: 1.0\n",
            "Epoch 9706/10000\n",
            "Step 0: Train Loss: 5.406106495797758e-08, Train Acc: 1.0\n",
            "Epoch 9707/10000\n",
            "Step 0: Train Loss: 6.588629020143344e-08, Train Acc: 1.0\n",
            "Epoch 9708/10000\n",
            "Step 0: Train Loss: 6.284661679956116e-08, Train Acc: 1.0\n",
            "Epoch 9709/10000\n",
            "Step 0: Train Loss: 5.122395307921579e-08, Train Acc: 1.0\n",
            "Epoch 9710/10000\n",
            "Step 0: Train Loss: 5.959215343409596e-08, Train Acc: 1.0\n",
            "Epoch 9711/10000\n",
            "Step 0: Train Loss: 7.049915495827008e-08, Train Acc: 1.0\n",
            "Epoch 9712/10000\n",
            "Step 0: Train Loss: 4.17468299929169e-08, Train Acc: 1.0\n",
            "Epoch 9713/10000\n",
            "Step 0: Train Loss: 5.565841121324411e-08, Train Acc: 1.0\n",
            "Epoch 9714/10000\n",
            "Step 0: Train Loss: 6.387182338585262e-08, Train Acc: 1.0\n",
            "Epoch 9715/10000\n",
            "Step 0: Train Loss: 7.015338354676715e-08, Train Acc: 1.0\n",
            "Epoch 9716/10000\n",
            "Step 0: Train Loss: 5.907920908043707e-08, Train Acc: 1.0\n",
            "Epoch 9717/10000\n",
            "Step 0: Train Loss: 6.604080482475183e-08, Train Acc: 1.0\n",
            "Epoch 9718/10000\n",
            "Step 0: Train Loss: 5.8662482871341126e-08, Train Acc: 1.0\n",
            "Epoch 9719/10000\n",
            "Step 0: Train Loss: 7.862918494083715e-08, Train Acc: 1.0\n",
            "Epoch 9720/10000\n",
            "Step 0: Train Loss: 7.135746926678621e-08, Train Acc: 1.0\n",
            "Epoch 9721/10000\n",
            "Step 0: Train Loss: 4.088865068752057e-08, Train Acc: 1.0\n",
            "Epoch 9722/10000\n",
            "Step 0: Train Loss: 4.63364244751574e-08, Train Acc: 1.0\n",
            "Epoch 9723/10000\n",
            "Step 0: Train Loss: 6.4145694977924e-08, Train Acc: 1.0\n",
            "Epoch 9724/10000\n",
            "Step 0: Train Loss: 4.265280395543414e-08, Train Acc: 1.0\n",
            "Epoch 9725/10000\n",
            "Step 0: Train Loss: 4.972185152496422e-08, Train Acc: 1.0\n",
            "Epoch 9726/10000\n",
            "Step 0: Train Loss: 5.3965642621278676e-08, Train Acc: 1.0\n",
            "Epoch 9727/10000\n",
            "Step 0: Train Loss: 5.4668781501732155e-08, Train Acc: 1.0\n",
            "Epoch 9728/10000\n",
            "Step 0: Train Loss: 6.862818224817602e-08, Train Acc: 1.0\n",
            "Epoch 9729/10000\n",
            "Step 0: Train Loss: 4.666999586788734e-08, Train Acc: 1.0\n",
            "Epoch 9730/10000\n",
            "Step 0: Train Loss: 6.04862790964944e-08, Train Acc: 1.0\n",
            "Epoch 9731/10000\n",
            "Step 0: Train Loss: 4.982915768891871e-08, Train Acc: 1.0\n",
            "Epoch 9732/10000\n",
            "Step 0: Train Loss: 5.619401477474639e-08, Train Acc: 1.0\n",
            "Epoch 9733/10000\n",
            "Step 0: Train Loss: 5.7899370631275815e-08, Train Acc: 1.0\n",
            "Epoch 9734/10000\n",
            "Step 0: Train Loss: 6.586167700106671e-08, Train Acc: 1.0\n",
            "Epoch 9735/10000\n",
            "Step 0: Train Loss: 5.583641993212041e-08, Train Acc: 1.0\n",
            "Epoch 9736/10000\n",
            "Step 0: Train Loss: 5.94725584335265e-08, Train Acc: 1.0\n",
            "Epoch 9737/10000\n",
            "Step 0: Train Loss: 7.16790822252733e-08, Train Acc: 1.0\n",
            "Epoch 9738/10000\n",
            "Step 0: Train Loss: 4.7635673183776817e-08, Train Acc: 1.0\n",
            "Epoch 9739/10000\n",
            "Step 0: Train Loss: 6.289429421713066e-08, Train Acc: 1.0\n",
            "Epoch 9740/10000\n",
            "Step 0: Train Loss: 5.1367020859061086e-08, Train Acc: 1.0\n",
            "Epoch 9741/10000\n",
            "Step 0: Train Loss: 5.651591195032779e-08, Train Acc: 1.0\n",
            "Epoch 9742/10000\n",
            "Step 0: Train Loss: 4.775455053618316e-08, Train Acc: 1.0\n",
            "Epoch 9743/10000\n",
            "Step 0: Train Loss: 6.39544808223036e-08, Train Acc: 1.0\n",
            "Epoch 9744/10000\n",
            "Step 0: Train Loss: 4.540665088370588e-08, Train Acc: 1.0\n",
            "Epoch 9745/10000\n",
            "Step 0: Train Loss: 5.365579980320945e-08, Train Acc: 1.0\n",
            "Epoch 9746/10000\n",
            "Step 0: Train Loss: 5.887702414497653e-08, Train Acc: 1.0\n",
            "Epoch 9747/10000\n",
            "Step 0: Train Loss: 7.181019867630312e-08, Train Acc: 1.0\n",
            "Epoch 9748/10000\n",
            "Step 0: Train Loss: 6.40977972921064e-08, Train Acc: 1.0\n",
            "Epoch 9749/10000\n",
            "Step 0: Train Loss: 6.70896227461526e-08, Train Acc: 1.0\n",
            "Epoch 9750/10000\n",
            "Step 0: Train Loss: 4.361850614031937e-08, Train Acc: 1.0\n",
            "Epoch 9751/10000\n",
            "Step 0: Train Loss: 4.261707786668012e-08, Train Acc: 1.0\n",
            "Epoch 9752/10000\n",
            "Step 0: Train Loss: 4.1818374540980585e-08, Train Acc: 1.0\n",
            "Epoch 9753/10000\n",
            "Step 0: Train Loss: 4.669366049370183e-08, Train Acc: 1.0\n",
            "Epoch 9754/10000\n",
            "Step 0: Train Loss: 5.782793621733617e-08, Train Acc: 1.0\n",
            "Epoch 9755/10000\n",
            "Step 0: Train Loss: 5.7720502155689246e-08, Train Acc: 1.0\n",
            "Epoch 9756/10000\n",
            "Step 0: Train Loss: 4.427405286833164e-08, Train Acc: 1.0\n",
            "Epoch 9757/10000\n",
            "Step 0: Train Loss: 4.041179124669725e-08, Train Acc: 1.0\n",
            "Epoch 9758/10000\n",
            "Step 0: Train Loss: 4.3356102708003164e-08, Train Acc: 1.0\n",
            "Epoch 9759/10000\n",
            "Step 0: Train Loss: 4.5525723635364557e-08, Train Acc: 1.0\n",
            "Epoch 9760/10000\n",
            "Step 0: Train Loss: 5.396559288328717e-08, Train Acc: 1.0\n",
            "Epoch 9761/10000\n",
            "Step 0: Train Loss: 5.3309893388586715e-08, Train Acc: 1.0\n",
            "Epoch 9762/10000\n",
            "Step 0: Train Loss: 5.608751152408331e-08, Train Acc: 1.0\n",
            "Epoch 9763/10000\n",
            "Step 0: Train Loss: 7.949890346026223e-08, Train Acc: 1.0\n",
            "Epoch 9764/10000\n",
            "Step 0: Train Loss: 3.3128035425988855e-08, Train Acc: 1.0\n",
            "Epoch 9765/10000\n",
            "Step 0: Train Loss: 7.27641449316252e-08, Train Acc: 1.0\n",
            "Epoch 9766/10000\n",
            "Step 0: Train Loss: 5.820943016487945e-08, Train Acc: 1.0\n",
            "Epoch 9767/10000\n",
            "Step 0: Train Loss: 4.2259454602344704e-08, Train Acc: 1.0\n",
            "Epoch 9768/10000\n",
            "Step 0: Train Loss: 3.749109112050064e-08, Train Acc: 1.0\n",
            "Epoch 9769/10000\n",
            "Step 0: Train Loss: 4.6562728783783314e-08, Train Acc: 1.0\n",
            "Epoch 9770/10000\n",
            "Step 0: Train Loss: 4.769523087588823e-08, Train Acc: 1.0\n",
            "Epoch 9771/10000\n",
            "Step 0: Train Loss: 4.2056889526520536e-08, Train Acc: 1.0\n",
            "Epoch 9772/10000\n",
            "Step 0: Train Loss: 4.168733269693803e-08, Train Acc: 1.0\n",
            "Epoch 9773/10000\n",
            "Step 0: Train Loss: 6.135594787792797e-08, Train Acc: 1.0\n",
            "Epoch 9774/10000\n",
            "Step 0: Train Loss: 4.0054128902511366e-08, Train Acc: 1.0\n",
            "Epoch 9775/10000\n",
            "Step 0: Train Loss: 4.556158117452469e-08, Train Acc: 1.0\n",
            "Epoch 9776/10000\n",
            "Step 0: Train Loss: 3.2937446547975924e-08, Train Acc: 1.0\n",
            "Epoch 9777/10000\n",
            "Step 0: Train Loss: 3.5750652216393064e-08, Train Acc: 1.0\n",
            "Epoch 9778/10000\n",
            "Step 0: Train Loss: 3.758655964247737e-08, Train Acc: 1.0\n",
            "Epoch 9779/10000\n",
            "Step 0: Train Loss: 4.333233860620567e-08, Train Acc: 1.0\n",
            "Epoch 9780/10000\n",
            "Step 0: Train Loss: 5.257095736510564e-08, Train Acc: 1.0\n",
            "Epoch 9781/10000\n",
            "Step 0: Train Loss: 4.382102858357939e-08, Train Acc: 1.0\n",
            "Epoch 9782/10000\n",
            "Step 0: Train Loss: 4.963830591009355e-08, Train Acc: 1.0\n",
            "Epoch 9783/10000\n",
            "Step 0: Train Loss: 4.026869149242884e-08, Train Acc: 1.0\n",
            "Epoch 9784/10000\n",
            "Step 0: Train Loss: 6.462243362648223e-08, Train Acc: 1.0\n",
            "Epoch 9785/10000\n",
            "Step 0: Train Loss: 5.025803062608247e-08, Train Acc: 1.0\n",
            "Epoch 9786/10000\n",
            "Step 0: Train Loss: 5.294038984970939e-08, Train Acc: 1.0\n",
            "Epoch 9787/10000\n",
            "Step 0: Train Loss: 3.45109043564662e-08, Train Acc: 1.0\n",
            "Epoch 9788/10000\n",
            "Step 0: Train Loss: 5.655235213453125e-08, Train Acc: 1.0\n",
            "Epoch 9789/10000\n",
            "Step 0: Train Loss: 5.1938563672138116e-08, Train Acc: 1.0\n",
            "Epoch 9790/10000\n",
            "Step 0: Train Loss: 5.5789218578183863e-08, Train Acc: 1.0\n",
            "Epoch 9791/10000\n",
            "Step 0: Train Loss: 5.514582213095309e-08, Train Acc: 1.0\n",
            "Epoch 9792/10000\n",
            "Step 0: Train Loss: 4.551381493911322e-08, Train Acc: 1.0\n",
            "Epoch 9793/10000\n",
            "Step 0: Train Loss: 4.535887043743969e-08, Train Acc: 1.0\n",
            "Epoch 9794/10000\n",
            "Step 0: Train Loss: 4.6539092579678254e-08, Train Acc: 1.0\n",
            "Epoch 9795/10000\n",
            "Step 0: Train Loss: 5.751751075422362e-08, Train Acc: 1.0\n",
            "Epoch 9796/10000\n",
            "Step 0: Train Loss: 5.6098901524137545e-08, Train Acc: 1.0\n",
            "Epoch 9797/10000\n",
            "Step 0: Train Loss: 4.1115114868262026e-08, Train Acc: 1.0\n",
            "Epoch 9798/10000\n",
            "Step 0: Train Loss: 4.0054100480801935e-08, Train Acc: 1.0\n",
            "Epoch 9799/10000\n",
            "Step 0: Train Loss: 3.8015645742461857e-08, Train Acc: 1.0\n",
            "Epoch 9800/10000\n",
            "Step 0: Train Loss: 4.7921847823317876e-08, Train Acc: 1.0\n",
            "Epoch 9801/10000\n",
            "Step 0: Train Loss: 4.29269455537451e-08, Train Acc: 1.0\n",
            "Epoch 9802/10000\n",
            "Step 0: Train Loss: 4.9459341511237653e-08, Train Acc: 1.0\n",
            "Epoch 9803/10000\n",
            "Step 0: Train Loss: 4.286740562520208e-08, Train Acc: 1.0\n",
            "Epoch 9804/10000\n",
            "Step 0: Train Loss: 4.001842413003942e-08, Train Acc: 1.0\n",
            "Epoch 9805/10000\n",
            "Step 0: Train Loss: 4.261710628838955e-08, Train Acc: 1.0\n",
            "Epoch 9806/10000\n",
            "Step 0: Train Loss: 3.645397228524416e-08, Train Acc: 1.0\n",
            "Epoch 9807/10000\n",
            "Step 0: Train Loss: 4.533496777980872e-08, Train Acc: 1.0\n",
            "Epoch 9808/10000\n",
            "Step 0: Train Loss: 3.7836812794012076e-08, Train Acc: 1.0\n",
            "Epoch 9809/10000\n",
            "Step 0: Train Loss: 3.825405769930512e-08, Train Acc: 1.0\n",
            "Epoch 9810/10000\n",
            "Step 0: Train Loss: 4.041179479941093e-08, Train Acc: 1.0\n",
            "Epoch 9811/10000\n",
            "Step 0: Train Loss: 3.786070834621569e-08, Train Acc: 1.0\n",
            "Epoch 9812/10000\n",
            "Step 0: Train Loss: 4.252173368968215e-08, Train Acc: 1.0\n",
            "Epoch 9813/10000\n",
            "Step 0: Train Loss: 4.444098067324376e-08, Train Acc: 1.0\n",
            "Epoch 9814/10000\n",
            "Step 0: Train Loss: 4.2700612823409756e-08, Train Acc: 1.0\n",
            "Epoch 9815/10000\n",
            "Step 0: Train Loss: 5.013905379769312e-08, Train Acc: 1.0\n",
            "Epoch 9816/10000\n",
            "Step 0: Train Loss: 5.685003756639162e-08, Train Acc: 1.0\n",
            "Epoch 9817/10000\n",
            "Step 0: Train Loss: 4.868454439588277e-08, Train Acc: 1.0\n",
            "Epoch 9818/10000\n",
            "Step 0: Train Loss: 2.5856378371713618e-08, Train Acc: 1.0\n",
            "Epoch 9819/10000\n",
            "Step 0: Train Loss: 4.026866662343309e-08, Train Acc: 1.0\n",
            "Epoch 9820/10000\n",
            "Step 0: Train Loss: 4.3260339310791096e-08, Train Acc: 1.0\n",
            "Epoch 9821/10000\n",
            "Step 0: Train Loss: 4.887557736310555e-08, Train Acc: 1.0\n",
            "Epoch 9822/10000\n",
            "Step 0: Train Loss: 3.141155602293111e-08, Train Acc: 1.0\n",
            "Epoch 9823/10000\n",
            "Step 0: Train Loss: 4.09004599077889e-08, Train Acc: 1.0\n",
            "Epoch 9824/10000\n",
            "Step 0: Train Loss: 3.571497941834423e-08, Train Acc: 1.0\n",
            "Epoch 9825/10000\n",
            "Step 0: Train Loss: 3.316383967444381e-08, Train Acc: 1.0\n",
            "Epoch 9826/10000\n",
            "Step 0: Train Loss: 3.2651303882857974e-08, Train Acc: 1.0\n",
            "Epoch 9827/10000\n",
            "Step 0: Train Loss: 4.179407397941759e-08, Train Acc: 1.0\n",
            "Epoch 9828/10000\n",
            "Step 0: Train Loss: 4.514426876767175e-08, Train Acc: 1.0\n",
            "Epoch 9829/10000\n",
            "Step 0: Train Loss: 3.485664024083235e-08, Train Acc: 1.0\n",
            "Epoch 9830/10000\n",
            "Step 0: Train Loss: 3.7288494070253364e-08, Train Acc: 1.0\n",
            "Epoch 9831/10000\n",
            "Step 0: Train Loss: 3.6203662290290595e-08, Train Acc: 1.0\n",
            "Epoch 9832/10000\n",
            "Step 0: Train Loss: 4.153228161385414e-08, Train Acc: 1.0\n",
            "Epoch 9833/10000\n",
            "Step 0: Train Loss: 4.261660535576084e-08, Train Acc: 1.0\n",
            "Epoch 9834/10000\n",
            "Step 0: Train Loss: 4.8720451673034404e-08, Train Acc: 1.0\n",
            "Epoch 9835/10000\n",
            "Step 0: Train Loss: 4.08647302663212e-08, Train Acc: 1.0\n",
            "Epoch 9836/10000\n",
            "Step 0: Train Loss: 4.1198468636594043e-08, Train Acc: 1.0\n",
            "Epoch 9837/10000\n",
            "Step 0: Train Loss: 3.5381159335656776e-08, Train Acc: 1.0\n",
            "Epoch 9838/10000\n",
            "Step 0: Train Loss: 5.2809348005666834e-08, Train Acc: 1.0\n",
            "Epoch 9839/10000\n",
            "Step 0: Train Loss: 4.3582385700347004e-08, Train Acc: 1.0\n",
            "Epoch 9840/10000\n",
            "Step 0: Train Loss: 3.802759351856366e-08, Train Acc: 1.0\n",
            "Epoch 9841/10000\n",
            "Step 0: Train Loss: 3.751495825099482e-08, Train Acc: 1.0\n",
            "Epoch 9842/10000\n",
            "Step 0: Train Loss: 3.826602323897532e-08, Train Acc: 1.0\n",
            "Epoch 9843/10000\n",
            "Step 0: Train Loss: 5.841163286390838e-08, Train Acc: 1.0\n",
            "Epoch 9844/10000\n",
            "Step 0: Train Loss: 4.0352031049906145e-08, Train Acc: 1.0\n",
            "Epoch 9845/10000\n",
            "Step 0: Train Loss: 3.764615641443925e-08, Train Acc: 1.0\n",
            "Epoch 9846/10000\n",
            "Step 0: Train Loss: 2.9814163582386755e-08, Train Acc: 1.0\n",
            "Epoch 9847/10000\n",
            "Step 0: Train Loss: 4.098384209783035e-08, Train Acc: 1.0\n",
            "Epoch 9848/10000\n",
            "Step 0: Train Loss: 3.139963666853873e-08, Train Acc: 1.0\n",
            "Epoch 9849/10000\n",
            "Step 0: Train Loss: 3.1506882436360684e-08, Train Acc: 1.0\n",
            "Epoch 9850/10000\n",
            "Step 0: Train Loss: 3.108965174192235e-08, Train Acc: 1.0\n",
            "Epoch 9851/10000\n",
            "Step 0: Train Loss: 5.0627214420728706e-08, Train Acc: 1.0\n",
            "Epoch 9852/10000\n",
            "Step 0: Train Loss: 4.409529097415543e-08, Train Acc: 1.0\n",
            "Epoch 9853/10000\n",
            "Step 0: Train Loss: 4.2783721454497936e-08, Train Acc: 1.0\n",
            "Epoch 9854/10000\n",
            "Step 0: Train Loss: 3.700235851056277e-08, Train Acc: 1.0\n",
            "Epoch 9855/10000\n",
            "Step 0: Train Loss: 3.275859228324407e-08, Train Acc: 1.0\n",
            "Epoch 9856/10000\n",
            "Step 0: Train Loss: 3.879048549038089e-08, Train Acc: 1.0\n",
            "Epoch 9857/10000\n",
            "Step 0: Train Loss: 4.1031384512280056e-08, Train Acc: 1.0\n",
            "Epoch 9858/10000\n",
            "Step 0: Train Loss: 3.414137594859312e-08, Train Acc: 1.0\n",
            "Epoch 9859/10000\n",
            "Step 0: Train Loss: 3.741924103906058e-08, Train Acc: 1.0\n",
            "Epoch 9860/10000\n",
            "Step 0: Train Loss: 3.8313668682121715e-08, Train Acc: 1.0\n",
            "Epoch 9861/10000\n",
            "Step 0: Train Loss: 3.086317335032618e-08, Train Acc: 1.0\n",
            "Epoch 9862/10000\n",
            "Step 0: Train Loss: 2.80260348262118e-08, Train Acc: 1.0\n",
            "Epoch 9863/10000\n",
            "Step 0: Train Loss: 2.5963725391875414e-08, Train Acc: 1.0\n",
            "Epoch 9864/10000\n",
            "Step 0: Train Loss: 3.4558603090317774e-08, Train Acc: 1.0\n",
            "Epoch 9865/10000\n",
            "Step 0: Train Loss: 4.031616995803233e-08, Train Acc: 1.0\n",
            "Epoch 9866/10000\n",
            "Step 0: Train Loss: 4.104338913180072e-08, Train Acc: 1.0\n",
            "Epoch 9867/10000\n",
            "Step 0: Train Loss: 4.8994330370533135e-08, Train Acc: 1.0\n",
            "Epoch 9868/10000\n",
            "Step 0: Train Loss: 3.846840712640187e-08, Train Acc: 1.0\n",
            "Epoch 9869/10000\n",
            "Step 0: Train Loss: 3.2496334512188696e-08, Train Acc: 1.0\n",
            "Epoch 9870/10000\n",
            "Step 0: Train Loss: 3.611988219631712e-08, Train Acc: 1.0\n",
            "Epoch 9871/10000\n",
            "Step 0: Train Loss: 3.336648290996891e-08, Train Acc: 1.0\n",
            "Epoch 9872/10000\n",
            "Step 0: Train Loss: 2.9289523695297248e-08, Train Acc: 1.0\n",
            "Epoch 9873/10000\n",
            "Step 0: Train Loss: 3.113729007964139e-08, Train Acc: 1.0\n",
            "Epoch 9874/10000\n",
            "Step 0: Train Loss: 4.621705684826338e-08, Train Acc: 1.0\n",
            "Epoch 9875/10000\n",
            "Step 0: Train Loss: 4.168729006437388e-08, Train Acc: 1.0\n",
            "Epoch 9876/10000\n",
            "Step 0: Train Loss: 3.892145628014987e-08, Train Acc: 1.0\n",
            "Epoch 9877/10000\n",
            "Step 0: Train Loss: 3.65732191198731e-08, Train Acc: 1.0\n",
            "Epoch 9878/10000\n",
            "Step 0: Train Loss: 3.476123211498816e-08, Train Acc: 1.0\n",
            "Epoch 9879/10000\n",
            "Step 0: Train Loss: 3.687107508199006e-08, Train Acc: 1.0\n",
            "Epoch 9880/10000\n",
            "Step 0: Train Loss: 4.779015938538578e-08, Train Acc: 1.0\n",
            "Epoch 9881/10000\n",
            "Step 0: Train Loss: 3.261548187083463e-08, Train Acc: 1.0\n",
            "Epoch 9882/10000\n",
            "Step 0: Train Loss: 3.452284147442697e-08, Train Acc: 1.0\n",
            "Epoch 9883/10000\n",
            "Step 0: Train Loss: 3.0422075525393666e-08, Train Acc: 1.0\n",
            "Epoch 9884/10000\n",
            "Step 0: Train Loss: 3.044591423417842e-08, Train Acc: 1.0\n",
            "Epoch 9885/10000\n",
            "Step 0: Train Loss: 4.0554503755174665e-08, Train Acc: 1.0\n",
            "Epoch 9886/10000\n",
            "Step 0: Train Loss: 2.6631264304910474e-08, Train Acc: 1.0\n",
            "Epoch 9887/10000\n",
            "Step 0: Train Loss: 2.8014051522973205e-08, Train Acc: 1.0\n",
            "Epoch 9888/10000\n",
            "Step 0: Train Loss: 3.399838632844876e-08, Train Acc: 1.0\n",
            "Epoch 9889/10000\n",
            "Step 0: Train Loss: 4.109091378268204e-08, Train Acc: 1.0\n",
            "Epoch 9890/10000\n",
            "Step 0: Train Loss: 3.453477503967406e-08, Train Acc: 1.0\n",
            "Epoch 9891/10000\n",
            "Step 0: Train Loss: 3.09465768566497e-08, Train Acc: 1.0\n",
            "Epoch 9892/10000\n",
            "Step 0: Train Loss: 2.4473626680787675e-08, Train Acc: 1.0\n",
            "Epoch 9893/10000\n",
            "Step 0: Train Loss: 4.4619465455753016e-08, Train Acc: 1.0\n",
            "Epoch 9894/10000\n",
            "Step 0: Train Loss: 2.5904123290843017e-08, Train Acc: 1.0\n",
            "Epoch 9895/10000\n",
            "Step 0: Train Loss: 2.856245195914653e-08, Train Acc: 1.0\n",
            "Epoch 9896/10000\n",
            "Step 0: Train Loss: 2.8884299396736424e-08, Train Acc: 1.0\n",
            "Epoch 9897/10000\n",
            "Step 0: Train Loss: 2.9480364815981375e-08, Train Acc: 1.0\n",
            "Epoch 9898/10000\n",
            "Step 0: Train Loss: 3.6406003545153e-08, Train Acc: 1.0\n",
            "Epoch 9899/10000\n",
            "Step 0: Train Loss: 3.763392442124314e-08, Train Acc: 1.0\n",
            "Epoch 9900/10000\n",
            "Step 0: Train Loss: 3.728818143144963e-08, Train Acc: 1.0\n",
            "Epoch 9901/10000\n",
            "Step 0: Train Loss: 3.1685715384810464e-08, Train Acc: 1.0\n",
            "Epoch 9902/10000\n",
            "Step 0: Train Loss: 4.3725648879444634e-08, Train Acc: 1.0\n",
            "Epoch 9903/10000\n",
            "Step 0: Train Loss: 2.511730912146959e-08, Train Acc: 1.0\n",
            "Epoch 9904/10000\n",
            "Step 0: Train Loss: 3.443938112468459e-08, Train Acc: 1.0\n",
            "Epoch 9905/10000\n",
            "Step 0: Train Loss: 2.9861844552669936e-08, Train Acc: 1.0\n",
            "Epoch 9906/10000\n",
            "Step 0: Train Loss: 2.1898673097098253e-08, Train Acc: 1.0\n",
            "Epoch 9907/10000\n",
            "Step 0: Train Loss: 3.582192320550348e-08, Train Acc: 1.0\n",
            "Epoch 9908/10000\n",
            "Step 0: Train Loss: 3.012409877101163e-08, Train Acc: 1.0\n",
            "Epoch 9909/10000\n",
            "Step 0: Train Loss: 4.163934264056479e-08, Train Acc: 1.0\n",
            "Epoch 9910/10000\n",
            "Step 0: Train Loss: 2.5927766600375435e-08, Train Acc: 1.0\n",
            "Epoch 9911/10000\n",
            "Step 0: Train Loss: 2.793062670036761e-08, Train Acc: 1.0\n",
            "Epoch 9912/10000\n",
            "Step 0: Train Loss: 2.8490912740153362e-08, Train Acc: 1.0\n",
            "Epoch 9913/10000\n",
            "Step 0: Train Loss: 2.6607448688764634e-08, Train Acc: 1.0\n",
            "Epoch 9914/10000\n",
            "Step 0: Train Loss: 3.392682401681668e-08, Train Acc: 1.0\n",
            "Epoch 9915/10000\n",
            "Step 0: Train Loss: 2.614251570776105e-08, Train Acc: 1.0\n",
            "Epoch 9916/10000\n",
            "Step 0: Train Loss: 2.9516108668303787e-08, Train Acc: 1.0\n",
            "Epoch 9917/10000\n",
            "Step 0: Train Loss: 3.205522247640147e-08, Train Acc: 1.0\n",
            "Epoch 9918/10000\n",
            "Step 0: Train Loss: 2.5081515531155674e-08, Train Acc: 1.0\n",
            "Epoch 9919/10000\n",
            "Step 0: Train Loss: 3.610813337218133e-08, Train Acc: 1.0\n",
            "Epoch 9920/10000\n",
            "Step 0: Train Loss: 4.442864920406464e-08, Train Acc: 1.0\n",
            "Epoch 9921/10000\n",
            "Step 0: Train Loss: 2.15649116341865e-08, Train Acc: 1.0\n",
            "Epoch 9922/10000\n",
            "Step 0: Train Loss: 3.2782267567199597e-08, Train Acc: 1.0\n",
            "Epoch 9923/10000\n",
            "Step 0: Train Loss: 2.8931859574754526e-08, Train Acc: 1.0\n",
            "Epoch 9924/10000\n",
            "Step 0: Train Loss: 2.2137118804721467e-08, Train Acc: 1.0\n",
            "Epoch 9925/10000\n",
            "Step 0: Train Loss: 3.391471636859933e-08, Train Acc: 1.0\n",
            "Epoch 9926/10000\n",
            "Step 0: Train Loss: 3.387913594110614e-08, Train Acc: 1.0\n",
            "Epoch 9927/10000\n",
            "Step 0: Train Loss: 3.7288309329142066e-08, Train Acc: 1.0\n",
            "Epoch 9928/10000\n",
            "Step 0: Train Loss: 2.6833877342369306e-08, Train Acc: 1.0\n",
            "Epoch 9929/10000\n",
            "Step 0: Train Loss: 3.5428605826837156e-08, Train Acc: 1.0\n",
            "Epoch 9930/10000\n",
            "Step 0: Train Loss: 3.477305199339753e-08, Train Acc: 1.0\n",
            "Epoch 9931/10000\n",
            "Step 0: Train Loss: 3.737189047114953e-08, Train Acc: 1.0\n",
            "Epoch 9932/10000\n",
            "Step 0: Train Loss: 3.012403482216541e-08, Train Acc: 1.0\n",
            "Epoch 9933/10000\n",
            "Step 0: Train Loss: 3.372417367586422e-08, Train Acc: 1.0\n",
            "Epoch 9934/10000\n",
            "Step 0: Train Loss: 3.1935812216943305e-08, Train Acc: 1.0\n",
            "Epoch 9935/10000\n",
            "Step 0: Train Loss: 2.5546466275727653e-08, Train Acc: 1.0\n",
            "Epoch 9936/10000\n",
            "Step 0: Train Loss: 2.9623237196574337e-08, Train Acc: 1.0\n",
            "Epoch 9937/10000\n",
            "Step 0: Train Loss: 2.228016882099837e-08, Train Acc: 1.0\n",
            "Epoch 9938/10000\n",
            "Step 0: Train Loss: 2.9241796539736242e-08, Train Acc: 1.0\n",
            "Epoch 9939/10000\n",
            "Step 0: Train Loss: 2.8788944561597418e-08, Train Acc: 1.0\n",
            "Epoch 9940/10000\n",
            "Step 0: Train Loss: 3.296118933349135e-08, Train Acc: 1.0\n",
            "Epoch 9941/10000\n",
            "Step 0: Train Loss: 2.67028017475468e-08, Train Acc: 1.0\n",
            "Epoch 9942/10000\n",
            "Step 0: Train Loss: 1.9276091833830833e-08, Train Acc: 1.0\n",
            "Epoch 9943/10000\n",
            "Step 0: Train Loss: 2.2590107562336925e-08, Train Acc: 1.0\n",
            "Epoch 9944/10000\n",
            "Step 0: Train Loss: 2.341261939875494e-08, Train Acc: 1.0\n",
            "Epoch 9945/10000\n",
            "Step 0: Train Loss: 3.2996975818377905e-08, Train Acc: 1.0\n",
            "Epoch 9946/10000\n",
            "Step 0: Train Loss: 3.832547079696269e-08, Train Acc: 1.0\n",
            "Epoch 9947/10000\n",
            "Step 0: Train Loss: 2.4068272708177574e-08, Train Acc: 1.0\n",
            "Epoch 9948/10000\n",
            "Step 0: Train Loss: 3.702624340462535e-08, Train Acc: 1.0\n",
            "Epoch 9949/10000\n",
            "Step 0: Train Loss: 2.4092148720455953e-08, Train Acc: 1.0\n",
            "Epoch 9950/10000\n",
            "Step 0: Train Loss: 1.7929053797161032e-08, Train Acc: 1.0\n",
            "Epoch 9951/10000\n",
            "Step 0: Train Loss: 1.937146976160875e-08, Train Acc: 1.0\n",
            "Epoch 9952/10000\n",
            "Step 0: Train Loss: 2.6571646216666522e-08, Train Acc: 1.0\n",
            "Epoch 9953/10000\n",
            "Step 0: Train Loss: 2.7060409024670662e-08, Train Acc: 1.0\n",
            "Epoch 9954/10000\n",
            "Step 0: Train Loss: 3.2245850434264867e-08, Train Acc: 1.0\n",
            "Epoch 9955/10000\n",
            "Step 0: Train Loss: 2.2268217492182885e-08, Train Acc: 1.0\n",
            "Epoch 9956/10000\n",
            "Step 0: Train Loss: 2.7525221213409168e-08, Train Acc: 1.0\n",
            "Epoch 9957/10000\n",
            "Step 0: Train Loss: 2.341261939875494e-08, Train Acc: 1.0\n",
            "Epoch 9958/10000\n",
            "Step 0: Train Loss: 2.3508007984673895e-08, Train Acc: 1.0\n",
            "Epoch 9959/10000\n",
            "Step 0: Train Loss: 2.4711997781423634e-08, Train Acc: 1.0\n",
            "Epoch 9960/10000\n",
            "Step 0: Train Loss: 1.95383673684546e-08, Train Acc: 1.0\n",
            "Epoch 9961/10000\n",
            "Step 0: Train Loss: 2.7906803978794414e-08, Train Acc: 1.0\n",
            "Epoch 9962/10000\n",
            "Step 0: Train Loss: 2.5236531087102776e-08, Train Acc: 1.0\n",
            "Epoch 9963/10000\n",
            "Step 0: Train Loss: 2.9015296831857995e-08, Train Acc: 1.0\n",
            "Epoch 9964/10000\n",
            "Step 0: Train Loss: 2.7334555952052142e-08, Train Acc: 1.0\n",
            "Epoch 9965/10000\n",
            "Step 0: Train Loss: 2.8300073395826075e-08, Train Acc: 1.0\n",
            "Epoch 9966/10000\n",
            "Step 0: Train Loss: 2.3615287503275795e-08, Train Acc: 1.0\n",
            "Epoch 9967/10000\n",
            "Step 0: Train Loss: 2.5904109079988302e-08, Train Acc: 1.0\n",
            "Epoch 9968/10000\n",
            "Step 0: Train Loss: 2.549875688373504e-08, Train Acc: 1.0\n",
            "Epoch 9969/10000\n",
            "Step 0: Train Loss: 2.6178270218224498e-08, Train Acc: 1.0\n",
            "Epoch 9970/10000\n",
            "Step 0: Train Loss: 2.8180979327885325e-08, Train Acc: 1.0\n",
            "Epoch 9971/10000\n",
            "Step 0: Train Loss: 2.800204157438202e-08, Train Acc: 1.0\n",
            "Epoch 9972/10000\n",
            "Step 0: Train Loss: 2.672661203462212e-08, Train Acc: 1.0\n",
            "Epoch 9973/10000\n",
            "Step 0: Train Loss: 2.3245730673693288e-08, Train Acc: 1.0\n",
            "Epoch 9974/10000\n",
            "Step 0: Train Loss: 2.4950411514623738e-08, Train Acc: 1.0\n",
            "Epoch 9975/10000\n",
            "Step 0: Train Loss: 2.5689404381523673e-08, Train Acc: 1.0\n",
            "Epoch 9976/10000\n",
            "Step 0: Train Loss: 2.892002726184728e-08, Train Acc: 1.0\n",
            "Epoch 9977/10000\n",
            "Step 0: Train Loss: 2.0992702687294695e-08, Train Acc: 1.0\n",
            "Epoch 9978/10000\n",
            "Step 0: Train Loss: 2.4461664693831153e-08, Train Acc: 1.0\n",
            "Epoch 9979/10000\n",
            "Step 0: Train Loss: 2.23039524627211e-08, Train Acc: 1.0\n",
            "Epoch 9980/10000\n",
            "Step 0: Train Loss: 2.800206999609145e-08, Train Acc: 1.0\n",
            "Epoch 9981/10000\n",
            "Step 0: Train Loss: 1.690385253994009e-08, Train Acc: 1.0\n",
            "Epoch 9982/10000\n",
            "Step 0: Train Loss: 3.434393391898993e-08, Train Acc: 1.0\n",
            "Epoch 9983/10000\n",
            "Step 0: Train Loss: 2.5069599729476977e-08, Train Acc: 1.0\n",
            "Epoch 9984/10000\n",
            "Step 0: Train Loss: 2.2935774168786338e-08, Train Acc: 1.0\n",
            "Epoch 9985/10000\n",
            "Step 0: Train Loss: 1.958603057516939e-08, Train Acc: 1.0\n",
            "Epoch 9986/10000\n",
            "Step 0: Train Loss: 2.6082881632305543e-08, Train Acc: 1.0\n",
            "Epoch 9987/10000\n",
            "Step 0: Train Loss: 2.9873753248921275e-08, Train Acc: 1.0\n",
            "Epoch 9988/10000\n",
            "Step 0: Train Loss: 2.1409960027085617e-08, Train Acc: 1.0\n",
            "Epoch 9989/10000\n",
            "Step 0: Train Loss: 3.156639749590795e-08, Train Acc: 1.0\n",
            "Epoch 9990/10000\n",
            "Step 0: Train Loss: 1.7249561778953648e-08, Train Acc: 1.0\n",
            "Epoch 9991/10000\n",
            "Step 0: Train Loss: 2.5081549281935622e-08, Train Acc: 1.0\n",
            "Epoch 9992/10000\n",
            "Step 0: Train Loss: 2.0611246043245046e-08, Train Acc: 1.0\n",
            "Epoch 9993/10000\n",
            "Step 0: Train Loss: 2.827624534518236e-08, Train Acc: 1.0\n",
            "Epoch 9994/10000\n",
            "Step 0: Train Loss: 2.511727537068964e-08, Train Acc: 1.0\n",
            "Epoch 9995/10000\n",
            "Step 0: Train Loss: 3.0386210880806175e-08, Train Acc: 1.0\n",
            "Epoch 9996/10000\n",
            "Step 0: Train Loss: 2.1755603540896118e-08, Train Acc: 1.0\n",
            "Epoch 9997/10000\n",
            "Step 0: Train Loss: 2.0110554999064334e-08, Train Acc: 1.0\n",
            "Epoch 9998/10000\n",
            "Step 0: Train Loss: 3.328300834937181e-08, Train Acc: 1.0\n",
            "Epoch 9999/10000\n",
            "Step 0: Train Loss: 2.0933107691689656e-08, Train Acc: 1.0\n",
            "Epoch 10000/10000\n",
            "Step 0: Train Loss: 2.4163631096030258e-08, Train Acc: 1.0\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'results' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m train_with_mi(model, train_dataset, test_dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmi_all_relu\u001b[39m\u001b[38;5;124m'\u001b[39m, results)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
          ]
        }
      ],
      "source": [
        "result = train_with_mi(model, train_dataset, test_dataset, epochs=10000, batch_size=128)\n",
        "results.append(result)\n",
        "\n",
        "np.save('mi_all_relu', results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([[13.24054035,  7.65960814,  5.91213722,  0.84694817],\n",
            "       [13.2178642 ,  7.86259113,  5.14114638,  3.05101336],\n",
            "       [13.16236175,  8.19987025,  5.87589755,  2.91110057],\n",
            "       [13.10679908,  8.86629176,  5.55292568,  3.06828084],\n",
            "       [13.0779017 ,  8.60797612,  5.24362645,  3.29390698],\n",
            "       [13.0803462 ,  8.69201213,  5.9041479 ,  3.21299038],\n",
            "       [12.9921747 ,  9.98404476,  7.53568816,  5.1295381 ],\n",
            "       [12.99044486, 10.00115323,  7.50872076,  5.0938365 ],\n",
            "       [12.99309655, 10.01482947,  7.505043  ,  5.07555474],\n",
            "       [12.98976871,  9.98723546,  7.59688869,  5.13333724],\n",
            "       [12.98921639,  9.98653146,  7.62050891,  5.19718784],\n",
            "       [12.98815135,  9.98597747,  7.6070397 ,  5.18680967],\n",
            "       [12.92465694, 10.14905226,  7.84027596,  5.24892198],\n",
            "       [12.92311623, 10.16298591,  7.84763406,  5.24268528],\n",
            "       [12.92374721, 10.2039414 ,  7.83785235,  5.24611415],\n",
            "       [12.92581462, 10.22883332,  7.82625365,  5.26555664],\n",
            "       [12.92528358, 10.20891456,  7.84818409,  5.28900014],\n",
            "       [12.9254848 , 10.19237996,  7.84987242,  5.27841912],\n",
            "       [12.9553203 , 10.28267983,  7.87034362,  5.23979388],\n",
            "       [12.95415164, 10.27922917,  7.87591472,  5.24272238],\n",
            "       [12.95337327, 10.27961352,  7.87655244,  5.2459851 ],\n",
            "       [12.95117823, 10.27824546,  7.87024136,  5.24256034],\n",
            "       [12.95149327, 10.27777146,  7.87626197,  5.24408378],\n",
            "       [12.95225299, 10.27868069,  7.87760982,  5.24916639],\n",
            "       [12.93708673, 10.13370934,  7.91846165,  5.10637684],\n",
            "       [12.93750885, 10.13375922,  7.91823242,  5.10627324],\n",
            "       [12.93770885, 10.13338269,  7.919377  ,  5.10614438],\n",
            "       [12.93770885, 10.13344127,  7.91854724,  5.10632284],\n",
            "       [12.93770885, 10.13395503,  7.918175  ,  5.1062229 ],\n",
            "       [12.93793532, 10.1340386 ,  7.91893129,  5.10657243],\n",
            "       [12.93380579, 10.09249935,  7.96274831,  5.04216588],\n",
            "       [12.93399891, 10.0914964 ,  7.96235324,  5.04218001],\n",
            "       [12.93394064, 10.09142082,  7.96221383,  5.04220673],\n",
            "       [12.93373666, 10.09268046,  7.961903  ,  5.04176135],\n",
            "       [12.93363666, 10.09252591,  7.96197944,  5.04066396],\n",
            "       [12.93379891, 10.09295773,  7.96166285,  5.04046235],\n",
            "       [12.92654043, 10.06393609,  7.97541495,  5.00277682],\n",
            "       [12.92640269, 10.06392047,  7.97533347,  5.00286013],\n",
            "       [12.92654043, 10.06374968,  7.97572893,  5.00254621],\n",
            "       [12.92620269, 10.06477959,  7.97538911,  5.00245507],\n",
            "       [12.92644043, 10.06551727,  7.97558739,  5.00259967],\n",
            "       [12.92644043, 10.06530729,  7.97567451,  5.00278782],\n",
            "       [12.92388493, 10.06232472,  7.9826379 ,  4.97619382],\n",
            "       [12.92388493, 10.06193789,  7.98228319,  4.97640729],\n",
            "       [12.92378493, 10.06178464,  7.98251826,  4.97676699],\n",
            "       [12.92348493, 10.06202867,  7.98236424,  4.97637272],\n",
            "       [12.92352268, 10.06209225,  7.98228683,  4.97624527],\n",
            "       [12.92352268, 10.06238977,  7.98216229,  4.97615339],\n",
            "       [12.92654879, 10.05045844,  7.99290724,  4.95052464],\n",
            "       [12.92651105, 10.05041744,  7.99304895,  4.95089895],\n",
            "       [12.92633056, 10.05060033,  7.99298498,  4.9509939 ],\n",
            "       [12.92647885, 10.05053128,  7.99257406,  4.95195701],\n",
            "       [12.92647885, 10.05086267,  7.99243806,  4.95191243],\n",
            "       [12.92647885, 10.05038932,  7.99292566,  4.95201462],\n",
            "       [12.918239  ,  9.86142095,  8.11821027,  4.54145688],\n",
            "       [12.91809997,  9.86213367,  8.11641265,  4.54186189],\n",
            "       [12.91786222,  9.86098186,  8.11684575,  4.54213672],\n",
            "       [12.91768174,  9.86268948,  8.11605268,  4.54344339],\n",
            "       [12.91760811,  9.86255797,  8.11701074,  4.54170081],\n",
            "       [12.91722172,  9.86241898,  8.1165178 ,  4.54280119],\n",
            "       [10.56633111,  7.49177542,  5.21696646,  2.5290021 ],\n",
            "       [10.56701775,  7.49200144,  5.21583845,  2.52815076],\n",
            "       [10.56494257,  7.49427801,  5.21419687,  2.52937583],\n",
            "       [10.56661653,  7.49486063,  5.21493471,  2.52966269],\n",
            "       [10.56565487,  7.49625449,  5.21388948,  2.52858212],\n",
            "       [10.56439785,  7.49596745,  5.21335592,  2.52933631],\n",
            "       [10.57416957,  7.63922692,  5.37513481,  2.6239776 ],\n",
            "       [10.57436725,  7.63913879,  5.37471789,  2.6242419 ],\n",
            "       [10.57421898,  7.63909214,  5.37467662,  2.624654  ],\n",
            "       [10.57431037,  7.63902509,  5.37507082,  2.62433251],\n",
            "       [10.57454133,  7.63936989,  5.37459663,  2.62410403],\n",
            "       [10.57488357,  7.63915006,  5.374743  ,  2.62345282],\n",
            "       [10.60284103,  7.71089953,  5.41189006,  2.9092802 ],\n",
            "       [10.60287878,  7.71209334,  5.4122185 ,  2.90923037],\n",
            "       [10.60293156,  7.71224742,  5.41195103,  2.90878843],\n",
            "       [10.60289847,  7.71237375,  5.41200116,  2.90847386],\n",
            "       [10.60289847,  7.71251397,  5.41203197,  2.90840662],\n",
            "       [10.60293156,  7.71207796,  5.4124714 ,  2.9092008 ],\n",
            "       [10.62388244,  7.79780288,  5.43945204,  3.00079996],\n",
            "       [10.62401362,  7.79731599,  5.43987707,  3.000585  ],\n",
            "       [10.6237517 ,  7.79745426,  5.43923165,  2.99791473],\n",
            "       [10.62377866,  7.79918858,  5.43742375,  2.99587748],\n",
            "       [10.62372408,  7.7992895 ,  5.43743642,  2.99650076],\n",
            "       [10.62379133,  7.79960841,  5.43974536,  2.99657781],\n",
            "       [10.6349371 ,  7.86745993,  5.46128977,  3.15644876],\n",
            "       [10.6349371 ,  7.86758131,  5.46156973,  3.1556832 ],\n",
            "       [10.63481383,  7.86709991,  5.46141029,  3.15599116],\n",
            "       [10.63418752,  7.86718963,  5.46108318,  3.15636812],\n",
            "       [10.63428618,  7.86696035,  5.46110116,  3.15696158],\n",
            "       [10.63428618,  7.86722668,  5.46091955,  3.15534265],\n",
            "       [10.63749599,  7.91364699,  5.46885768,  3.45380723],\n",
            "       [10.63749599,  7.91363592,  5.46815406,  3.45387489],\n",
            "       [10.63749599,  7.91326821,  5.46833545,  3.45360948],\n",
            "       [10.63758664,  7.91330696,  5.46817319,  3.45360948],\n",
            "       [10.63762001,  7.91310605,  5.46851408,  3.45365908],\n",
            "       [10.63762001,  7.91348228,  5.46801628,  3.45414858],\n",
            "       [10.65640491,  7.97088735,  5.4938943 ,  3.56196536],\n",
            "       [10.65648855,  7.97049995,  5.49405531,  3.56205061],\n",
            "       [10.65670389,  7.97077281,  5.49377001,  3.56189062],\n",
            "       [10.65670389,  7.97097328,  5.4927048 ,  3.56238692],\n",
            "       [10.65676934,  7.97092696,  5.49175419,  3.56203664],\n",
            "       [10.65693983,  7.96974098,  5.49182725,  3.56250249],\n",
            "       [10.69802578,  8.17572667,  5.62382331,  3.42272242],\n",
            "       [10.69848988,  8.17617726,  5.62463978,  3.42425509],\n",
            "       [10.69834173,  8.17929898,  5.62065366,  3.42568925],\n",
            "       [10.69935474,  8.18118899,  5.62657238,  3.4243739 ],\n",
            "       [10.69971214,  8.17775876,  5.62744587,  3.42083466],\n",
            "       [10.69954108,  8.17645452,  5.62715147,  3.42053612],\n",
            "       [10.07477336,  6.65545395,  5.27146382,  1.90727112],\n",
            "       [10.07366227,  6.65741929,  5.27085446,  1.90750098],\n",
            "       [10.07376061,  6.65898098,  5.26888508,  1.90722246],\n",
            "       [10.07307936,  6.65657767,  5.2681038 ,  1.90770771],\n",
            "       [10.07408007,  6.65638047,  5.26540512,  1.90737305],\n",
            "       [10.07497928,  6.65592725,  5.26515494,  1.90693767],\n",
            "       [10.14031055,  6.93495989,  5.43571619,  2.10533982],\n",
            "       [10.1402625 ,  6.93477844,  5.43629859,  2.10572783],\n",
            "       [10.14048641,  6.93507994,  5.43619559,  2.10510399],\n",
            "       [10.14024662,  6.93577754,  5.43592397,  2.10514645],\n",
            "       [10.14002685,  6.93590118,  5.43633518,  2.10485133],\n",
            "       [10.13979294,  6.9376148 ,  5.43687405,  2.10502425],\n",
            "       [10.18640995,  7.03658184,  5.36857837,  2.24953928],\n",
            "       [10.18630822,  7.03666929,  5.36865171,  2.24959337],\n",
            "       [10.18563417,  7.03607309,  5.36868572,  2.25030095],\n",
            "       [10.18437825,  7.03693733,  5.36830605,  2.25045124],\n",
            "       [10.18446459,  7.03697794,  5.36606758,  2.25056523],\n",
            "       [10.18496241,  7.03721203,  5.36719825,  2.25048142],\n",
            "       [10.23117798,  7.15097566,  5.38524848,  2.30766936],\n",
            "       [10.2316041 ,  7.15078569,  5.38514812,  2.30696382],\n",
            "       [10.23153813,  7.15137368,  5.38413054,  2.30708436],\n",
            "       [10.23127022,  7.1512985 ,  5.38434404,  2.30775025],\n",
            "       [10.23160989,  7.15038403,  5.38415089,  2.30904637],\n",
            "       [10.2318763 ,  7.15032677,  5.3840634 ,  2.30878162],\n",
            "       [10.25659068,  7.27262205,  5.42039515,  2.29421373],\n",
            "       [10.25746279,  7.2728356 ,  5.4207308 ,  2.2942372 ],\n",
            "       [10.25748843,  7.27254187,  5.42006456,  2.29330412],\n",
            "       [10.25638953,  7.27293715,  5.41989653,  2.29234704],\n",
            "       [10.25641812,  7.27328253,  5.4196146 ,  2.29268234],\n",
            "       [10.25587358,  7.27440758,  5.42055701,  2.29211016]]), array([[3.31178178, 2.47448246, 1.95750073, 1.43829452],\n",
            "       [3.30764404, 2.67957364, 2.33707841, 1.56217081],\n",
            "       [3.30257983, 2.81157673, 2.48905322, 1.84833606],\n",
            "       [3.29964208, 2.7736162 , 2.5373634 , 1.72109615],\n",
            "       [3.29818611, 2.81735818, 2.54092939, 1.77032751],\n",
            "       [3.28761799, 2.84508922, 2.56844118, 1.92255749],\n",
            "       [3.30531927, 3.22093571, 2.99411174, 2.54614642],\n",
            "       [3.30529006, 3.22327735, 2.99414407, 2.52652243],\n",
            "       [3.30661258, 3.22472206, 2.99685817, 2.50862207],\n",
            "       [3.30648377, 3.22470363, 3.00037666, 2.52789541],\n",
            "       [3.30501757, 3.22238661, 2.99545703, 2.54174092],\n",
            "       [3.30571757, 3.2214062 , 2.99670685, 2.53646301],\n",
            "       [3.30831757, 3.25368126, 3.11968344, 2.61206127],\n",
            "       [3.30827274, 3.25411915, 3.12251342, 2.63070723],\n",
            "       [3.30923709, 3.25360234, 3.12700754, 2.64683814],\n",
            "       [3.30900758, 3.25337737, 3.12565499, 2.64778641],\n",
            "       [3.3080616 , 3.25201972, 3.12486139, 2.64496274],\n",
            "       [3.3082608 , 3.25488182, 3.12256277, 2.6350501 ],\n",
            "       [3.3087616 , 3.2580516 , 3.14468333, 2.70262432],\n",
            "       [3.30842385, 3.25803001, 3.14530527, 2.70407877],\n",
            "       [3.30791757, 3.25813798, 3.14661129, 2.69986892],\n",
            "       [3.30781757, 3.25789453, 3.14536716, 2.70067151],\n",
            "       [3.3076659 , 3.258667  , 3.14473633, 2.70270006],\n",
            "       [3.3077659 , 3.25891644, 3.14566247, 2.70427939],\n",
            "       [3.30965143, 3.25775502, 3.16857484, 2.74400805],\n",
            "       [3.30965143, 3.25785502, 3.16870944, 2.74376545],\n",
            "       [3.30965143, 3.25795505, 3.1687339 , 2.74378771],\n",
            "       [3.30965143, 3.25803554, 3.16855974, 2.74343802],\n",
            "       [3.30965143, 3.25795505, 3.16857773, 2.74348102],\n",
            "       [3.30955143, 3.25787953, 3.16847611, 2.74348002],\n",
            "       [3.30945336, 3.25879135, 3.17974188, 2.74638628],\n",
            "       [3.30950838, 3.2585931 , 3.17972956, 2.74570358],\n",
            "       [3.30950838, 3.25866722, 3.1798507 , 2.74566363],\n",
            "       [3.30945336, 3.2587712 , 3.17964151, 2.74588764],\n",
            "       [3.30945336, 3.2586712 , 3.17929162, 2.74584178],\n",
            "       [3.30945336, 3.2586981 , 3.17915698, 2.74583167],\n",
            "       [3.31005112, 3.26109605, 3.18285458, 2.7541197 ],\n",
            "       [3.31005112, 3.26105331, 3.18302844, 2.75407843],\n",
            "       [3.31005112, 3.26110313, 3.18307072, 2.75389414],\n",
            "       [3.31005112, 3.26120313, 3.18287231, 2.75420005],\n",
            "       [3.31005112, 3.26107073, 3.18282669, 2.75404885],\n",
            "       [3.31005112, 3.26093299, 3.18285279, 2.75416118],\n",
            "       [3.31002885, 3.2602669 , 3.18367786, 2.76236388],\n",
            "       [3.31002885, 3.26064239, 3.18337214, 2.7627374 ],\n",
            "       [3.31012885, 3.26063031, 3.18346675, 2.76248375],\n",
            "       [3.31002885, 3.26063031, 3.18354012, 2.76218765],\n",
            "       [3.31012885, 3.26063031, 3.18340445, 2.76178531],\n",
            "       [3.31012885, 3.26043954, 3.18341241, 2.76177918],\n",
            "       [3.30981467, 3.26333197, 3.18312722, 2.76690551],\n",
            "       [3.30981467, 3.26339423, 3.18285015, 2.76708379],\n",
            "       [3.30981467, 3.2636184 , 3.18274999, 2.76706414],\n",
            "       [3.30981467, 3.26353197, 3.1829469 , 2.7674927 ],\n",
            "       [3.30981467, 3.26333696, 3.18274666, 2.76745932],\n",
            "       [3.30981467, 3.26365439, 3.18299915, 2.76676248],\n",
            "       [3.30804627, 3.25542258, 3.1534959 , 2.68412172],\n",
            "       [3.30810853, 3.25576605, 3.15363837, 2.68261415],\n",
            "       [3.30804627, 3.25611043, 3.15337423, 2.68245385],\n",
            "       [3.30799841, 3.2559382 , 3.15335873, 2.68204542],\n",
            "       [3.30809841, 3.25631283, 3.15373325, 2.6817653 ],\n",
            "       [3.30809841, 3.25660154, 3.15368181, 2.68177569],\n",
            "       [2.62760635, 3.1694424 , 3.19508138, 2.97095298],\n",
            "       [2.62743169, 3.16925499, 3.19518077, 2.97104196],\n",
            "       [2.62707246, 3.16916323, 3.19547734, 2.97109759],\n",
            "       [2.62682476, 3.16886886, 3.1949677 , 2.97115891],\n",
            "       [2.62695387, 3.1688565 , 3.19510108, 2.97108939],\n",
            "       [2.62681897, 3.16845134, 3.19532127, 2.97122401],\n",
            "       [2.63004122, 3.18029301, 3.19598234, 2.96826873],\n",
            "       [2.63022507, 3.18025981, 3.19604748, 2.96862114],\n",
            "       [2.63022202, 3.18034699, 3.19611821, 2.96868392],\n",
            "       [2.63021319, 3.18023269, 3.19636068, 2.96924466],\n",
            "       [2.63007437, 3.18000814, 3.19626686, 2.96921754],\n",
            "       [2.62995691, 3.17978509, 3.19635162, 2.96917357],\n",
            "       [2.62452624, 3.18144998, 3.19720014, 2.96961882],\n",
            "       [2.62440947, 3.18170639, 3.19720933, 2.9695945 ],\n",
            "       [2.62436425, 3.18104671, 3.1972489 , 2.96908666],\n",
            "       [2.62421252, 3.1812592 , 3.19721476, 2.96880502],\n",
            "       [2.62426509, 3.18127916, 3.19701502, 2.96884496],\n",
            "       [2.62441016, 3.18140614, 3.19724227, 2.9688674 ],\n",
            "       [2.6242547 , 3.18373115, 3.19546638, 2.96708355],\n",
            "       [2.62413452, 3.1836178 , 3.19540405, 2.96742095],\n",
            "       [2.62415998, 3.1836178 , 3.19530928, 2.96730384],\n",
            "       [2.62415998, 3.18387557, 3.19520724, 2.96717488],\n",
            "       [2.62414881, 3.18384519, 3.19555028, 2.96703099],\n",
            "       [2.6240155 , 3.18360116, 3.19557614, 2.96690913],\n",
            "       [2.62472035, 3.18446528, 3.19157771, 2.96195852],\n",
            "       [2.62472035, 3.18446528, 3.19128083, 2.96193265],\n",
            "       [2.62472035, 3.18437075, 3.19116649, 2.96186177],\n",
            "       [2.62472035, 3.18439585, 3.1911369 , 2.96185502],\n",
            "       [2.62472035, 3.18439585, 3.19102682, 2.96181951],\n",
            "       [2.62463178, 3.18438144, 3.1911667 , 2.96178031],\n",
            "       [2.62393801, 3.18859284, 3.19185553, 2.95647048],\n",
            "       [2.62393801, 3.18859284, 3.19206851, 2.95630794],\n",
            "       [2.62393801, 3.18849284, 3.19188868, 2.95630829],\n",
            "       [2.62393801, 3.18878579, 3.19186176, 2.95616328],\n",
            "       [2.6238983 , 3.18868579, 3.19189611, 2.95623248],\n",
            "       [2.6238983 , 3.18891447, 3.1919394 , 2.95623203],\n",
            "       [2.6268972 , 3.19281741, 3.18382338, 2.93982824],\n",
            "       [2.62686969, 3.19283194, 3.18378918, 2.93976831],\n",
            "       [2.62689546, 3.19283194, 3.18377645, 2.94004471],\n",
            "       [2.62678142, 3.19290583, 3.18391169, 2.93994182],\n",
            "       [2.62678142, 3.19291418, 3.18390369, 2.94000965],\n",
            "       [2.62672416, 3.19293225, 3.18375388, 2.93947786],\n",
            "       [2.61071842, 3.18085152, 3.13102119, 2.75916587],\n",
            "       [2.61105079, 3.18019137, 3.13060823, 2.75954204],\n",
            "       [2.61120517, 3.17976427, 3.13042964, 2.75928444],\n",
            "       [2.61098095, 3.17974739, 3.1307432 , 2.75990618],\n",
            "       [2.61114401, 3.17954732, 3.13041866, 2.75962514],\n",
            "       [2.61133604, 3.17959899, 3.13000294, 2.75879311],\n",
            "       [2.194245  , 3.22523825, 3.01255442, 2.74361082],\n",
            "       [2.19387705, 3.2259377 , 3.01224819, 2.7432263 ],\n",
            "       [2.19351488, 3.22632486, 3.01207485, 2.74306882],\n",
            "       [2.19343803, 3.2261461 , 3.0124723 , 2.74322113],\n",
            "       [2.19404981, 3.22603918, 3.01186179, 2.74337129],\n",
            "       [2.19447527, 3.22559025, 3.01200068, 2.74361567],\n",
            "       [2.21611709, 3.24620929, 3.06659729, 2.73542036],\n",
            "       [2.21609392, 3.24601981, 3.06645051, 2.73514083],\n",
            "       [2.21603164, 3.24625065, 3.06612249, 2.73470133],\n",
            "       [2.21596032, 3.24629539, 3.06667023, 2.73502104],\n",
            "       [2.21569265, 3.24614644, 3.06670119, 2.73474033],\n",
            "       [2.21552949, 3.24626495, 3.06687106, 2.73503572],\n",
            "       [2.22750282, 3.25353526, 3.06900921, 2.74763775],\n",
            "       [2.22769309, 3.25351075, 3.06854078, 2.747461  ],\n",
            "       [2.22735612, 3.25374793, 3.06876951, 2.74763641],\n",
            "       [2.2273696 , 3.25374787, 3.06849557, 2.74737998],\n",
            "       [2.22728683, 3.25396307, 3.0688491 , 2.74729168],\n",
            "       [2.22731443, 3.25398821, 3.06853449, 2.74728131],\n",
            "       [2.22798207, 3.25798132, 3.07275847, 2.76556384],\n",
            "       [2.22798815, 3.25798132, 3.07277139, 2.76571939],\n",
            "       [2.22783021, 3.25814148, 3.07253775, 2.76549419],\n",
            "       [2.22769251, 3.25802326, 3.0723204 , 2.76551333],\n",
            "       [2.22790872, 3.258161  , 3.07254529, 2.76551316],\n",
            "       [2.22778184, 3.258161  , 3.07270933, 2.76595992],\n",
            "       [2.2344995 , 3.26101759, 3.07945672, 2.77260199],\n",
            "       [2.23451327, 3.2610421 , 3.07945673, 2.77251189],\n",
            "       [2.23457759, 3.2610421 , 3.07926739, 2.77266917],\n",
            "       [2.23457399, 3.2610421 , 3.07910656, 2.77273102],\n",
            "       [2.23448025, 3.26104507, 3.07915646, 2.77288562],\n",
            "       [2.23411804, 3.26104507, 3.07899493, 2.77290795]]))\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D7y47XvyPsGs"
      },
      "outputs": [],
      "source": [
        "np.save('mi_all_relu', result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 138, 4)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "a = np.load('mi_all_relu.npy')\n",
        "print(a.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(80, 4)\n",
            "[[3.31178178 2.47448246 1.95750073 1.43829452]\n",
            " [3.30764404 2.67957364 2.33707841 1.56217081]\n",
            " [3.30257983 2.81157673 2.48905322 1.84833606]\n",
            " [3.29964208 2.7736162  2.5373634  1.72109615]\n",
            " [3.29818611 2.81735818 2.54092939 1.77032751]\n",
            " [3.28761799 2.84508922 2.56844118 1.92255749]\n",
            " [3.30531927 3.22093571 2.99411174 2.54614642]\n",
            " [3.30529006 3.22327735 2.99414407 2.52652243]\n",
            " [3.30661258 3.22472206 2.99685817 2.50862207]\n",
            " [3.30648377 3.22470363 3.00037666 2.52789541]\n",
            " [3.30501757 3.22238661 2.99545703 2.54174092]\n",
            " [3.30571757 3.2214062  2.99670685 2.53646301]\n",
            " [3.30831757 3.25368126 3.11968344 2.61206127]\n",
            " [3.30827274 3.25411915 3.12251342 2.63070723]\n",
            " [3.30923709 3.25360234 3.12700754 2.64683814]\n",
            " [3.30900758 3.25337737 3.12565499 2.64778641]\n",
            " [3.3080616  3.25201972 3.12486139 2.64496274]\n",
            " [3.3082608  3.25488182 3.12256277 2.6350501 ]\n",
            " [3.3087616  3.2580516  3.14468333 2.70262432]\n",
            " [3.30842385 3.25803001 3.14530527 2.70407877]\n",
            " [3.30791757 3.25813798 3.14661129 2.69986892]\n",
            " [3.30781757 3.25789453 3.14536716 2.70067151]\n",
            " [3.3076659  3.258667   3.14473633 2.70270006]\n",
            " [3.3077659  3.25891644 3.14566247 2.70427939]\n",
            " [3.30965143 3.25775502 3.16857484 2.74400805]\n",
            " [3.30965143 3.25785502 3.16870944 2.74376545]\n",
            " [3.30965143 3.25795505 3.1687339  2.74378771]\n",
            " [3.30965143 3.25803554 3.16855974 2.74343802]\n",
            " [3.30965143 3.25795505 3.16857773 2.74348102]\n",
            " [3.30955143 3.25787953 3.16847611 2.74348002]\n",
            " [3.30945336 3.25879135 3.17974188 2.74638628]\n",
            " [3.30950838 3.2585931  3.17972956 2.74570358]\n",
            " [3.30950838 3.25866722 3.1798507  2.74566363]\n",
            " [3.30945336 3.2587712  3.17964151 2.74588764]\n",
            " [3.30945336 3.2586712  3.17929162 2.74584178]\n",
            " [3.30945336 3.2586981  3.17915698 2.74583167]\n",
            " [3.31005112 3.26109605 3.18285458 2.7541197 ]\n",
            " [3.31005112 3.26105331 3.18302844 2.75407843]\n",
            " [3.31005112 3.26110313 3.18307072 2.75389414]\n",
            " [3.31005112 3.26120313 3.18287231 2.75420005]\n",
            " [3.31005112 3.26107073 3.18282669 2.75404885]\n",
            " [3.31005112 3.26093299 3.18285279 2.75416118]\n",
            " [3.31002885 3.2602669  3.18367786 2.76236388]\n",
            " [3.31002885 3.26064239 3.18337214 2.7627374 ]\n",
            " [3.31012885 3.26063031 3.18346675 2.76248375]\n",
            " [3.31002885 3.26063031 3.18354012 2.76218765]\n",
            " [3.31012885 3.26063031 3.18340445 2.76178531]\n",
            " [3.31012885 3.26043954 3.18341241 2.76177918]\n",
            " [3.30981467 3.26333197 3.18312722 2.76690551]\n",
            " [3.30981467 3.26339423 3.18285015 2.76708379]\n",
            " [3.30981467 3.2636184  3.18274999 2.76706414]\n",
            " [3.30981467 3.26353197 3.1829469  2.7674927 ]\n",
            " [3.30981467 3.26333696 3.18274666 2.76745932]\n",
            " [3.30981467 3.26365439 3.18299915 2.76676248]\n",
            " [3.30804627 3.25542258 3.1534959  2.68412172]\n",
            " [3.30810853 3.25576605 3.15363837 2.68261415]\n",
            " [3.30804627 3.25611043 3.15337423 2.68245385]\n",
            " [3.30799841 3.2559382  3.15335873 2.68204542]\n",
            " [3.30809841 3.25631283 3.15373325 2.6817653 ]\n",
            " [3.30809841 3.25660154 3.15368181 2.68177569]\n",
            " [2.62760635 3.1694424  3.19508138 2.97095298]\n",
            " [2.62743169 3.16925499 3.19518077 2.97104196]\n",
            " [2.62707246 3.16916323 3.19547734 2.97109759]\n",
            " [2.62682476 3.16886886 3.1949677  2.97115891]\n",
            " [2.62695387 3.1688565  3.19510108 2.97108939]\n",
            " [2.62681897 3.16845134 3.19532127 2.97122401]\n",
            " [2.63004122 3.18029301 3.19598234 2.96826873]\n",
            " [2.63022507 3.18025981 3.19604748 2.96862114]\n",
            " [2.63022202 3.18034699 3.19611821 2.96868392]\n",
            " [2.63021319 3.18023269 3.19636068 2.96924466]\n",
            " [2.63007437 3.18000814 3.19626686 2.96921754]\n",
            " [2.62995691 3.17978509 3.19635162 2.96917357]\n",
            " [2.62452624 3.18144998 3.19720014 2.96961882]\n",
            " [2.62440947 3.18170639 3.19720933 2.9695945 ]\n",
            " [2.62436425 3.18104671 3.1972489  2.96908666]\n",
            " [2.62421252 3.1812592  3.19721476 2.96880502]\n",
            " [2.62426509 3.18127916 3.19701502 2.96884496]\n",
            " [2.62441016 3.18140614 3.19724227 2.9688674 ]\n",
            " [2.6242547  3.18373115 3.19546638 2.96708355]\n",
            " [2.62413452 3.1836178  3.19540405 2.96742095]]\n"
          ]
        }
      ],
      "source": [
        "mi_xt_mean = a[0][:80]\n",
        "print(mi_xt_mean.shape)\n",
        "mi_ty_mean = a[1][:80]\n",
        "print(mi_ty_mean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(27, 4)\n"
          ]
        }
      ],
      "source": [
        "IXM_N = mi_xt_mean[0,:110]\n",
        "IYM_N = mi_ty_mean[0,:110]\n",
        "\n",
        "T =1\n",
        "\n",
        "for i in range(1,mi_xt_mean.shape[0]):\n",
        "    \n",
        "    A_ = i <= 4 and i % 1 == 0  \n",
        "    A0 = i > 4 and i <= 10 and i % 2 == 0    \n",
        "    A1 = i > 10 and i <= 30 and i % 3 == 0     \n",
        "    A2 = i > 30 and i % 3 == 0    \n",
        "\n",
        "    #if A0 or A1 or A2:\n",
        "    if  A0 or A1 or A2:\n",
        "        mean_vec_xt = np.mean(mi_xt_mean[T:i+1,:],axis=0)\n",
        "        mean_vec_yt = np.mean(mi_ty_mean[T:i+1,:],axis=0)  \n",
        "                             \n",
        "        #print(T,i+1)                      \n",
        "        IXM_N = np.append(IXM_N,mean_vec_xt)\n",
        "        IYM_N = np.append(IYM_N,mean_vec_yt)\n",
        "        \n",
        "        T = i+1\n",
        "\n",
        "        \n",
        "IXM_N = np.reshape(IXM_N,(-1,4))\n",
        "IYM_N = np.reshape(IYM_N,(-1,4))\n",
        "print(IXM_N.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(80, 4)\n",
            "(80, 4)\n"
          ]
        }
      ],
      "source": [
        "IXM_N, IYM_N = mi_ty_mean, mi_ty_mean\n",
        "print(IXM_N.shape)\n",
        "print(IYM_N.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(78, 4)\n",
            "(78, 4)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "T=3\n",
        "IXM_N, IYM_N = np.zeros((mi_xt_mean.shape[0]-T+1,4)), np.zeros((mi_ty_mean.shape[0]-T+1,4))\n",
        "\n",
        "for i in range(4):\n",
        "    IXM_N[:, i] = pd.Series(mi_xt_mean[:, i]).rolling(T).mean().iloc[T-1:].values\n",
        "    IYM_N[:, i] = pd.Series(mi_ty_mean[:, i]).rolling(T).mean().iloc[T-1:].values\n",
        "    \n",
        "    \n",
        "print(IXM_N.shape)\n",
        "print(IYM_N.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_16224\\75744796.py:44: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
            "  plt.tight_layout(rect=[0, 0, 0.9, 1])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "# matplotlib.use('TkAgg')\n",
        "#matplotlib.use('Agg')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "#import seaborn as sns\n",
        "#sns.set_style('darkgrid')\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "gs = gridspec.GridSpec(4, 2)\n",
        "\n",
        "COLORBAR_MAX_EPOCHS = 15000\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "sm = plt.cm.ScalarMappable(cmap='gnuplot', norm=plt.Normalize(vmin=0, vmax=COLORBAR_MAX_EPOCHS))\n",
        "sm._A = []\n",
        "\n",
        "n_epoch = IXM_N.shape[0]\n",
        "\n",
        "PLOT_LAYERS = [0]\n",
        "for epoch in range(0, n_epoch):\n",
        "    c = sm.to_rgba(220 * (epoch))\n",
        "    xmvals = IXM_N[epoch]\n",
        "    ymvals = IYM_N[epoch]\n",
        "    \n",
        "    plt.plot(xmvals, ymvals, c=c, alpha=0.1, zorder=1)\n",
        "    plt.scatter(xmvals, ymvals, s=40, facecolors=[c for _ in PLOT_LAYERS], edgecolor='none', zorder=2)\n",
        "\n",
        "plt.ylim([1, 3.5])\n",
        "plt.xlim([4, 14])\n",
        "plt.xlabel('I(X;M)')\n",
        "plt.ylabel('I(Y;M)')\n",
        "plt.title('Tanh')\n",
        "\n",
        "# colorbar\n",
        "cbaxes = fig.add_axes([0.92, 0.125, 0.03, 0.75])\n",
        "plt.colorbar(sm, label='Epoch', cax=cbaxes)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
        "\n",
        "plt.show(block=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
